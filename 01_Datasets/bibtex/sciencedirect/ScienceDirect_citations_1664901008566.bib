@article{EKINS2014115,
title = {Progress in computational toxicology},
journal = {Journal of Pharmacological and Toxicological Methods},
volume = {69},
number = {2},
pages = {115-140},
year = {2014},
issn = {1056-8719},
doi = {https://doi.org/10.1016/j.vascn.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1056871913003250},
author = {Sean Ekins},
keywords = {Bayesian, Computational toxicology, Machine learning, Support Vector Machine},
abstract = {Introduction: Computational methods have been widely applied to toxicology across pharmaceutical, consumer product and environmental fields over the past decade. Progress in computational toxicology is now reviewed. Methods: A literature review was performed on computational models for hepatotoxicity (e.g. for drug-induced liver injury (DILI)), cardiotoxicity, renal toxicity and genotoxicity. In addition various publications have been highlighted that use machine learning methods. Several computational toxicology model datasets from past publications were used to compare Bayesian and Support Vector Machine (SVM) learning methods. Results: The increasing amounts of data for defined toxicology endpoints have enabled machine learning models that have been increasingly used for predictions. It is shown that across many different models Bayesian and SVM perform similarly based on cross validation data. Discussion: Considerable progress has been made in computational toxicology in a decade in both model development and availability of larger scale or ‘big data’ models. The future efforts in toxicology data generation will likely provide us with hundreds of thousands of compounds that are readily accessible for machine learning models. These models will cover relevant chemistry space for pharmaceutical, consumer product and environmental applications.}
}
@article{ONG201443,
title = {Improving record linkage performance in the presence of missing linkage data},
journal = {Journal of Biomedical Informatics},
volume = {52},
pages = {43-54},
year = {2014},
note = {Special Section: Methods in Clinical Research Informatics},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000197},
author = {Toan C. Ong and Michael V. Mannino and Lisa M. Schilling and Michael G. Kahn},
keywords = {Record linkage, Missing data, Data quality, Comparative effectiveness research, Quasi-identifiers},
abstract = {Introduction
Existing record linkage methods do not handle missing linking field values in an efficient and effective manner. The objective of this study is to investigate three novel methods for improving the accuracy and efficiency of record linkage when record linkage fields have missing values.
Methods
By extending the Fellegi–Sunter scoring implementations available in the open-source Fine-grained Record Linkage (FRIL) software system we developed three novel methods to solve the missing data problem in record linkage, which we refer to as: Weight Redistribution, Distance Imputation, and Linkage Expansion. Weight Redistribution removes fields with missing data from the set of quasi-identifiers and redistributes the weight from the missing attribute based on relative proportions across the remaining available linkage fields. Distance Imputation imputes the distance between the missing data fields rather than imputing the missing data value. Linkage Expansion adds previously considered non-linkage fields to the linkage field set to compensate for the missing information in a linkage field. We tested the linkage methods using simulated data sets with varying field value corruption rates.
Results
The methods developed had sensitivity ranging from .895 to .992 and positive predictive values (PPV) ranging from .865 to 1 in data sets with low corruption rates. Increased corruption rates lead to decreased sensitivity for all methods.
Conclusions
These new record linkage algorithms show promise in terms of accuracy and efficiency and may be valuable for combining large data sets at the patient level to support biomedical and clinical research.}
}
@article{STEHLIK201653,
title = {Missing chaos in global climate change data interpreting?},
journal = {Ecological Complexity},
volume = {25},
pages = {53-59},
year = {2016},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X1500121X},
author = {M. Stehlík and J. Dušek and J. Kiseľák},
keywords = {Stochasticity, Determinism, Entropy, Chaos, Wetland ecosystem, Kullback–Leibler (KL) divergence},
abstract = {The main problem of ecological data modeling is their interpretation and its correct understanding. This problem cannot be solved solely by a big data collection. To sufficiently understand ecosystems we need to know how these processes behave and how they respond to internal and external factors. Similarly, we need to know the behavior of processes that are involved in the climate system and the biosphere of the earth. In order to characterize precisely the behavior of individual elements and ecosystems we need to use deterministic, stochastic and chaotic behavior. Unfortunately, the chaotic part of systems is typically completely ignored in almost all approaches. Ignoring of chaotical part leads to many biased outcomes. To overcome this gap we model chaotic system behavior by random iterated function system which provides a generic guideline for such data management. This also allows to replicate a complexity and chaos of ecosystem.}
}
@article{ELLIOTT201561,
title = {The overlooked potential of Generalized Linear Models in astronomy-II: Gamma regression and photometric redshifts},
journal = {Astronomy and Computing},
volume = {10},
pages = {61-72},
year = {2015},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2015.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213133715000037},
author = {J. Elliott and R.S. {de Souza} and A. Krone-Martins and E. Cameron and E.E.O. Ishida and J. Hilbe},
keywords = {Techniques: photometric, Methods: statistical, Methods: analytical, Galaxies: distances and redshifts},
abstract = {Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ∼1% for simulated and ∼2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely thousands of galaxies. Our software is made publicly available as a user-friendly package developed in Python, R and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope.}
}
@article{LI2014187,
title = {Big Biological Data: Challenges and Opportunities},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {12},
number = {5},
pages = {187-189},
year = {2014},
note = {Special Issue: Translational Omics},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022914001041},
author = {Yixue Li and Luonan Chen}
}
@article{BUCKOW201622,
title = {Changing requirements and resulting needs for IT-infrastructure for longitudinal research in the neurosciences},
journal = {Neuroscience Research},
volume = {102},
pages = {22-28},
year = {2016},
note = {Trajectories in mental illness},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2014.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0168010214001813},
author = {Karoline Buckow and Matthias Quade and Otto Rienhoff and Sara Y. Nussbeck},
keywords = {IT-infrastructure, Metadata, Identity management, Data quality, Neuroscience, Infrastructure methodology},
abstract = {The observation of growing “difficulties” in IT-infrastructures in neuroscience research during the last years led to a search for reasons and an analysis on how this phenomenon is reflected in the scientific literature. With a retrospective analysis of nine examples of multicenter research projects in the neurosciences and a literature review the observation was systematically analyzed. Results show that the rise in complexity mainly stems from two reasons: (1) more and more need for information on quality and context of research data (metadata) and (2) long-term requirements to handle the consent and identity/pseudonyms of study participants and biomaterials in relation to legal requirements. The combination of these two aspects together with very long study times and data evaluation periods are components of the subjectively perceived “difficulties”. A direct consequence of this result is that big multicenter trials are becoming part of integrated research data environments and are not standing alone for themselves anymore. This drives up the resource needs regarding the IT-infrastructure in neuroscience research. In contrast to these findings, literature on this development is scarce and the problem probably underestimated.}
}
@article{KRAEMER201619,
title = {Progress and Challenges in Infectious Disease Cartography},
journal = {Trends in Parasitology},
volume = {32},
number = {1},
pages = {19-29},
year = {2016},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2015.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S147149221500207X},
author = {Moritz U.G. Kraemer and Simon I. Hay and David M. Pigott and David L. Smith and G.R. William Wint and Nick Golding},
keywords = {spatial epidemiology, mapping, disease modelling, pathogen dispersal, human mobility},
abstract = {Quantitatively mapping the spatial distributions of infectious diseases is key to both investigating their epidemiology and identifying populations at risk of infection. Important advances in data quality and methodologies have allowed for better investigation of disease risk and its association with environmental factors. However, incorporating dynamic human behavioural processes in disease mapping remains challenging. For example, connectivity among human populations, a key driver of pathogen dispersal, has increased sharply over the past century, along with the availability of data derived from mobile phones and other dynamic data sources. Future work must be targeted towards the rapid updating and dissemination of appropriately designed disease maps to guide the public health community in reducing the global burden of infectious disease.}
}
@article{PATIRE2015325,
title = {How much GPS data do we need?},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {325-342},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15000662},
author = {Anthony D. Patire and Matthew Wright and Boris Prodhomme and Alexandre M. Bayen},
keywords = {Data fusion, Probe data, GPS, Travel time, Estimation},
abstract = {With the rapid growth of communications technologies, GPS, and the mobile internet, an increasing amount of real-time location information is collected by private companies and could be marketed for retail. This body of data offers transportation agencies potential opportunities to improve operations, but it also presents unique challenges. This article investigates the question of how much GPS data is needed to power a traffic information system capable of providing accurate speed (and thus travel time) information. A hybrid data framework is proposed to use real-time, GPS-based, point-speed data from mobile sources to augment previous investments in existing fixed sensors. In addition, a systematic analysis of the performance trade-offs among a menu of data sources is described. The results presented in this article were generated from the first procurement of streaming probe data from the private sector conducted on behalf of the California Department of Transportation and executed by UC Berkeley. Third-party data were incorporated with loop detector data and travel times were estimated within the bounds of driver variability. This achievement was repeated over multiple weeks and multiple congested freeway sites. Our findings indicate that penetration rates for GPS-based probe data are now suitable for travel time estimation on selected corridors. Data fusion makes possible the effective use of data from multiple sources or providers; when data from multiple sources are fused, superior results are obtained. On a freeway that is already instrumented with loop detectors, better travel time performance may be achieved by fusing a relatively small amount of probe data than by doubling the number of loop detectors. Finally, the answer to how much GPS data is needed must address issues of data quality in terms of sample rate and penetration rate.}
}
@article{ZANIN20161,
title = {Combining complex networks and data mining: Why and how},
journal = {Physics Reports},
volume = {635},
pages = {1-44},
year = {2016},
note = {Combining complex networks and data mining: Why and how},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S037015731630062X},
author = {M. Zanin and D. Papo and P.A. Sousa and E. Menasalvas and A. Nicchi and E. Kubik and S. Boccaletti},
keywords = {Complex networks, Data mining, Big Data},
abstract = {The increasing power of computer technology does not dispense with the need to extract meaningful information out of data sets of ever growing size, and indeed typically exacerbates the complexity of this task. To tackle this general problem, two methods have emerged, at chronologically different times, that are now commonly used in the scientific community: data mining and complex network theory. Not only do complex network analysis and data mining share the same general goal, that of extracting information from complex systems to ultimately create a new compact quantifiable representation, but they also often address similar problems too. In the face of that, a surprisingly low number of researchers turn out to resort to both methodologies. One may then be tempted to conclude that these two fields are either largely redundant or totally antithetic. The starting point of this review is that this state of affairs should be put down to contingent rather than conceptual differences, and that these two fields can in fact advantageously be used in a synergistic manner. An overview of both fields is first provided, some fundamental concepts of which are illustrated. A variety of contexts in which complex network theory and data mining have been used in a synergistic manner are then presented. Contexts in which the appropriate integration of complex network metrics can lead to improved classification rates with respect to classical data mining algorithms and, conversely, contexts in which data mining can be used to tackle important issues in complex network theory applications are illustrated. Finally, ways to achieve a tighter integration between complex networks and data mining, and open lines of research are discussed.}
}
@incollection{REDMAN2014xi,
title = {Foreword by Thomas C. Redman},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xi-xii},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.06001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124055476060016},
author = {Thomas C. Redman}
}
@article{JANCZEWSKI2016123,
title = {What's so different about differential response? A multilevel and longitudinal analysis of child neglect investigations},
journal = {Children and Youth Services Review},
volume = {67},
pages = {123-132},
year = {2016},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2016.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0190740916301645},
author = {Colleen E. Janczewski and Joshua P. Mersky},
keywords = {Differential response, Alternative response, Multilevel model, CPS systems, Neglect, Longitudinal analysis},
abstract = {Differential response (DR) is a system reform that allows child protective services (CPS) agencies to divert low-to-moderate risk families from an investigative track to an alternate track that does not require a maltreatment disposition or identification of an alleged perpetrator. Knowledge of how DR alters the flow of cases through CPS systems has been restricted by methodological limitations in prior research. This study uses cross-sectional and longitudinal data from the National Child Abuse and Neglect Data System (NCANDS) child file to examine the extent to which DR implementation affects the number and demographic composition of cases investigated for neglect. Results from multivariate, multilevel cross-sectional analysis of 2010 data indicated that investigations were 2.4 times more likely to be substantiated in DR counties than in non-DR counties. Children with a previous substantiated report were also more likely to have a current report substantiated and this difference was significantly greater in DR counties than in non-DR counties. Child race and ethnicity did not predict substantiation decisions. Results from a mixed-effect longitudinal analysis of 997,512 cases from 269 counties between 2001 and 2010 suggest that the rate of investigations fell sharply nationwide within three years of DR implementation. However, substantiation rates did not change as a result of DR implementation. Instead, analysis indicated differences between DR and non-DR counties emerged before the launch of DR. The findings highlight the benefit of using “big data” and longitudinal analysis to assess large-scale policy changes.}
}
@article{JANSSEN2015363,
title = {Big and Open Linked Data (BOLD) in government: A challenge to transparency and privacy?},
journal = {Government Information Quarterly},
volume = {32},
number = {4},
pages = {363-368},
year = {2015},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X15001069},
author = {Marijn Janssen and Jeroen {van den Hoven}},
keywords = {Open government, Interoperability, Transparency, Privacy, Big data, Open data, E-government, Privacy-by-design, Transparency-by-design},
abstract = {Big and Open Linked Data (BOLD) results in new opportunities and have the potential to transform government and its interactions with the public. BOLD provides the opportunity to analyze the behavior of individuals, increase control, and reduce privacy. At the same time BOLD can be used to create an open and transparent government. Transparency and privacy are considered as important societal and democratic values that are needed to inform citizens and let them participate in democratic processes. Practices in these areas are changing with the rise of BOLD. Although intuitively appealing, the concepts of transparency and privacy have many interpretations and are difficult to conceptualize, which makes it often hard to implement them. Transparency and privacy should be conceptualized as complex, non-dichotomous constructs interrelated with other factors. Only by conceptualizing these values in this way, the nature and impact of BOLD on privacy and transparency can be understood, and their levels can be balanced with security, safety, openness and other socially-desirable values.}
}
@article{SCHWABE201629,
title = {A framework for geometric quantification and forecasting of cost uncertainty for aerospace innovations},
journal = {Progress in Aerospace Sciences},
volume = {84},
pages = {29-47},
year = {2016},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0376042115300324},
author = {Oliver Schwabe and Essam Shehab and John Erkoyuncu},
keywords = {Cost estimate, Geometry, Symmetry, Topology, Uncertainty},
abstract = {Quantification and forecasting of cost uncertainty for aerospace innovations is challenged by conditions of small data which arises out of having few measurement points, little prior experience, unknown history, low data quality, and conditions of deep uncertainty. Literature research suggests that no frameworks exist which specifically address cost estimation under such conditions. In order to provide contemporary cost estimating techniques with an innovative perspective for addressing such challenges a framework based on the principles of spatial geometry is described. The framework consists of a method for visualising cost uncertainty and a dependency model for quantifying and forecasting cost uncertainty. Cost uncertainty is declared to represent manifested and unintended future cost variance with a probability of 100% and an unknown quantity and innovative starting conditions considered to exist when no verified and accurate cost model is available. The shape of data is used as an organising principle and the attribute of geometrical symmetry of cost variance point clouds used for the quantification of cost uncertainty. The results of the investigation suggest that the uncertainty of a cost estimate at any future point in time may be determined by the geometric symmetry of the cost variance data in its point cloud form at the time of estimation. Recommendations for future research include using the framework to determine the “most likely values” of estimates in Monte Carlo simulations and generalising the dependency model introduced. Future work is also recommended to reduce the framework limitations noted.}
}
@article{THEURER2015495,
title = {Using Feedlot Operational Data to Make Valid Conclusions for Improving Health Management},
journal = {Veterinary Clinics of North America: Food Animal Practice},
volume = {31},
number = {3},
pages = {495-508},
year = {2015},
note = {Feedlot Production Medicine},
issn = {0749-0720},
doi = {https://doi.org/10.1016/j.cvfa.2015.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0749072015000407},
author = {Miles E. Theurer and David G. Renter and Brad J. White},
keywords = {Epidemiology, Feedlot, Data, Distribution}
}
@article{MENDOZAPARRA2014268,
title = {Assessing quality standards for ChIP-seq and related massive parallel sequencing-generated datasets: When rating goes beyond avoiding the crisis},
journal = {Genomics Data},
volume = {2},
pages = {268-273},
year = {2014},
issn = {2213-5960},
doi = {https://doi.org/10.1016/j.gdata.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213596014000671},
author = {Marco Antonio Mendoza-Parra and Hinrich Gronemeyer},
keywords = {ChIP sequencing, Massive parallel sequencing, Quality control, Omics data mining},
abstract = {Massive parallel DNA sequencing combined with chromatin immunoprecipitation and a large variety of DNA/RNA-enrichment methodologies is at the origin of data resources of major importance. Indeed these resources, available for multiple genomes, represent the most comprehensive catalogue of (i) cell, development and signal transduction-specified patterns of binding sites for transcription factors (‘cistromes’) and for transcription and chromatin modifying machineries and (ii) the patterns of specific local post-translational modifications of histones and DNA (‘epigenome’) or of regulatory chromatin binding factors. In addition, (iii) the resources specifying chromatin structure alterations are emerging. Importantly, these types of “omics” datasets populate increasingly public repositories and provide highly valuable resources for the exploration of general principles of cell function in a multi-dimensional genome–transcriptome–epigenome–chromatin structure context. However, data mining is critically dependent on the data quality, an issue that, surprisingly, is still largely ignored by scientists and well-financed consortia, data repositories and scientific journals. So what determines the quality of ChIP-seq experiments and the datasets generated therefrom and what refrains scientists from associating quality criteria to their data? In this ‘opinion’ we trace the various parameters that influence the quality of this type of datasets, as well as the computational efforts that were made until now to qualify them. Moreover, we describe a universal quality control (QC) certification approach that provides a quality rating for ChIP-seq and enrichment-related assays. The corresponding QC tool and a regularly updated database, from which at present the quality parameters of more than 8000 datasets can be retrieved, are freely accessible at www.ngs-qc.org.}
}
@incollection{PARIKH2013493,
title = {Chapter 20 - Query Suggestion with Large Scale Data},
editor = {C.R. Rao and Venu Govindaraju},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {31},
pages = {493-518},
year = {2013},
booktitle = {Handbook of Statistics},
issn = {0169-7161},
doi = {https://doi.org/10.1016/B978-0-444-53859-8.00020-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444538598000205},
author = {Nish Parikh and Gyanit Singh and Neel Sundaresan},
keywords = {Query Suggestions, Related searches, Query reformulation, Search, Big data, Recommender system},
abstract = {Explosive growth of information has created a challenge for search engines in various domains to handle large scale data. It is still difficult for search engines to fully understand user intent in many scenarios. To address this, most search engines provide assistive features to the user which help the users in managing their information need. Query Suggestion (Related Searches) is one such feature which is an integral part of all search engines. It helps steer users toward queries which are more likely to help them succeed in their search missions. There has been extensive research in this field. In this chapter, we discuss state-of-the-art techniques to build a Query Suggestion system. Specifically, we describe the strengths and limitations of different approaches. We also describe salient characteristics of large scale data sets like query corpora and click-stream logs. We walk the reader through the design, implementation, and evaluation of large scale Query Suggestion systems in practice. We show how challenges related to sparsity in the long tail, biases in user data, and speed of algorithms can be tackled at industry scale.}
}
@article{BRONSELAER2015121,
title = {Pointwise multi-valued fusion},
journal = {Information Fusion},
volume = {25},
pages = {121-133},
year = {2015},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253514001183},
author = {Antoon Bronselaer and Daan {Van Britsom} and Guy {De Tré}},
keywords = {Set, Multiset, Multi-valued fusion},
abstract = {Assessment and improvement of data quality is a major challenge with any modern information source. An aspect of data quality that has gained a lot of interest in the past decades, is the detection and fusion of duplicate data. This paper contributes to the field of duplicate data fusion by investigating a framework of fusion functions. In particular, it is observed that multisets are a data structure for which little is known concerning fusion theory. Therefore, a class of multi-valued functions called pointwise fusion functions, is proposed and investigated. An extensive list of properties is defined in order to compare the behavior of multi-valued fusion functions. Some specific pointwise fusion functions are investigated with respect to the defined properties and they are evaluated in different fusion scenarios. Next, some quality measures are discussed and their usefulness in the different fusion scenarios is discussed.}
}
@article{LONG201636,
title = {Redefining Chinese city system with emerging new data},
journal = {Applied Geography},
volume = {75},
pages = {36-48},
year = {2016},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0143622816302867},
author = {Ying Long},
keywords = {Urban morphology, Urban function, Human activity, Street network, City evolution},
abstract = {Modern Chinese cities are defined from the administrative view and classified into several administrative categories, which makes it inconsistent between Chinese cities and their counterparts in western countries. Without easy access to fine-scale data, researchers have to rely heavily on statistical and aggregated indicators available in officially released yearbooks, to understand Chinese city system. Not to mention the data quality of yearbooks, it is problematic that a large number of towns or downtown areas of counties are not addressed in yearbooks. To address this issue, as a following study of Long et al. (2016), we have redefined the Chinese city system, using percolation theory in the light of newly emerging big/open data. In this paper, we propose our alternative definition of a city with road/street junctions, and present the methodology for extracting city system for the whole country with national wide road junctions. A city is defined as “a spatial cluster with a minimum of 100 road/street junctions within a 300 m distance threshold”. Totally we identify 4629 redefined cities with a total urban area of 64,144 km2 for the whole China. We observe total city number increases from 2273 in 2009 to 4629 in 2014. We find that expanded urban area during 2009 and 2014, comparing with urban areas in 2009 are associated with 73.3% road junction density, 25.3% POI density and 5.5% online comment density. In addition, we benchmark our results with the conventional Chinese city system by using yearbooks.}
}
@article{GEISSBUHLER20131,
title = {Trustworthy reuse of health data: A transnational perspective},
journal = {International Journal of Medical Informatics},
volume = {82},
number = {1},
pages = {1-9},
year = {2013},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2012.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S138650561200202X},
author = {A. Geissbuhler and C. Safran and I. Buchan and R. Bellazzi and S. Labkoff and K. Eilenberg and A. Leese and C. Richardson and J. Mantas and P. Murray and G. {De Moor}},
keywords = {Electronic health data, Electronic health records, Privacy, Big data, Policy, Interoperability, European Union},
abstract = {Background
The widespread adoption of electronic health records (EHRs) is accelerating the collection of sensitive clinical data. The availability of these data raises privacy concerns, yet sharing the data is essential for public health, longitudinal patient care, and clinical research.
Method
Following previous work in the United States [1], [2], the International Medical Informatics Association convened the 2012 European Summit on Trustworthy Reuse of Health Data. Over 100 delegates representing national governments, academia, patient groups, industry, and the European Commission participated. In all, 21 countries were represented. The agenda was designed to solicit a wide range of perspectives on trustworthy reuse of health data from the participants.
Results and conclusions
Delegates agreed that the “government” should provide oversight, that the reuse should be “fully regulated,” and that the patient should be “fully informed.” One important reflection was that doing nothing will have negative implications across the European Union (EU). First, continued fragmented parallel non-standards-based developments in multiple sectors entail a substantial duplication of costs and human effort. Second, a failure to work jointly across the stakeholders on common policy frameworks will forego a crucial opportunity to boost key EU markets (pharmaceuticals, health technology and devices, and eHealth solutions) and counter global competition. Finally, and crucially, the lack of harmonized policy across EU nations for trustworthy reuse of health data risks patient safety. The productive dialog, initiated with multiple stakeholders from government, academia, and industry, will have to continue, in order to address the many remaining issues outlined in this white paper.}
}
@article{AMPOFO20154368,
title = {Strengthening the influenza vaccine virus selection and development process: Report of the 3rd WHO Informal Consultation for Improving Influenza Vaccine Virus Selection held at WHO headquarters, Geneva, Switzerland, 1–3 April 2014},
journal = {Vaccine},
volume = {33},
number = {36},
pages = {4368-4382},
year = {2015},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2015.06.090},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X15009056},
author = {William K. Ampofo and Eduardo Azziz-Baumgartner and Uzma Bashir and Nancy J. Cox and Rodrigo Fasce and Maria Giovanni and Gary Grohmann and Sue Huang and Jackie Katz and Alla Mironenko and Talat Mokhtari-Azad and Pretty Multihartina Sasono and Mahmudur Rahman and Pathom Sawanpanyalert and Marilda Siqueira and Anthony L. Waddell and Lillian Waiboci and John Wood and Wenqing Zhang and Thedi Ziegler},
keywords = {Influenza vaccine viruses, Vaccine virus selection, WHO recommendations},
abstract = {Despite long-recognized challenges and constraints associated with their updating and manufacture, influenza vaccines remain at the heart of public health preparedness and response efforts against both seasonal and potentially pandemic influenza viruses. Globally coordinated virological and epidemiological surveillance is the foundation of the influenza vaccine virus selection and development process. Although national influenza surveillance and reporting capabilities are being strengthened and expanded, sustaining and building upon recent gains has become a major challenge. Strengthening the vaccine virus selection process additionally requires the continuation of initiatives to improve the timeliness and representativeness of influenza viruses shared by countries for detailed analysis by the WHO Global Influenza Surveillance and Response System (GISRS). Efforts are also continuing at the national, regional, and global levels to better understand the dynamics of influenza transmission in both temperate and tropical regions. Improved understanding of the degree of influenza seasonality in tropical countries of the world should allow for the strengthening of national vaccination policies and use of the most appropriate available vaccines. There remain a number of limitations and difficulties associated with the use of HAI assays for the antigenic characterization and selection of influenza vaccine viruses by WHOCCs. Current approaches to improving the situation include the more-optimal use of HAI and other assays; improved understanding of the data produced by neutralization assays; and increased standardization of serological testing methods. A number of new technologies and associated tools have the potential to revolutionize influenza surveillance and response activities. These include the increasingly routine use of whole genome next-generation sequencing and other high-throughput approaches. Such approaches could not only become key elements in outbreak investigations but could drive a new surveillance paradigm. However, despite the advances made, significant challenges will need to be addressed before next-generation technologies become routine, particularly in low-resource settings. Emerging approaches and techniques such as synthetic genomics, systems genetics, systems biology and mathematical modelling are capable of generating potentially huge volumes of highly complex and diverse datasets. Harnessing the currently theoretical benefits of such bioinformatics (“big data”) concepts for the influenza vaccine virus selection and development process will depend upon further advances in data generation, integration, analysis and dissemination. Over the last decade, growing awareness of influenza as an important global public health issue has been coupled to ever-increasing demands from the global community for more-equitable access to effective and affordable influenza vaccines. The current influenza vaccine landscape continues to be dominated by egg-based inactivated and live attenuated vaccines, with a small number of cell-based and recombinant vaccines. Successfully completing each step in the annual influenza vaccine manufacturing cycle will continue to rely upon timely and regular communication between the WHO GISRS, manufacturers and regulatory authorities. While the pipeline of influenza vaccines appears to be moving towards a variety of niche products in the near term, it is apparent that the ultimate aim remains the development of effective “universal” influenza vaccines that offer longer-lasting immunity against a broad range of influenza A subtypes.}
}
@article{BAUERMARSCHALLINGER201484,
title = {Optimisation of global grids for high-resolution remote sensing data},
journal = {Computers & Geosciences},
volume = {72},
pages = {84-93},
year = {2014},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2014.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300414001629},
author = {Bernhard Bauer-Marschallinger and Daniel Sabel and Wolfgang Wagner},
keywords = {Remote sensing, High resolution, Big data, Global grid, Projection, Sampling, Equi7 Grid},
abstract = {Upcoming remote sensing systems onboard satellites will generate unprecedented volumes of spatial data, hence challenging processing facilities in terms of storage and processing capacities. Thus, an efficient handling of remote sensing data is of vital importance, demanding a well-suited definition of spatial grids for the data׳s storage and manipulation. For high-resolution image data, regular grids defined by map projections have been identified as practicable, cognisant of their drawbacks due to geometric distortions. To this end, we defined a new metric named grid oversampling factor (GOF) that estimates local data oversampling appearing during projection of generic satellite images to a regular raster grid. Based on common map projections, we defined sets of spatial grids optimised to minimise data oversampling. Moreover, they ensure that data undersampling cannot occur at any location. From the resulting GOF-values we concluded that equidistant projections are most suitable, with a global mean oversampling of 2% when using a system of seven continental grids (introduced under the name Equi7 Grid). Opposed to previous studies that suggested equal-area projections, we recommend the Plate Carrée, the Equidistant Conic and the Equidistant Azimuthal projection for global, hemispherical and continental grids, respectively.}
}
@article{ENEIORDACHE2009404,
title = {Developing Regulatory-compliant Electronic Case Report Forms for Clinical Trials: Experience with The Demand Trial},
journal = {Journal of the American Medical Informatics Association},
volume = {16},
number = {3},
pages = {404-408},
year = {2009},
issn = {1067-5027},
doi = {https://doi.org/10.1197/jamia.M2787},
url = {https://www.sciencedirect.com/science/article/pii/S1067502709000243},
author = {Bogdan Ene-Iordache and Sergio Carminati and Luca Antiga and Nadia Rubis and Piero Ruggenenti and Giuseppe Remuzzi and Andrea Remuzzi},
abstract = {The use of electronic case report forms (CRF) to gather data in randomized clinical trials has grown to progressively replace paper-based forms. Computerized form designs must ensure the same data quality expected of paper CRF, by following Good Clinical Practice rules. Electronic data capture (EDC) tools must also comply with applicable statutory and regulatory requirements. Here the authors focus on the development of computerized systems for clinical trials implementing FDA and EU recommendations and regulations, and describe a laptop-based electronic CRF used in a randomized, multicenter clinical trial.}
}
@incollection{SHERMAN2015301,
title = {Chapter 12 - Data Integration Processes},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {301-333},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000125},
author = {Rick Sherman},
keywords = {Access and delivery services, Data ingestion, Data integration services, Data integration tool, Data profiling, Data quality, Data transformation, Data transport, Extract and load services, Hand coding, Operations management, Process management},
abstract = {Data integration activities are the most time- and resource-consuming portion of building an enterprise business intelligence (BI) environment. Using data integration tools, in particular extract, transform, and load (ETL) tools has been the best practice in BI for more than a decade, but manually coded extracts are still the mainstay of the industry. The current generation of tools consists of full-fledged data integration suites that include ETL, real-time integration, message streaming, Web services, complex event processing, and data virtualization functionality. The suites can support data integration processes in traditional batch mode, near real time and in real time if the right connectivity is in place to support it. In addition, many suites offer extensions for data quality, data cleansing, data profiling, and master data management functionality. Data integration services include access and delivery (extract and load), data ingestion, data profiling, data transformation, data quality, process management, operations management, and data transport.}
}
@article{GILL20151186,
title = {A Framework for Distributed Cleaning of Data Streams},
journal = {Procedia Computer Science},
volume = {52},
pages = {1186-1191},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009564},
author = {Saul Gill and Brian Lee},
keywords = {Internet of Things, Data Stream Cleaning, Model Based Cleaning, Regression, Declarative Cleaning, Distributed Cleaning, Sensor Data Modelling},
abstract = {Vast and ever increasing quantities of data are produced by sensors in the Internet of Things (IoT). The quality of this data can be very variable due to problems with sensors, incorrect calibration etc. Data quality can be greatly enhanced by cleaning the data before it reaches its end user. This paper reports on the construction of a distributed cleaning system (DCS) to clean data streams in real-time for an environmental case-study. A combination of declarative and statistical model based cleaning methods are applied and initial results are reported.}
}
@article{LIU201540,
title = {An Effective Probabilistic Skyline Query Process on Uncertain Data Streams},
journal = {Procedia Computer Science},
volume = {63},
pages = {40-47},
year = {2015},
note = {The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.310},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915024382},
author = {Chuan-Ming Liu and Syuan-Wei Tang},
keywords = {Probabilistic Skyline, Uncertain Data, Data Streams, Query Processing},
abstract = {With the evolution of technology, the ways to acquire data and the applications of data are more diverse. As data volume con- tinuously grows, the data quality may not be high as usual. The data can be defected, imprecise or inaccurate due to the process of data acquiring. Recently, the skyline query is widely used in data analysis to derive the results that meets more than one spe- cific condition simultaneously. For example, the forest monitoring system, which collects the temperature and humidity of the surrounding environment with sensors, to monitor the disasters. Using the skyline query, the zones of potential fire hazards can be found in time, where the temperature is high and the humidity is low. In the mentioned application, the derived data change with time. We refer to such data as data streams. The constant change and uncertainty of data make the query process difficult and need more computations. Thus, how to have an effective skyline query process in terms of time and space over uncertain data streams becomes crucial. In this paper, we discuss this problem and propose an effective approach, Efficient Probabilistic Skyline Update (EPSU), using a new data structure by augmenting the R-tree structure. The relevant algorithms are analyzed and discussed. Last, we perform the simulated experiments extensively with synthetic data to validate the EPSU approach. The results show that EPSU can effectively compute the probabilistic skyline query in terms of the time and space and outperforms the existing ones.}
}
@article{WENDELSDORF20157,
title = {Empowered genome community: leveraging a bioinformatics platform as a citizen–scientist collaboration tool},
journal = {Applied & Translational Genomics},
volume = {6},
pages = {7-10},
year = {2015},
note = {What is Translational Bioinformatics},
issn = {2212-0661},
doi = {https://doi.org/10.1016/j.atg.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212066115300235},
author = {Katherine Wendelsdorf and Sohela Shah},
keywords = {Crowd source, Next-generation sequencing, Variant Analysis, Citizen science, Big data},
abstract = {There is on-going effort in the biomedical research community to leverage Next Generation Sequencing (NGS) technology to identify genetic variants that affect our health. The main challenge facing researchers is getting enough samples from individuals either sick or healthy – to be able to reliably identify the few variants that are causal for a phenotype among all other variants typically seen among individuals. At the same time, more and more individuals are having their genome sequenced either out of curiosity or to identify the cause of an illness. These individuals may benefit from of a way to view and understand their data. QIAGEN's Ingenuity Variant Analysis is an online application that allows users with and without extensive bioinformatics training to incorporate information from published experiments, genetic databases, and a variety of statistical models to identify variants, from a long list of candidates, that are most likely causal for a phenotype as well as annotate variants with what is already known about them in the literature and databases. Ingenuity Variant Analysis is also an information sharing platform where users may exchange samples and analyses. The Empowered Genome Community (EGC) is a new program in which QIAGEN is making this on-line tool freely available to any individual who wishes to analyze their own genetic sequence. EGC members are then able to make their data available to other Ingenuity Variant Analysis users to be used in research. Here we present and describe the Empowered Genome Community in detail. We also present a preliminary, proof-of-concept study that utilizes the 200 genomes currently available through the EGC. The goal of this program is to allow individuals to access and understand their own data as well as facilitate citizen–scientist collaborations that can drive research forward and spur quality scientific dialogue in the general public.}
}
@article{HOXHA2016176,
title = {Leveraging dialog systems research to assist biomedical researchers’ interrogation of Big Clinical Data},
journal = {Journal of Biomedical Informatics},
volume = {61},
pages = {176-184},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416300259},
author = {Julia Hoxha and Chunhua Weng},
keywords = {Health communication, Comparative effectiveness research, Information storage and retrieval},
abstract = {The worldwide adoption of electronic health records (EHR) promises to accelerate clinical research, which lies at the heart of medical advances. However, the interrogation of such Big Data by clinical researchers can be laborious and error-prone, involving iterative and ineffective communication of data requests to data analysts. Research on this communication process is rare. There also exists no contemporary system that offers intelligent solutions to assist clinical researchers in their quest for clinical data. In this article, we first provide a detailed characterization of the challenges encountered in this communication space. Second, we identify promising synergies between fields studying human-to-human and human–machine communication that can shed light on biomedical data query mediation. We propose a mixed-initiative dialog-based approach to support autonomous clinical data access and recommend needed technology development and communication study for accelerating clinical research.}
}
@article{BUNYAVANICH201531,
title = {Systems biology of asthma and allergic diseases: A multiscale approach},
journal = {Journal of Allergy and Clinical Immunology},
volume = {135},
number = {1},
pages = {31-42},
year = {2015},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2014.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0091674914014869},
author = {Supinda Bunyavanich and Eric E. Schadt},
keywords = {Systems biology, network, asthma, allergy, atopic, genome, transcriptome, epigenome, microbiome, metabolome, individual health profile, big data},
abstract = {Systems biology is an approach to understanding living systems that focuses on modeling diverse types of high-dimensional interactions to develop a more comprehensive understanding of complex phenotypes manifested by the system. High-throughput molecular, cellular, and physiologic profiling of populations is coupled with bioinformatic and computational techniques to identify new functional roles for genes, regulatory elements, and metabolites in the context of the molecular networks that define biological processes associated with system physiology. Given the complexity and heterogeneity of asthma and allergic diseases, a systems biology approach is attractive, as it has the potential to model the myriad connections and interdependencies between genetic predisposition, environmental perturbations, regulatory intermediaries, and molecular sequelae that ultimately lead to diverse disease phenotypes and treatment responses across individuals. The increasing availability of high-throughput technologies has enabled system-wide profiling of the genome, transcriptome, epigenome, microbiome, and metabolome, providing fodder for systems biology approaches to examine asthma and allergy at a more holistic level. In this article we review the technologies and approaches for system-wide profiling, as well as their more recent applications to asthma and allergy. We discuss approaches for integrating multiscale data through network analyses and provide perspective on how individually captured health profiles will contribute to more accurate systems biology views of asthma and allergy.}
}
@article{PURSS2015135,
title = {Unlocking the Australian Landsat Archive – From dark data to High Performance Data infrastructures},
journal = {GeoResJ},
volume = {6},
pages = {135-140},
year = {2015},
note = {Rescuing Legacy Data for Future Science},
issn = {2214-2428},
doi = {https://doi.org/10.1016/j.grj.2015.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2214242815000182},
author = {Matthew B.J. Purss and Adam Lewis and Simon Oliver and Alex Ip and Joshua Sixsmith and Ben Evans and Roger Edberg and Glenn Frankish and Lachlan Hurst and Tai Chan},
keywords = {Big data, Landsat, High Performance Data, High Performance Computing, Data rescue, Earth Observation},
abstract = {Earth Observation data acquired by the Landsat missions are of immense value to the global community and constitute the world’s longest continuous civilian Earth Observation program. However, because of the costs of data storage infrastructure these data have traditionally been stored in raw form on tape storage infrastructures which introduces a data retrieval and processing overhead that limits the efficiency of use of this data. As a consequence these data have become ‘dark data’ with only limited use in a piece-meal and labor intensive manner. The Unlocking the Landsat Archive project was set up in 2011 to address this issue and to help realize the true value and potential of these data. The key outcome of the project was the migration of the raw Landsat data that was housed in tape archives at Geoscience Australia to High Performance Data facilities hosted by the National Computational Infrastructure (a super computer facility located at the Australian National University). Once this migration was completed the data were calibrated to produce a living and accessible archive of sensor and scene independent data products derived from Landsat-5 and Landsat-7 data for the period 1998–2012. The calibrated data were organized into High Performance Data structures, underpinned by ISO/OGC standards and web services, which have opened up a vast range of opportunities to efficiently apply these data to applications across multiple scientific domains.}
}
@incollection{VANDERLANS2012253,
title = {Chapter 13 - The Future of Data Virtualization},
editor = {Rick F. {van der Lans}},
booktitle = {Data Virtualization for Business Intelligence Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {253-266},
year = {2012},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-394425-2},
doi = {https://doi.org/10.1016/B978-0-12-394425-2.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123944252000137},
author = {Rick F. {van der Lans}}
}
@article{SAITO2016116,
title = {Optimal room charge and expected sales under discrete choice models with limited capacity},
journal = {International Journal of Hospitality Management},
volume = {57},
pages = {116-131},
year = {2016},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2016.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278431916300792},
author = {Taiga Saito and Akihiko Takahashi and Hiroshi Tsuda},
keywords = {Hotels in Kyoto, Revenue management, Online booking, Discrete choice model},
abstract = {In this paper, we introduce a model that incorporates features of the fully transparent hotel booking systems and enables estimates of hotel choice probabilities in a group based on the room charges. Firstly, we extract necessary information for the estimation from big data of online booking for major four hotels near Kyoto station.11The data were provided by National Institute of Informatics. Then, we consider a nested logit model as well as a multinomial logit model for the choice behavior of the customers, where the number of rooms available for booking for each hotel are possibly limited. In addition, we apply the model to an optimal room charge problem for a hotel that aims to maximize its expected sales of a certain room type in the transparent online booking systems. We show numerical examples of the maximization problem using the data of the four hotels of November 2012 which is a high season in Kyoto city. This model is useful in that hotel managers as well as hotel investors, such as hotel REITs and hotel funds, are able to predict the potential sales increase of hotels from online booking data and make use of the result as a tool for investment decisions.}
}
@article{ISIK201313,
title = {Business intelligence success: The roles of BI capabilities and decision environments},
journal = {Information & Management},
volume = {50},
number = {1},
pages = {13-23},
year = {2013},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2012.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720612000833},
author = {Öykü Işık and Mary C. Jones and Anna Sidorova},
keywords = {Business intelligence, BI, Decision environment, BI capabilities, BI success, PLS},
abstract = {This study examines the role of the decision environment in how well business intelligence (BI) capabilities are leveraged to achieve BI success. We examine the decision environment in terms of the types of decisions made and the information processing needs of the organization. Our findings suggest that technological capabilities such as data quality, user access and the integration of BI with other systems are necessary for BI success, regardless of the decision environment. However, the decision environment does influence the relationship between BI success and capabilities, such as the extent to which BI supports flexibility and risk in decision making.}
}
@article{GARCIA20161,
title = {Tutorial on practical tips of the most influential data preprocessing algorithms in data mining},
journal = {Knowledge-Based Systems},
volume = {98},
pages = {1-29},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004785},
author = {Salvador García and Julián Luengo and Francisco Herrera},
keywords = {Data preprocessing, Data reduction, Missing values imputation, Noise filtering, Dimensionality reduction, Instance reduction, Discretization, Data mining},
abstract = {Data preprocessing is a major and essential stage whose main goal is to obtain final data sets that can be considered correct and useful for further data mining algorithms. This paper summarizes the most influential data preprocessing algorithms according to their usage, popularity and extensions proposed in the specialized literature. For each algorithm, we provide a description, a discussion on its impact, and a review of current and further research on it. These most influential algorithms cover missing values imputation, noise filtering, dimensionality reduction (including feature selection and space transformations), instance reduction (including selection and generation), discretization and treatment of data for imbalanced preprocessing. They constitute all among the most important topics in data preprocessing research and development. This paper emphasizes on the most well-known preprocessing methods and their practical study, selected after a recent, generic book on data preprocessing that does not deepen on them. This manuscript also presents an illustrative study in two sections with different data sets that provide useful tips for the use of preprocessing algorithms. In the first place, we graphically present the effects on two benchmark data sets for the preprocessing methods. The reader may find useful insights on the different characteristics and outcomes generated by them. Secondly, we use a real world problem presented in the ECDBL’2014 Big Data competition to provide a thorough analysis on the application of some preprocessing techniques, their combination and their performance. As a result, five different cases are analyzed, providing tips that may be useful for readers.}
}
@incollection{BERANGER20161,
title = {1 - The Shift towards a Connected, Assessed and Personalized Medicine Centered Upon Medical Datasphere Processing},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {1-95},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500014},
author = {Jérôme Béranger},
keywords = {Economic market, Ethical risks, Law and regulation, Medical Big Data, Medical Datasphere Processing, Medicine 4.0, Open Data, Personal health data, Personalized Medicine, Quantified Self},
abstract = {Abstract:
Today’s world corresponds to a universe where digital data is omnipresent, thus opening up prospects around reality that we have never known before. Hence, we are witnessing the emergence of the process of “datafication” which consists of digitizing and assessing everything, so that data emerges from written works, locations, individual actions or even fingerprints. Such a phenomenon contributes to transforming our ecosystem by providing the possibility of analyzing infinite quantities of increasing amounts of data, the acceptability of both approximation and disorder and the search for correlations rather than relationships between cause and effect. It may be observed that this notion of “correlations” stemming from biology has been used for a long time in economics.}
}
@article{2016iii,
title = {Contents},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {iii-iv},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(16)30161-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16301613}
}
@article{ZIBERT2016217,
title = {Particulate matter (PM10) patterns in Europe: An exploratory data analysis using non-negative matrix factorization},
journal = {Atmospheric Environment},
volume = {132},
pages = {217-228},
year = {2016},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2016.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1352231016301741},
author = {Janez Žibert and Jure Cedilnik and Jure Pražnikar},
keywords = {Particular matter, Non-negative matrix factorization, Space-time patterns, Synoptic situations},
abstract = {In last decade space-density of monitoring stations increased, in to addition also air pollution modeling made big progress. Using diversity of big data can lead to better knowledge about air pollution at continental scale. The focus of presented study is the data-driven approach using non-negative matrix factorization to provide new insights and to study the characteristic space-time particulate-matter patterns across Europe. We analyzed the PM10 concentrations obtained from 1097 monitoring stations (AirBase data) and the Monitoring Atmospheric Composition and Climate (MACC) modeled fields for a period of 3 years. We distinguished five characteristic patterns obtained from the AirBase data and five patterns from the MACC data. A comparison between the AirBase and MACC data shows a good spatial overlap for the east Europe, central Europe and the Mediterranean patterns. However, it should be noted that an analysis of the MACC data revealed two additional marine patterns: the Celtic and the North Seas. The Po Valley and Balkan patterns were very clearly identified when analyzing the AirBase data. In order to better understand the influence of the synoptic situation on the particulate-matter concentrations the synoptic meteorological situations were additionally analyzed. The cold season, low wind and very stable conditions, which can last for several days, is the most common situation linked to high concentrations of anthropogenic air pollution with particulate matter. In contrast, for the Mediterranean pattern the most common situation (high factor loadings) is observed during the summer period. This pattern also exhibits a clearer annual cycle. A closer look at the sea-salt patterns (Celtic and North Seas) shows low time-series correlations between these two factors. Nevertheless, the physical mechanism is the same: a steep gradient between the cyclone and the anti-cyclone that causes high winds and, consequently, higher sea-salt production.}
}
@incollection{2015xiii,
title = {Preface},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xiii-xvi},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.05001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378050018}
}
@article{LIVI2015567,
title = {Granular modeling and computing approaches for intelligent analysis of non-geometric data},
journal = {Applied Soft Computing},
volume = {27},
pages = {567-574},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2014.08.072},
url = {https://www.sciencedirect.com/science/article/pii/S1568494614004682},
author = {Lorenzo Livi and Antonello Rizzi and Alireza Sadeghian},
keywords = {Granular modeling and computing, Analysis of non-geometric data, Dissimilarity measure, Soft computing, Pattern recognition},
abstract = {Data analysis techniques have been traditionally conceived to cope with data described in terms of numeric vectors. The reason behind this fact is that numeric vectors have a well-defined and clear geometric interpretation, which facilitates the analysis from the mathematical viewpoint. However, the state-of-the-art research on current topics of fundamental importance, such as smart grids, networks of dynamical systems, biochemical and biophysical systems, intelligent trading systems, multimedia content-based retrieval systems, and social networks analysis, deal with structured and non-conventional information characterizing the data, providing richer and hence more complex patterns to be analyzed. As a consequence, representing patterns by complex (relational) structures and defining suitable, usually non-metric, dissimilarity measures is becoming a consolidated practice in related fields. However, as the data sources become more complex, the capability of judging over the data quality (or reliability) and related interpretability issues can be seriously compromised. For this purpose, automated methods able to synthesize relevant information, and at the same time rigorously describe the uncertainty in the available datasets, are very important: information granulation is the key aspect in the analysis of complex data. In this paper, we discuss our general viewpoint on the adoption of information granulation techniques in the general context of soft computing and pattern recognition, conceived as a fundamental approach towards the challenging problem of automatic modeling of complex systems. We focus on the specific setting of processing the so-called non-geometric data, which diverges significantly from what has been done so far in the related literature. We highlight the motivations, the founding concepts, and finally we provide the high-level conceptualization of the proposed data analysis framework.}
}
@article{WANG201656,
title = {Credible and energy-aware participant selection with limited task budget for mobile crowd sensing},
journal = {Ad Hoc Networks},
volume = {43},
pages = {56-70},
year = {2016},
note = {Smart Wireless Access Networks and Systems for Smart Cities},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2016.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1570870516300300},
author = {Wendong Wang and Hui Gao and Chi Harold Liu and Kin K. Leung},
keywords = {Crowd sensing, Incentive, Reputation, Quality of information, Difficulty of task},
abstract = {Crowd sensing campaigns encourage ordinary people to collect and share sensing data by using their carried smart devices. However, new challenges that must be faced have arisen. One of them is how to allocate tasks to the most appropriate participants when considering their different incentive requirements and credibility, in order to best satisfy the quality-of-information (QoI) requirements of multiple concurrent tasks, with different, and limited budget constraints. Another challenge is how to maximize participants’ rewards to encourage them to contribute sensing data continuously. To this end, in this paper, we first propose a crowd sensing system, that aims to address the above two challenges, where the system considers the benefits of both platform and participants. Then, a participant reputation definition and update method is proposed, that takes participant’s willingness and contributed data quality into consideration. Last, we introduce two metrics called “QoI satisfaction” and “Difficulty of Task (DoT)”. The former quantifies how much collected sensing data can satisfy the multi-dimensional task’s QoI requirements in terms of data quality, granularity and quantity, and the later aids participants to choose proper tasks to maximize their rewards. Finally, we compare our proposed scheme with existing methods via extensive simulations based on a real dataset. Extensive simulation results well justify the effectiveness and robustness of our approach.}
}
@incollection{LEE2015341,
title = {Chapter 19 - Intelligent Factory Agents with Predictive Analytics for Asset Management},
editor = {Paulo Leitão and Stamatis Karnouskos},
booktitle = {Industrial Agents},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {341-360},
year = {2015},
isbn = {978-0-12-800341-1},
doi = {https://doi.org/10.1016/B978-0-12-800341-1.00019-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800341100019X},
author = {Jay Lee and Hung-An Kao and Hossein Davari Ardakani and David Siegel},
keywords = {Prognostics and health management, Predictive manufacturing, Intelligence maintenance, Condition monitoring, Asset management},
abstract = {The conceptual framework of predictive manufacturing systems starts with data acquisition of the monitored assets. Using appropriate sensor installations, various signals such as vibration, pressure, etc., can be extracted. In addition, historical data can be harvested for further data mining. Communication protocols, such as MTConnect and OPC, can help users record controller signals. When all the data are aggregated, this amalgamation is called “Big Data.” The transforming agent consists of several components: an integrated platform, predictive analytics, and visualization tools. The algorithms found in the Watchdog Agent® can be categorized into four sections: signal processing and feature extraction, health assessment, performance prediction, and fault diagnosis. By utilizing the tools, health information (such as current condition, remaining useful life, failure mode, etc.) can be effectively conveyed in terms of a radar chart, fault map, risk chart, and health degradation curves.}
}
@article{ALBAFERNANDEZ2016352,
title = {On the similarity analysis of spatial patterns},
journal = {Spatial Statistics},
volume = {18},
pages = {352-362},
year = {2016},
issn = {2211-6753},
doi = {https://doi.org/10.1016/j.spasta.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2211675316300458},
author = {M.V. Alba-Fernández and F.J. Ariza-López and M. Dolores Jiménez-Gamero and J. Rodríguez-Avi},
keywords = {Space-filling curve, Multinomial distribution, Simulation},
abstract = {The evaluation of the spatial similarity of two observed point patterns is an important issue in spatial data quality assessment. In this work we propose a formal procedure that takes advantage of the joint use of space-filling curves and the multinomial model in order to establish a statistical test to compare spatial point patterns. In this mix, the space-filling curves offer a mechanism to order the 2D, 3D or n-D space and the multinomial distribution the statistical approach for testing homogeneity. A simulation method is proposed in order to analyze the applied performance of this idea.}
}
@incollection{KRISHNAN2013127,
title = {Chapter 6 - Data Warehousing Revisited},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {127-145},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000064},
author = {Krish Krishnan},
keywords = {Inmon’s DW 2.0, DSS 2.0, database, scalability, complexity, workloads, data warehouse, star schema, ETL, datamart, analytics, business intelligence, BUS architecture},
abstract = {The goal of this chapter is to revisit data warehousing as we know it today, and the issues that have confronted the world of data warehousing leading to lack of adoption and often failure of data warehouses.}
}
@article{LODIGIANI2016566,
title = {A PageRank-based Reputation Model for VGI Data},
journal = {Procedia Computer Science},
volume = {98},
pages = {566-571},
year = {2016},
note = {The 7th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2016)/The 6th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2016)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916322335},
author = {Carlo Lodigiani and Michele Melchiori},
keywords = {VGI, volunteered geographic information, reputation model, trust model, VGI data model, PageRank ;},
abstract = {Quality of data is one of the key issues in the domain of Volunteered geographic information (VGI). To this purpose, in literature VGI data has been sometime compared with authoritative geospatial data. Evaluation of single contributions to VGI databases is more relevant for some applications and typically relies on evaluating reputation of contributors and using it as proxy measures for data quality. In this paper, we present a novel approach for reputation evaluation that is based on the well known PageRank algorithm for Web pages. We use a simple model for describing different versions of a geospatial entity in terms of corrections and completions. Authors, VGI contributions and their mutual relationships are modeled as nodes of a graph. In order to evaluate reputation of authors and contributions in the graph we propose an algorithm that is based on the personalized version of PageRank.}
}
@article{SIOUTAS2015211,
title = {D-P2P-Sim+: A novel distributed framework for P2P protocols performance testing},
journal = {Journal of Systems and Software},
volume = {100},
pages = {211-233},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002416},
author = {S. Sioutas and E. Sakkopoulos and A. Panaretos and D. Tsoumakos and P. Gerolymatos and G. Tzimas and Y. Manolopoulos},
keywords = {Distributed storage systems, IoT and Web 2.0 applications, P2P data management},
abstract = {In recent technologies like IoT (Internet of Things) and Web 2.0, a critical problem arises with respect to storing and processing the large amount of collected data. In this paper we develop and evaluate distributed infrastructures for storing and processing large amount of such data. We present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The proposed framework provides the appropriate integrated set of tools that allows applications processing large amount of data, to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes–billions of keys). The key aim is to provide the appropriate environment that contributes in taking decisions regarding the choice of the protocol in storage P2P systems for a variety of big data applications. Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup. D-P2P-Sim+ framework is publicly available at http://code.google.com/p/d-p2p-sim/downloads/list.}
}
@article{FISKE2016145,
title = {How to publish rigorous experiments in the 21st century},
journal = {Journal of Experimental Social Psychology},
volume = {66},
pages = {145-147},
year = {2016},
note = {Rigorous and Replicable Methods in Social Psychology},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0022103116000032},
author = {Susan T. Fiske},
keywords = {Experiments, Replicability, Internal validity, External validity, Theory},
abstract = {Crises provide an opportunity for the field to take stock, as do the articles in this special issue. Constructive advice for 21st century publication standards includes appropriate theory, internal validity, and external validity. First, well-grounded theory can produce a priori plausibility, testable logic, and a focus on the ideas involved, all cumulatively informed by meta-analysis across studies. Second, internal validity benefits from both exploratory work and confirmatory analyses on well-powered samples that require systematic detection and principled decisions about data quality. Inferences benefit from manipulated mediation analysis and from careful interpretation without over-claiming. Finally, external validity profits from a variety of exact and conceptual replications, best evaluated by meta-analysis.}
}
@article{DURGALAKSHMI2015551,
title = {Progonosis and Modelling of Breast Cancer and its Growth Novel Naive Bayes},
journal = {Procedia Computer Science},
volume = {50},
pages = {551-553},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915006031},
author = {B. Durgalakshmi and V. Vijayakumar},
keywords = {Naive Bayes, Hadoop.Feature Selection},
abstract = {Cancer is a crucial disease which kills the people worldwide. To reduce cancer death rate is to detect it earlier. It depends upon the age, blood group, food habits, genetic combination, and heredity. Even though predicting cancer is generally clinical and biological in nature, in general we used some of the computation methods and artificial intelligence to predict breast cancer through images and rough set values.In this paper we are using revised edition of classifier (i.e) Mahout Naive Bayes and the performance and accuracy can be calculated}
}
@article{CAI2015206,
title = {Evaluating the impact of odors from the 1955 landfills in China using a bottom-up approach},
journal = {Journal of Environmental Management},
volume = {164},
pages = {206-214},
year = {2015},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2015.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0301479715302619},
author = {Bofeng Cai and Jinnan Wang and Ying Long and Wanxin Li and Jianguo Liu and Zhe Ni and Xin Bo and Dong Li and Jianghao Wang and Xuejing Chen and Qingxian Gao and Lixiao Zhang},
keywords = {Landfill, Odor impact ranges, Bottom-up, Uncertainty evaluation},
abstract = {Landfill odors have created a major concern for the Chinese public. Based on the combination of a first order decay (FOD) model and a ground-level point source Gaussian dispersion model, the impacts from odors emitted from the 1955 landfills in China are evaluated in this paper. Our bottom-up approach uses basic data related to each landfill to achieve a more accurate and comprehensive understanding of impact of landfill odors. Results reveal that the average radius of impact of landfill odors in China is 796 m, while most landfills (46.85%) are within the range of 400–1000 m, in line with the results from previous studies. The total land area impacted by odors has reached 837,476 ha, accounting for 0.09% of China's land territory. Guangdong and Sichuan provinces have the largest land areas impacted by odors, while Tibet Autonomous Region and Tianjin Municipality have the smallest. According to the CALPUFF (California Puff) model and an analysis of social big data, the overall uncertainty of our calculation of the range of odor impacts is roughly −32.88% to 32.67%. This type of study is essential for gaining an accurate and detailed estimation of the affected human population and will prove valuable for addressing the current Not In My Back Yard (NIMBY) challenge in China.}
}
@article{OBERG20155262,
title = {Lessons learned in the analysis of high-dimensional data in vaccinomics},
journal = {Vaccine},
volume = {33},
number = {40},
pages = {5262-5270},
year = {2015},
note = {Systems Vaccinology: How Big Data Can Accelerate Vaccine Development},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2015.04.088},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X15005745},
author = {Ann L. Oberg and Brett A. McKinney and Daniel J. Schaid and V. Shane Pankratz and Richard B. Kennedy and Gregory A. Poland},
keywords = {Vaccines, Vaccination, Systems biology, Immunogenetics, Data interpretation, statistical},
abstract = {The field of vaccinology is increasingly moving toward the generation, analysis, and modeling of extremely large and complex high-dimensional datasets. We have used data such as these in the development and advancement of the field of vaccinomics to enable prediction of vaccine responses and to develop new vaccine candidates. However, the application of systems biology to what has been termed “big data,” or “high-dimensional data,” is not without significant challenges—chief among them a paucity of gold standard analysis and modeling paradigms with which to interpret the data. In this article, we relate some of the lessons we have learned over the last decade of working with high-dimensional, high-throughput data as applied to the field of vaccinomics. The value of such efforts, however, is ultimately to better understand the immune mechanisms by which protective and non-protective responses to vaccines are generated, and to use this information to support a personalized vaccinology approach in creating better, and safer, vaccines for the public health.}
}
@incollection{IMHOFF2012xiii,
title = {Foreword},
editor = {Rick F. {van der Lans}},
booktitle = {Data Virtualization for Business Intelligence Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xiii-xiv},
year = {2012},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-394425-2},
doi = {https://doi.org/10.1016/B978-0-12-394425-2.00019-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123944252000198},
author = {Claudia Imhoff}
}
@incollection{MCKNIGHT2014144,
title = {Chapter Fourteen - An Elegant Architecture Where Information Flows},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {144-157},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012408056000014X},
author = {William McKnight},
keywords = {business intelligence, information architecture, leadership, information technology, information management maturity, big data, data warehouse, cloud computing, data virtualization, master data management, data warehouse appliance, analytics, cubes, multidimensional databases, data warehouse consolidation, data marts, data stream processing, NoSQL},
abstract = {Guidelines for absorbing the information in the book and putting it into practice.}
}
@article{HEIPKE20161,
title = {Theme issue “State-of-the-art in photogrammetry, remote sensing and spatial information science”},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {1-2},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2016.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271616000630},
author = {Christian Heipke and Marguerite Madden and Zhilin Li and Ian Dowman}
}
@article{HOLZINGER2015141,
title = {Introduction to the special issue on “interactive data analysis”},
journal = {Information Processing & Management},
volume = {51},
number = {2},
pages = {141-143},
year = {2015},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2014.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306457314001101},
author = {Andreas Holzinger and Gabriella Pasi}
}
@article{SORANI20151546,
title = {It Is Time for Bowel-Omics},
journal = {Archives of Physical Medicine and Rehabilitation},
volume = {96},
number = {8},
pages = {1546-1547},
year = {2015},
issn = {0003-9993},
doi = {https://doi.org/10.1016/j.apmr.2015.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0003999315004633},
author = {Marco D. Sorani}
}
@incollection{BERANGER2016xi,
title = {Introduction},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {xi-xxxvi},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500087},
author = {Jérôme Béranger}
}
@article{LONGO2014471,
title = {Fact – Centered ETL: A Proposal for Speeding Business Analytics up},
journal = {Procedia Technology},
volume = {16},
pages = {471-480},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.114},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314003417},
author = {Antonella Longo and Sara Giacovelli and Mario A. Bochicchio},
keywords = {Business Intelligence, ETL, Process & Conceptual Models, Operational Intelligence ;},
abstract = {In real-time Business Analytics scenarios, like Business Activity Monitoring (BAM) or Operational Intelligence, speeding up ETL is fundamental to provide business users with up-to-date data in order to support decision-making process and to optimize business operations. Conventional ETL processes extract data from heterogeneous operational data sources, transform them according to some predefined semantic and syntactic rules, and finally load results into a new (relational or data warehouse) model, in order to be processed both for business monitoring or analysis. Usually the whole loading process is presented as a technical aspect, far from business facts. In real-time Business Analytics context the traditional ETL approach can still be valid, provided that ETL process has to be designed in a way that circumscribes individual business events and facts and makes them independent one from another. In this context we propose a control-flow- based approach to ETL process modeling, which starts from business facts identification, and represents ETL processes using BPMN notation, which is the foundation for machine-readable code. Our main contribution consists in a proposal for structuring ETL processes and related objects and in its application to a business case.}
}
@article{DIMITRIU2014840,
title = {A New Paradigm for Accounting through Cloud Computing},
journal = {Procedia Economics and Finance},
volume = {15},
pages = {840-846},
year = {2014},
note = {Emerging Markets Queries in Finance and Business (EMQ 2013)},
issn = {2212-5671},
doi = {https://doi.org/10.1016/S2212-5671(14)00541-3},
url = {https://www.sciencedirect.com/science/article/pii/S2212567114005413},
author = {Otilia Dimitriu and Marian Matei},
keywords = {cloud computing, accounting model, cloud accounting, Software as a Service;},
abstract = {Nowadays, the impact of globalization, the rapid advances in science and technology, the rise of big data, the wide spread of internet-based applications and even standardization have created the proper context for the emergence of a new concept -cloud accounting. The digitization of business, the increasing amplitude of virtual reality, the metamorphosis of traditional computer business schemes towards cloud-based solutions are underlying drivers of change that shape the actual principles of the market. On the other hand, accounting is an essential component of the framework that sustains any enterprise's activity. The focus of this article is the impact of the cloud computing paradigm on the accounting domain. We highlight different points of view and definitions assigned to the concept of ‘cloud accounting’, and also, the benefits and possible risks determined by the adoption of these services, particularly in relation to the accounting department. Our approach is specifically centered on the financial implications and the pricing offerings that come along with cloud computing. Furthermore, the paper analyzes the key aspects that should be considered by any company when deciding to choose the right accounting system. Understanding the specific requirements of the business is vital - be it online or offline.}
}
@article{BATAINEH2016472,
title = {Monetizing Personal Data: A Two-Sided Market Approach},
journal = {Procedia Computer Science},
volume = {83},
pages = {472-479},
year = {2016},
note = {The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.04.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916302447},
author = {Ahmed Saleh Bataineh and Rabeb Mizouni and May El Barachi and Jamal Bentahar},
keywords = {Personal data monetization, Data quality, Data pricing, Two-sided market, Demand curve, Supply curve, Mobile phone sensing},
abstract = {Mobile phone-based sensing is a new paradigm that aims at using smartpohnes to answer sensing requests and collect useful data. Nowadays, a wide variety of domains ranging from health-care applications to pollution monitoring are benefiting from such collected data. However, despite its increasing popularity and the huge amount of data provided by users, there is no platform where mobile phone owners can effectively sell their data. In this paper, we propose the idea of a data monetization platform using two-sided market theory. In this platform, the data is viewed as an economic good and the data sharing activity is considered as an economic transaction. The proposed platform considers the case of abundant data. An experimental analysis is conducted to compare our approach against the peer-to-peer model using a real case study from the health care domain. We show that our proposed platform has the potential to generate higher profit for both data providers and data consumers.}
}
@article{MINOR2016216,
title = {Safeguarding Structural Data Repositories against Bad Apples},
journal = {Structure},
volume = {24},
number = {2},
pages = {216-220},
year = {2016},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2015.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0969212616000022},
author = {Wladek Minor and Zbigniew Dauter and John R. Helliwell and Mariusz Jaskolski and Alexander Wlodawer},
abstract = {Structural biology research generates large amounts of data, some deposited in public databases or repositories, but a substantial remainder never becomes available to the scientific community. In addition, some of the deposited data contain less or more serious errors that may bias the results of data mining. Thorough analysis and discussion of these problems is needed to ameliorate this situation. This perspective is an attempt to propose some solutions and encourage both further discussion and action on the part of the relevant organizations, in particular the PDB and various bodies of the International Union of Crystallography.}
}
@incollection{2013335,
title = {Index},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {335-346},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000271}
}
@incollection{ALLEN201537,
title = {Chapter 3 - Architecture and Technology Considerations},
editor = {Mark Allen and Dalton Cervo},
booktitle = {Multi-Domain Master Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {37-54},
year = {2015},
isbn = {978-0-12-800835-5},
doi = {https://doi.org/10.1016/B978-0-12-800835-5.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008355000038},
author = {Mark Allen and Dalton Cervo},
keywords = {Architecture, technology, tools, solutions, vendors, platforms, data hub, registry, hybrid, transaction, persistent},
abstract = {This chapter provides perspective on various data architecture scenarios, as well as how and where current technologies can help support and enable a multi-domain model. It covers how to determine the capability needs and use cases for the technology solutions and explores the limitations, such that one platform (or even multiple tools) cannot address all requirements.}
}
@incollection{2013229,
title = {Glossary},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {229-245},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.09982-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767099822}
}
@article{WYLLIE2015267,
title = {Role of data warehousing in healthcare epidemiology},
journal = {Journal of Hospital Infection},
volume = {89},
number = {4},
pages = {267-270},
year = {2015},
note = {Proceedings from the 9th Healthcare Infection Society International Conference},
issn = {0195-6701},
doi = {https://doi.org/10.1016/j.jhin.2015.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0195670115000523},
author = {D. Wyllie and J. Davies},
keywords = {Data management, Epidemiology, Healthcare data},
abstract = {Summary
Electronic storage of healthcare data, including individual-level risk factors for both infectious and other diseases, is increasing. These data can be integrated at hospital, regional and national levels. Data sources that contain risk factor and outcome information for a wide range of conditions offer the potential for efficient epidemiological analysis of multiple diseases. Opportunities may also arise for monitoring healthcare processes. Integrating diverse data sources presents epidemiological, practical, and ethical challenges. For example, diagnostic criteria, outcome definitions, and ascertainment methods may differ across the data sources. Data volumes may be very large, requiring sophisticated computing technology. Given the large populations involved, perhaps the most challenging aspect is how informed consent can be obtained for the development of integrated databases, particularly when it is not easy to demonstrate their potential. In this article, we discuss some of the ups and downs of recent projects as well as the potential of data warehousing for antimicrobial resistance monitoring.}
}
@incollection{SIMON201445,
title = {Chapter 4 - Surveying Relevant Enterprise Data Management Technologies},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {45-61},
year = {2014},
isbn = {978-0-12-801539-1},
doi = {https://doi.org/10.1016/B978-0-12-801539-1.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015391000046},
author = {Alan Simon},
keywords = {Databases, Database Management Systems, DBMS, Master Data, Data Virtualization, Metadata, Data Quality, Data Profiling, Data Governance, Data Interchange, ETL, Data Mining, Data Security, Data Archiving, Maturity Model},
abstract = {Many different technologies are part of nearly every modern enterprise data management environment. A high-level understanding of the various categories and how they fit together is critical for enterprise data management strategists and architects as they build out their roadmaps.}
}
@incollection{GUDIVADA20163,
title = {Chapter 1 - Cognitive Computing: Concepts, Architectures, Systems, and Applications},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {3-38},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300451},
author = {V.N. Gudivada},
keywords = {Cognitive computing, Cognitive architectures, Cognitive models, Cognitive systems, Cognitive applications, Cognitive computing systems, Data science},
abstract = {Cognitive computing is an emerging field ushered in by the synergistic confluence of cognitive science, data science, and an array of computing technologies. Cognitive science theories provide frameworks to describe various models of human cognition including how information is represented and processed by the brain. Data science provides processes and systems to extract knowledge from both structured and unstructured data. Cognitive computing employs the computing discipline's theories, methods, and tools to model human cognition. The recent advances in data science and computing disciplines—neuromorphic processors, big data, predictive modeling, machine learning, natural language understanding, and cloud computing—are accelerating advances in cognitive science and cognitive computing. The overarching goal of this chapter is to provide an interdisciplinary introduction to cognitive computing. The focus is on breadth to provide a unified view of the discipline. The chapter begins with an overview of cognitive science, data science, and cognitive computing. The principal technology enablers of cognitive computing are presented next. An overview of three major categories of cognitive architectures is presented, which is followed by a description of cognitive computing systems and their applications. Trends and future research directions in cognitive computing are discussed. The chapter concludes by listing various cognitive computing resources.}
}
@article{NIELSON2015124,
title = {Leveraging biomedical informatics for assessing plasticity and repair in primate spinal cord injury},
journal = {Brain Research},
volume = {1619},
pages = {124-138},
year = {2015},
note = {Repair stratagies for spinal cord injury},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2014.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S000689931401470X},
author = {Jessica L. Nielson and Jenny Haefeli and Ernesto A. Salegio and Aiwen W. Liu and Cristian F. Guandique and Ellen D. Stück and Stephanie Hawbecker and Rod Moseanko and Sarah C. Strand and Sharon Zdunowski and John H. Brock and Roland R. Roy and Ephron S. Rosenzweig and Yvette S. Nout-Lomas and Gregoire Courtine and Leif A Havton and Oswald Steward and V. {Reggie Edgerton} and Mark H. Tuszynski and Michael S. Beattie and Jacqueline C. Bresnahan and Adam R. Ferguson},
keywords = {Non-human primate, Spinal cord injury, Bioinformatics, Big-data, Syndromics, Statistics, Translation, Plasticity, Recovery, Motor function, Sensory function, Autonomic function},
abstract = {Recent preclinical advances highlight the therapeutic potential of treatments aimed at boosting regeneration and plasticity of spinal circuitry damaged by spinal cord injury (SCI). With several promising candidates being considered for translation into clinical trials, the SCI community has called for a non-human primate model as a crucial validation step to test efficacy and validity of these therapies prior to human testing. The present paper reviews the previous and ongoing efforts of the California Spinal Cord Consortium (CSCC), a multidisciplinary team of experts from 5 University of California medical and research centers, to develop this crucial translational SCI model. We focus on the growing volumes of high resolution data collected by the CSCC, and our efforts to develop a biomedical informatics framework aimed at leveraging multidimensional data to monitor plasticity and repair targeting recovery of hand and arm function. Although the main focus of many researchers is the restoration of voluntary motor control, we also describe our ongoing efforts to add assessments of sensory function, including pain, vital signs during surgery, and recovery of bladder and bowel function. By pooling our multidimensional data resources and building a unified database infrastructure for this clinically relevant translational model of SCI, we are now in a unique position to test promising therapeutic strategies׳ efficacy on the entire syndrome of SCI. We review analyses highlighting the intersection between motor, sensory, autonomic and pathological contributions to the overall restoration of function. This article is part of a Special Issue entitled SI: Spinal cord injury.}
}
@incollection{2014189,
title = {Index},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {189-195},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00032-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000321}
}
@article{LEPHUOC201625,
title = {The Graph of Things: A step towards the Live Knowledge Graph of connected things},
journal = {Journal of Web Semantics},
volume = {37-38},
pages = {25-35},
year = {2016},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826816000196},
author = {Danh Le-Phuoc and Hoan {Nguyen Mau Quoc} and Hung {Ngo Quoc} and Tuan {Tran Nhat} and Manfred Hauswirth},
keywords = {Internet of Things, Graph of Things, Linked Stream Data, Real-time search engine},
abstract = {The Internet of Things (IoT) with billions of connected devices has been generating an enormous amount of data every hour. Connecting every data item generated by IoT to the rest of the digital world to turn this data into meaningful actions will create new capabilities, richer experiences, and unprecedented economic opportunities for businesses, individuals, and countries. However, providing an integrated view for exploring and querying such data at real-time is extremely challenging due to its Big Data natures: big volume, fast real-time update and messy data sources. To address this challenge, we provide a unified integrated and live view for heterogeneous IoT data sources using Linked Data, called the Graph of Things (GoT). GoT is backed by a scalable and elastic software stack to deal with billions of records of historical and static datasets in conjunction with millions of triples being fetched and enriched to connect to GoT per hour in real time. GoT makes approximately a half of million stream data sources queryable via a SPARQL endpoint and a continuous query channel that enable us to create a live explorer of GoT (http://graphofthings.org/) with just HTML and Javascript.}
}
@article{DUTTA2014264,
title = {Development of an intelligent environmental knowledge system for sustainable agricultural decision support},
journal = {Environmental Modelling & Software},
volume = {52},
pages = {264-272},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213002351},
author = {Ritaban Dutta and Ahsan Morshed and Jagannath Aryal and Claire D'Este and Aruneema Das},
keywords = {Feature, Semantic matching, Knowledge integration, i-EKbase, Linked Open Data cloud},
abstract = {The purpose of this research was to develop a knowledge recommendation architecture based on unsupervised machine learning and unified resource description framework (RDF) for integrated environmental sensory data sources. In developing this architecture, which is very useful for agricultural decision support systems, we considered web based large-scale dynamic data mining, contextual knowledge extraction, and integrated knowledge representation methods. Five different environmental data sources were considered to develop and test the proposed knowledge recommendation framework called Intelligent Environmental Knowledgebase (i-EKbase); including Bureau of Meteorology SILO, Australian Water Availability Project, Australian Soil Resource Information System, Australian National Cosmic Ray Soil Moisture Monitoring Facility, and NASA's Moderate Resolution Imaging Spectroradiometer. Unsupervised clustering techniques based on Principal Component Analysis (PCA), Fuzzy-C-Means (FCM) and Self-organizing map (SOM) were used to create a 2D colour knowledge map representing the dynamics of the i-EKbase to provide “prior knowledge” about the integrated knowledgebase. Prior availability of recommendations from the knowledge base could potentially optimize the accessibility and usability issues related to big data sets and minimize the overall application costs. RDF representation has made i-EKbase flexible enough to publish and integrate on the Linked Open Data cloud. This newly developed system was evaluated as an expert agricultural decision support for sustainable water resource management case study in Australia at Tasmania with promising results.}
}
@article{LORD2015S11,
title = {Data from the Internet: New methods for automated insomnia interventions},
journal = {Sleep Medicine},
volume = {16},
pages = {S11-S12},
year = {2015},
note = {6th World Congress on Sleep Medicine, 21st to 25th March 2015, Seoul, South Korea},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2015.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1389945715001252},
author = {H. Lord and F. Thorndike and C. Morin and L. Gonder-Frederick and M. Quigg and K. Ingersol and L. Ritterband}
}
@article{SANGUPAMBAMWILU2016108,
title = {Design science research contribution to business intelligence in the cloud — A systematic literature review},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {108-122},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003623},
author = {Odette {Sangupamba Mwilu} and Isabelle Comyn-Wattiau and Nicolas Prat},
keywords = {Business intelligence, Analytics, Cloud computing, Design science research, Systematic literature review},
abstract = {Business intelligence (BI) helps managers make informed decisions. In the age of big data, BI technology provides essential support for decision making. Cloud computing also attracts many organizations because of its potential: ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, servers, storage, applications, and services). This paper focuses on the deployment of BI in the cloud, from the vantage point of design science research (DSR). We produce a state of the art of research pertaining to BI in the cloud, following the methodology of systematic literature review. This literature review especially exhibits the different artifacts proposed by design science researchers regarding BI in the cloud. To structure the literature review, we propose a framework composed of two dimensions: artifact type and BI step. In particular, we propose a typology of artifact types, refining the coarse-grained typology commonly used in DSR. We use the two-dimensional framework both to map the current state of DSR regarding BI in the cloud, and to elicit future research avenues in terms of design science artifacts for BI in the cloud. The contribution is threefold: the literature review may help DSR researchers get an overview of this active research domain; the two-dimensional framework facilitates the understanding of different research streams; finally, the proposed future topics may guide researchers in identifying promising research avenues.}
}
@incollection{WILLIAMS2016185,
title = {Chapter 8 - General Management Perspectives on Technical Topics},
editor = {Steve Williams},
booktitle = {Business Intelligence Strategy and Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {185-208},
year = {2016},
isbn = {978-0-12-809198-2},
doi = {https://doi.org/10.1016/B978-0-12-809198-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128091982000087},
author = {Steve Williams},
keywords = {BI strategy, BI technical strategy, data architecture, BI infrastructure, BI and IT shared services},
abstract = {While BI investments need to be business-driven in order to create value, there are important technical topics that come into play and affect BI program performance. These topics are more strategic than the choices of which particular technology products are most suitable. From a general management perspective, making effective technical choices demands solid business and technical thinking. On the business side, a key determination is how best to provide the technical infrastructure needed for BI execution. On the technical side, a key challenge for CIOs is resolving often-encountered differences in goals, methods, and incentives between mainstream IT best practices and BI best practices. CIOs also face the challenge of a rapidly-evolving BI technical landscape. From both perspectives, the choices are important because they affect investment levels, total cost of ownership of BI infrastructure, ability to execute, switching costs, pace of adoption, ability to differentiate, and the return on BI investment.}
}
@article{PERERA201691,
title = {Marine Engine Centered Localized Models for Sensor Fault Detection under Ship Performance Monitoring},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {91-96},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324405},
author = {Lokukaluge P. Perera},
keywords = {Sensor Fault Detection, Principal Component Analysis, Big Data, Statistical Data Analysis, Gaussian Mixture Models, Engine-Propeller combinator Diagram},
abstract = {Abstract:
Sensor fault detection under marine engine centered localized models of an engine propeller combinator diagram is presented in this study. The proposed approach consists of two detection levels to identify of sensor fault situations in an onboard data acquisition system of a vessel. Each parameter in ship performance and navigation data can have a realistic data range (i.e. a threshold relates to the variance), where the parameter can vary. If the sensor reads a value beyond this parameter range, then that data point is categorized as a sensor fault situation by the first fault detection level. However, some sensor faults are located within this data range and that cannot identify by this detection level. Such complex sensor fault situations are detected by the second fault detection level by considering the proposed localized models. These localized models are derived with respect to the operating regions of an engine-propeller combinator diagram, where the respective data points are clustered by Gaussian mixture models with an expectation maximization algorithm. Each data cluster is examined through principal component analysis and projected into the bottom principal component to identify such complex sensor fault situations. A data set of ship performance and navigation information of a selected vessel is used through these sensor fault detection levels and the successful results on identifying such sensor fault situations are also presented in this study.}
}
@article{ARIZALOPEZ2015657,
title = {Using International Standards to Control the Positional Quality of Spatial Data},
journal = {Photogrammetric Engineering & Remote Sensing},
volume = {81},
number = {8},
pages = {657-668},
year = {2015},
issn = {0099-1112},
doi = {https://doi.org/10.14358/PERS.81.8.657},
url = {https://www.sciencedirect.com/science/article/pii/S009911121530207X},
author = {F.J. Ariza-López and J. Rodríguez-Avi},
abstract = {Abstract
A positional quality control method based on the application of the International Standard ISO 2859 is proposed. This entails a common framework for dealing with the control of all other spatial data quality components (e.g., completeness, consistency, etc.). We propose a relationship between the parameters “acceptable quality level” and “limiting quality” of the international standard and positional quality by means of observed error models. This proposal does not require any assumption for positional errors (e.g., normality), which means that the application is universal. It can be applied to any type of positional and geometric controls (points, line-strings), to any dimension (1D, 2D, 3D, etc.) and with parametric or non-parametric error models (e.g., lidar). This paper introduces ISO 2859, presents the statistical bases of the proposal and develops two examples of application, the first dealing with a lot-by-lot control and the second, isolated lot control.}
}
@incollection{HOOPS201645,
title = {Chapter 4 - Immunoinformatics Cyberinfrastructure for Modeling and Analytics},
editor = {Josep Bassaganya-Riera},
booktitle = {Computational Immunology},
publisher = {Academic Press},
pages = {45-61},
year = {2016},
isbn = {978-0-12-803697-6},
doi = {https://doi.org/10.1016/B978-0-12-803697-6.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128036976000047},
author = {Stefan Hoops and Bruno W. Sobral and Pawel Michalak and Vida Abedi and Barbara Kronsteiner and Raquel Hontecillas and Monica Viladomiu and Josep Bassaganya-Riera},
keywords = {Informatics, cyberinfrastructure, LIMS, HPC, PATRIC, MIEP, IMMPORT, GEO, CINET},
abstract = {The twenty-first century immunology research requires the collaboration of a team of scientists with different backgrounds and expertise including immunologists, biochemists, molecular biologists, bioinformaticians, statisticians, and mathematicians to address important immunological questions. For this team to work seamlessly and efficiently we require an immunoinformatics cyberinfrastructure that supports the collection of experimental data (in vitro, ex vivo, preclinical, and clinical) and metadata, the aggregation, analysis and visualization of this data and presentation of analysis results, the model building, calibration and analysis process, and finally the maintenance of quality assurance. This chapter describes the cyberinfrastructure required to integrate computational modeling, big data analytics, portal science, and procedural knowledge to engineer synthetic systems-wide information processing representations of immune responses.}
}
@incollection{2013257,
title = {Index},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-261},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.09981-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767099810}
}
@incollection{JIANG2016435,
title = {Chapter 15 - Holistic atlases of functional networks and interactions (HAFNI)},
editor = {Guorong Wu and Dinggang Shen and Mert R. Sabuncu},
booktitle = {Machine Learning and Medical Imaging},
publisher = {Academic Press},
pages = {435-454},
year = {2016},
series = {The Elsevier and MICCAI Society Book Series},
isbn = {978-0-12-804076-8},
doi = {https://doi.org/10.1016/B978-0-12-804076-8.00015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040768000153},
author = {X. Jiang and D. Zhu and T. Liu},
keywords = {fMRI, Sparse learning, Functional networks, Alzheimer’s disease, Prenatal alcohol exposure},
abstract = {After decades of active research using in vivo functional magnetic resonance imaging (fMRI), there has been numerous evidence that human brain functions emerge from and are realized by the interaction of multiple concurrent neural networks. Due to the lack of effective computational brain mapping approaches and the limitation of functional neuroimaging data quality/quantity, it is still challenging to faithfully and robustly reconstruct concurrent functional networks from fMRI data and to quantitatively investigate their interactions at network level. Therefore it is largely unknown how multiple interacting functional networks spatially overlap and jointly perform brain functions. In response, by developing innovative sparse representation of whole-brain fMRI signals, we found that many robust and reproducible functional networks, including both task-evoked and intrinsic connectivity networks, are simultaneously activated and distributed in distant neuroanatomic regions with substantially spatial overlaps, thus forming an initial collection of holistic atlases of functional networks and interactions (HAFNI).}
}
@article{FELDMAN20151629,
title = {The New Data Frontier: Special issue of Research Policy},
journal = {Research Policy},
volume = {44},
number = {9},
pages = {1629-1632},
year = {2015},
note = {The New Data Frontier},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2015.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0048733315000359},
author = {Maryann Feldman and Martin Kenney and Francesco Lissoni},
keywords = {Digital data, Administrative records, Social science research}
}
@article{WAGNER201637,
title = {Analyzing human driving data an approach motivated by data science methods},
journal = {Chaos, Solitons & Fractals},
volume = {90},
pages = {37-45},
year = {2016},
note = {Challenges in Data Science},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2016.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S096007791630039X},
author = {Peter Wagner and Ronald Nippold and Sebastian Gabloner and Martin Margreiter},
keywords = {Car-following, Big data analysis, Maximal information content},
abstract = {By analyzing a large data-base of car-driving data in a generic way, a few elementary facts on car-following have been found out. The inferences stem from the application of the mutual information to detect correlations to the data. Arguably, the most interesting fact is that the acceleration of the following vehicle depends mostly on the speed-difference to the lead vehicle. This seems to be a causal relationship, since acceleration follows speed-difference with an average delay of 0.5 s. Furthermore, the car-following process organizes itself in such a manner that there is a strong relation between speed and distance to the vehicle in front. In most cases, this is the dominant relationship in car-following. Additionally, acceleration depends only weakly on distance, which may be surprising and is at odds to a number of simple models that state an exclusive dependency between acceleration and distance.}
}
@article{FRENKEL201518,
title = {A never-ending search for the truth: Thermodynamics in the uncertain era of the internet},
journal = {The Journal of Chemical Thermodynamics},
volume = {84},
pages = {18-40},
year = {2015},
issn = {0021-9614},
doi = {https://doi.org/10.1016/j.jct.2014.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0021961414003875},
author = {Michael Frenkel},
keywords = {Dynamic data evaluation, Global Information Systems, Thermophysical property data, Data-driven technologies, Experimental data quality},
abstract = {This article is based on the 20th Rossini Lecture delivered on 28 July 2014 at the opening of the 23rd International Conference on Chemical Thermodynamics in Durban, South Africa. In the last several decades, enormous progress in material and computer sciences has led, in many scientific disciplines, to fundamental improvements in experimental measurement technologies. That, in combination with new communication technologies and gradually increasing societal commitment to support public scientific research, has resulted in an unprecedented growth in the “production” of the reported experimental results. These trends together with dramatically growing demand for thermophysical and thermochemical property data related to new chemical processes and products necessitated development of dynamic methods of critical data evaluation within a framework of the concept of Global Information Systems in Science which was developed and implemented for the field of thermodynamics at the Thermodynamics Research Center (TRC) of the US National Institute of Standards and Technology (NIST). Principal advantages of this approach in comparison with the traditional static methods of critical data evaluation are illustrated. Major components of the developed system and its impact on such areas of human activities as information delivery, data publication, chemical process design, chemical product design, experiment planning, and education are discussed. The systems and software tools designed for global validation of experimental values are outlined. A variety of experimental data-driven technologies for thermophysical property prediction developed with the use of the elements of the Global Information System in Thermodynamics, including those based on surrogate mixture models, group contributions, QSPR, UNIFAC, and Monte Carlo molecular simulation, are described.}
}
@incollection{MCKNIGHT2014179,
title = {Chapter Seventeen - Organizational Change Management: The Soft Stuff is the Hard Stuff},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {179-188},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00017-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000175},
author = {William McKnight},
keywords = {organization change management, project management, leadership, stakeholder management, organizational training},
abstract = {The disparity between expecting change and managing it—the “change gap”—is growing at an unprecedented pace. This has put many information management (IM) shops into traction as they initiate the projects needed to stay competitive. At the same time, IM professionals must concern themselves about the organization’s acceptance of these efforts. To be successful, initiatives must transform the enterprise, but it takes more than wishful thinking to bridge the gap. IM projects are uniquely able to uncover the need for realigning mindsets. Consolidating information in a data warehouse or a master data management (MDM) hub, applying rules to improve data quality, or implementing workflow/sharing mechanisms all require organizational change. However, the complexities of engaging in behavioral and enterprise transformation are too often underestimated at great peril, because the “soft stuff” is truly hard.}
}
@incollection{WILLIAMS2016151,
title = {Chapter 7 - Meeting the Challenges of Enterprise BI},
editor = {Steve Williams},
booktitle = {Business Intelligence Strategy and Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {151-184},
year = {2016},
isbn = {978-0-12-809198-2},
doi = {https://doi.org/10.1016/B978-0-12-809198-2.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128091982000075},
author = {Steve Williams},
keywords = {BI strategy, enterprise BI, BI program management, BI risks, BI execution},
abstract = {Business intelligence is ultimately about improving business performance. While the concept of leveraging BI for improved business processes and enhanced business performance management is straightforward, the journey from concept to reality is complicated. There are predictable execution challenges to meet—some that cross organizational boundaries, some that business units must meet, and some that fall squarely in the lap of the information technology unit. From an enterprise perspective, achieving BI success is essentially a general management challenge, one that requires effective adaptation, synchronization, and cross-unit execution of six major workflows. More broadly, achieving BI success as an enterprise requires strong business leadership and an ability to balance BI activities with the many other business improvement activities that are always underway at most companies. Meeting the typical challenges of enterprise BI is the subject of this chapter.}
}
@article{ARNAIZGONZALEZ201669,
title = {Fusion of instance selection methods in regression tasks},
journal = {Information Fusion},
volume = {30},
pages = {69-79},
year = {2016},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1566253515001141},
author = {Álvar Arnaiz-González and Marcin Blachnik and Mirosław Kordos and César García-Osorio},
keywords = {Instance selection, Regression, Ensemble models},
abstract = {Data pre-processing is a very important aspect of data mining. In this paper we discuss instance selection used for prediction algorithms, which is one of the pre-processing approaches. The purpose of instance selection is to improve the data quality by data size reduction and noise elimination. Until recently, instance selection has been applied mainly to classification problems. Very few recent papers address instance selection for regression tasks. This paper proposes fusion of instance selection algorithms for regression tasks to improve the selection performance. As the members of the ensemble two different families of instance selection methods are evaluated: one based on distance threshold and the other one on converting the regression task into a multiple class classification task. Extensive experimental evaluation performed on the two regression versions of the Edited Nearest Neighbor (ENN) and Condensed Nearest Neighbor (CNN) methods showed that the best performance measured by the error value and data size reduction are in most cases obtained for the ensemble methods.}
}
@incollection{2013267,
title = {Appendix A - Customer Case Studies},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {267-287},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00024-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000246}
}
@article{KNAPEN2016156,
title = {Determining structural route components from GPS traces},
journal = {Transportation Research Part B: Methodological},
volume = {90},
pages = {156-171},
year = {2016},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2016.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0191261516302296},
author = {Luk Knapen and Irith Ben-Arroyo Hartman and Daniel Schulz and Tom Bellemans and Davy Janssens and Geert Wets},
keywords = {Travel behavior, Route choice, Route decomposition, Transportation modeling, GPS traces, Graph theory},
abstract = {Analysis of GPS traces shows that people often do not use the least cost path through the transportation network while making trips. This leads to the question which structural path characteristics can be used to construct realistic route choice sets for use in traffic simulation models. In this paper, we investigate the hypothesis that, for utilitarian trips, the route between origin and destination consists of a small number of concatenated least cost paths. The hypothesis is verified by analyzing routes extracted from large sets of recorded GPS traces which constitute revealed preference information. Trips have been extracted from the traces and for each trip the path in the transportation network is determined by map matching. This is followed by a path decomposition phase for which the algorithm constitutes the first contribution of this paper. There are multiple ways to split a given path in a directed graph into a minimal number of subpaths of minimal cost. By calculating two specific path splittings, it is possible to identify subsets of the vertices (splitVertexSuites) that can be used to generate every possible minimum path splitting by taking one vertex from each such subset. As a second contribution, we show how the extracted information is used in microscopic travel simulation. The distribution for the size of the minimum decomposition, extracted from the GPS traces, can be used in constrained enumeration methods for route choice set generation. The sets of vertices that can act as boundary vertices separating consecutive route parts contain way points (landmarks) having a particular meaning to their user. The paper explains the theoretical aspects of route splitting as well as the process to extract splitVertexSuites from big data. It reports statistical distributions extracted from sets of GPS traces for both multimodal person movements and unimodal car trips.}
}
@article{HORGAN2014226,
title = {European data-driven economy: A lighthouse initiative on Personalised Medicine},
journal = {Health Policy and Technology},
volume = {3},
number = {4},
pages = {226-233},
year = {2014},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2211883714000677},
author = {Denis Horgan and Mário Romão and Richard Torbett and Angela Brand}
}
@article{SIESLING20151039,
title = {Uses of cancer registries for public health and clinical research in Europe: Results of the European Network of Cancer Registries survey among 161 population-based cancer registries during 2010–2012},
journal = {European Journal of Cancer},
volume = {51},
number = {9},
pages = {1039-1049},
year = {2015},
note = {Cancer Registries in Europe for Cancer Surveillance and Research: EUROCOURSE Perspectives for Unity in Diversity},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2014.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0959804914008168},
author = {S. Siesling and W.J. Louwman and A. Kwast and C. {van den Hurk} and M. O’Callaghan and S. Rosso and R. Zanetti and H. Storm and H. Comber and E. Steliarova-Foucher and J.W. Coebergh},
keywords = {Cancer registry, Public health research, Clinical research, Outcomes research, Survey},
abstract = {Aim
To provide insight into cancer registration coverage, data access and use in Europe. This contributes to data and infrastructure harmonisation and will foster a more prominent role of cancer registries (CRs) within public health, clinical policy and cancer research, whether within or outside the European Research Area.
Methods
During 2010–12 an extensive survey of cancer registration practices and data use was conducted among 161 population-based CRs across Europe. Responding registries (66%) operated in 33 countries, including 23 with national coverage.
Results
Population-based oncological surveillance started during the 1940–50s in the northwest of Europe and from the 1970s to 1990s in other regions. The European Union (EU) protection regulations affected data access, especially in Germany and France, but less in the Netherlands or Belgium. Regular reports were produced by CRs on incidence rates (95%), survival (60%) and stage for selected tumours (80%). Evaluation of cancer control and quality of care remained modest except in a few dedicated CRs. Variables evaluated were support of clinical audits, monitoring adherence to clinical guidelines, improvement of cancer care and evaluation of mass cancer screening. Evaluation of diagnostic imaging tools was only occasional.
Conclusion
Most population-based CRs are well equipped for strengthening cancer surveillance across Europe. Data quality and intensity of use depend on the role the cancer registry plays in the politico, oncomedical and public health setting within the country. Standard registration methodology could therefore not be translated to equivalent advances in cancer prevention and mass screening, quality of care, translational research of prognosis and survivorship across Europe. Further European collaboration remains essential to ensure access to data and comparability of the results.}
}
@article{SRIVASTAVA2015470,
title = {Elliptic Curves for Data Provenance},
journal = {Procedia Computer Science},
volume = {45},
pages = {470-476},
year = {2015},
note = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.03.082},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500318X},
author = {Kriti Srivastava and Gaurav Nand},
keywords = {Data Provenance, Elliptic curve cryptography, Secure Provenance},
abstract = {Securing provenance data in distributed environment is a challenge and with the rapid increase in its size, volume and variety it becomes more challenging. Provenance data are more sensitive than actual data as they include the workflow or the chain kind of structure. A little change can be disastrous. There are various existing security algorithms and frameworks but distributed nature of infrastructure and large volume of data makes the implementation of existing security models very complex. This paper discusses the security challenges of provenance data and proposes a secure way to store provenance data in highly distributed infrastructure.}
}
@article{WHITMORE2014622,
title = {Using open government data to predict war: A case study of data and systems challenges},
journal = {Government Information Quarterly},
volume = {31},
number = {4},
pages = {622-630},
year = {2014},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2014.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X14001154},
author = {Andrew Whitmore},
keywords = {Open data, Case study, Research methods, Military conflict, Data quality, System design, Government portal},
abstract = {The ability to predict future military engagements would be a boon to combatants, contracting companies, investors, and other stakeholders. While governments may seek to conceal plans for impending conflict, they must spend large sums of money mobilizing and equipping soldiers in preparation for deployment. Thus, examining government spending patterns might yield insight into future military conflict. This article reports on an attempt to explore the possibility of using open U.S. Department of Defense (D.O.D.) contracting data to identify patterns of spending activity that can predict future military engagement. The research in this article followed a two-stage method. The first stage involved the exploration of the research question in the context of a specific case, the U.S. invasion of Iraq in 2003. The second stage assessed the open government contracting data used in the research and classified data and systems problems that were encountered according to an established analytical framework for open data barriers. The analysis demonstrated that the use of U.S. D.O.D. contracting data to predict future war has promise; however, a number of problems with the data and online portal prevented the derivation of conclusive, generalizable results. These problems were related to the open data barriers of task complexity and information quality. A detailed description of how these barriers manifested and directions for overcoming them are presented.}
}
@incollection{BARRY201371,
title = {Chapter 7 - Technical Forces Driving the Adoption of Cloud Computing},
editor = {Douglas K. Barry and David Dick},
booktitle = {Web Services, Service-Oriented Architectures, and Cloud Computing (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {71-79},
year = {2013},
series = {The Savvy Manager's Guides},
isbn = {978-0-12-398357-2},
doi = {https://doi.org/10.1016/B978-0-12-398357-2.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123983572000075},
author = {Douglas K. Barry and David Dick}
}
@incollection{LOSHIN2013xiii,
title = {Preface},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xiii-xx},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00016-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000168},
author = {David Loshin}
}
@article{MAKHOUL2015149,
title = {Residual energy-based adaptive data collection approach for periodic sensor networks},
journal = {Ad Hoc Networks},
volume = {35},
pages = {149-160},
year = {2015},
note = {Special Issue on Big Data Inspired Data Sensing, Processing and Networking Technologies},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1570870515001687},
author = {Abdallah Makhoul and Hassan Harb and David Laiymani},
keywords = {Periodic sensor networks (PSNs), Adaptive sampling models, Residual energy, Real data measurements},
abstract = {Due to its potential applications and the density of the deployed sensors, distributed wireless sensor networks are one of the highly anticipated key contributors of the big data in the future. Consequently, massive data collected by the sensors beside the limited battery power are the main limitations imposed by such networks. In this paper, we consider a periodic sensor networks (PSNs) where sensors transmit their data to the sink on a periodic basis. We propose an efficient adaptive model of data collection dedicated to PSN, in order to increase the network lifetime and to reduce the huge amount of the collected data. The main idea behind this approach is to allow each sensor node to adapt its sampling rate to the physical changing dynamics. In this way, the oversampling can be minimized and the power efficiency of the overall network system can be further improved. The proposed method is based on the dependence of measurements variance while taking into account the residual energy that varies over time. We study three well known statistical tests based on One-Way Anova model. Then, we propose a multiple levels activity model that uses behavior functions modeled by modified Bezier curves to define application classes and allow for sampling adaptive rate. Experiments on real sensors data show that our approach can be effectively used to minimize the amount of data retrieved by the network and conserve energy of the sensors, without loss of fidelity/accuracy.}
}
@article{BROWN2015246,
title = {Country-risk measurement and analysis: A new conceptualization and managerial tool},
journal = {International Business Review},
volume = {24},
number = {2},
pages = {246-265},
year = {2015},
issn = {0969-5931},
doi = {https://doi.org/10.1016/j.ibusrev.2014.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0969593114001176},
author = {Christopher L. Brown and S. Tamer Cavusgil and A. Wayne Lord},
keywords = {Big data, Country risk, Critical thinking, Education, Index, Organizational context, State development, Strategy},
abstract = {Country risk analysis has been a topic of investigation for decades, often focused on forecasting the risks to business profitability and assets when investing in a country. While there have been gradual improvements in the analytic techniques and overall breadth of the research, many scholars and practitioners continue to focus on limited conceptualizations of risk, measures with a relatively small number of variables, and/or expert analysis. Others point to the need to expand the inquiry, produce better tools and models, take advantage of the greater availability of data and enhanced computing techniques, and tackle puzzles differently. Advancing this discussion, we make the case for a new conceptualization and measurement of country-level risk and introduce the Robinson Country Risk Index (RCRI), a tool which incorporates four broad dimensions—Governance, Economics, Operations, and Society (GEOS). Within this holistic macrostructure, the RCRI encompasses 70 sub-dimensions, 126 countries, and, at present, 8 years of data. Its ecological conceptualization, multifaceted goals, and embedded functionalities complement and offer advantages over other risk indexes. The RCRI addresses concerns surrounding the conceptualization and measurement of country risk and provides a dynamic new instrument for educators, researchers, and practitioners.}
}
@incollection{RIDGE2015239,
title = {Chapter 19 - Closing Remarks},
editor = {Enda Ridge},
booktitle = {Guerrilla Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {239-242},
year = {2015},
isbn = {978-0-12-800218-6},
doi = {https://doi.org/10.1016/B978-0-12-800218-6.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002186000199},
author = {Enda Ridge}
}
@article{SULLIVAN201431,
title = {The eBird enterprise: An integrated approach to development and application of citizen science},
journal = {Biological Conservation},
volume = {169},
pages = {31-40},
year = {2014},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2013.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0006320713003820},
author = {Brian L. Sullivan and Jocelyn L. Aycrigg and Jessie H. Barry and Rick E. Bonney and Nicholas Bruns and Caren B. Cooper and Theo Damoulas and André A. Dhondt and Tom Dietterich and Andrew Farnsworth and Daniel Fink and John W. Fitzpatrick and Thomas Fredericks and Jeff Gerbracht and Carla Gomes and Wesley M. Hochachka and Marshall J. Iliff and Carl Lagoze and Frank A. {La Sorte} and Matthew Merrifield and Will Morris and Tina B. Phillips and Mark Reynolds and Amanda D. Rodewald and Kenneth V. Rosenberg and Nancy M. Trautmann and Andrea Wiggins and David W. Winkler and Weng-Keen Wong and Christopher L. Wood and Jun Yu and Steve Kelling},
keywords = {eBird, Citizen-science},
abstract = {Citizen-science projects engage volunteers to gather or process data to address scientific questions. But citizen-science projects vary in their ability to contribute usefully for science, conservation, or public policy. eBird has evolved from a basic citizen-science project into a collective enterprise, taking a novel approach to citizen science by developing cooperative partnerships among experts in a wide range of fields: population and distributions, conservation biologists, quantitative ecologists, statisticians, computer scientists, GIS and informatics specialists, application developers, and data administrators. The goal is to increase data quantity through participant recruitment and engagement, but also to quantify and control for data quality issues such as observer variability, imperfect detection of species, and both spatial and temporal bias in data collection. Advances at the interface among ecology, statistics, and computer science allow us to create new species distribution models that provide accurate estimates across broad spatial and temporal scales with extremely detailed resolution. eBird data are openly available and used by a broad spectrum of students, teachers, scientists, NGOs, government agencies, land managers, and policy makers. Feedback from this broad data use community helps identify development priorities. As a result, eBird has become a major source of biodiversity data, increasing our knowledge of the dynamics of species distributions, and having a direct impact on the conservation of birds and their habitats.}
}
@incollection{KRISHNAN2013179,
title = {Chapter 9 - New Technologies Applied to Data Warehousing},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {179-196},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405891000009X},
author = {Krish Krishnan},
keywords = {data warehouse appliance (DWA), Big Data appliance, Cloud computing, scalability, data virtualization, in-memory computing},
abstract = {The goal of this chapter is to provide you with the use of new technologies to build the data warehouse, including the appliances, cloud computing, and in-memory architectures. We will examine each architecture in detail in this chapter.}
}
@article{SLINGSBY2013848,
title = {Visual analysis of social networks in space and time using smartphone logs},
journal = {Pervasive and Mobile Computing},
volume = {9},
number = {6},
pages = {848-864},
year = {2013},
note = {Mobile Data Challenge},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2013.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574119213000849},
author = {Aidan Slingsby and Roger Beecham and Jo Wood},
keywords = {Big data, Human behaviour, Spatiotemporal, Social networks, Visual analysis},
abstract = {We designed and applied interactive visualisation techniques for investigating how social networks are embedded in time and space, using data collected from smartphone logs. Our interest in spatial aspects of social networks is that they may reveal associations between participants missed by simply making contact through smartphone devices. Four linked and co-ordinated views of spatial, temporal, individual and social network aspects of the data, along with demographic and attitudinal variables, helped add context to the behaviours we observed. Using these techniques, we were able to characterise spatial and temporal aspects of participants’ social networks and suggest explanations for some of them. This provides some validation of our techniques. Unexpected deficiencies in the data that became apparent prompted us to evaluate the dataset in more detail. Contrary to what we expected, we found significant gaps in participant records, particularly in terms of location, a poorly connected sample of participants and asymmetries in reciprocal call logs. Although the data captured are of high quality, deficiencies such as these remain and are likely to have a significant impact on interpretations relating to spatial aspects of the social network. We argue that appropriately-designed interactive visualisation techniques–afforded by our flexible prototyping approach–are effective in identifying and characterising data inconsistencies. Such deficiencies are likely to exist in other similar datasets, and although the visual approaches we discuss for identifying data problems may not be scalable, the categories of problems we identify may be used to inform attempts to systematically account for errors in larger smartphone datasets.}
}
@article{BROUS2016303,
title = {Governing Asset Management Data Infrastructures},
journal = {Procedia Computer Science},
volume = {95},
pages = {303-310},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.339},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325121},
author = {Paul Brous and Paulien Herder and Marijn Janssen},
keywords = {complex adaptive systems, CAS, data infrastructure, asset management, data quality, data governance},
abstract = {Organizations are increasingly looking to trusted data to drive their decision making process. Trusted data has a clear, defined and consistent quality which meets the expectations of the user. Data infrastructures which produce trusted data and provide organizations with the capability to make the right decisions at the right time are socio-technical networks, consisting of technical infrastructures and actor networks, and as such they are often complex and adaptive. Critical issues, challenges, and dilemmas can be identified while looking at data infrastructures as a socio-technical systems. This paper explores conditions and factors for effective and sustainable development of data infrastructures in organizations and suggests that the inherent complexity of data infrastructures requires a multi-faceted way of data governance. Several predefined components of data infrastructures which contain the behavior of agents through various coordination mechanisms have been developed to model the effect of data governance on data infrastructures. These components can be further customized to model an empirical situation more closely. Finally, the paper suggests institutionalization of data governance within an organization as a unifying concept towards the effectiveness and sustainability of data infrastructures, recognizing their inherent complexities. The approach is illustrated with a case study in the asset management domain.}
}
@incollection{2013xv,
title = {Preface},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xv-xviii},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.09979-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767099792}
}
@incollection{WANG2015xi,
title = {Foreword},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xi-xii},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.06001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378060014},
author = {Richard Wang}
}
@article{JETZEK201689,
title = {Managing complexity across multiple dimensions of liquid open data: The case of the Danish Basic Data Program},
journal = {Government Information Quarterly},
volume = {33},
number = {1},
pages = {89-104},
year = {2016},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X15300186},
author = {Thorhildur Jetzek},
keywords = {Open data, Open data infrastructure, Liquid open data, System-of-systems governance, Value generating mechanisms},
abstract = {Current literature on open government data has uncovered a wide range of challenges related to these important initiatives. The problems encountered include: insufficient data quality and interoperability, problems regarding governance and motivation, lack of capabilities, and heterogeneous political and ideological agendas. A common open data infrastructure might resolve some of these problems, however, implementing such an infrastructure is a highly complex task. This longitudinal case study of the Danish Basic Data Program (BDP) is intended to improve our understanding of the challenges related to providing open access to government data through open data infrastructure. The BDP aims to improve the quality of selected government data, make them more coherent, and improve accessibility through the implementation of a common data distribution platform. The program is expected to increase government efficiency and stimulate innovation. This case study describes the evolution of the BDP and identifies the main structural elements of an open data infrastructure. Data analysis uncovered four tensions, which are identified as key challenges of an open data infrastructure implementation. These tensions are presented with four suggested governance strategies that were used in the BDP case. The main contribution of the paper is a process model where the main phases and mechanisms of an open data infrastructure implementation, use and impacts are identified and explained.}
}