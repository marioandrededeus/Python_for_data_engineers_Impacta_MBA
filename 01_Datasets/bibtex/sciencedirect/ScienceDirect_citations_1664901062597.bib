@incollection{LINSTEDT2016123,
title = {Chapter 5 - Intermediate Data Vault Modeling},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {123-150},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000052},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {data warehouse, data vault, satellites, link entity, data modeling},
abstract = {Due to the complexity of data warehouses and the underlying business requirements, more complex Data Vault entities are typically required and introduced throughout this chapter. They extend the basic entities discussed in the previous chapter. The various special types of satellites are covered, including overloaded satellites, multi-active satellites, status-tracking satellites, effectivity satellites, record tracking satellites, and computed satellites. Extended link entities are covered as well, including link-to-link relationships, same-as links, hierarchical links, nonhistorized links, nondescriptive links, computed aggregate links, and exploration links. For each link entity, the technical or business reason for adding them to the Data Vault will be explained.}
}
@incollection{TALBURT201517,
title = {Chapter 2 - Entity Identity Information and the CSRUD Life Cycle Model},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {17-29},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000028},
author = {John R. Talburt and Yinle Zhou},
keywords = {Entity Identity Information, Information Life Cycle, POSMAD, CRUD, CSRUD},
abstract = {Chapter 2 lays the foundation for the book’s theme – recognizing and understanding the role of life cycle management in the context of entity information supporting master data management. The chapter defines a life cycle model called CSRUD as an extension and adaptation of existing models for general information life cycle management to the specific context of entity identity information.}
}
@article{GARCIA2016488,
title = {DSS from an RE Perspective: A systematic mapping},
journal = {Journal of Systems and Software},
volume = {117},
pages = {488-507},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.03.046},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300061},
author = {Stephany García and Oscar Romero and Ruth Raventós},
keywords = {Decision support systems, Requirements engineering, Business intelligence},
abstract = {Decision support systems (DSS) provide a unified analytical view of business data to better support decision-making processes. Such systems have shown a high level of user satisfaction and return on investment. However, several surveys stress the high failure rate of DSS projects. This problem results from setting the wrong requirements by approaching DSS in the same way as operational systems, whereas a specific approach is needed. Although this is well-known, there is still a surprising gap on how to address requirements engineering (RE) for DSS. To overcome this problem, we conducted a systematic mapping study to identify and classify the literature on DSS from an RE perspective. Twenty-seven primary studies that addressed the main stages of RE were selected, mapped, and classified into 39 models, 27 techniques, and 54 items of guidance. We have also identified a gap in the literature on how to design the DSS main constructs (typically, the data warehouse and data flows) in a methodological manner from the business needs. We believe this study will help practitioners better address the RE stages of DSS projects.}
}
@article{SHROUF2015235,
title = {Energy management based on Internet of Things: practices and framework for adoption in production management},
journal = {Journal of Cleaner Production},
volume = {100},
pages = {235-246},
year = {2015},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615002760},
author = {Fadi Shrouf and Giovanni Miragliotta},
keywords = {Internet of Things, Energy consumption awareness, Energy-efficient production management practices, Energy management practices, Framework},
abstract = {In today's manufacturing scenario, rising energy prices, increasing ecological awareness, and changing consumer behaviors are driving decision-makers to prioritize green manufacturing. The Internet of Things paradigm promises to increase the visibility and awareness of energy consumption, thanks to smart sensors and smart meters at the machine and production line level. Consequently, real-time energy consumption data from manufacturing processes can be collected easily, and then analyzed, to improve energy-aware decision-making. Relying on a comprehensive literature review and on experts' insight, this paper contributes to the understanding of energy-efficient production management practices that are enhanced and enabled by the Internet of Things technology. In addition, it discusses the benefits that can be obtained thanks to adopting such management practices. Eventually, a framework is presented to support the integration of gathered energy data into a company's information technology tools and platforms. This is done with the ultimate goal of highlighting how operational and tactical decision-making processes could leverage on such data in order to improve energy efficiency, and therefore competitiveness, of manufacturing companies. With the outcomes of this paper, energy managers can approach the Internet of Things adoption in a benefit-driven manner, addressing those energy management practices that are more aligned with company maturity, measurable data and available information systems and tools.}
}
@incollection{KOLTAY201661,
title = {Chapter 2 - The Nature of Information Literacy},
editor = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics},
booktitle = {Research 2.0 and the Future of Information Literacy},
publisher = {Chandos Publishing},
pages = {61-110},
year = {2016},
isbn = {978-0-08-100075-5},
doi = {https://doi.org/10.1016/B978-0-08-100075-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100075500002X},
author = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics},
keywords = {New literacies, Culture, The turns of library and information science, Evidence-based practice, Scientific literacy, Academic literacy, Media literacy, Data literacy, Metaliteracy, Transliteracy, Digital literacy, Information overload, Personal information management},
abstract = {As the title suggests, this chapter describes the nature of information literacy. First, we examine some of its definitions, relevant declarations, and frameworks, and then put it into the context of new literacies. A number of other contexts follow this: reading and writing, the relationship to culture, and the turns of library and information science. We also give a picture of related literacies, such as digital literacy, media literacy, academic literacy, and a number of others. Lastly, we pay attention to such “borderline” fields of information literacy as information overload and personal information management.}
}
@article{PASSALACQUA2015174,
title = {Analyzing high resolution topography for advancing the understanding of mass and energy transfer through landscapes: A review},
journal = {Earth-Science Reviews},
volume = {148},
pages = {174-193},
year = {2015},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0012825215300015},
author = {Paola Passalacqua and Patrick Belmont and Dennis M. Staley and Jeffrey D. Simley and J Ramon Arrowsmith and Collin A. Bode and Christopher Crosby and Stephen B. DeLong and Nancy F. Glenn and Sara A. Kelly and Dimitri Lague and Harish Sangireddy and Keelin Schaffrath and David G. Tarboton and Thad Wasklewicz and Joseph M. Wheaton},
keywords = {Lidar, Geomorphic features, Change detection, Point cloud, InSAR, IfSAR, Bathymetry, Structure from Motion, Filtering},
abstract = {The study of mass and energy transfer across landscapes has recently evolved to comprehensive considerations acknowledging the role of biota and humans as geomorphic agents, as well as the importance of small-scale landscape features. A contributing and supporting factor to this evolution is the emergence over the last two decades of technologies able to acquire high resolution topography (HRT) (meter and sub-meter resolution) data. Landscape features can now be captured at an appropriately fine spatial resolution at which surface processes operate; this has revolutionized the way we study Earth-surface processes. The wealth of information contained in HRT also presents considerable challenges. For example, selection of the most appropriate type of HRT data for a given application is not trivial. No definitive approach exists for identifying and filtering erroneous or unwanted data, yet inappropriate filtering can create artifacts or eliminate/distort critical features. Estimates of errors and uncertainty are often poorly defined and typically fail to represent the spatial heterogeneity of the dataset, which may introduce bias or error for many analyses. For ease of use, gridded products are typically preferred rather than the more information-rich point cloud representations. Thus many users take advantage of only a fraction of the available data, which has furthermore been subjected to a series of operations often not known or investigated by the user. Lastly, standard HRT analysis work-flows are yet to be established for many popular HRT operations, which has contributed to the limited use of point cloud data. In this review, we identify key research questions relevant to the Earth-surface processes community within the theme of mass and energy transfer across landscapes and offer guidance on how to identify the most appropriate topographic data type for the analysis of interest. We describe the operations commonly performed from raw data to raster products and we identify key considerations and suggest appropriate work-flows for each, pointing to useful resources and available tools. Future research directions should stimulate further development of tools that take advantage of the wealth of information contained in the HRT data and address the present and upcoming research needs such as the ability to filter out unwanted data, compute spatially variable estimates of uncertainty and perform multi-scale analyses. While we focus primarily on HRT applications for mass and energy transfer, we envision this review to be relevant beyond the Earth-surface processes community for a much broader range of applications involving the analysis of HRT.}
}
@incollection{TEKINERDOGAN20161,
title = {Chapter 1 - Quality concerns in large-scale and complex software-intensive systems},
editor = {Ivan Mistrik and Richard Soley and Nour Ali and John Grundy and Bedir Tekinerdogan},
booktitle = {Software Quality Assurance},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-17},
year = {2016},
isbn = {978-0-12-802301-3},
doi = {https://doi.org/10.1016/B978-0-12-802301-3.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023013000016},
author = {Bedir Tekinerdogan and Nour Ali and John Grundy and Ivan Mistrik and Richard Soley},
keywords = {Software quality management, software quality assurance, software quality metrics},
abstract = {Software quality management (SQM) is the collection of all processes that ensure that software products, services, and life cycle process implementations meet organizational software quality objectives and achieve stakeholder satisfaction. SQM comprises three basic subcategories: software quality planning, software quality assurance (SQA), and software quality control and software process improvement. This chapter provides a general overview of the SQA domain and discuss the related concept. A conceptual model for software quality framework is provided together with the current approaches for SQA. The chapter concludes with some of the identified challenges and future challenges regarding SQA.}
}
@incollection{HOLMES2001137,
title = {7 - Data mining},
editor = {L M M Tijskens and M L A T M Hertog and B M Nicolaï},
booktitle = {Food Process Modelling},
publisher = {Woodhead Publishing},
pages = {137-155},
year = {2001},
series = {Woodhead Publishing Series in Food Science, Technology and Nutrition},
isbn = {978-1-85573-565-1},
doi = {https://doi.org/10.1533/9781855736375.2.137},
url = {https://www.sciencedirect.com/science/article/pii/B9781855735651500146},
author = {G. Holmes and T.C. Smith}
}
@article{GAO2015749,
title = {Cloud-enabled prognosis for manufacturing},
journal = {CIRP Annals},
volume = {64},
number = {2},
pages = {749-772},
year = {2015},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2015.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S000785061500150X},
author = {R. Gao and L. Wang and R. Teti and D. Dornfeld and S. Kumara and M. Mori and M. Helu},
keywords = {Predictive model, Condition monitoring, Cloud manufacturing},
abstract = {Advanced manufacturing depends on the timely acquisition, distribution, and utilization of information from machines and processes across spatial boundaries. These activities can improve accuracy and reliability in predicting resource needs and allocation, maintenance scheduling, and remaining service life of equipment. As an emerging infrastructure, cloud computing provides new opportunities to achieve the goals of advanced manufacturing. This paper reviews the historical development of prognosis theories and techniques and projects their future growth enabled by the emerging cloud infrastructure. Techniques for cloud computing are highlighted, as well as the influence of these techniques on the paradigm of cloud-enabled prognosis for manufacturing. Finally, this paper discusses the envisioned architecture and associated challenges of cloud-enabled prognosis for manufacturing.}
}
@incollection{TRENBERTH1991571,
title = {Working Group 1: Observations},
editor = {M.E. Schlesinger},
series = {Developments in Atmospheric Science},
publisher = {Elsevier},
volume = {19},
pages = {571-582},
year = {1991},
booktitle = {Greenhouse-Gas-Induced Climatic Change: A Critical Appraisal of Simulations and Observations},
issn = {0167-5117},
doi = {https://doi.org/10.1016/B978-0-444-88351-3.50043-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044488351350043X},
author = {K. Trenberth and J. Angell and R. Barry and R. Bradley and H. Diaz and W. Elliott and R. Etkins and C. Folland and R. Jenne and P. Jones and T. Karl and S. Levitus and A. Oort and D. Parker and C. Ropelewski and K. Vinnikov and T. Wigley}
}
@article{OH20154,
title = {Short-term Travel-time Prediction on Highway: A Review of the Data-driven Approach},
journal = {Transport Reviews},
volume = {35},
number = {1},
pages = {4-32},
year = {2015},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2014.992496},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722002896},
author = {Simon Oh and Young-Ji Byon and Kitae Jang and Hwasoo Yeo},
keywords = {highway travel-time prediction, traffic forecasting, data-driven approach, statistical modelling, artificial Intelligence, pattern searching},
abstract = {Near future travel-time information is one of the most critical factors that travellers consider before making trip decisions. In efforts to provide more reliable future travel-time estimations, transportation engineers have examined various techniques developed in the last three decades. However, there have not been sufficiently systematic and through reviews so far. In order to effectively support various transportation strategies and applications including Intelligent Transportation Systems (ITS), it is necessary to apply appropriate forecasting methods for matching circumstances in a timely manner. This paper conducts a comprehensive review study focusing on literatures, including modern techniques proposed recently, related to travel time and traffic condition predictions that are based on ‘data-driven' approaches. Based on the underlying mechanisms and theoretical principles, different approaches are categorized as parametric (linear regression and time series) and non-parametric approaches (artificial intelligence and pattern searching). Then, the approaches are analysed for their strengths, potential weaknesses, and performances from five main perspectives that are prediction range, accuracy, efficiency, applicability, and robustness.}
}
@article{KNORCHEN201513,
title = {Implementation of a near-real time cross-border web-mapping platform on airborne particulate matter (PM) concentration with open-source software},
journal = {Computers & Geosciences},
volume = {74},
pages = {13-26},
year = {2015},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2014.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S009830041400226X},
author = {Achim Knörchen and Gunnar Ketzler and Christoph Schneider},
keywords = {Geographical information system, Open-source software, Web GIS, Airborne particulate matter, Near-real time, Open data},
abstract = {Although Europe has been growing together for the past decades, cross-border information platforms on environmental issues are still scarce. With regard to the establishment of a web-mapping tool on airborne particulate matter (PM) concentration for the Euregio Meuse-Rhine located in the border region of Belgium, Germany and the Netherlands, this article describes the research on methodical and technical backgrounds implementing such a platform. An open-source solution was selected for presenting the data in a Web GIS (OpenLayers/GeoExt; both JavaScript-based), applying other free tools for data handling (Python), data management (PostgreSQL), geo-statistical modelling (Octave), geoprocessing (GRASS GIS/GDAL) and web mapping (MapServer). The multilingual, made-to-order online platform provides access to near-real time data on PM concentration as well as additional background information. In an open data section, commented configuration files for the Web GIS client are being made available for download. Furthermore, all geodata generated by the project is being published under public domain and can be retrieved in various formats or integrated into Desktop GIS as Web Map Services (WMS).}
}
@article{HUANG20161,
title = {Understanding human activity patterns based on space-time-semantics},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {121},
pages = {1-10},
year = {2016},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0924271616303203},
author = {Wei Huang and Songnian Li},
keywords = {Human mobility, Human activity, Spatiotemporal pattern, Semantic pattern, Topic model, Social media, Twitter},
abstract = {Understanding human activity patterns plays a key role in various applications in an urban environment, such as transportation planning and traffic forecasting, urban planning, public health and safety, and emergency response. Most existing studies in modeling human activity patterns mainly focus on spatiotemporal dimensions, which lacks consideration of underlying semantic context. In fact, what people do and discuss at some places, inferring what is happening at the places, cannot be simple neglected because it is the root of human mobility patterns. We believe that the geo-tagged semantic context, representing what individuals do and discuss at a place and a specific time, drives a formation of specific human activity pattern. In this paper, we aim to model human activity patterns not only based on space and time but also with consideration of associated semantics, and attempt to prove a hypothesis that similar mobility patterns may have different motivations. We develop a spatiotemporal-semantic model to quantitatively express human activity patterns based on topic models, leading to an analysis of space, time and semantics. A case study is conducted using Twitter data in Toronto based on our model. Through computing the similarities between users in terms of spatiotemporal pattern, semantic pattern and spatiotemporal-semantic pattern, we find that only a small number of users (2.72%) have very similar activity patterns, while the majority (87.14%) show different activity patterns (i.e., similar spatiotemporal patterns and different semantic patterns, similar semantic patterns and different spatiotemporal patterns, or different in both). The population of users that has very similar activity patterns is decreased by 56.41% after incorporating semantic information in the corresponding spatiotemporal patterns, which can quantitatively prove the hypothesis.}
}
@article{HUME2016352,
title = {Current applications and future directions for the CDISC Operational Data Model standard: A methodological review},
journal = {Journal of Biomedical Informatics},
volume = {60},
pages = {352-362},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416000381},
author = {Sam Hume and Jozef Aerts and Surendra Sarnikar and Vojtech Huser},
keywords = {ODM, Define-XML, CDISC, Interoperability, Clinical trial, EHR},
abstract = {Introduction
In order to further advance research and development on the Clinical Data Interchange Standards Consortium (CDISC) Operational Data Model (ODM) standard, the existing research must be well understood. This paper presents a methodological review of the ODM literature. Specifically, it develops a classification schema to categorize the ODM literature according to how the standard has been applied within the clinical research data lifecycle. This paper suggests areas for future research and development that address ODM’s limitations and capitalize on its strengths to support new trends in clinical research informatics.
Methods
A systematic scan of the following databases was performed: (1) ABI/Inform, (2) ACM Digital, (3) AIS eLibrary, (4) Europe Central PubMed, (5) Google Scholar, (5) IEEE Xplore, (7) PubMed, and (8) ScienceDirect. A Web of Science citation analysis was also performed. The search term used on all databases was “CDISC ODM.” The two primary inclusion criteria were: (1) the research must examine the use of ODM as an information system solution component, or (2) the research must critically evaluate ODM against a stated solution usage scenario. Out of 2686 articles identified, 266 were included in a title level review, resulting in 183 articles. An abstract review followed, resulting in 121 remaining articles; and after a full text scan 69 articles met the inclusion criteria.
Results
As the demand for interoperability has increased, ODM has shown remarkable flexibility and has been extended to cover a broad range of data and metadata requirements that reach well beyond ODM’s original use cases. This flexibility has yielded research literature that covers a diverse array of topic areas. A classification schema reflecting the use of ODM within the clinical research data lifecycle was created to provide a categorized and consolidated view of the ODM literature. The elements of the framework include: (1) EDC (Electronic Data Capture) and EHR (Electronic Health Record) infrastructure; (2) planning; (3) data collection; (4) data tabulations and analysis; and (5) study archival. The analysis reviews the strengths and limitations of ODM as a solution component within each section of the classification schema. This paper also identifies opportunities for future ODM research and development, including improved mechanisms for semantic alignment with external terminologies, better representation of the CDISC standards used end-to-end across the clinical research data lifecycle, improved support for real-time data exchange, the use of EHRs for research, and the inclusion of a complete study design.
Conclusions
ODM is being used in ways not originally anticipated, and covers a diverse array of use cases across the clinical research data lifecycle. ODM has been used as much as a study metadata standard as it has for data exchange. A significant portion of the literature addresses integrating EHR and clinical research data. The simplicity and readability of ODM has likely contributed to its success and broad implementation as a data and metadata standard. Keeping the core ODM model focused on the most fundamental use cases, while using extensions to handle edge cases, has kept the standard easy for developers to learn and use.}
}
@article{FORTINO201462,
title = {BodyCloud: A SaaS approach for community Body Sensor Networks},
journal = {Future Generation Computer Systems},
volume = {35},
pages = {62-79},
year = {2014},
note = {Special Section: Integration of Cloud Computing and Body Sensor Networks; Guest Editors: Giancarlo Fortino and Mukaddim Pathan},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X13002793},
author = {Giancarlo Fortino and Daniele Parisi and Vincenzo Pirrone and Giuseppe {Di Fatta}},
keywords = {Body Sensor Networks, Cloud computing, Software engineering, SaaS, Sensor data as a service, Analytics as a service},
abstract = {Body Sensor Networks (BSNs) have been recently introduced for the remote monitoring of human activities in a broad range of application domains, such as health care, emergency management, fitness and behavior surveillance. BSNs can be deployed in a community of people and can generate large amounts of contextual data that require a scalable approach for storage, processing and analysis. Cloud computing can provide a flexible storage and processing infrastructure to perform both online and offline analysis of data streams generated in BSNs. This paper proposes BodyCloud, a SaaS approach for community BSNs that supports the development and deployment of Cloud-assisted BSN applications. BodyCloud is a multi-tier application-level architecture that integrates a Cloud computing platform and BSN data streams middleware. BodyCloud provides programming abstractions that allow the rapid development of community BSN applications. This work describes the general architecture of the proposed approach and presents a case study for the real-time monitoring and analysis of cardiac data streams of many individuals.}
}
@incollection{LASZEWSKI2012261,
title = {Chapter 11 - Sybase Migrations from a Systems Integrator Perspective, and Case Study},
editor = {Tom Laszewski and Prakash Nauduri},
booktitle = {Migrating to the Cloud},
publisher = {Syngress},
address = {Boston},
pages = {261-281},
year = {2012},
isbn = {978-1-59749-647-6},
doi = {https://doi.org/10.1016/B978-1-59749-647-6.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781597496476000119},
author = {Tom Laszewski and Prakash Nauduri},
abstract = {Publisher Summary
mLogica is a specialized systems integrator with primary expertise in database performance management. mLogica consults on Sybase database performance challenges and database migration issues, and provides database products and solutions for customers globally, from premium New York City financial services firms, to leading California media and entertainment companies, to stock exchanges and banks from Bangladesh and the Maldives Islands. This chapter deals primarily with mLogica's experience in working with clients who have asked mLogica to help them evaluate and/or support them with their database migration initiatives from Sybase Adaptive Service Enterprise (ASE) to Oracle. mLogica provides assessments on the strategic pros and cons of Sybase-to-Oracle migrations from business and technology perspectives. mLogica analyzes existing trends in the database marketplace, what business users are telling us, what database trainers are reporting in regard to database cross-training, what analysts are saying, and the investments that both Sybase and Oracle have made in terms of database innovation. Based on this information, mLogica prepares a business case and justification for migration from Sybase to Oracle for its customers. This chapter offers what mLogica considers to be best practices and lessons learned when migrating a Sybase database to Oracle, from the perspective of an Oracle systems integrator as well as a systems integrator that has been managing and improving the performance of Sybase installations worldwide. The chapter concludes with a case study that provides details regarding the experiences of a global telecommunications company that migrated from a Sybase and PowerBuilder environment to an Oracle and .NET environment.}
}
@incollection{KOLTAY2016131,
title = {Chapter 4 - Conclusion: Shaping Forces, Future Challenges},
editor = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics},
booktitle = {Research 2.0 and the Future of Information Literacy},
publisher = {Chandos Publishing},
pages = {131-154},
year = {2016},
isbn = {978-0-08-100075-5},
doi = {https://doi.org/10.1016/B978-0-08-100075-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081000755000043},
author = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics}
}
@incollection{PAL2016445,
title = {Chapter 14 - Extending cloud-based applications with mobile opportunistic networks: Security issues and privacy challenges},
editor = {Ciprian Dobre and Fatos Xhafa},
booktitle = {Pervasive Computing},
publisher = {Academic Press},
address = {Boston},
pages = {445-481},
year = {2016},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803663-1},
doi = {https://doi.org/10.1016/B978-0-12-803663-1.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128036631000140},
author = {S. Pal and W. Moreira},
keywords = {Mobile devices, Cloud-based solutions, Challenged environments, Social collaborations, Mobile opportunistic networks},
abstract = {Mobile devices (eg, smart phones or PDAs) are becoming increasingly popular. One important feature of such devices is the ability of users to take advantage of cloud-based solutions at their convenience. However, accessing cloud-based solutions in challenged environments is not so trivial, since such solutions rely on the availability of connectivity. For instance, in sparse or rural areas that lack the proper infrastructure, and in high-density, urban areas, with access networks that are restricted or suffer from interference, connectivity cannot be assumed. In such situations, the use of mobile opportunistic networks may enable users to send/retrieve information. Communication in these types of networks depends upon the local user’s mobile network connections and social collaborations. In this chapter, we answer the following research question: What are the security issues and privacy challenges that make such mobile opportunistic/cloud-based networks vulnerable? We begin by addressing the networking and functional challenges related to these networks. Next, we describe a set of constraints intrinsic to mobile opportunistic/cloud-based networks and explore these constraints to understand the potential threat model based on storage issues and resource management challenges. Finally, we identify the challenges to effectively support secure and flexible data transmission in a privacy-preserving manner in such networks, while considering the mobility of the users, confidentiality of the user’s sensitive information, as well as the limitations of battery and storage of the mobile devices.}
}
@article{DESSI2016366,
title = {A machine-learning approach to ranking RDF properties},
journal = {Future Generation Computer Systems},
volume = {54},
pages = {366-377},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15001764},
author = {Andrea Dessi and Maurizio Atzori},
keywords = {Semantic web, Machine learning, Fast property ranking, User experience},
abstract = {In this paper we address the problem of providing an order of relevance, or ranking, among entities’ properties used in RDF datasets, Linked Data and SPARQL endpoints. We first motivate the importance of ranking RDF properties by providing two killer applications for the problem, namely property tagging and entity visualization. Moved by the desiderata of these applications, we propose to apply Machine Learning to Rank (MLR) techniques to the problem of ranking RDF properties. Our devised solution is based on a deep empirical study of all the dimensions involved: feature selection, MLR algorithm and Model training. The major advantages of our approach are the following: (a) flexibility/personalization, as the properties’ relevance can be user-specified by personalizing the training set in a supervised approach, or set by a novel automatic classification approach based on SWiPE; (b) speed, since it can be applied without computing frequencies over the whole dataset, leveraging existing fast MLR algorithms; (c) effectiveness, as it can be applied even when no ontology data is available by using novel dataset-independent features; (d) precision, which is high both in terms of f-measure and Spearman’s rho. Experimental results show that the proposed MLR framework outperform the two existing approaches found in literature which are related to RDF property ranking.}
}
@article{GEUM2016176,
title = {How to generate creative ideas for innovation: a hybrid approach of WordNet and morphological analysis},
journal = {Technological Forecasting and Social Change},
volume = {111},
pages = {176-187},
year = {2016},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516301354},
author = {Youngjung Geum and Yongtae Park},
keywords = {innovation, idea generation, creativity, morphology, WordNet},
abstract = {A creative ideation process occupies a substantial part of the innovation process. Among many techniques for ideation, morphology analysis has been employed as a prevalent method, whose success is critically affected by its dimensions and values. Despite the gravity of determining dimensions and values, previous literature has been simply subject to manual construction by some experts, which leads to significant subjectivity and bias in morphology building. For this reason, an analytic and objective way of morphology building is highly required. In response, this paper suggests a new way of morphology building to enhance creative ideation using WordNet. WordNet is a large lexical database of English, which provides a hierarchical network dictionary of words. WordNet's hierarchical relationship characteristic fits morphology analysis as its nature comes from a hierarchical structure of dimensions and values. The use of WordNet can be an excellent remedy for morphology building by employing two types of relationships: meronym/holonym for dimension construction and hyponym/ hypernym for value construction. Since dimension construction extends the contents of horizontal axis of morphology, it is called horizontal extension. Similarly, value construction extends the contents of vertical axis of morphology, thus it is referred to as vertical extension of morphology.}
}
@incollection{BAYON201673,
title = {Chapter 4 - Bioinformatics Tools in Epigenomics Studies},
editor = {Mario F. Fraga and Agustín F. Fernández},
booktitle = {Epigenomics in Health and Disease},
publisher = {Academic Press},
address = {Boston},
pages = {73-107},
year = {2016},
isbn = {978-0-12-800140-0},
doi = {https://doi.org/10.1016/B978-0-12-800140-0.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128001400000042},
author = {Gustavo F. Bayón and Agustín F. Fernández and Mario F. Fraga},
keywords = {Epigenomics, DNA methylation, histone, microarray, NGS, bioinformatics},
abstract = {One of the most common problems in Epigenomics studies comes from the average size of the data set generated by modern sequencing and microarray platforms. Traditional analyses of data become unfeasible when dealing with huge sets of reads or big matrices, thus forcing the analyst not only to keep the biologic question in mind but also to master several, unrelated software tools. This book chapter aims to provide a gentle introduction to the vast universe of available software tools for the analysis of Epigenomics data. A general introduction to the most common epigenetic alterations is followed by a classification of available analysis tools. Although there are tools that are not mentioned in this chapter, those presented here will be a good starting point for anyone interested in Epigenomics Data Analysis.}
}
@article{SHIN2015119,
title = {On condition based maintenance policy},
journal = {Journal of Computational Design and Engineering},
volume = {2},
number = {2},
pages = {119-127},
year = {2015},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2288430014000141},
author = {Jong-Ho Shin and Hong-Bae Jun},
keywords = {Condition-based maintenance, Predictive maintenance, Prognostic and health management},
abstract = {In the case of a high-valuable asset, the Operation and Maintenance (O&M) phase requires heavy charges and more efforts than the installation (construction) phase, because it has long usage life and any accident of an asset during this period causes catastrophic damage to an industry. Recently, with the advent of emerging Information Communication Technologies (ICTs), we can get the visibility of asset status information during its usage period. It gives us new challenging issues for improving the efficiency of asset operations. One issue is to implement the Condition-Based Maintenance (CBM) approach that makes a diagnosis of the asset status based on wire or wireless monitored data, predicts the assets abnormality, and executes suitable maintenance actions such as repair and replacement before serious problems happen. In this study, we have addressed several aspects of CBM approach: definition, related international standards, procedure, and techniques with the introduction of some relevant case studies that we have carried out.}
}
@incollection{SHEIKH201321,
title = {Chapter 2 - Information Continuum},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {21-40},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000025},
author = {Nauman Sheikh},
keywords = {search and lookup, counts, summaries, reporting, data warehousing, ETL, data integration, GIS, dashboards, metrics, analytics models, decision strategies, model governance, geo-spacing},
abstract = {Now that we have been able to put some boundaries around defining analytics—a necessary step to provide a step-by-step detailed guideline on conceiving and building analytics projects—we will put some context around the maturity needed to introduce an analytics-driven culture. This context primarily deals with how information is consumed and how that consumption evolves toward a higher value of information leading to automated decisions.}
}
@incollection{2014425,
title = {Index},
editor = {Matthew S. Hull and Diana M. Bowman},
booktitle = {Nanotechnology Environmental Health and Safety (Second Edition)},
publisher = {William Andrew Publishing},
edition = {Second Edition},
address = {Oxford},
pages = {425-436},
year = {2014},
series = {Micro and Nano Technologies},
isbn = {978-1-4557-3188-6},
doi = {https://doi.org/10.1016/B978-1-4557-3188-6.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978145573188600030X}
}
@incollection{MATHENY2014309,
title = {Chapter 11 - Generation of Knowledge for Clinical Decision Support: Statistical and Machine Learning Techniques},
editor = {Robert A. Greenes},
booktitle = {Clinical Decision Support (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {309-337},
year = {2014},
isbn = {978-0-12-398476-0},
doi = {https://doi.org/10.1016/B978-0-12-398476-0.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123984760000117},
author = {Michael E. Matheny and Lucila Ohno-Machado},
keywords = {Artificial neural networks, CDS knowledge generation, classification trees, logistic regression models},
abstract = {This chapter begins with historical backgrounds of knowledge generation for clinical decision support systems. It then reviews the methodologies of the most commonly used diagnostic and prognostic models in the medical domain, and discusses specific strengths and weaknesses of alternative modeling methods. Popular examples of some modeling methods are discussed; since the focus is on models that have been utilized in practice, the discussion concentrates on logistic regression models, classification trees, and artificial neural networks. It concludes with a discussion on current directions for the field.}
}
@incollection{DROGSETH2015299,
title = {Chapter 15 - The CMDB System Moves to Cloud and Beyond!},
editor = {Dennis Nils Drogseth and Rick Sturm and Dan Twing},
booktitle = {CMDB Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {299-324},
year = {2015},
isbn = {978-0-12-801265-9},
doi = {https://doi.org/10.1016/B978-0-12-801265-9.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012659000159},
author = {Dennis Nils Drogseth and Rick Sturm and Dan Twing},
keywords = {cloud, service-aware, asset, DevOps, analytics, automation, federation, tipping point},
abstract = {Like the Three-Tiered Roadmap, this chapter looks both backward and forward, but from a different vantage point and with a much stronger emphasis on looking forward. We begin by providing insights from other deployments after the first 12 months—a tipping point when the CMDB System should have already begun to show real value to a wide variety of stakeholders. We then examine some timely research addressing the CMDB System opportunities for expansion and growth. These include Service-Aware Asset Management, business service delivery across the cloud, analytics, and DevOps.}
}
@article{DUYCK20151059,
title = {Sloop: A pattern retrieval engine for individual animal identification},
journal = {Pattern Recognition},
volume = {48},
number = {4},
pages = {1059-1073},
year = {2015},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2014.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0031320314002763},
author = {James Duyck and Chelsea Finn and Andy Hutcheon and Pablo Vera and Joaquin Salas and Sai Ravela},
keywords = {Photo-identification, Animal biometrics, Individual identification, Relevance feedback, Crowdsourcing, Conservation, Scale-cascaded alignment, Local features, Hybrid shape contexts, Gecko, Skink, Whale shark, Salamander},
abstract = {Identifying individuals in photographs of animals collected over time is a non-invasive approach for ecological monitoring and conservation. This paper describes the design and use of Sloop, the first image retrieval system for individual animal identification incorporating crowd-sourced relevance feedback. Sloop׳s iterative retrieval strategy using hierarchical and aggregated matching and relevance feedback consistently improves deformation and correspondence-based approaches for individual identification across several species. Its crowdsourcing strategy is successful in utilizing relevance feedback on a large scale. Sloop is in operational use. The user experience and results are presented here to facilitate the creation of a community-based individual identification system for conservation planning.}
}
@article{AHMADIZELETI2016535,
title = {Exploring the economic value of open government data},
journal = {Government Information Quarterly},
volume = {33},
number = {3},
pages = {535-551},
year = {2016},
note = {Open and Smart Governments: Strategies, Tools, and Experiences},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16300077},
author = {Fatemeh {Ahmadi Zeleti} and Adegboyega Ojo and Edward Curry},
keywords = {Open government, Open Data Business Models, Open data industry, Values disciplines, Business models, Business model framework},
abstract = {Business models for open data have emerged in response to the economic opportunities presented by the increasing availability of open data. However, scholarly efforts providing elaborations, rigorous analysis and comparison of open data models are very limited. This could be partly attributed to the fact that most discussions on Open Data Business Models (ODBMs) are predominantly in the practice community. This shortcoming has resulted in a growing list of ODBMs which, on closer examination, are not clearly delineated and lack clear value orientation. This has made the understanding of value creation and exploitation mechanisms in existing open data businesses difficult and challenging to transfer. Following the Design Science Research (DSR) tradition, we developed a 6-Value (6-V) business model framework as a design artifact to facilitate the explication and detailed analysis of existing ODBMs in practice. Based on the results from the analysis, we identify business model patterns and emerging core value disciplines for open data businesses. Our results not only help streamline existing ODBMs and help in linking them to the overall business strategy, but could also guide governments in developing the required capabilities to support and sustain the business models.}
}
@article{HEISE201245,
title = {Integrating open government data with stratosphere for more transparency},
journal = {Journal of Web Semantics},
volume = {14},
pages = {45-56},
year = {2012},
note = {Special Issue on Dealing with the Messiness of the Web of Data},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826812000364},
author = {Arvid Heise and Felix Naumann},
keywords = {Data integration, Data cleansing, Record linkage, Data fusion, Parallel query processing, Map-reduce},
abstract = {Governments are increasingly publishing their data to enable organizations and citizens to browse and analyze the data. However, the heterogeneity of this Open Government Data hinders meaningful search, analysis, and integration and thus limits the desired transparency. In this article, we present the newly developed data integration operators of the Stratosphere parallel data analysis framework to overcome the heterogeneity. With declaratively specified queries, we demonstrate the integration of well-known government data sources and other large open data sets at technical, structural, and semantic levels. Furthermore, we publish the integrated data on the Web in a form that enables users to discover relationships between persons, government agencies, funds, and companies. The evaluation shows that linking person entities of different data sets results in a good precision of 98.3% and a recall of 95.2%. Moreover, the integration of large data sets scales well on up to eight machines.}
}
@article{FERREIRA2015212,
title = {Forward looking sonar mosaicing for Mine Countermeasures},
journal = {Annual Reviews in Control},
volume = {40},
pages = {212-226},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S136757881500053X},
author = {Fausto Ferreira and Vladimir Djapic and Michele Micheli and Massimo Caccia},
keywords = {Mosaicing, Forward looking sonar, Automatic Target Recognition, Mine Countermeasures},
abstract = {Forward looking sonars (FLS) are nowadays popular for many different applications. In particular, they can be used for Automatic Target Recognition (ATR) in the context of Mine Countermeasures. Currently, ATR techniques are applied to raw data which generates many false positives and the need for human supervision. Mosaicing FLS data increases target contrast and thus reduces false positive rate. Moreover, it implies a considerable data size reduction which is important if one thinks of exchange of data in real time through an acoustic channel with very limited bandwidth. Results of applying a real-time mosaicing algorithm to FLS data generated during Mine Countermeasures missions are shown and discussed thoroughly in this article.}
}
@article{FEDOROWICZ2014302,
title = {Design observations for interagency collaboration},
journal = {Government Information Quarterly},
volume = {31},
number = {2},
pages = {302-316},
year = {2014},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2013.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X14000057},
author = {Jane Fedorowicz and Steve Sawyer and Christine B. Williams and M. Lynne Markus and Martin Dias and Michael Tyworth and Sonia Gantman and Dax Jacobson and Arthur P. Tomasino and Robert Schrier},
keywords = {Inter-organizational information sharing systems, Design, Digital government, Public safety networks},
abstract = {We present 14 design observations for public safety networks (PSNs) and describe how they may apply more broadly to a wider range of inter-organizational systems within the public sector. A PSN is an interagency collaboration focused on developing and using information systems in support of information sharing and functional interoperability among public safety organizations engaged in law enforcement, criminal justice, and emergency response. We base our design observations upon an analysis of an extensive survey of 80 PSNs plus 6 in-depth case studies. The design observations identify commonalities that can guide agencies participating in interagency collaborations in addressing the interlocking issues they face. Our goal in presenting this set of design observations is to: (1) encourage improved PSN systems design and (2) draw attention to the importance of jointly addressing governance and technological considerations when designing PSNs.}
}
@article{BERGER2014143,
title = {A Questionnaire to Assess the Relevance and Credibility of Observational Studies to Inform Health Care Decision Making: An ISPOR-AMCP-NPC Good Practice Task Force Report},
journal = {Value in Health},
volume = {17},
number = {2},
pages = {143-156},
year = {2014},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2013.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1098301514000096},
author = {Marc L. Berger and Bradley C. Martin and Don Husereau and Karen Worley and J. Daniel Allen and Winnie Yang and Nicole C. Quon and C. Daniel Mullins and Kristijan H. Kahler and William Crown},
keywords = {bias, checklist, comparative effectiveness research, confounding, consensus, credibility, decision making, prospective observational study, quality, questionnaire, relevance, retrospective observational study, validity},
abstract = {Evidence-based health care decisions are best informed by comparisons of all relevant interventions used to treat conditions in specific patient populations. Observational studies are being performed to help fill evidence gaps. Widespread adoption of evidence from observational studies, however, has been limited because of various factors, including the lack of consensus regarding accepted principles for their evaluation and interpretation. Two task forces were formed to develop questionnaires to assist decision makers in evaluating observational studies, with one Task Force addressing retrospective research and the other Task Force addressing prospective research. The intent was to promote a structured approach to reduce the potential for subjective interpretation of evidence and drive consistency in decision making. Separately developed questionnaires were combined into a single questionnaire consisting of 33 items. These were divided into two domains: relevance and credibility. Relevance addresses the extent to which findings, if accurate, apply to the setting of interest to the decision maker. Credibility addresses the extent to which the study findings accurately answer the study question. The questionnaire provides a guide for assessing the degree of confidence that should be placed from observational studies and promotes awareness of the subtleties involved in evaluating those.}
}
@article{HE201683,
title = {Data synthesis in the Community Land Model for ecosystem simulation},
journal = {Journal of Computational Science},
volume = {13},
pages = {83-95},
year = {2016},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316300059},
author = {Hongsheng He and Dali Wang and Yang Xu and Jindong Tan},
keywords = {Data synthesis, Data analysis, Machine learning, Affinity Propagation, ARIMA model},
abstract = {Though many ecosystem states are physically observable, the number of measured variables is limited owning to the constraints of practical environments and onsite sensors. It is therefore beneficial to only measure fundamental variables that determine the behavior of the whole ecosystem, and to simulate other variables with the measured ones. This paper proposes an approach to extract fundamental variables from simulated or observed ecosystem data, and to synthesize the other variables using the fundamental variables. Because the relation of variables in the ecosystem depends on sampling time and frequencies, a region of interest (ROI) is determined using a sliding window on time series with a predefined sampling point and frequency. Within each ROI, system variables are clustered in accordance with a group of selective features by a combination of Affinity Propagation and k-Nearest-Neighbor. In each cluster, the unobserved variables are synthesized from selected fundamental variables using a linear fitting model with ARIMA errors. In the experiment, we studied the performance of variable clustering and data synthesis under a community-land-model based simulation platform. The performance of data synthesis is evaluated by data fitting errors in prediction and forecasting, and the change of system dynamics when synthesized data are in the loop. The experiment proves the high accuracy of the proposed approach in time-series analysis and synthesis for ecosystem simulation.}
}
@article{ZUCCA2012157,
title = {Towards a World Desertification Atlas. Relating and selecting indicators and data sets to represent complex issues},
journal = {Ecological Indicators},
volume = {15},
number = {1},
pages = {157-170},
year = {2012},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2011.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X11002998},
author = {C. Zucca and R. Della Peruta and R. Salvia and S. Sommer and M. Cherlet},
keywords = {UNCCD, Land degradation, Indicator frameworks, Indicator selection, Indicator ranking, Global data sets},
abstract = {Mapping land degradation and desertification (LDD) at the global scale still is a conceptual and operational challenge. The present study has been performed in the frame of the WAD (new World Atlas of Desertification) initiative. The objective of the paper is to test a structured procedure to identify relevant indicators for an effective representation of complex global LDD issues, based on available data sets (both geo-spatial and statistical), and conform to the UNCCD (United Nations Convention to Combat Desertification) requirements. Available indicators and data sets were reviewed and the collected information was organized through a database (DB). The proposed selection procedure is semi-quantitative and based on expert ranking decisions made within the relational DB environment, which allows for transparent definition and iterative refinement of the selection rules. The procedure was operationally tested on a concrete case study (“deforestation in Sub-Saharan Africa”) by using available global data sets. Questions related to global data availability and to major conceptual aspects such as the concept of “minimum indicator set”, the indicator frameworks, the ranking and selection criteria are raised and discussed.}
}
@article{MO20152851,
title = {Exploring immunological mechanisms of the whole sporozoite vaccination against P. falciparum malaria},
journal = {Vaccine},
volume = {33},
number = {25},
pages = {2851-2857},
year = {2015},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2015.04.056},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X15005204},
author = {Annie X.Y. Mo and John Pesce and B. Fenton Hall},
keywords = {Malaria, , Sporozoite, Vaccine, Immunology},
abstract = {Great progress has been made in the development of whole sporozoite vaccines including the manufacturing of cryopreserved Plasmodium falciparum sporozoites (PfSPZ) suitable for clinical application. Such whole sporozoites are being used for clinical studies of controlled human malaria infection (CHMI) as well as for evaluation of candidate vaccine approaches (both attenuated sporozoites and infectious sporozoites administered with chemoprophylaxis) and as reagents for immunology and cell biology assays. CHMI studies with whole sporozoites provide a great opportunity to better understand the intrinsic mechanisms of resistance to P. falciparum (e.g. due to sickle cell trait and other hemoglobinopathies) as well as host responses to an initial P. falciparum infection. High-level protective efficacy has been demonstrated in a small number of volunteers after intravenous (IV) inoculation of radiation-attenuated PfSPZ or in those who were exposed to live PfSPZ while on malaria chemoprophylaxis. These advances and data warrant further investigations of the immunological mechanism(s) whereby whole sporozoite inoculation elicits protective immunity in order to facilitate whole sporozoite vaccine development. The National Institute of Allergy and Infectious Diseases (NIAID) convened a workshop on Sept. 2–3, 2014 involving participation of international experts in the field of malaria vaccine development, and in basic and clinical immunology research. The workshop discussed the current understanding of host immune responses to whole malaria sporozoite inoculation, identified gaps in knowledge, resources to facilitate progress, and applicable new technologies and approaches to accelerate immunologic and vaccinologic studies and biomarker identification. This report summarizes the discussions and major conclusions from the workshop participants.}
}
@article{JIANG201689,
title = {A framework based on hidden Markov model with adaptive weighting for microcystin forecasting and early-warning},
journal = {Decision Support Systems},
volume = {84},
pages = {89-103},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616300124},
author = {P. Jiang and X. Liu and J. Zhang and X. Yuan},
keywords = {Decision support systems, Framework, Hidden Markov model, Adaptive exponential weighting, Microcystin forecasting, Early warning of risk},
abstract = {Harmful algal blooms during the eutrophication process produce toxins, such as microcystins (MCs), which endanger the ecosystems and human health. Accurate forecasting and early-warning of MCs can provide theoretical guidance for quick identification of risk in water management systems. The variation of MC concentration is affected by not only the status quo of numerous manifest biotic and abiotic factors, but also a hidden variable that represents the uncertainty of variations of these factors. Traditional approaches focus on fitting data precisely but less consider such a hidden variable, which would experience formidable barriers when encountering fluctuations in time-serial data. In this study, to address the forecasting problem with a hidden state variable and the problem of early-warning-of-risk, we build a novel integrated framework which is consist of three parts: 1) a forecasting model based on a Principal Component Analysis (PCA) and an improved Continuous Hidden Markov Model (CHMM) with adaptive exponential weighting (AEW), where the AEW-CHMM is proposed to forecast both the single-step-ahead concentration for general points and fluctuating points, and the three-step-ahead concentration existing immediately after the fluctuating point; 2) Bayesian hierarchical modeling for a ratio estimation; and 3) revised guidelines for the risk-level grading. The case study tests a real dataset of one shallow lake with the proposed approaches and other supervised machine learning methods. Computational results demonstrate that the proposed approaches are effective to offer an intelligent decision support tool for MC forecasting and early warning of risk by risk-level grading.}
}
@incollection{NOVIKOVA2013123,
title = {Chapter Three - NDAR: A Model Federal System for Secondary Analysis in Developmental Disabilities Research},
editor = {Richard C. Urbano},
series = {International Review of Research in Developmental Disabilities},
publisher = {Academic Press},
volume = {45},
pages = {123-153},
year = {2013},
booktitle = {Using Secondary Datasets to Understand Persons with Developmental Disabilities and their Families},
issn = {2211-6095},
doi = {https://doi.org/10.1016/B978-0-12-407760-7.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077607000037},
author = {S.I. Novikova and D.M. Richman and K. Supekar and L. Barnard-Brak and D. Hall},
keywords = {Secondary analysis, Data respository, Database, Autism, Developmental disabilities},
abstract = {The National Database for Autism Research (NDAR) is a human-subject data repository on tens of thousands of research participants. Approved researchers have access to an unprecedented volume of item-level clinical, genomic, and imaging data. Data are shared quickly using both a common data standard and innovative tools for experiment definition, which provide the level of detail needed for efficient use of the repository. As described, early adopters have used it to conduct secondary data analysis. Now, with an ever-increasing volume of research data being made available, and new methods for data query, data download, and computation in place, this initiative is becoming vital to those interested in scientific discovery in autism or is being used as a model by other research communities.}
}
@article{XU201616,
title = {Contribution of soil respiration to the global carbon equation},
journal = {Journal of Plant Physiology},
volume = {203},
pages = {16-28},
year = {2016},
note = {Plants facing Changing Climate},
issn = {0176-1617},
doi = {https://doi.org/10.1016/j.jplph.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0176161716301742},
author = {Ming Xu and Hua Shang},
keywords = {Soil microbial decomposition, Soil respiration measurement, Soil respiration model, Global annual soil respiration, Belowground carbon allocation},
abstract = {Soil respiration (Rs) is the second largest carbon flux next to GPP between the terrestrial ecosystem (the largest organic carbon pool) and the atmosphere at a global scale. Given their critical role in the global carbon cycle, Rs measurement and modeling issues have been well reviewed in previous studies. In this paper, we briefly review advances in soil organic carbon (SOC) decomposition processes and the factors affecting Rs. We examine the spatial and temporal distribution of Rs measurements available in the literature and found that most of the measurements were conducted in North America, Europe, and East Asia, with major gaps in Africa, East Europe, North Asia, Southeast Asia, and Australia, especially in dry ecosystems. We discuss the potential problems of measuring Rs on slope soils and propose using obliquely-cut soil collars to solve the existing problems. We synthesize previous estimates of global Rs flux and find that the estimates ranged from 50 PgC/yr to 98 PgC/yr and the error associated with each estimation was also high (4 PgC/yr to 33.2 PgC/yr). Using a newly integrated database of Rs measurements and the MODIS vegetation map, we estimate that the global annual Rs flux is 94.3 PgC/yr with an estimation error of 17.9 PgC/yr at a 95% confidence level. The uneven distribution of Rs measurements limits our ability to improve the accuracy of estimation. Based on the global estimation of Rs flux, we found that Rs is highly correlated with GPP and NPP at the biome level, highlighting the role of Rs in global carbon budgets.}
}
@article{SCHWABE20151,
title = {Uncertainty quantification metrics for whole product life cycle cost estimates in aerospace innovation},
journal = {Progress in Aerospace Sciences},
volume = {77},
pages = {1-24},
year = {2015},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2015.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0376042115000433},
author = {O. Schwabe and E. Shehab and J. Erkoyuncu},
keywords = {Cost estimation, Cost readiness, Innovation, Uncertainty quantification, Whole product life cycle},
abstract = {The lack of defensible methods for quantifying cost estimate uncertainty over the whole product life cycle of aerospace innovations such as propulsion systems or airframes poses a significant challenge to the creation of accurate and defensible cost estimates. Based on the axiomatic definition of uncertainty as the actual prediction error of the cost estimate, this paper provides a comprehensive overview of metrics used for the uncertainty quantification of cost estimates based on a literature review, an evaluation of publicly funded projects such as part of the CORDIS or Horizon 2020 programs, and an analysis of established approaches used by organizations such NASA, the U.S. Department of Defence, the ESA, and various commercial companies. The metrics are categorized based on their foundational character (foundations), their use in practice (state-of-practice), their availability for practice (state-of-art) and those suggested for future exploration (state-of-future). Insights gained were that a variety of uncertainty quantification metrics exist whose suitability depends on the volatility of available relevant information, as defined by technical and cost readiness level, and the number of whole product life cycle phases the estimate is intended to be valid for. Information volatility and number of whole product life cycle phases can hereby be considered as defining multi-dimensional probability fields admitting various uncertainty quantification metric families with identifiable thresholds for transitioning between them. The key research gaps identified were the lacking guidance grounded in theory for the selection of uncertainty quantification metrics and lacking practical alternatives to metrics based on the Central Limit Theorem. An innovative uncertainty quantification framework consisting of; a set-theory based typology, a data library, a classification system, and a corresponding input-output model are put forward to address this research gap as the basis for future work in this field.}
}
@article{ABUSHANAB2015453,
title = {Reengineering the open government concept: An empirical support for a proposed model},
journal = {Government Information Quarterly},
volume = {32},
number = {4},
pages = {453-463},
year = {2015},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X15000787},
author = {Emad A. Abu-Shanab},
keywords = {Open government, e-Government, Reengineering open government, Research framework, Participation, Collaboration, Transparency, Accountability, Empowerment, Empirical test, Jordan},
abstract = {Open government is a new phenomenon that attracted much research in recent years. The major dimensions of open government included some redundancy with respect to the indications of these concepts. Through an extensive literature review, this study tried to breakdown the three major pillars of open government into their known sub-dimensions. The second step was to summarize the basic concepts reported in the literature and map them to these dimensions. Finally, this work tried to synthesize the basic concepts into four major dimensions. The proposed dimensions are: transparency, information accountability, collaboration and empowerment. The second objective of this work is to validate this proposed model by utilizing an empirical test using confirmatory factor analysis. The research model proposed tried to predict Jordanians' intentions to use e-government services using the four proposed dimensions of open government. Empirical results supported our premise and indicated a good fit of dimensions and acceptable loadings on each dimension. Also, the regression test predicted the intention to use e-government websites with an acceptable coefficient of determination (R2=0.409). Empirical results indicated a significant prediction of intention to use e-government website by all dimensions proposed with more weight for information accountability. The reengineered model was supported by the data and calls for more validation by researchers.}
}
@article{AALTO201464,
title = {Large scale data acquisition of simultaneous MRI and speech},
journal = {Applied Acoustics},
volume = {83},
pages = {64-75},
year = {2014},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2014.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X14000528},
author = {Daniel Aalto and Olli Aaltonen and Risto-Pekka Happonen and Päivi Jääsaari and Atle Kivelä and Juha Kuortti and Jean-Marc Luukinen and Jarmo Malinen and Tiina Murtola and Riitta Parkkola and Jani Saunavaara and Tero Soukka and Martti Vainio},
keywords = {Speech production, Speech recording, MRI, Noise reduction, Formant analysis, Vocal tract resonance},
abstract = {We describe an arrangement for simultaneous recording of speech and vocal tract geometry in patients undergoing surgery involving this area. Experimental design is considered from an articulatory phonetic point of view. The speech signals are recorded with an acoustic-electrical arrangement. The vocal tract is simultaneously imaged with MRI. A MATLAB-based system controls the timing of speech recording and MR image acquisition. The speech signals are cleaned from acoustic MRI noise by an adaptive signal processing algorithm. Finally, a vowel data set from pilot experiments is qualitatively compared both with validation data from the anechoic chamber and with Helmholtz resonances of the vocal tract volume, obtained using FEM.}
}
@incollection{HUGHES2013251,
title = {Chapter 8 - Adapting Agile for Data Warehousing},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing Project Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {251-302},
year = {2013},
isbn = {978-0-12-396463-2},
doi = {https://doi.org/10.1016/B978-0-12-396463-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964632000089},
author = {Ralph Hughes}
}
@article{TEIZER2015225,
title = {Status quo and open challenges in vision-based sensing and tracking of temporary resources on infrastructure construction sites},
journal = {Advanced Engineering Informatics},
volume = {29},
number = {2},
pages = {225-238},
year = {2015},
note = {Infrastructure Computer Vision},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2015.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034615000336},
author = {Jochen Teizer},
keywords = {Building information modeling, Computer vision and machine learning, Resource location tracking and progress monitoring, Safety and health, Sensors: photo and video cameras, unmanned aerial vehicles, Surveying: laser scanning, photo- and videogrammetry},
abstract = {Modern construction projects require sufficient planning and management of resources to become successful. Core issues are tasks that deal with maintaining the schedule, such as procuring materials, guaranteeing the supply chain, controlling the work status, and monitoring safety and quality. Timely feedback of project status aids project management by providing accurate percentages of task completions and appropriately allocating resources (workforce, equipment, material) to coordinate the next work packages. However, current methods for measuring project status or progress, especially on large infrastructure projects, are mostly based on manual assessments. Recent academic research and commercial development has focused on semi- or fully-automated approaches to collect and process images of evolving worksites. Preliminary results are promising and show capturing, analyzing, and documenting construction progress and linking to information models is possible. This article presents first an overview to vision-based sensing technology available for temporary resource tracking at infrastructure construction sites. Second, it provides the status quo of research applications by highlighting exemplary case. Third, a discussion follows on existing advantages and current limitations of vision based sensing and tracking. Open challenges that need to be addressed in future research efforts conclude this paper.}
}
@incollection{2012991,
title = {Chapter 16 - The Future of Text and Web Analytics},
editor = {Gary Miner and Dursun Delen and John Elder and Andrew Fast and Thomas Hill and Robert A. Nisbet},
booktitle = {Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications},
publisher = {Academic Press},
address = {Boston},
pages = {991-1005},
year = {2012},
isbn = {978-0-12-386979-1},
doi = {https://doi.org/10.1016/B978-0-12-386979-1.00044-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012386979100044X}
}
@article{CHRISTMANN20161,
title = {On the robustness of regularized pairwise learning methods based on kernels},
journal = {Journal of Complexity},
volume = {37},
pages = {1-33},
year = {2016},
issn = {0885-064X},
doi = {https://doi.org/10.1016/j.jco.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885064X16300474},
author = {Andreas Christmann and Ding-Xuan Zhou},
keywords = {Machine learning, Pairwise loss function, Regularized risk, Robustness},
abstract = {Regularized empirical risk minimization including support vector machines plays an important role in machine learning theory. In this paper regularized pairwise learning (RPL) methods based on kernels will be investigated. One example is regularized minimization of the error entropy loss which has recently attracted quite some interest from the viewpoint of consistency and learning rates. This paper shows that such RPL methods and also their empirical bootstrap have additionally good statistical robustness properties, if the loss function and the kernel are chosen appropriately. We treat two cases of particular interest: (i) a bounded and non-convex loss function and (ii) an unbounded convex loss function satisfying a certain Lipschitz type condition.}
}
@article{CARON20164,
title = {The Internet of Things (IoT) and its impact on individual privacy: An Australian perspective},
journal = {Computer Law & Security Review},
volume = {32},
number = {1},
pages = {4-15},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001661},
author = {Xavier Caron and Rachelle Bosua and Sean B. Maynard and Atif Ahmad},
keywords = {Australian Privacy Principles (APPs), Authentication, Hacking risk, Individual privacy, Internet of Things, Privacy legislation, Security, Surveillance and ubiquity},
abstract = {The Internet of Things (IoT) heralds a new era of computing whereby every imaginable object is equipped with, or connected to a smart device allowing data collection and communication through the Internet. The IoT challenges individual privacy in terms of the collection and use of individuals' personal data. This study assesses the extent to which the Australian Privacy Principles protect individual privacy associated with data collection through the IoT. A systematic literature review identified four key privacy themes that represent issues related to the collection of individuals' data through the IoT: unauthorised surveillance, uncontrolled data generation and use, inadequate authentication and information security risks. These four themes are used to critically analyse the Australian Privacy Principle's (APPs) protection of individual data. Findings indicate that (1) the APPs do not adequately protect individual privacy of data collected through the IoT, and (2) future privacy legislation must consider the implications of global reach of IoT services, and ubiquity and security of IoT data collection with respect to individual privacy.}
}
@incollection{2016341,
title = {Index},
editor = {Rajkumar Buyya and Amir {Vahid Dastjerdi}},
booktitle = {Internet of Things},
publisher = {Morgan Kaufmann},
pages = {341-354},
year = {2016},
isbn = {978-0-12-805395-9},
doi = {https://doi.org/10.1016/B978-0-12-805395-9.00026-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053959000265}
}
@article{COBO20153,
title = {25years at Knowledge-Based Systems: A bibliometric analysis},
journal = {Knowledge-Based Systems},
volume = {80},
pages = {3-13},
year = {2015},
note = {25th anniversary of Knowledge-Based Systems},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2014.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115000076},
author = {M.J. Cobo and M.A. Martínez and M. Gutiérrez-Salcedo and H. Fujita and E. Herrera-Viedma},
keywords = {Bibliometrics, Science mapping, Citations, Co-word analysis, h-index},
abstract = {In commemoration of the Anniversary 25th of KnoSys we present a bibliometric analysis of the scientific content of the journal during the period 1991–2014. This analysis shows the conceptual evolution of the journal and some of its performance bibliometric indicators based on citations, as the evolution of its impact factor, its h-index, and its most cited authors/documents.}
}
@article{DEANGELIS20152254,
title = {Survival variations by country and age for lymphoid and myeloid malignancies in Europe 2000–2007: Results of EUROCARE-5 population-based study},
journal = {European Journal of Cancer},
volume = {51},
number = {15},
pages = {2254-2268},
year = {2015},
note = {Survival of Cancer Patients in Europe, 1999–2007: The EUROCARE-5 Study},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959804915007789},
author = {Roberta {De Angelis} and Pamela Minicozzi and Milena Sant and Luigino {Dal Maso} and David H. Brewster and Gemma Osca-Gelis and Otto Visser and Marc Maynadié and Rafael Marcos-Gragera and Xavier Troussard and Dominic Agius and Paolo Roazzi and Elisabetta Meneghini and Alain Monnereau and M. Hackl and N. Zielonke and W. Oberaigner and E. {Van Eycken} and K. Henau and Z. Valerianova and N. Dimitrova and M. Sekerija and M. Zvolský and L. Dušek and H. Storm and G. Engholm and M. Mägi and T. Aareleid and N. Malila and K. Seppä and M. Velten and X. Troussard and V. Bouvier and G. Launoy and A.V. Guizard and J. Faivre and A.M. Bouvier and P. Arveux and M. Maynadié and A.S. Woronoff and M. Robaszkiewicz and I. Baldi and A. Monnereau and B. Tretarre and N. Bossard and A. Belot and M. Colonna and F. Molinié and S. Bara and C. Schvartz and B. Lapôtre-Ledoux and P. Grosclaude and M. Meyer and R. Stabenow and S. Luttmann and A. Eberle and H. Brenner and A. Nennecke and J. Engel and G. Schubert-Fritschle and J. Kieschke and J. Heidrich and B. Holleczek and A. Katalinic and J.G. Jónasson and L. Tryggvadóttir and H. Comber and G. Mazzoleni and A. Bulatko and C. Buzzoni and A. Giacomin and A. {Sutera Sardo} and P. Mancuso and S. Ferretti and E. Crocetti and A. Caldarella and G. Gatta and M. Sant and H. Amash and C. Amati and P. Baili and F. Berrino and S. Bonfarnuzzo and L. Botta and F. {Di Salvo} and R. Foschi and C. Margutti and E. Meneghini and P. Minicozzi and A. Trama and D. Serraino and L. {Dal Maso} and R. {De Angelis} and M. Caldora and R. Capocaccia and E. Carrani and S. Francisci and S. Mallone and D. Pierannunzio and P. Roazzi and S. Rossi and M. Santaquilani and A. Tavilla and F. Pannozzo and S. Busco and L. Bonelli and M. Vercelli and V. Gennaro and P. Ricci and M. Autelitano and G. Randi and M. {Ponz De Leon} and C. Marchesi and C. Cirilli and M. Fusco and M.F. Vitale and M. Usala and A. Traina and R. Staiti and F. Vitale and B. Ravazzolo and M. Michiara and R. Tumino and P. {Giorgi Rossi} and E. {Di Felice} and F. Falcini and A. Iannelli and O. Sechi and R. Cesaraccio and S. Piffer and A. Madeddu and F. Tisano and S. Maspero and A.C. Fanetti and R. Zanetti and S. Rosso and P. Candela and T. Scuderi and F. Stracci and F. Bianconi and G. Tagliabue and P. Contiero and A.P. {Dei Tos} and S. Guzzinati and S. Pildava and G. Smailyte and N. Calleja and D. Agius and T.B. Johannesen and J. Rachtan and S. Gózdz and R. Mezyk and J. Blaszczyk and M. Bebenek and M. Bielska-Lasota and G. {Forjaz de Lacerda} and M.J. Bento and C. Castro and A. Miranda and A. Mayer-da-Silva and F. Nicula and D. Coza and C. {Safaei Diba} and M. Primic-Zakelj and E. Almar and C. Ramírez and M. Errezola and J. Bidaurrazaga and A. Torrella-Ramos and J.M. {Díaz García} and R. Jimenez-Chillaron and R. Marcos-Gragera and A. {Izquierdo Font} and M.J. Sanchez and D.Y.L. Chang and C. Navarro and M.D. Chirlaque and C. Moreno-Iribas and E. Ardanaz and J. Galceran and M. Carulla and M. Lambe and S. Khan and M. Mousavi and C. Bouchardy and M. Usel and S.M. Ess and H. Frick and M. Lorez and S.M. Ess and C. Herrmann and A. Bordoni and A. Spitale and I. Konzelmann and O. Visser and V. Lemmens and M. Coleman and C. Allemani and B. Rachet and J. Verne and N. Easey and G. Lawrence and T. Moran and J. Rashbass and M. Roche and J. Wilkinson and A. Gavin and C. Donnelly and D.H. Brewster and D.W. Huws and C. White and R. Otter},
keywords = {Relative survival, Europe, Cancer registry, Lymphoma, Leukaemia, Hodgkin lymphoma, Non-Hodgkin lymphoma, Multiple myeloma},
abstract = {Background
Significant advances in the management of patients with lymphoid and myeloid malignancies entered clinical practice in the early 2000’s. The EUROCARE-5 study database provides an opportunity to assess the impact of these changes at the population level by country in Europe. We provide survival estimates for clinically relevant haematological malignancies (HM), using the International Classification of Diseases for Oncology 3, by country, gender and age in Europe.
Methods
We estimated age-standardised relative survival using the complete cohort approach for 625,000 adult patients diagnosed in 2000–2007 and followed up to 2008. Survival information was provided by 89 participating cancer registries from 29 European countries. Mean survival in Europe was calculated as the population weighted average of country-specific estimates.
Results
On average in Europe, 5-year relative survival was highest for Hodgkin lymphoma (81%; 40,625 cases), poorest for acute myeloid leukaemia (17%; 57,026 cases), and intermediate for non-Hodgkin lymphoma (59%; 329,204 cases), chronic myeloid leukaemia (53%; 17,713 cases) and plasma cell neoplasms (39%; 94,024 cases). Survival was generally lower in Eastern Europe and highest in Central and Northern Europe. Wider between country differences (>10%) were observed for malignancies that benefited from therapeutic advances, such as chronic myeloid leukaemia, chronic lymphocytic leukaemia, follicular lymphoma, diffuse large B-cell lymphoma and multiple myeloma. Lower differences (<10%) were observed for Hodgkin lymphoma.
Conclusions
Delayed or reduced access to innovative and appropriate therapies could plausibly have contributed to the observed geographical disparities between European regions and countries. Population based survival by morphological sub-type is important for measuring outcomes of HM management. To better inform quality of care research, the collection of detailed clinical information at the population level should be prioritised.}
}
@article{B201597,
title = {Efficient identity matching using static pruning q-gram indexing approach},
journal = {Decision Support Systems},
volume = {73},
pages = {97-108},
year = {2015},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2015.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167923615000421},
author = {Khairul Nizam B. and Shahrul Azman M.N.},
keywords = {Q-gram indexing, Static index pruning, Identity matching, Identity management},
abstract = {Information overload is a growing problem for information management and analytics in many organizations. Identity matching techniques are used to manage and resolve millions of identity records in diverse domains such as health care information, telecom subscribers, insurance holders, law offenders, and the census. In this paper, we propose an identity matching technique that is efficient for large datasets without compromising matching effectiveness. Our experimental results provide strong evidence that our proposed identity matching technique outperforms the adaptive detection identity matching technique in terms of efficiency and effectiveness, reducing the number of required comparisons by almost 98% and the completion time by 97%, with promising scalability results. Furthermore, our proposed technique achieves better matching results than the most trusted pairwise identity matching approach.}
}
@incollection{WAPOLE2015209,
title = {Chapter 8 - Data Management and Research Integrity},
editor = {Mark A. Suckow and Bill J. Yates},
booktitle = {Research Regulatory Compliance},
publisher = {Academic Press},
pages = {209-242},
year = {2015},
isbn = {978-0-12-420058-6},
doi = {https://doi.org/10.1016/B978-0-12-420058-6.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124200586000083},
author = {Shannon Wapole and David A. Stone},
keywords = {Data management, Federal agencies, National Institutes of Health, National Institute of Justice, US Department of Agriculture},
abstract = {There is not great uniformity among funding agencies when it comes to data management requirements. Each agency has different expectations from researchers. Some agencies require data management plans at the application stage while others are more concerned with data sharing at the postaward stage. Researchers must be aware of the specific agencies requirements. Once funding is secured, the research should implement a data management plan and follow it with continuity throughout the funding lifecycle. Institutions should consider having internal data management policies particularly when it comes to issues of data ownership and data retention. In the event a researcher compromises the integrity of their data, misconduct policies should also be put into place. If proper data management protocols are observed and followed the integrity of the research will be secure.}
}
@article{CHRISTMANN2007347,
title = {Robust learning from bites for data mining},
journal = {Computational Statistics & Data Analysis},
volume = {52},
number = {1},
pages = {347-361},
year = {2007},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167947306004853},
author = {Andreas Christmann and Ingo Steinwart and Mia Hubert},
keywords = {Breakdown point, Convex risk minimization, Data mining, Distributed computing, Influence function, Logistic regression, Robustness, Scalability, Statistical machine learning, Support vector machine},
abstract = {Some methods from statistical machine learning and from robust statistics have two drawbacks. Firstly, they are computer-intensive such that they can hardly be used for massive data sets, say with millions of data points. Secondly, robust and non-parametric confidence intervals for the predictions according to the fitted models are often unknown. A simple but general method is proposed to overcome these problems in the context of huge data sets. An implementation of the method is scalable to the memory of the computer and can be distributed on several processors to reduce the computation time. The method offers distribution-free confidence intervals for the median of the predictions. The main focus is on general support vector machines (SVM) based on minimizing regularized risks. As an example, a combination of two methods from modern statistical machine learning, i.e. kernel logistic regression and ε-support vector regression, is used to model a data set from several insurance companies. The approach can also be helpful to fit robust estimators in parametric models for huge data sets.}
}
@article{DENG201586,
title = {Phylogeny, divergence times, and historical biogeography of the angiosperm family Saxifragaceae},
journal = {Molecular Phylogenetics and Evolution},
volume = {83},
pages = {86-98},
year = {2015},
issn = {1055-7903},
doi = {https://doi.org/10.1016/j.ympev.2014.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1055790314003959},
author = {Jia-bin Deng and Bryan T. Drew and Evgeny V. Mavrodiev and Matthew A. Gitzendanner and Pamela S. Soltis and Douglas E. Soltis},
keywords = {Saxifragaceae, , Heucheroids, Saxifragoids, Divergence times, S-DIVA},
abstract = {Saxifragaceae (Saxifragales) contain approximately 640 species and 33 genera, about half of which are monotypic. Due to factors such as morphological stasis, convergent morphological evolution, and disjunct distributions, relationships within Saxifragaceae have historically been troublesome. The family occurs primarily in mountainous regions of the Northern Hemisphere, with the highest generic and species diversity in western North America, but disjunct taxa are known from southern South America. Here, we integrate broad gene (56 loci) and taxon (223 species) sampling strategies, both the most comprehensive to date within Saxifragaceae, with fossil calibrations and geographical distribution data to address relationships, divergence times, and historical biogeography among major lineages of Saxifragaceae. Two previously recognized main clades, the heucheroids (eight groups+Saniculiphyllum) and saxifragoids (Saxifraga s.s.), were re-affirmed by our phylogenetic analyses. Relationships among the eight heucheroid groups, as well as the phylogenetic position of Saniculiphyllum within the heucheroids, were resolved with mostly high support. Divergence time estimates indicate that Saxifragaceae began to diversify ca. 38.37 million years ago (Mya; 95% HPD=30.99–46.11Mya) in the Mid-Late Eocene, and that the two major lineages, the heucheroids and saxifragoids, began to diversify approximately 30.04Mya (95% HPD=23.87–37.15Mya) and 30.85 Mya (95% HPD=23.47–39.33Mya), respectively. We reconstructed ancestral geographic areas using statistical dispersal-vicariance (S-DIVA). These analyses indicate several radiations within Saxifragaceae: one in eastern Asia and multiple radiations in western North America. Our results also demonstrate that large amounts of sequence data coupled with broad taxon sampling can help resolve clade relationships that have thus far seemed intractable.}
}
@article{ANAGNOSTOPOULOS201690,
title = {A delay-resilient and quality-aware mechanism over incomplete contextual data streams},
journal = {Information Sciences},
volume = {355-356},
pages = {90-109},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516301670},
author = {Christos Anagnostopoulos and Kostas Kolomvatsos},
keywords = {Incomplete multivariate context streams, Quality of streaming data, Internet of Things, Optimal stopping theory},
abstract = {We study the case of scheduling a Contextual Information Process (CIP) over incomplete multivariate contextual data streams coming from sensing devices in Internet of Things (IoT) environments. CIPs like data fusion, concept drift detection, and predictive analytics adopt window-based methods for processing continuous stream queries. CIPs involve the continuous evaluation of functions over contextual attributes (e.g., air pollutants measurements from environmental sensors) possibly incomplete (i.e., containing missing values) thus degrading the quality of the CIP results. We introduce a mechanism, which monitors the quality of the contextual streaming values and then optimally determines the appropriate time to activate a CIP. CIP is optimally delayed in hopes of observing in the near future higher quality of contextual values in terms of validity, freshness and presence. Our time-optimized mechanism activates a CIP when the expected quality is maximized taking also into account the induced cost of delay and an aging framework of freshness over contextual values. We propose two analytical time-based stochastic optimization models and provide extensive sensitivity analysis. We provide a comparative assessment with sliding window-centric models found in the literature and showcase the efficiency of our mechanism on improving the quality of results of a CIP.}
}
@article{KLUG201566,
title = {Operationalizing environmental indicators for real time multi-purpose decision making and action support},
journal = {Ecological Modelling},
volume = {295},
pages = {66-74},
year = {2015},
note = {Use of ecological indicators in models},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2014.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0304380014001902},
author = {Hermann Klug and Alexander Kmoch},
keywords = {WebGIS, Sensors, SDI, Early warning system, Water resources management, OGC},
abstract = {Within the last decades tremendous progress has been made in analysing, characterising and understanding the processes, functions, and structures of the environment. Numerous indicators have been proposed and operationalised using computing techniques. However, many of the approaches are based on specific case study areas and the transfer of approaches is hampered due to incompatible data formats, data availability limitations, and/or unavailable modelling routines. Information on modelling routines, existing result datasets, and updates of previously derived analyses are missing. Considering the recent technological and methodological developments, environmental modelling providing indicators for decision support is likely to change in the next decade. This research provides a heuristic conceptual basis for driving the next generation of real-time multi-purpose data assembling, evaluating, modelling, and visualisation towards the operationalisation of decisions. Turning field observations into useful (near) real-time decision support information is demonstrated based on a hydrological example of future Integrated Water Resources Management. This paper describes new ways of near real-time indicator processing using Wireless Sensor Networks and standardised web services. Publicly available and standardised environmental information as Open Geospatial Consortium compliant Sensor Observation Services with its data formats Observations & Measurements and Water Markup Language 2.0 automatically feed into Web Processing Services for timely information delivery, discovery and access of the spatially explicit environmental conditions as pull and push based web services accompanied with notification for immediate actions in crisis times.}
}
@article{GILGARCIA2016524,
title = {Conceptualizing smartness in government: An integrative and multi-dimensional view},
journal = {Government Information Quarterly},
volume = {33},
number = {3},
pages = {524-534},
year = {2016},
note = {Open and Smart Governments: Strategies, Tools, and Experiences},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2016.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16300284},
author = {J. Ramon Gil-Garcia and Jing Zhang and Gabriel Puron-Cid},
keywords = {Smart government, Smart city, Sustainability, Openness, Effectiveness, Efficiency, Innovation, Creativeness, Information technologies, Data, Evidence-based decision making},
abstract = {Smartness has recently emerged as a desirable characteristic of governments, cities, communities, infrastructures, and devices. Within the public sector, smart city has become a popular term and municipal governments around the world are using multiple strategies to become smarter. However, there is no consensus about what smartness means and how to identify its key components or dimensions. Some definitions highlight information technology and data, while others pay attention to sustainability, openness, innovation, or resiliency. Based on a review of current literature, this paper identifies multiple dimensions of smartness and proposes an integrative view that highlights how each dimension contributes to the understanding and development of smart governments. We argue that smartness should be conceptualized in a broad and multifaceted way. The framework we present serves as a foundation to understand and measure smartness in government and provides guidelines for the comprehensive development of smart governments. Some of the dimensions have been identified and studied explicitly in the realm of smart government. A number of other dimensions are embedded in the literature as individual characteristics of a good government; although they are not explicitly referenced in relationship to smart government, we argue that they are important components of a government being smart. The paper also suggests that public managers do not control all dimensions equally. Some dimensions could be seen as relatively direct outputs of their actions, while others could be better understood as outcomes that could be affected, but not solely determined, by strategic interventions or deliberate actions.}
}
@incollection{GUTIERREZ2014278,
title = {1.15 - Toward Sustainable Water Resource Management: Challenges and Opportunities},
editor = {Satinder Ahuja},
booktitle = {Comprehensive Water Quality and Purification},
publisher = {Elsevier},
address = {Waltham},
pages = {278-287},
year = {2014},
isbn = {978-0-12-382183-6},
doi = {https://doi.org/10.1016/B978-0-12-382182-9.00018-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123821829000189},
author = {S.C. Gutierrez},
keywords = {Contaminants, Drinking water, Governance, Innovation, Sustainability, Technology, US Environmental Protection Agency, Wastewater treatment, Water availability, Water efficiency, Water quality, Water resource management, Water reuse},
abstract = {The United States has derived significant economic benefit from an abundant and high-quality water supply. The ability of the nation to continue this pace into the future is uncertain because of a number of significant challenges. These include increasing water demand because of increased population growth, declining water quality, aging water infrastructure, climate change, and competition for the resource. A number of actions could be taken to put the country on a more sustainable path; including improved water information, increased water efficiency and expansion of water availability; establishment and tracking of national-scale water-relevant sustainability metrics; and development of new governance models.}
}
@incollection{GUPTA2014345,
title = {Chapter 19 - Next Generation Sequencing and Its Applications},
editor = {Ashish S. Verma and Anchal Singh},
booktitle = {Animal Biotechnology},
publisher = {Academic Press},
address = {San Diego},
pages = {345-367},
year = {2014},
isbn = {978-0-12-416002-6},
doi = {https://doi.org/10.1016/B978-0-12-416002-6.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160026000195},
author = {Anuj Kumar Gupta and U.D. Gupta},
keywords = {Animal Biotechnology, Human Health, Next Generation Sequencing, NGS Chemistries, NGS Technologies},
abstract = {Next generation sequencing (NGS) has revolutionized nearly every area of biotechnology. It has been applied to various aspects of biological science, including animal, human, and plant biotechnology. The enormous information produced by NGS assists in understanding genomic variations, disease mechanisms, and resistance, thus helping development of better diagnostics, therapies, and breeds.}
}
@article{ORDONEZ201438,
title = {Extending ER models to capture database transformations to build data sets for data mining},
journal = {Data & Knowledge Engineering},
volume = {89},
pages = {38-54},
year = {2014},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X13001298},
author = {Carlos Ordonez and Sofian Maabout and David Sergio Matusevich and Wellington Cabrera},
keywords = {ER model, Data mining, Transformation, Denormalization, Aggregation},
abstract = {In a data mining project developed on a relational database, a significant effort is required to build a data set for analysis. The main reason is that, in general, the database has a collection of normalized tables that must be joined, aggregated and transformed in order to build the required data set. Such scenario results in many complex SQL queries that are written independently from each other, in a disorganized manner. Therefore, the database grows with many tables and views that are not present as entities in the ER model and similar SQL queries are written multiple times, creating problems in database evolution and software maintenance. In this paper, we classify potential database transformations, we extend an ER diagram with entities capturing database transformations and we introduce an algorithm which automates the creation of such extended ER model. We present a case study with a public database illustrating database transformations to build a data set to compute a typical data mining model.}
}
@article{POLITI2016791,
title = {Assessing the utility of geospatial technologies to investigate environmental change within lake systems},
journal = {Science of The Total Environment},
volume = {543},
pages = {791-806},
year = {2016},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2015.09.136},
url = {https://www.sciencedirect.com/science/article/pii/S004896971530797X},
author = {Eirini Politi and John S. Rowan and Mark E.J. Cutler},
keywords = {Lake change, Ecosystem services, Remote sensing, Geospatial technology, Catchment pressures},
abstract = {Over 50% of the world's population live within 3km of rivers and lakes highlighting the on-going importance of freshwater resources to human health and societal well-being. Whilst covering c. 3.5% of the Earth's non-glaciated land mass, trends in the environmental quality of the world's standing waters (natural lakes and reservoirs) are poorly understood, at least in comparison with rivers, and so evaluation of their current condition and sensitivity to change are global priorities. Here it is argued that a geospatial approach harnessing existing global datasets, along with new generation remote sensing products, offers the basis to characterise trajectories of change in lake properties e.g., water quality, physical structure, hydrological regime and ecological behaviour. This approach furthermore provides the evidence base to understand the relative importance of climatic forcing and/or changing catchment processes, e.g. land cover and soil moisture data, which coupled with climate data provide the basis to model regional water balance and runoff estimates over time. Using examples derived primarily from the Danube Basin but also other parts of the World, we demonstrate the power of the approach and its utility to assess the sensitivity of lake systems to environmental change, and hence better manage these key resources in the future.}
}
@article{ANTONIOU2015129,
title = {W-SPSA in practice: Approximation of weight matrices and calibration of traffic simulation models},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {59},
pages = {129-146},
year = {2015},
note = {Special Issue on International Symposium on Transportation and Traffic Theory},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15001710},
author = {Constantinos Antoniou and Carlos {Lima Azevedo} and Lu Lu and Francisco Pereira and Moshe Ben-Akiva},
keywords = {Calibration algorithms, Dynamic traffic assignment, Microscopic traffic simulation, Large-scale applications, Optimisation, Heuristics},
abstract = {The development and calibration of complex traffic models demands parsimonious techniques, because such models often involve hundreds of thousands of unknown parameters. The Weighted Simultaneous Perturbation Stochastic Approximation (W-SPSA) algorithm has been proven more efficient than its predecessor SPSA (Spall, 1998), particularly in situations where the correlation structure of the variables is not homogeneous. This is crucial in traffic simulation models where effectively some variables (e.g. readings from certain sensors) are strongly correlated, both in time and space, with some other variables (e.g. certain OD flows). In situations with reasonably sized traffic networks, the difference is relevant considering computational constraints. However, W-SPSA relies on determining a proper weight matrix (W) that represents those correlations, and such a process has been so far an open problem, and only heuristic approaches to obtain it have been considered. This paper presents W-SPSA in a formally comprehensive way, where effectively SPSA becomes an instance of W-SPSA, and explores alternative approaches for determining the matrix W. We demonstrate that, relying on a few simplifications that marginally affect the final solution, we can obtain W matrices that considerably outperform SPSA. We analyse the performance of our proposed algorithm in two applications in motorway networks in Singapore and Portugal, using a dynamic traffic assignment model and a microscopic traffic simulator, respectively.}
}
@article{GAGLIARDI2015357,
title = {Material data matter — Standard data format for engineering materials},
journal = {Technological Forecasting and Social Change},
volume = {101},
pages = {357-365},
year = {2015},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0040162515002772},
author = {Dimitri Gagliardi},
keywords = {Innovation in materials data models, Standardisation, Materials data, Engineering materials},
abstract = {Standardisation is often linked to the very existence of an industry and its evolutionary dynamics especially in sectors where activities are particularly reliant on network economies. This paper investigates the interests of the stakeholders of the materials industry in the effort of setting the basis for standard-compliant formats for engineering materials test data used in materials test certificates (TCs). Test certificates provide a guarantee of origin, characteristics and materials specifications. A transition to standardised electronic-test certificates (E-TCs) is thought to offer advantages over the static alternatives currently in use. This work highlights that the stakeholders involved in the process have a general need for a standard compliant data model yet their particular interests might not be completely aligned. The choice of an open, non-proprietary, data format and compatibility of the standard with legacy technological solutions prove to be critical for the implementation of standardised E-TCs.}
}
@article{DONG201458,
title = {Mobile agent-based energy-aware and user-centric data collection in wireless sensor networks},
journal = {Computer Networks},
volume = {74},
pages = {58-70},
year = {2014},
note = {Special Issue on Mobile Computing for Content/Service-Oriented Networking Architecture},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2014.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1389128614003156},
author = {Mianxiong Dong and Kaoru Ota and Laurence T. Yang and Shan Chang and Hongzi Zhu and Zhenyu Zhou},
keywords = {Wireless sensor network, Mobile agent, Mobile server, Data gathering},
abstract = {We design an efficient data-gathering system in wireless sensor networks (WSNs) with mobile agents to achieve energy- and time-efficient collection as well as intelligent monitoring to adapt to the numerous demands of users. We first consider a data-gathering system called MAMS where mobile agents (MAs) and a mobile server (MS) collaboratively collect data. MAs collect data over the WSN and intelligently return this to the MS. We then develop dynamic itinerary planning approach for an MA (DIPMA) to find an optimal itinerary that provides more flexible services using widespread WSNs to users. We focus on two key challenges: (1) developing a new data-searching mechanism for making an MA’s itinerary under specified requirements and (2) designing data structures with minimal information stored in sensor nodes, where an MA decides on the next destination based on the information. We validate the proposed solutions by simulation experiments and show DIPMA outperforms the random migration of MAs in terms of execution time by considering the search accuracy of nodes that detect events.}
}
@incollection{ABRAMOVICI2016159,
title = {Chapter 10 - Epidemiology of epilepsy},
editor = {Michael J. Aminoff and François Boller and Dick F. Swaab},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {138},
pages = {159-171},
year = {2016},
booktitle = {Neuroepidemiology},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-12-802973-2.00010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029732000100},
author = {S. Abramovici and A. Bagić},
keywords = {epilepsy, epidemiology, etiology, incidence, prevalence, prevention, refractory epilepsy, seizure, stigma, surveillance},
abstract = {Modern epidemiology of epilepsy maximizes the benefits of advanced diagnostic methods and sophisticated techniques for case ascertainment in order to increase the diagnostic accuracy and representativeness of the cases and cohorts studied, resulting in better comparability of similarly performed studies. Overall, these advanced epidemiologic methods are expected to yield a better understanding of diverse risk factors, high-risk populations, seizure triggers, multiple and poorly understood causes of epilepsy, including the increasing and complex role of genetics, and establish the natural course of treated and untreated epilepsy and syndromes – all of which form the foundation of an attempt to prevent epileptogenesis as the primary prophylaxis of epilepsy. Although data collection continues to improve, epidemiologists still need to overcome definition and coding variability, insufficient documentation, as well as the interplay of socioeconomic factors and stigma. As most of the 65–70 million people with epilepsy live outside of resource-rich countries, extensive underdiagnosis, misdiagnosis, and undertreatment are likely. Epidemiology will continue to provide the necessary information to the medical community, public, and regulators as the foundation for improved health policies, targeted education, and advanced measures of prevention and prognostication of the most common severe brain disorder.}
}
@article{VANDERHOORN201521,
title = {A multi-layered risk exposure assessment approach for the shipping industry},
journal = {Transportation Research Part A: Policy and Practice},
volume = {78},
pages = {21-33},
year = {2015},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2015.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0965856415001184},
author = {Stephen {Vander Hoorn} and Sabine Knapp},
keywords = {Total risk exposure, Binary logistic regression, Spatial statistics, Incident models, Uncertainties, Monetary value at risk},
abstract = {Maritime administration and coastal states have become more aware of the need to enhance risk mitigation strategies primarily due to increased worldwide shipping activities, changing safety qualities of the world fleet and limited resources to deploy mitigation strategies. This paper introduces an innovative multi-layered framework to assess, predict and mitigate potential harm. The proposed approach addresses known restrictions of risk assessments in shipping. These restrictions are the lack of scalability to apply risk assessments over large areas using an automated routine, the absence of recognizing that the world fleet is heterogeneous, the lack of integrating location specific environmental conditions such as wind, currents or waves and most importantly, the lack of recognizing the uncertainties associated with each factor especially for predictions. The proposed framework is based on the idea of integrating various layers representing the most important factors that can influence risk in order to estimate and predict risk exposure for a given area. As proof of concept of the underlying ideas, the outcome of a pilot project with the Australian Maritime Safety Authority is presented which demonstrates the integration of the first two layers and is based on a unique and comprehensive combination of data. The results of selected endpoints of risk exposure compare well with observations. The article also discusses the integration of the remaining layers including the recognition and addition of uncertainties in the future.}
}
@article{STEENBRUGGEN201386,
title = {Data from telecommunication networks for incident management: An exploratory review on transport safety and security},
journal = {Transport Policy},
volume = {28},
pages = {86-102},
year = {2013},
note = {Special Issue on Transportation Pricing Policies Special Issue on Transport Security - Policies and Empirical Perspectives},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2012.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X12001400},
author = {John Steenbruggen and Maria Teresa Borzacchiello and Peter Nijkamp and Henk Scholten},
keywords = {Incident management (IM), Mobile phones, Telecommunication network, Situation awareness, Transport security, Transport safety},
abstract = {Problems such as traffic congestion and environmental sustainability are forcing us to review our long-term plans for transport, whose aim should be to develop and improve safety, security and effectiveness of the transportation systems. The consequences of traffic accidents are not only limited to road travellers (congestion, delays), but may also affect the area surrounding the incidents, for instance, the release of chemical substances. The lack of a real-time assessment of the mobility consequences of an incident, as well as of its wider consequences for the surrounding area, in terms of security and safety, hampers the decision-makers ability to respond effectively to an incident and to manage its consequences. After recognizing the great potential of electronic data in supporting traffic management and interpreting space–time geography, the paper focuses on the use of data obtained from cellular-phone networks in supporting incident management, with particular attention to transport safety and security. The literature review, coupled with the description of some illustrations of applications in the city of Amsterdam, allows highlighting the advantages and limitations of this technology in the field of transport safety and security.}
}
@incollection{PALSHIKAR2014577,
title = {Chapter 36 - Financial Security against Money Laundering: A Survey},
editor = {Babak Akhgar and Hamid R. Arabnia},
booktitle = {Emerging Trends in ICT Security},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {577-590},
year = {2014},
isbn = {978-0-12-411474-6},
doi = {https://doi.org/10.1016/B978-0-12-411474-6.00036-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114746000360},
author = {Girish Keshav Palshikar and Manoj Apte},
keywords = {anti-money laundering, financial security, underground economy, data mining},
abstract = {Money laundering (ML) is a serious problem for the economies and financial institutions around the world. Financial institutions get used by organized criminals and terrorists as vehicles of large-scale money laundering, which presents the institutions with challenges of regulatory compliance, maintaining financial security, preserving goodwill and reputation, and avoiding operational risks like liquidity crunch and lawsuits. Hence prevention, detection, and control of ML is crucial for the financial security and risk management of financial institutions. Realizing the gravity of ML, various nations have started anti-ML (AML) activities, along with cooperative international efforts, including Financial Action Task Force, Egmont Group and Wolfsberg Group. This chapter begins with an overview of ML, discusses commonly used methods of ML, and the anti-ML efforts worldwide. After surveying some analytics techniques used to estimate the extent of ML, some data-mining techniques reported in the literature for detection of ML episodes (instances) are surveyed.}
}
@incollection{VERNOOIJ201669,
title = {Chapter 5 - Population imaging in neuroepidemiology},
editor = {Michael J. Aminoff and François Boller and Dick F. Swaab},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {138},
pages = {69-90},
year = {2016},
booktitle = {Neuroepidemiology},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-12-802973-2.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029732000057},
author = {M.W. Vernooij and M. {de Groot} and D. Bos},
keywords = {epidemiology, population, MRI, CT, imaging, quantification, aging, stroke, dementia},
abstract = {Neuroepidemiologic studies have traditionally focused on studying associations between determinants and neurologic outcomes, while treating the pathway in between both as a “black box.” With the rise of noninvasive, advanced neuroimaging techniques, it has become possible to directly study brain changes occurring in this “black box.” This importantly aids to unravel disease pathways, find new markers of disease, or identify subjects at risk of disease. Imaging in neuroepidemiologic studies is also called population neuroimaging. This chapter discusses the rationale of population neuroimaging, the different imaging modalities that can be applied, and the various ways to extract visual or quantitative information from these images. Population neuroimaging is a fast-progressing field, partly due to new techniques and partly due to the growing need for collaboration, harmonization, and standardization among studies. Considerations for future applications of imaging in neuroepidemiology are discussed against this background.}
}
@incollection{SUN2016147,
title = {Chapter Three - Integrative Analysis of Multi-omics Data for Discovery and Functional Studies of Complex Human Diseases},
editor = {Theodore Friedmann and Jay C. Dunlap and Stephen F. Goodwin},
series = {Advances in Genetics},
publisher = {Academic Press},
volume = {93},
pages = {147-190},
year = {2016},
issn = {0065-2660},
doi = {https://doi.org/10.1016/bs.adgen.2015.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0065266015000516},
author = {Yan V. Sun and Yi-Juan Hu},
keywords = {DNA methylation, Epigenome, Gene expression, Genomic epidemiology, GWAS, Integrative genomics, Metabolome, Proteome, Quantitative trait loci, Transcriptome},
abstract = {Complex and dynamic networks of molecules are involved in human diseases. High-throughput technologies enable omics studies interrogating thousands to millions of makers with similar biochemical properties (eg, transcriptomics for RNA transcripts). However, a single layer of “omics” can only provide limited insights into the biological mechanisms of a disease. In the case of genome-wide association studies, although thousands of single nucleotide polymorphisms have been identified for complex diseases and traits, the functional implications and mechanisms of the associated loci are largely unknown. Additionally, the genomic variants alone are not able to explain the changing disease risk across the life span. DNA, RNA, protein, and metabolite often have complementary roles to jointly perform a certain biological function. Such complementary effects and synergistic interactions between omic layers in the life course can only be captured by integrative study of multiple molecular layers. Building upon the success in single-omics discovery research, population studies started adopting the multi-omics approach to better understanding the molecular function and disease etiology. Multi-omics approaches integrate data obtained from different omic levels to understand their interrelation and combined influence on the disease processes. Here, we summarize major omics approaches available in population research, and review integrative approaches and methodologies interrogating multiple omic layers, which enhance the gene discovery and functional analysis of human diseases. We seek to provide analytical recommendations for different types of multi-omics data and study designs to guide the emerging multi-omic research, and to suggest improvement of the existing analytical methods.}
}
@incollection{2013415,
title = {Subject Index},
editor = {Ali Hurson},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {90},
pages = {415-427},
year = {2013},
booktitle = {Connected Computing Environment},
issn = {0065-2458},
doi = {https://doi.org/10.1016/B978-0-12-408091-1.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080911000130}
}
@article{RUSS2016328,
title = {The probable foundations of sustainabilism: Information, energy and entropy based definition of capital, Homo Sustainabiliticus and the need for a “new gold”},
journal = {Ecological Economics},
volume = {130},
pages = {328-338},
year = {2016},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2016.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921800916304827},
author = {Meir Russ},
keywords = {New networked knowledge-based global economy, Intellectual capital, Human capital, Entropy, , “New gold”, Sustainabilism},
abstract = {This conceptual, interdisciplinary paper will start with an introduction to the new-networked knowledge-based global economy and the importance of intellectual and, specifically, human, capital. Next, an advanced definition of human and other forms of capital using information, energy and entropy will be introduced. This will be followed by a discussion of the premises framing the study of economics and will focus on the role of law in the economy. Afterwards, the paper will suggest the addition of a new model of humans that should serve as the base for the concept of law, the homo sustainabiliticus. Ensuing this discussion and consistent with the newly proposed definition of capital, a proposal for a new currency (“new gold”) will be offered. This proposal suggests viewing usable, renewable energy, knowledge and data as the most important assets for the 21st century and is seen as the building block for the new sustainabilistic economy.}
}
@article{YAN2016201,
title = {Enhancing quality of statistic monitoring models by training set design with active learning approach},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {151},
pages = {201-218},
year = {2016},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2015.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169743915003317},
author = {Zhengbing Yan and Junghui Chen},
keywords = {Active learning, Gaussian process model, Latent structure statistical model, Process monitoring},
abstract = {The objective of this paper is to enhance the quality of the process monitoring models by designing a training set through an active learning approach. Although conventional process monitoring models are effective in many manufacturing processes, these models falter when confronted by a set of training data with poor quality or a small volume of training data. As the limitations of the monitoring models become increasingly obvious in face of even more complex manufacturing processes, in this work, the active learning process monitoring (AL-PM) model is developed. To design a good training set, Gaussian process (GP) models are first used to construct the relationships between the score variables of the latent structure model and the designable process variables because the GP model is capable of providing the accurate predictive mean and variance. The variance can quantify its prediction uncertainty. Second, the uncertainty index is presented and utilized to adequately explore for which regions the new data samples should be used to enhance the quality of the monitoring model. The proposed AL-PM model can be applied to any types of latent structure-based monitoring models. Its effectiveness and promising results have been demonstrated by its applications to a numerical example and a penicillin benchmark process.}
}
@incollection{TALBURT201531,
title = {Chapter 3 - A Deep Dive into the Capture Phase},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {31-51},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800537800003X},
author = {John R. Talburt and Yinle Zhou},
keywords = {Data profiling, data matching, benchmarking, truth sets, review indicators},
abstract = {This chapter describes the start of the CSRUD Life Cycle with initial capture and storage of entity identity information. It also discusses the importance of understanding the characteristics of the data, properly preparing the data, selecting identity attributes, and coming up with matching strategies. Perhaps most importantly, it discusses the methods and techniques for evaluating ER outcomes.}
}
@article{FAHLGREN20151520,
title = {A Versatile Phenotyping System and Analytics Platform Reveals Diverse Temporal Responses to Water Availability in Setaria},
journal = {Molecular Plant},
volume = {8},
number = {10},
pages = {1520-1535},
year = {2015},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2015.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1674205215002683},
author = {Noah Fahlgren and Maximilian Feldman and Malia A. Gehan and Melinda S. Wilson and Christine Shyu and Douglas W. Bryant and Steven T. Hill and Colton J. McEntee and Sankalpi N. Warnasooriya and Indrajit Kumar and Tracy Ficor and Stephanie Turnipseed and Kerrigan B. Gilbert and Thomas P. Brutnell and James C. Carrington and Todd C. Mockler and Ivan Baxter},
keywords = {abiotic/environmental stress, water relations, bioinformatics, development, phenotyping},
abstract = {Phenotyping has become the rate-limiting step in using large-scale genomic data to understand and improve agricultural crops. Here, the Bellwether Phenotyping Platform for controlled-environment plant growth and automated multimodal phenotyping is described. The system has capacity for 1140 plants, which pass daily through stations to record fluorescence, near-infrared, and visible images. Plant Computer Vision (PlantCV) was developed as open-source, hardware platform-independent software for quantitative image analysis. In a 4-week experiment, wild Setaria viridis and domesticated Setaria italica had fundamentally different temporal responses to water availability. While both lines produced similar levels of biomass under limited water conditions, Setaria viridis maintained the same water-use efficiency under water replete conditions, while Setaria italica shifted to less efficient growth. Overall, the Bellwether Phenotyping Platform and PlantCV software detected significant effects of genotype and environment on height, biomass, water-use efficiency, color, plant architecture, and tissue water status traits. All ∼79 000 images acquired during the course of the experiment are publicly available.}
}
@incollection{2016463,
title = {Index},
editor = {Daniel Vallero},
booktitle = {Translating Diverse Environmental Data into Reliable Information},
publisher = {Academic Press},
pages = {463-486},
year = {2016},
isbn = {978-0-12-812446-8},
doi = {https://doi.org/10.1016/B978-0-12-812446-8.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124468180016}
}
@article{SULLIVAN20092282,
title = {eBird: A citizen-based bird observation network in the biological sciences},
journal = {Biological Conservation},
volume = {142},
number = {10},
pages = {2282-2292},
year = {2009},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2009.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S000632070900216X},
author = {Brian L. Sullivan and Christopher L. Wood and Marshall J. Iliff and Rick E. Bonney and Daniel Fink and Steve Kelling},
keywords = {eBird, Citizen-science, Observation network, Scale-dependent analysis, Bird observations},
abstract = {New technologies are rapidly changing the way we collect, archive, analyze, and share scientific data. For example, over the next several years it is estimated that more than one billion autonomous sensors will be deployed over large spatial and temporal scales, and will gather vast quantities of data. Networks of human observers play a major role in gathering scientific data, and whether in astronomy, meteorology, or observations of nature, they continue to contribute significantly. In this paper we present an innovative use of the Internet and information technologies that better enhances the opportunity for citizens to contribute their observations to science and the conservation of bird populations. eBird is building a web-enabled community of bird watchers who collect, manage, and store their observations in a globally accessible unified database. Through its development as a tool that addresses the needs of the birding community, eBird sustains and grows participation. Birders, scientists, and conservationists are using eBird data worldwide to better understand avian biological patterns and the environmental and anthropogenic factors that influence them. Developing and shaping this network over time, eBird has created a near real-time avian data resource producing millions of observations per year.}
}
@incollection{HOLMBERG2016105,
title = {3 - The Future},
editor = {Kim Holmberg},
booktitle = {Altmetrics for Information Professionals},
publisher = {Chandos Publishing},
pages = {105-129},
year = {2016},
isbn = {978-0-08-100273-5},
doi = {https://doi.org/10.1016/B978-0-08-100273-5.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100273500003X},
author = {Kim Holmberg},
keywords = {altmetrics, impact, reach, measurement, indicator, metrics, social media, social media metrics, social media analysis, standards, gaming, manipulation, future, future research, critique},
abstract = {The third and final part of the book will start with a discussion about how altmetrics are created partly by researchers and partly by the public, how altmetrics can be detected and collected, and how they can be used to measure reach and impact. This final part of the book will envision possible directions of how to measure impact and how the indication of different levels of impact could be used in altmetrics. This part will continue by presenting some of the concerns with altmetrics. A possible concern related to altmetrics that will be discussed in this book is the possible unintentional and intentional gaming of the metrics that may occur and that may have a significant impact on the results from any analysis where these “alternative” metrics are being used. The book will end with a discussion about future trends in altmetric research and some possible directions where the area may develop.}
}
@article{SAHOO201621,
title = {Insight: An ontology-based integrated database and analysis platform for epilepsy self-management research},
journal = {International Journal of Medical Informatics},
volume = {94},
pages = {21-30},
year = {2016},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616301381},
author = {Satya S. Sahoo and Priya Ramesh and Elisabeth Welter and Ashley Bukach and Joshua Valdez and Curtis Tatsuoka and Yvan Bamps and Shelley Stoll and Barbara C. Jobst and Martha Sajatovic},
abstract = {We present Insight as an integrated database and analysis platform for epilepsy self-management research as part of the national Managing Epilepsy Well Network. Insight is the only available informatics platform for accessing and analyzing integrated data from multiple epilepsy self-management research studies with several new data management features and user-friendly functionalities. The features of Insight include, (1) use of Common Data Elements defined by members of the research community and an epilepsy domain ontology for data integration and querying, (2) visualization tools to support real time exploration of data distribution across research studies, and (3) an interactive visual query interface for provenance-enabled research cohort identification. The Insight platform contains data from five completed epilepsy self-management research studies covering various categories of data, including depression, quality of life, seizure frequency, and socioeconomic information. The data represents over 400 participants with 7552 data points. The Insight data exploration and cohort identification query interface has been developed using Ruby on Rails Web technology and open source Web Ontology Language Application Programming Interface to support ontology-based reasoning. We have developed an efficient ontology management module that automatically updates the ontology mappings each time a new version of the Epilepsy and Seizure Ontology is released. The Insight platform features a Role-based Access Control module to authenticate and effectively manage user access to different research studies. User access to Insight is managed by the Managing Epilepsy Well Network database steering committee consisting of representatives of all current collaborating centers of the Managing Epilepsy Well Network. New research studies are being continuously added to the Insight database and the size as well as the unique coverage of the dataset allows investigators to conduct aggregate data analysis that will inform the next generation of epilepsy self-management studies.}
}
@article{IONITA200613,
title = {Mapping Tumor-Suppressor Genes with Multipoint Statistics from Copy-Number–Variation Data},
journal = {The American Journal of Human Genetics},
volume = {79},
number = {1},
pages = {13-22},
year = {2006},
issn = {0002-9297},
doi = {https://doi.org/10.1086/504354},
url = {https://www.sciencedirect.com/science/article/pii/S0002929707600044},
author = {Iuliana Ionita and Raoul-Sam Daruwala and Bud Mishra},
abstract = {Array-based comparative genomic hybridization (arrayCGH) is a microarray-based comparative genomic hybridization technique that has been used to compare tumor genomes with normal genomes, thus providing rapid genomic assays of tumor genomes in terms of copy-number variations of those chromosomal segments that have been gained or lost. When properly interpreted, these assays are likely to shed important light on genes and mechanisms involved in the initiation and progression of cancer. Specifically, chromosomal segments, deleted in one or both copies of the diploid genomes of a group of patients with cancer, point to locations of tumor-suppressor genes (TSGs) implicated in the cancer. In this study, we focused on automatic methods for reliable detection of such genes and their locations, and we devised an efficient statistical algorithm to map TSGs, using a novel multipoint statistical score function. The proposed algorithm estimates the location of TSGs by analyzing segmental deletions (hemi- or homozygous) in the genomes of patients with cancer and the spatial relation of the deleted segments to any specific genomic interval. The algorithm assigns, to an interval of consecutive probes, a multipoint score that parsimoniously captures the underlying biology. It also computes a P value for every putative TSG by using concepts from the theory of scan statistics. Furthermore, it can identify smaller sets of predictive probes that can be used as biomarkers for diagnosis and therapeutics. We validated our method using different simulated artificial data sets and one real data set, and we report encouraging results. We discuss how, with suitable modifications to the underlying statistical model, this algorithm can be applied generally to a wider class of problems (e.g., detection of oncogenes).}
}
@incollection{LAURENT2015137,
title = {4 - Privacy Management and Protection of Personal Data},
editor = {Maryline Laurent and Samia Bouzefrane},
booktitle = {Digital Identity Management},
publisher = {Elsevier},
pages = {137-205},
year = {2015},
isbn = {978-1-78548-004-1},
doi = {https://doi.org/10.1016/B978-1-78548-004-1.50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480041500043},
author = {Maryline Laurent and Claire Levallois-Barth},
keywords = {Ambient intelligence, Anonymity.online (AN.ON), APPEL languages and ontological requirements, Application program interfaces (APIs), Electronic product code (EPC), The onion router (TOR), Radio frequency identification (RFID), Social network services (SNS), Traffic anonymity techniques},
abstract = {In this chapter, we will give detailed consideration to the ways in which the use of technology can infringe on privacy. We will consider technologies that have been around for the last decade revolutionizing the way in which we buy, communicate, contribute and obtain information, and emerging technologies, which use communicating objects (radio frequency identification (RFID), sensors, smartdust, etc.) to provide future solutions for facilitating everyday life. This development creates issues concerning the application of legal provisions, particularly concerning the collection, use and transmission of personal data. The existing rules, defined in the late 1970s, serve to defend the privacy of users, alongside other fundamental rights and liberties, including freedom of movement and self-determination. This legal framework is applicable to all technologies, whether they concern direct identification data (names and surnames) or indirect identification elements (biometric elements, DNA).}
}
@article{JAZAYERI2014219,
title = {A geometric and semantic evaluation of 3D data sourcing methods for land and property information},
journal = {Land Use Policy},
volume = {36},
pages = {219-230},
year = {2014},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2013.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0264837713001543},
author = {Ida Jazayeri and Abbas Rajabifard and Mohsen Kalantari},
keywords = {3D cadastre, Land administration, Building reconstruction, UAS, Photogrammetry},
abstract = {Urbanisation, the development of high-rise apartments and the advent of complex building structures creates unique challenges that cannot be met by 2D land and property information. These include inter-related titles and complex plans relating to (i) the land parcel and (ii) the building, both internal (indoor plans) and external attributes (roof and façade). Incorporating the third dimension into the land development cycle can potentially address such challenges by providing data that describes both the land parcel and building in 3D. This move towards 3D data administration requires the development of new 3D data processes, including 3D data sourcing, which forms the focus of this research. Following an examination of current 2D methods in land and property information registration, a framework of the requirements for sourcing 3D land and property information is suggested and potential methods are discussed. Focussing on the geometric and semantic components an evaluation of the methods is developed and applied. The results highlight methods based on photogrammetry, laser scanning, mobile mapping, Unmanned Aerial Systems (UAS) and Building Information Modelling (BIM) to source integrated 3D information for both the internal and external attributes of a building and corresponding land parcel.}
}
@article{CHANG2016167,
title = {Organisational sustainability modelling—An emerging service and analytics model for evaluating Cloud Computing adoption with two case studies},
journal = {International Journal of Information Management},
volume = {36},
number = {1},
pages = {167-179},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2015.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0268401215000882},
author = {Victor Chang and Robert John Walters and Gary Brian Wills},
keywords = {Organisational sustainability modelling (OSM), Cloud Computing, Emerging Services and Analytics, OSM case studies},
abstract = {Cloud Computing is an emerging technology which promises to bring with it great benefits to all types of computing activities including business support. However, the full commitment to Cloud Computing necessary to gain the full benefit is a major project for any organisation, since it necessitates adoption of new business processes and attitudes to computing services in addition to the immediately obvious systems changes. Hence the evaluation of a Cloud Computing project needs to consider the balance of benefits and risks to the organisation in the full context of the environment in which it operates; it is not sufficient or appropriate to examine technical considerations alone. In this paper, we consider the application of CAPM, a well established approach used for the analysis of risks and benefits of commercial projects to Cloud adoption projects and propose a revised and improved technique, OSM. To support the validity of OSM, two full case studies are presented. In the first, we describe an application of the approach to the iSolutions Group at University of Southampton, which focuses on evaluations of Cloud Computing service improvement. We then illustrate the use of OSM for measuring learning satisfaction of two cohort groups at the University of Greenwich. The results confirm the advantages of using OSM. We conclude that OSM can analyse the risk and return status of Cloud Computing services and help organisations that adopt Cloud Computing to evaluate and review their Cloud Computing projects and services. OSM is an emerging service and analytics model supported by several case studies.}
}
@article{KSHETRI20161274,
title = {Creation, deployment, diffusion and export of Sub-Saharan Africa-originated information technology-related innovations},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part B},
pages = {1274-1287},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216302377},
author = {Nir Kshetri},
keywords = {Information technology-related innovations, iCow, Large and small scale innovations, mPedigree, Sub-Saharan Africa},
abstract = {A number of high profile innovations have been created in Sub-Saharan Africa. In this paper, we examine the mechanisms associated with the development, deployment, diffusion, and export of SSA-originated innovations. The paper gives special consideration to the relative roles and contributions of local and outside resources in the creation and deployment of innovations in SSA economies. A key focus of the article is on the roles of local infrastructural facilities, systems and services in affecting the diffusion of SSA-originated innovations. Also discussed are the features of SSA-originated innovations that explain the diffusion rates. It provides a detailed analysis and description of the key characteristics of SSA-originated innovations that can increase the possibility of being internationalized or exported to other countries. Finally, we analyze how such mechanisms vary across large and small scale innovations.}
}
@article{YEO20011134,
title = {An adaptive protocol for real-time fax communications over Internet},
journal = {Computer Communications},
volume = {24},
number = {12},
pages = {1134-1146},
year = {2001},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(00)00342-X},
url = {https://www.sciencedirect.com/science/article/pii/S014036640000342X},
author = {C.K Yeo and S.C Hui and I.Y Soon and B.S Lee},
keywords = {Internet faxing, Adaptive fax communication, Real-time transmission},
abstract = {Internet faxing allows users from different locations to exchange fax documents without incurring the high costs associated with international toll charges. This paper proposes a protocol, Adaptive Internet Fax Protocol (AIFP) which provides a basic framework for Internet fax transmission. It comprises three secondary protocols, which take care of issues like capability exchange, change of data socket, network load feedback and data transfer. AIFP is able to facilitate real-time fax transmission over unreliable Internet and make delivery confirmation possible. In a heavy jitter network, AIFP is able to provide at least session mode transmission albeit not real time.}
}
@article{TORRES2015198,
title = {Whither the U.S. National Ocean Policy Implementation Plan?},
journal = {Marine Policy},
volume = {53},
pages = {198-212},
year = {2015},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2014.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X14003145},
author = {H. Torres and F. Muller-Karger and D. Keys and H. Thornton and M. Luther and K. Alsharif},
keywords = {Ocean, Policy, Coastal, Marine, Implementation plan, United States},
abstract = {The need for a statutory framework to manage valuable marine resources in the United States is highlighted by problems such as fragmented ocean governance and increasing conflict over the use of ocean spaces. On July 19, 2010 President Obama signed Executive Order 13547 to create a National Ocean Policy (NOP) for the United States. A subsequent Implementation Plan, released in 2013, set up hundreds of actions to be accomplished between 2013 and 2025 to address economic, community, scientific and other issues. Progress implementing the NOP appears to have stalled. The purpose of this paper is to give an overview of the NOP and its Implementation Plan, and then discuss what needs to be done to bring the vision it set forth to fruition.}
}
@article{HALL2015126,
title = {Cardinality constraints on qualitatively uncertain data},
journal = {Data & Knowledge Engineering},
volume = {99},
pages = {126-150},
year = {2015},
note = {Selected Papers from the 33rd International Conference on Conceptual Modeling (ER 2014)},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2015.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1500035X},
author = {Neil Hall and Henning Koehler and Sebastian Link and Henri Prade and Xiaofang Zhou},
keywords = {Data and knowledge visualization, Data models, Database semantics, Management of integrity constraints, Requirements engineering},
abstract = {Modern applications require advanced techniques and tools to process large volumes of uncertain data. For that purpose we introduce cardinality constraints as a principled tool to control the occurrences of uncertain data. Uncertainty is modeled qualitatively by assigning to each object a degree of possibility by which the object occurs in an uncertain instance. Cardinality constraints are assigned a degree of certainty that stipulates on which objects they hold. Our framework empowers users to model uncertainty in an intuitive way, without the requirement to put a precise value on it. Our class of cardinality constraints enjoys a natural possible world semantics, which is exploited to establish several tools to reason about them. We characterize the associated implication problem axiomatically and algorithmically in linear input time. Furthermore, we show how to visualize any given set of our cardinality constraints in the form of an Armstrong sketch. Even though the problem of finding an Armstrong sketch is precisely exponential, our algorithm computes a sketch with conservative use of time and space. Data engineers may therefore compute Armstrong sketches that they can jointly inspect with domain experts in order to consolidate the set of cardinality constraints meaningful for a given application domain.}
}
@article{VERSLYPPE201442,
title = {StrainInfo introduces electronic passports for microorganisms},
journal = {Systematic and Applied Microbiology},
volume = {37},
number = {1},
pages = {42-50},
year = {2014},
issn = {0723-2020},
doi = {https://doi.org/10.1016/j.syapm.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0723202013001872},
author = {Bert Verslyppe and Wim {De Smet} and Bernard {De Baets} and Paul {De Vos} and Peter Dawyndt},
keywords = {StrainInfo, Strain passport, Strain browser, Biological resource center, Semantic integration},
abstract = {Microbiology builds upon biological material deposited in biological resource centers (BRCs) as a reference framework for collaborative research. BRCs assign so-called strain numbers to label the deposited material and are responsible for long-term preservation and worldwide distribution of the material. Cultured microorganisms can be deposited into multiple BRCs and BRCs also mutually exchange their holdings. As a result, many different strain numbers can be attached to biological material that stems from the same isolate. In practice, this material is considered equivalent and used interchangeably. This implies that finding information on given biological material requires all equivalent strain numbers to be used when searching. StrainInfo introduces strain passports for microorganisms: a uniform overview of information known about a given microbial strain. It contains all known equivalent strain numbers and information on the exchange history, sequences and related literature of the strain. Each passport has an associated strain browser that gives direct access to the underlying BRC catalog entries on which the passport was based. Taxon, sequence and literature passports are implemented in a similar manner. In addition to web pages that serve human users, integrated information is also offered in machine readable formats useful for automated, large-scale analysis. StrainInfo is envisioned to be an open platform integrating microbial information. This platform can form the basis for new methods of microbiological research, leveraging the vast amount of electronic information available online. StrainInfo is available from http://www.StrainInfo.net.}
}
@article{PONS2014243,
title = {Automatic and improved radiometric correction of Landsat imagery using reference values from MODIS surface reflectance images},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {33},
pages = {243-254},
year = {2014},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303243414001354},
author = {X. Pons and L. Pesquer and J. Cristóbal and O. González-Guerrero},
keywords = {Radiometric correction, Landsat, MODIS, Pseudoinvariant area},
abstract = {Radiometric correction is a prerequisite for generating high-quality scientific data, making it possible to discriminate between product artefacts and real changes in Earth processes as well as accurately produce land cover maps and detect changes. This work contributes to the automatic generation of surface reflectance products for Landsat satellite series. Surface reflectances are generated by a new approach developed from a previous simplified radiometric (atmospheric+topographic) correction model. The proposed model keeps the core of the old model (incidence angles and cast-shadows through a digital elevation model [DEM], Earth–Sun distance, etc.) and adds new characteristics to enhance and automatize ground reflectance retrieval. The new model includes the following new features: (1) A fitting model based on reference values from pseudoinvariant areas that have been automatically extracted from existing reflectance products (Terra MODIS MOD09GA) that were selected also automatically by applying quality criteria that include a geostatistical pattern model. This guarantees the consistency of the internal and external series, making it unnecessary to provide extra atmospheric data for the acquisition date and time, dark objects or dense vegetation. (2) A spatial model for atmospheric optical depth that uses detailed DEM and MODTRAN simulations. (3) It is designed so that large time-series of images can be processed automatically to produce consistent Landsat surface reflectance time-series. (4) The approach can handle most images, acquired now or in the past, regardless of the processing system, with the exception of those with extremely high cloud coverage. The new methodology has been successfully applied to a series of near 300 images of the same area including MSS, TM and ETM+ imagery as well as to different formats and processing systems (LPGS and NLAPS from the USGS; CEOS from ESA) for different degrees of cloud coverage (up to 60%) and SLC-off. Reflectance products have been validated with some example applications: time series robustness (for a pixel in a pseudoinvariant area, deviations are only 1.04% on average along the series), spectral signatures generation (visually coherent with the MODIS ones, but more similar between dates), and classification (up to 4 percent points better than those obtained with the original manual method or the CDR products). In conclusion, this new approach, that could also be applied to other sensors with similar band configurations, offers a fully automatic and reasonably good procedure for the new era of long time-series of spatially detailed global remote sensing data.}
}
@article{LAFORSOWCZYNIK2016433,
title = {Monitoring migrants or making migrants ‘misfit’? Data protection and human rights perspectives on Dutch identity management practices regarding migrants},
journal = {Computer Law & Security Review},
volume = {32},
number = {3},
pages = {433-449},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916300243},
author = {Karolina {La Fors-Owczynik}},
keywords = {Migrants, Data protection, Human rights, Risk, Biometrics, Identification, Profiling, Immigration, Border control, Law enforcement},
abstract = {Record numbers of migrants and refugees fleeing violence and poverty in parts of Africa and the Middle East present the European Union with unprecedented challenges, including in determining their identity as well as status. In recent years problems of identifying immigrants have been addressed in order to fight identity fraud and illegal entry of migrants. As a result, a wide variety of digital systems have been introduced to orchestrate an effective, preventative modus of identification of migrants. Digital systems are in particular geared towards spotting those migrants who (are about to) commit identity fraud or who enter the territory of EU member states illegally. Although the key aim of the digital systems is framed to protect the administrative, geographic and legal borders of the member state and the safety of its population, empirically based studies demonstrate that these systems bring new risks for migrants themselves. This article intends to contribute to the discussion on the use of digital systems for managing the movement of migrants by analysing identification and risk assessment systems from the perspective of the new European data protection regime and the European Convention on Human Rights. For this purpose, two identification systems – the so-called INS console within the Dutch immigration and border sector, and the PROGIS console within the law enforcement sector – are analysed. A third is the Advanced Passenger Information system operated at Schiphol Airport by border control and immigration services. Against the background of the position of many migrants finding themselves at risk in their home country and of the two legislative frameworks mentioned above, this article addresses two issues. First, the analysis focuses on how migrants are perceived by digital monitoring practices: are they themselves at risk, non-risk or do they pose a risk? In the EU, migrants must prove that their case is worthy of asylum status because they are ‘at risk’ from political unrest or other life-threatening circumstances in their home country. Yet, empirical data gathered through semi-structured interviews show that simply abiding by the standards during an enrolment process of the INS console, rather than being ‘at risk’, a migrant can easily be categorised as ‘posing a risk’ (La Fors-Owczynik & Van der Ploeg, 2015). Second, this article aims to investigate what the capacity of the new data protection regime is in protecting migrants from being framed as ‘a risk’ or a ‘misfit’ stemming from the use of digital systems. Given this second aim, the following discussion also intends to explore the extent to which the European Convention on Human Rights can provide an additional legal remedy for migrants being digitally categorised in a manner that is detrimental to them.}
}
@incollection{SHEIKH2013147,
title = {Chapter 9 - Analytics Implementation Methodology},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {147-165},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000098},
author = {Nauman Sheikh},
keywords = {analytics governance, ETL, automated decisions, decision strategies, life cycle, waterfall, analytics datamart, decision variables, analytics variables, characteristics, FICO, Experian, audit and control, syntactic and semantic profiling},
abstract = {This chapter provides an overview of the analytics project methodology that needs to be followed to enable the successful implementation of an analytics project. Consistent with the theme throughout this book, analytics projects are not a one-time activity, rather a new business lifestyle, and as more and more processes get innovative and customized to the customer, product, and operational needs, analytics-driven impetus into those processes will be a key requirement. Therefore, analytics projects have to be built and managed with a structured and methodical approach that is reliable and repeatable. The reliability is far more important for analytics since daily business decisions are made relying on analytics output.}
}
@article{SANTORO201618,
title = {Contributing to the GEO Model Web implementation: A brokering service for business processes},
journal = {Environmental Modelling & Software},
volume = {84},
pages = {18-34},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216302389},
author = {Mattia Santoro and Stefano Nativi and Paolo Mazzetti},
keywords = {Business process, Brokering, Interoperability, GEOSS, Model web},
abstract = {In the Earth system science domain, most current digital infrastructures are able to support data access rather than provide answers to complex questions – noticeably, supporting the ability to address the what-if questions posed by users. To this purpose, integrated modeling is an indispensable element. This work follows a recognizable gap characterized by high-level steps towards model integration: the Science-to-Information Technology barrier. This manuscript introduces an innovative approach to address such a gap in the “Model Web” framework initiative promoted by GEO (Group on Earth Observation) on its GEOSS (Global Earth Observation System of Systems) program. The methodological approach consists of five technologically neutral steps to build executable workflows from abstract business processes. A high-level architecture, based on the brokering pattern, is introduced to implement the proposed approach. By extending the GEOSS brokering framework, a proof-of-concept implementation is presented, argued, and ultimately experimentations and conclusions are discussed.}
}
@article{KIFLU2016338,
title = {Improving resistivity survey resolution at sites with limited spatial extent using buried electrode arrays},
journal = {Journal of Applied Geophysics},
volume = {135},
pages = {338-355},
year = {2016},
note = {New trends in Induced Polarization},
issn = {0926-9851},
doi = {https://doi.org/10.1016/j.jappgeo.2016.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926985116304116},
author = {H. Kiflu and S. Kruse and M.H. Loke and P.B. Wilkinson and D. Harro},
keywords = {Resistivity inversion, Tomography, Optimized arrays, Sinkhole karst features, MERIT},
abstract = {Electrical resistivity tomography (ERT) surveys are widely used in geological, environmental and engineering studies. However, the effectiveness of surface ERT surveys is limited by decreasing resolution with depth and near the ends of the survey line. Increasing the array length will increase depth of investigation, but may not be possible at urban sites where access is limited. One novel method of addressing these limitations while maintaining lateral coverage is to install an array of deep electrodes. Referred to here as the Multi-Electrode Resistivity Implant Technique (MERIT), self-driving pointed electrodes are implanted at depth below each surface electrode in an array, using direct-push technology. Optimal sequences of readings have been identified with the “Compare R” method of Wilkinson. Numerical, laboratory, and field case studies are applied to examine the effectiveness of the MERIT method, particularly for use in covered karst terrain. In the field case studies, resistivity images are compared against subsurface structure defined from borings, GPR surveys, and knowledge of prior land use. In karst terrain where limestone has a clay overburden, traditional surface resistivity methods suffer from lack of current penetration through the shallow clay layer. In these settings, the MERIT method is found to improve resolution of features between the surface and buried array, as well as increasing depth of penetration and enhancing imaging capabilities at the array ends. The method functions similar to a cross-borehole array between horizontal boreholes, and suffers from limitations common to borehole arrays. Inversion artifacts are common at depths close to the buried array, and because some readings involve high geometric factors, inversions are more susceptible to noise than traditional surface arrays. Results are improved by using errors from reciprocal measurements to weight the data during the inversion.}
}
@incollection{2015125,
title = {Appendix B - Glossary},
editor = {Jules J. Berman},
booktitle = {Repurposing Legacy Data},
publisher = {Elsevier},
address = {Boston},
pages = {125-160},
year = {2015},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-802882-7},
doi = {https://doi.org/10.1016/B978-0-12-802882-7.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128028827000166}
}
@article{PRATAMA201689,
title = {An incremental meta-cognitive-based scaffolding fuzzy neural network},
journal = {Neurocomputing},
volume = {171},
pages = {89-105},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215008449},
author = {Mahardhika Pratama and Jie Lu and Sreenatha Anavatti and Edwin Lughofer and Chee-Peng Lim},
keywords = {Evolving fuzzy systems, Fuzzy neural networks, Meta-cognitive learning, Sequential learning},
abstract = {The idea of meta-cognitive learning has enriched the landscape of evolving systems, because it emulates three fundamental aspects of human learning: what-to-learn; how-to-learn; and when-to-learn. However, existing meta-cognitive algorithms still exclude Scaffolding theory, which can realize a plug-and-play classifier. Consequently, these algorithms require laborious pre- and/or post-training processes to be carried out in addition to the main training process. This paper introduces a novel meta-cognitive algorithm termed GENERIC-Classifier (gClass), where the how-to-learn part constitutes a synergy of Scaffolding Theory – a tutoring theory that fosters the ability to sort out complex learning tasks, and Schema Theory – a learning theory of knowledge acquisition by humans. The what-to-learn aspect adopts an online active learning concept by virtue of an extended conflict and ignorance method, making gClass an incremental semi-supervised classifier, whereas the when-to-learn component makes use of the standard sample reserved strategy. A generalized version of the Takagi-Sugeno Kang (TSK) fuzzy system is devised to serve as the cognitive constituent. That is, the rule premise is underpinned by multivariate Gaussian functions, while the rule consequent employs a subset of the non-linear Chebyshev polynomial. Thorough empirical studies, confirmed by their corresponding statistical tests, have numerically validated the efficacy of gClass, which delivers better classification rates than state-of-the-art classifiers while having less complexity.}
}
@article{TIXIER2016102,
title = {Application of machine learning to construction injury prediction},
journal = {Automation in Construction},
volume = {69},
pages = {102-114},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516300966},
author = {Antoine J.-P. Tixier and Matthew R. Hallowell and Balaji Rajagopalan and Dean Bowman},
keywords = {Machine learning, Construction safety, Predictive modeling, Injury prevention, Random Forest, Boosting, Attribute},
abstract = {The needs to ground construction safety-related decisions under uncertainty on knowledge extracted from objective, empirical data are pressing. Although construction research has considered machine learning (ML) for more than two decades, it had yet to be applied to safety concerns. We applied two state-of-the-art ML models, Random Forest (RF) and Stochastic Gradient Tree Boosting (SGTB), to a data set of carefully featured attributes and categorical safety outcomes, extracted from a large pool of textual construction injury reports via a highly accurate Natural Language Processing (NLP) tool developed by past research. The models can predict injury type, energy type, and body part with high skill (0.236<RPSS<0.436), outperforming the parametric models found in the literature. The high predictive skill reached suggests that injuries do not occur at random, and that therefore construction safety should be studied empirically and quantitatively rather than strictly being approached through the analysis of subjective data, expert opinion, and with a regulatory and managerial perspective. This opens the gate to a new research field, where construction safety is considered an empirically grounded quantitative science. Finally, the absence of predictive skill for the output variable injury severity suggests that unlike other safety outcomes, injury severity is mainly random, or that extra layers of predictive information should be used in making predictions, like the energy level in the environment. In the context of construction safety analysis, this study makes important strides in that the results provide reliable probabilistic forecasts of likely outcomes should an accident occur, and show great potential for integration with building information modeling and work packaging due to the binary and physical nature of the input variables. Such data-driven predictions had been absent from the field since its inception.}
}
@article{MOEYERSOMS201580,
title = {Comprehensible software fault and effort prediction: A data mining approach},
journal = {Journal of Systems and Software},
volume = {100},
pages = {80-90},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002295},
author = {Julie Moeyersoms and Enric {Junqué de Fortuny} and Karel Dejaeger and Bart Baesens and David Martens},
keywords = {Rule extraction, Software fault and effort prediction, Comprehensibility},
abstract = {Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data.}
}
@article{MYERS201316,
title = {Eco-informatics modelling via semantic inference},
journal = {Information Systems},
volume = {38},
number = {1},
pages = {16-32},
year = {2013},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2012.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306437912000531},
author = {Trina Myers and Ian Atkinson},
keywords = {Semantic Web, Ontology engineering, Scientific workflows, Knowledge systems},
abstract = {There is a demand for new and evolved research practices resulting from the so called “data deluge” emerging from high volume digital collection methods. As the volume of raw data increases traditional data processing methodologies, especially those involving manual manipulation are becoming increasingly difficult to manage. This paper presents the “Semantic Reef” architecture that offers an alternative approach to the development, application and execution of observational hypotheses involving studies of coral reef ecosystems. The Semantic Reef Knowledge Representation system is an eco-informatics application designed to assist in the integration of remotely sensed data streams and historic data sets supporting flexible hypothesis design and knowledge extraction. The system is an ontology-based architecture built to allow researchers to combine disjoint data sets into a single Knowledge Base for modelling the impact of climate change on coral reef ecosystems. The Knowledge Base consists of a hierarchy of ontologies developed to maximise usability and reusability by separating data instances from the concept descriptions. The model can be effectively reused to extract or disclose phenomena of any coral reef. This paper both demonstrates and describes a performance analysis of the Semantic Reef knowledge system.}
}
@article{STERNBERG2016282,
title = {An analysis of Brazilian flight delays based on frequent patterns},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {95},
pages = {282-298},
year = {2016},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2016.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516301740},
author = {Alice Sternberg and Diego Carvalho and Leonardo Murta and Jorge Soares and Eduardo Ogasawara},
keywords = {Flight delays, Association rules, Commercial aviation, Delay reasons},
abstract = {In this paper we applied data indexing techniques combined with association rules to unveil hidden patterns of flight delays. Considering Brazilian flight data and guided by six research questions related to causes, moments, differences, and relationships between airports and airlines, we evaluated and quantified all attributes that may lead to delays, showing not only the main patterns, but also their chances of occurrence in the entire network, in each airport and airline. We observed that Brazilian flight system has difficulties to recover from previous delays and when operating under adverse meteorological conditions, delays occurrences may increase up to 216%.}
}
@article{WONG201529,
title = {Development of a wireless inspection and notification system with minimum monitoring hardware for real-time vehicle engine health inspection},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {29-45},
year = {2015},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15002417},
author = {Pak Kin Wong and Chi Man Vong and Ka In Wong and Zi-Qian Ma},
keywords = {In-use vehicle inspection, Radio frequency identification, Maximum spanning tree, Internet of things},
abstract = {While many standards have been stipulated to control vehicular emissions, current inspection program for examining the engine health of in-use vehicles is practically ineffective and time-consuming. In particular, in-use vehicles are only required for inspection yearly, but huge amount of emissions may have been produced from malfunctioned engines daily. A new wireless inspection and notification system (WINS) is therefore proposed to monitor the vehicle engine health on the street in situ. The principle of WINS is to wirelessly examine some of the engine parameters through radio frequency identification (RFID) and traffic lights. RFID tags are installed on vehicles to collect the engine health information, whereas RFID interrogators are installed on traffic lights for wireless data transmission. Experiments were carried out to evaluate the effectiveness of the proposed WINS, and the results show that the proposed WINS is more convenient and economical than traditional vehicle inspection system. Moreover, as there are more than hundreds of traffic lights in the traffic network of a city, a maximum spanning tree (MAXST) algorithm is proposed to determine the suitable number of RFID devices required in the network so that the implementation cost, system loading and missing rate can be optimized. Different from the typical spanning tree algorithm in operational research, the MAXST algorithm has a domain-specific rule and weight calculation method for this application. To verify the methodology, simulations on the traffic networks of Shenzhen, New York and London were conducted. Results show that only 25–40% of traffic lights of the traffic networks are necessary for installation of RFID interrogators, with a rate of 2–7% that the vehicle owners may be able to escape the location of RFID interrogators.}
}
@article{KIM201644,
title = {Third-party mobile app developers’ continued participation in platform-centric ecosystems: An empirical investigation of two different mechanisms},
journal = {International Journal of Information Management},
volume = {36},
number = {1},
pages = {44-59},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401215000894},
author = {Hyung Jin Kim and Inchan Kim and Hogeun Lee},
keywords = {Platform-centric ecosystems, Third-party developers, App store, Mobile apps, Dual model},
abstract = {Platform-centric ecosystems run by Apple, Google, Microsoft, etc. enable the companies to magnify the values of their products and services on an unprecedented scale, by harnessing third-party add-on software such as mobile apps. Despite the importance, however, there is a dearth of empirical research that investigates how third-party developers’ continued participation is actually determined. This paper examined two different mechanisms increasing dedication to a platform and constraining exit from the platform, respectively. Specific factors in each mechanism and their casual relationships were tested and discussed in the context of Apple’s mobile platform.}
}