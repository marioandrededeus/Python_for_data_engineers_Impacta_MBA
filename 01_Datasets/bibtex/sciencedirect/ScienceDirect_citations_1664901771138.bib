@article{MAO2021125081,
title = {How can bicycle-sharing have a sustainable future? A research based on life cycle assessment},
journal = {Journal of Cleaner Production},
volume = {282},
pages = {125081},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.125081},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620351258},
author = {Guozhu Mao and Tianyi Hou and Xi Liu and Jian Zuo and Abdul-Hakim Ibrahim Kiyawa and Pingping Shi and Sukhbir Sandhu},
keywords = {Life cycle assessment, Bicycle-sharing, Environmental impact, Sustainable},
abstract = {Bicycle-sharing is experiencing explosive growth in China, and it is expected to be an environmental friendly practice, however, mass production and insufficient recycling of shared bicycles may bring great negative environmental impact. In this study, we conduct a life cycle assessment (LCA) of bicycle-sharing in China to estimate the negative environmental impacts of the stages of the whole life cycle with nine environmental impact categories. The results show that the production stage contributes to the greatest negative environmental impacts by an average rate of 81.18% among different impact categories, much higher than other stages, i.e. the use stage, daily management and transportation stage and waste treatment and recycling stage. Specifically, the use of aluminum at the production stage contributes to the most in nine environmental impacts categories (55.43% on average). And rubber is another relatively important contributor to these nine environmental impact categories (16.27% on average). In addition, the cumulative production of excessive parts is expected to bring further environmental impacts at the production stage. To promote the sustainable development of bicycle-sharing, we discuss various potential opportunities at the production and maintenance stage, materials selection for better durability and longer service life, the structure design for the ease of maintenance. Besides, at the waste treatment and recycling stage, we promote that the recycling system is necessary for generating environmental benefits considering the potential huge supply and demand of the market. The construction of a bicycle-sharing industry chain may be an effective way for the sustainable development of bicycle-sharing in the future.}
}
@article{KRITTANAWONG202188,
title = {Social media and predictive analysis regarding dietary approaches to stop hypertension},
journal = {Progress in Cardiovascular Diseases},
volume = {68},
pages = {88-90},
year = {2021},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0033062021000748},
author = {Chayakrit Krittanawong and Scott Kaplin and W.H. Wilson Tang and Hani Jneid and Salim S. Virani and Franz H. Messerli},
keywords = {Social media, DASH, DASH diet, Social media analysis, Facebook, Twitter}
}
@article{THAKKAR202112,
title = {Comparative anatomization of data mining and fuzzy logic techniques used in diabetes prognosis},
journal = {Clinical eHealth},
volume = {4},
pages = {12-23},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588914120300113},
author = {Harshil Thakkar and Vaishnavi Shah and Hiteshri Yagnik and Manan Shah},
keywords = {Data mining, Fuzzy logic, Diabetes, Health care},
abstract = {Diabetes is an ailment in which glucose level increase in at high rates in blood due to body’s inability to metabolize it. This happens when body does not produce sufficient amount of insulin or it does not respond to it properly. Critical and long-term health issues arise if diabetes is not handled or properly treated which includes: heart problems, disorders of the lungs, skin and liver complications, nerve damage, etc. With increasing number of diabetic patients, its early detection becomes essential. In this paper, our major focus areas are data mining and fuzzy logic techniques used in diabetes diagnosis. Data mining is used for locating patterns in huge datasets using a composition of different methods of machine learning, database manipulations and statistics. Data mining offers a lot of methods to inspect large data considering the expected outcome to find the hidden knowledge. Fuzzy logic is similar to human reasoning system and hence it can handle the uncertainties found in the data of medical diagnosis. These systems are called expert systems. The fuzzy expert systems (FES) analyze the knowledge from the available data which might be vague and suggests linguistic concept with huge approximation as its core to medical texts. In this paper, the methodology section delivers the pipeline of various tasks such as selecting the dataset, preprocessing the data by applying numerous methods such as standardization, normalization etc. After that, feature extraction technique is implemented on the dataset for improving the accuracy and finally dataset worked on data mining and fuzzy logic various classification algorithms. While analyzing different data mining methods, the accuracy computed through random forest classifiers as high as 99.7% and in case of numerous fuzzy logic approaches, high precision and low complexity was found to contribute a fairly high accuracy of 96%.}
}
@article{FISCHER2021268,
title = {Current applications of artificial intelligence in vascular surgery},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {268-271},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S089579672100065X},
author = {Uwe M. Fischer and Paula K. Shireman and Judith C. Lin},
abstract = {ABSTRACT
Basic foundations of artificial intelligence (AI) include analyzing large amounts of data, recognizing patterns, and predicting outcomes. At the core of AI are well-defined areas, such as machine learning, natural language processing, artificial neural networks, and computer vision. Although research and development of AI in health care is being conducted in many medical subspecialties, only a few applications have been implemented in clinical practice. This is true in vascular surgery, where applications are mostly in the translational research stage. These AI applications are being evaluated in the realms of vascular diagnostics, perioperative medicine, risk stratification, and outcome prediction, among others. Apart from the technical challenges of AI and research outcomes on safe and beneficial use in patient care, ethical issues and policy surrounding AI will present future challenges for its successful implementation. This review will give a brief overview and a basic understanding of AI and summarize the currently available and used clinical AI applications in vascular surgery.}
}
@article{RUMMENS2021125,
title = {The effect of spatiotemporal resolution on predictive policing model performance},
journal = {International Journal of Forecasting},
volume = {37},
number = {1},
pages = {125-133},
year = {2021},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2020.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169207020300558},
author = {Anneleen Rummens and Wim Hardyns},
keywords = {Predictive policing, Crime forecasting, Spatiotemporal forecasting, Decision making, Predictive modeling},
abstract = {Being able to anticipate crime such that new crime events can be dealt with effectively or prevented entirely, leads police forces worldwide to look at applying predictive policing, which provides predictions of times and places at risk for crime, such that proactive preventative measures can be taken. Ideally, predictive policing models predict crime at a high spatio-temporal level, while also providing optimal prediction performance. The main objective of this paper is therefore to evaluate the impact of varying grid resolution, temporal resolution and historical time frame on prediction performance. To investigate this, we analyse home burglary data from a large city in Belgium and predict new crime events using a range of parameter values, comparing the resulting prediction performances. Given the potential prediction performance costs associated with prediction at a high spatio-temporal resolution, consideration should be given to balance practical requirements with performance requirements.}
}
@article{ZHENG2021105610,
title = {Trilemma and tripartition: The regulatory paradigms of cross-border personal data transfer in the EU, the U.S. and China},
journal = {Computer Law & Security Review},
volume = {43},
pages = {105610},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105610},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000832},
author = {Guan Zheng},
keywords = {Trilemma, Tripartition, Personal data protection, Free cross-border flow, National jurisdiction},
abstract = {The regulation of the cross-border transfer of personal data is a major issue of globalization in the digital era. The key point for lawmakers is how to choose two of the following three elements in the trilemma: personal data protection, free transborder flow of information and the expansion of national jurisdiction. The EU, the U.S. and China adopt their own decisions, resulting in three inherently incompatible legislative paradigms, which has led to the restricted flow of personal data around the world as well as the free flow in three different regions, with the EU, the U.S. and China as the center of each region. In this way, the regulating paradigms of cross-border personal data transfer presents a pattern of tripartition.}
}
@article{FORGET2021100886,
title = {Is multi-source feedback the future of perioperative medicine?},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {40},
number = {3},
pages = {100886},
year = {2021},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2021.100886},
url = {https://www.sciencedirect.com/science/article/pii/S2352556821000904},
author = {Patrice Forget and Karuna Dahlberg}
}
@article{HANEL2021210,
title = {Impact of Cyber-physically enhanced manufacturing on the product requirement documentation in high-tech applications},
journal = {Procedia CIRP},
volume = {102},
pages = {210-215},
year = {2021},
note = {18th CIRP Conference on Modeling of Machining Operations (CMMO), Ljubljana, Slovenia, June 15-17, 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121007794},
author = {Albrecht Hänel and André Seidel and Carl Willy Mehling and Alexander Dementyev and Karol Kozak and Rudi Seidel and Uwe Teicher and Arvid Hellmich and Welf-Guntram Drossel and Steffen Ihlenfeldt},
keywords = {Digital Manufacturing System, Modelling, Process control, Cyber-physical production},
abstract = {Conventional machining of high-tech parts and components is often associated with complex processes, long lead times and small to medium batch sizes. Obviously, there is great interest to shorten process development periods and increase process reliability. The transformation of conventional machines into cyber-physical production systems (CPPS) promises significant improvements here. In fact, CPPS allows real-time data acquisition and processing, enabling to target integrated process improvement and quality assurance. In the present work, a methodical procedure for structured data aggregation is introduced for a Cyber-physical enhanced machining process while being explained from a high-tech application point of view. This includes data generation, extraction, transfer and storage, data-consolidation, linkage, visualization and interpretation. Finally, the paper illustrates these aspects in terms of the effects on the “digital product requirements” in order to achieve the presented “analytics-ready” data model using the example of a high-tech application.}
}
@article{AGHDAM2021105903,
title = {The Role of the Internet of Things in Healthcare: Future Trends and Challenges},
journal = {Computer Methods and Programs in Biomedicine},
volume = {199},
pages = {105903},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105903},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720317363},
author = {Zahra Nasiri Aghdam and Amir Masoud Rahmani and Mehdi Hosseinzadeh},
keywords = {Internet of Things, Healthcare, Future Trends, Systematic Review},
abstract = {Background and Objective
With the recent advances in the Internet of Things (IoT), the field has become more and more developed in healthcare. The Internet of things will help physicians and hospital staff perform their duties comfortably and intelligently. With the latest advanced technologies, most of the challenges of using IoT have been resolved, and this technology can be a great revolution and has many benefits in the future of digital. Healthcare is one of the most useful areas for IoT use. The most important application of IoT is to monitor and make quick decisions in critical situations. Thanks to this technology-based treatment approach, there is an unprecedented opportunity to better the quality and productivity of treatments and better the patient's well-being and better government funding.
Methods
In this paper, we provide a comprehensive overview of the primary uses of IoT in healthcare. We used the Systematic Literature Review (SLR) method to analyze and comparison articles published in this field between 2015 and March 2020.
Results
A comprehensive taxonomy is presented based on the contents of the articles under study. In this article, a brief overview of selected articles based on research questions is given and highlights the most critical challenges and case studies for the future use of IoT in healthcare.
Conclusions
According to a detailed study of the 89 articles and a glimpse into about 208 articles, challenges and future trends in healthcare have been identified.}
}
@incollection{PLOTKIN202193,
title = {Chapter 5 - Training the Business Data Stewards},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {93-104},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822132700005X},
author = {David Plotkin},
keywords = {Training, Data Quality, Data Stewards, skills, roles, responsibilities},
abstract = {The outline of a training guide for new data stewards is presented here, along with explanations of items that are not otherwise included in this book.}
}
@article{HUANG2021129558,
title = {Evaluating the performance of LBSM data to estimate the gross domestic product of China at multiple scales: A comparison with NPP-VIIRS nighttime light data},
journal = {Journal of Cleaner Production},
volume = {328},
pages = {129558},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129558},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621037379},
author = {Ziwei Huang and Shaoying Li and Feng Gao and Fang Wang and Jinyao Lin and Ziling Tan},
keywords = {Location based social media data, NPP/VIIRS, Economic, OLS, GWR, China},
abstract = {Regional economic development evaluation is essential for understanding social and environmental issues. Although the nighttime light (NTL) data have been proved to be effective in economic estimation, it cannot reflect the human activities that occur during the daytime. Recently, with the widespread use of smart mobile devices, the location based social media (LBSM) data are increasingly being used as a proxy for real-time human activities. However, little work was carried out to explore the potential of LBSM data in estimating economic development at different scales in China. This study filled this gap by evaluating the effectiveness of Tencent user density (TUD) data, a typical type of LBSM data in China, in Gross Domestic Product (GDP) modeling at the provincial, municipal, and county scales. In this study, we employed holiday and non-holiday TUD sample data to simulate the annual TUD data, and compared it with the new generation NTL data, NPP/VIIRS images. The results showed that although the simulated annual TUD data does not perform better than NPP/VIIRS-NTL data in provincial and municipal GDP estimation, it outperforms NPP/VIIRS-NTL data at the county scale. More importantly, the simulated annual TUD data are much more powerful and reliable than NPP/VIIRS-NTL data in underdeveloped areas with complex terrain, such as the Northwest and Southwest China, as well as in more developed areas with separation of work and housing, such as the North China and South China. This is mainly because TUD data can reduce the impact of natural factors such as terrain on data collection as well as reflect both daytime and nighttime human activities. This study confirmed that the LBSM-TUD data is a potential and promising data source for economic modeling in small scale areas of China, which will help to support China's regional economic evaluation.}
}
@article{CENTOBELLI2021120463,
title = {Surfing blockchain wave, or drowning? Shaping the future of distributed ledgers and decentralized technologies},
journal = {Technological Forecasting and Social Change},
volume = {165},
pages = {120463},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120463},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312890},
author = {Piera Centobelli and Roberto Cerchione and Emilio Esposito and Eugenio Oropallo},
keywords = {Bibliometric analysis, Block-chain, Decentralized technology, Distributed ledger, Literature review, Network analysis, Performance analysis, Traceability, Tracking, Transparency, trust},
abstract = {Blockchain is a promising technology whose four TRs (TRaceability, TRacking, TRansparency, TRrust) features are bound to revolutionize material, information, financial flows and transactions inside and outside organisations. Many studies have been published showing the potential of this disruptive technology in many fields and this number is growing exponentially in recent years. This enormous amount of papers calls for a more systematic approach to analyse the overall trend in this research field. A bibliometric approach based on performance analysis and network analysis techniques is used to examine the evolution of blockchain technology research. Firstly, this paper contributes to the body of literature by discussing the most influential countries, authors, subject areas and journals of the current blockchain research. Secondly, this paper identifies six main clusters of blockchain-related research contributions and, based on the analysis on centrality and density measures, it classifies research themes in motor themes, basic themes, emerging or disappearing themes, and specialised themes. Despite the majority of contributions belong to the computer science subject area, many papers belonging to the technology management subject area provide pivotal insights for practitioners and policy makers. Specifically, they may exploit the results of this research to rethink many traditional processes in the light of blockchain technology implementation, exploit the benefits of the four TRs to manage processes, automate common tasks, generate actionable results, and improve daily operations.}
}
@article{MILLECAM20211922,
title = {Coming of age of Allotrope: Proceedings from the Fall 2020 Allotrope Connect},
journal = {Drug Discovery Today},
volume = {26},
number = {8},
pages = {1922-1928},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621001653},
author = {Todd Millecam and Austin J. Jarrett and Naomi Young and Dana E. Vanderwall and Dennis {Della Corte}},
keywords = {Precompetitive consortium, Semantics, Metadata, Standard, Harmonization, Laboratory IT, Allotrope, Digital Lab},
abstract = {The Allotrope Foundation (AF) is a group of pharmaceutical, device vendor, and software companies that develops and releases technologies [the Allotrope Data Format (ADF), the Allotrope Foundation Ontology (AFO), and the Allotrope Data Models (ADM)] to simplify the exchange of electronic data. We present here the first comprehensive history of the AF, its structure, a list of members and partners, and an introduction to the technologies. Finally, we provide current insights into the adoption and development of the technologies by summarizing the Fall 2020 Allotrope Connect virtual conference. This overview provides an easy access to the AF and highlights opportunities for collaboration.}
}
@article{SADRI2021102882,
title = {Fog data management: A vision, challenges, and future directions},
journal = {Journal of Network and Computer Applications},
volume = {174},
pages = {102882},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102882},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303465},
author = {Ali Akbar Sadri and Amir Masoud Rahmani and Morteza Saberikamarposhti and Mehdi Hosseinzadeh},
keywords = {Fog computing, Internet of things, Data management, Data processing, Data analytics, Data storage, Data security, Systematic literature review},
abstract = {Cloud computing with its key facets and its inherent advantages still faces several challenges in the Internet of Things (IoT) ecosystem. The distance among the IoT end devices and cloud computing might be a problem for latency-sensitive applications such as catastrophe management and content transference applications. Fog computing is a novel paradigm to address such issues that playacts a significant role in massive and real-time data management systems in an IoT environment. Particularly IoT data management by fog computing is one important phase for latency reduction in latency-sensitive applications and necessary to generate more skilled knowledge and intelligent decisions. In this study, we used the SLR (systematic literature review) method to survey fog data management to understand the various topics and main contexts in this domain that have been newly offered. The target of this article is classifying and analyzing the researches about the fog data management domain which has been released from 2014 to 2019. A context-based taxonomy is offered for fog data management including data processing, data storage and data security based on the context of papers that are elected with the SLR method in our study. Based on presented technical taxonomy, the grouped papers in any context are compared with each other pursuant to some metrics of fog data management reference model. Then, for any selected research, the new findings, advantages, and weaknesses are debated. Finally, based on studies the open issues in fog data management and their related challenges for future researches are highlighted.}
}
@article{SUN2021106276,
title = {High resolution 3D terrestrial LiDAR for cotton plant main stalk and node detection},
journal = {Computers and Electronics in Agriculture},
volume = {187},
pages = {106276},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106276},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921002933},
author = {Shangpeng Sun and Changying Li and Peng W. Chee and Andrew H. Paterson and Cheng Meng and Jingyi Zhang and Ping Ma and Jon S. Robertson and Jeevan Adhikari},
keywords = {High throughput phenotyping, Terrestrial LiDAR, Three-dimensional skeleton, Plant node detection, Minimum spanning tree},
abstract = {Dense three-dimensional point clouds provide opportunities to retrieve detailed characteristics of plant organ-level phenotypic traits, which are helpful to better understand plant architecture leading to its improvements via new plant breeding approaches. In this study, a high-resolution terrestrial LiDAR was used to acquire point clouds of plants under field conditions, and a data processing pipeline was developed to detect plant main stalks and nodes, and then to extract two phenotypic traits including node number and main stalk length. The proposed method mainly consisted of three steps: first, extract skeletons from original point clouds using a Laplacian-based contraction algorithm; second, identify the main stalk by converting a plant skeleton point cloud to a graph; and third, detect nodes by finding the intersection between the main stalk and branches. Main stalk length was calculated by accumulating the distance between two adjacent points from the lowest to the highest point of the main stalk. Experimental results based on 26 plants showed that the proposed method could accurately measure plant main stalk length and detect nodes; the average R2 and mean absolute percentage error were 0.94 and 4.3% for the main stalk length measurements and 0.7 and 5.1% for node counting, respectively, for point numbers between 80,000 and 150,000 for each plant. Three-dimensional point cloud-based high throughput phenotyping may expedite breeding technologies to improve crop production.}
}
@article{THINGO2021505,
title = {Evaluation of deep learning algorithms for national scale landslide susceptibility mapping of Iran},
journal = {Geoscience Frontiers},
volume = {12},
number = {2},
pages = {505-519},
year = {2021},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2020.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674987120301687},
author = {Phuong Thao {Thi Ngo} and Mahdi Panahi and Khabat Khosravi and Omid Ghorbanzadeh and Narges Kariminejad and Artemi Cerda and Saro Lee},
keywords = {CNN, RNN, Deep learning, Landslide, Iran},
abstract = {The identification of landslide-prone areas is an essential step in landslide hazard assessment and mitigation of landslide-related losses. In this study, we applied two novel deep learning algorithms, the recurrent neural network (RNN) and convolutional neural network (CNN), for national-scale landslide susceptibility mapping of Iran. We prepared a dataset comprising 4069 historical landslide locations and 11 conditioning factors (altitude, slope degree, profile curvature, distance to river, aspect, plan curvature, distance to road, distance to fault, rainfall, geology and land-sue) to construct a geospatial database and divided the data into the training and the testing dataset. We then developed RNN and CNN algorithms to generate landslide susceptibility maps of Iran using the training dataset. We calculated the receiver operating characteristic (ROC) curve and used the area under the curve (AUC) for the quantitative evaluation of the landslide susceptibility maps using the testing dataset. Better performance in both the training and testing phases was provided by the RNN algorithm (AUC ​= ​0.88) than by the CNN algorithm (AUC ​= ​0.85). Finally, we calculated areas of susceptibility for each province and found that 6% and 14% of the land area of Iran is very highly and highly susceptible to future landslide events, respectively, with the highest susceptibility in Chaharmahal and Bakhtiari Province (33.8%). About 31% of cities of Iran are located in areas with high and very high landslide susceptibility. The results of the present study will be useful for the development of landslide hazard mitigation strategies.}
}
@article{BANDARA2021103497,
title = {Construing online consumers’ information privacy decisions: The impact of psychological distance},
journal = {Information & Management},
volume = {58},
number = {7},
pages = {103497},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103497},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000719},
author = {Ruwan J. Bandara and Mario Fernando and Shahriar Akter},
keywords = {Construal level theory, Psychological distance, Privacy concerns, Privacy behavior, Privacy empowerment},
abstract = {The role of subjective distance and mental representations in understanding consumers’ information privacy decisions is underexplored in the literature. This study draws on construal level theory and power-responsibility equilibrium framework of privacy to explain consumer privacy behavior based on the interplay between three psychological constructs, namely, privacy concerns, privacy empowerment, and the psychological distance of privacy. This study empirically validates the psychological distance of privacy construct and the results indicate the capability of psychological distance to explain privacy behavior and to moderate the relationship between privacy concerns and privacy behavior. The findings also suggest that empowered consumers’ privacy behavior does not vary despite the degree of psychological distance. Our findings have implications for the privacy scholarship, consumers and e-commerce system developers.}
}
@article{WU2021106593,
title = {Digitalization and decentralization driving transactive energy Internet: Key technologies and infrastructures},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {126},
pages = {106593},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0142061520328210},
author = {Ying Wu and Yanpeng Wu and Josep M. Guerrero and Juan C. Vasquez},
keywords = {Digitalization, Decentralization, Energy Internet, IoT, Blockchain, Microgrids, Energy router, Energy transaction},
abstract = {With the increasing access of renewable energy resources and fast ubiquitous connection of everything, the traditional one-way power flow from centralized generation to end consumers will give way to bidirectional-way power flow with multidirectional energy network among central grids and distributed prosumers. To empower the prosumer-centric Energy Internet (EI) and enhance the integration of energy-aware services, digitalization and decentralization are the key enablers to achieve transactive EI. This article presents a systematic overview on how Internet of Things (IoT) drives the digitalization of transactive EI and how blockchain empowers the decentralization of transactive EI. A comprehensive discussion on the key infrastructures is provided for presenting how to implement digitalization and decentralization of transactive EI, including the last mile “Advanced metering infrastructure” (AMI), renewables integrator “smart inverter”, energy flow adjuster “energy router”, and coordinator “Microgrid”. Challenges and future trends are discussed from an extensive point of view, including energy physical space, data cyber space and human social space.}
}
@article{RASHID20213,
title = {Artificial Intelligence Effecting a Paradigm Shift in Drug Development},
journal = {SLAS Technology},
volume = {26},
number = {1},
pages = {3-15},
year = {2021},
note = {Special Collection: Artificial Intelligence in Process Automation},
issn = {2472-6303},
doi = {https://doi.org/10.1177/2472630320956931},
url = {https://www.sciencedirect.com/science/article/pii/S2472630322010858},
author = {Masturah Bte Mohd Abdul Rashid},
keywords = {artificial intelligence, drug development, drug discovery, industry},
abstract = {The inverse relationship between the cost of drug development and the successful integration of drugs into the market has resulted in the need for innovative solutions to overcome this burgeoning problem. This problem could be attributed to several factors, including the premature termination of clinical trials, regulatory factors, or decisions made in the earlier drug development processes. The introduction of artificial intelligence (AI) to accelerate and assist drug development has resulted in cheaper and more efficient processes, ultimately improving the success rates of clinical trials. This review aims to showcase and compare the different applications of AI technology that aid automation and improve success in drug development, particularly in novel drug target identification and design, drug repositioning, biomarker identification, and effective patient stratification, through exploration of different disease landscapes. In addition, it will also highlight how these technologies are translated into the clinic. This paradigm shift will lead to even greater advancements in the integration of AI in automating processes within drug development and discovery, enabling the probability and reality of attaining future precision and personalized medicine.
摘要
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。
초록
약물 개발 비용과 약물의 성공적인 시장 통합 간의 역상관관계는 이러한 급증하는 문제들을 극복하기 위한 혁신적인 해법이 필요하다는 인식을 제기했다. 이러한 문제는 임상시험의 조기 종료, 규제 요인 또는 초기 약물 개발 과정에서 이루어진 결정을 포함한 여러 요인들에서 기인할 수 있다. 약물 개발을 가속화하고 지원하기 위한 인공지능(artificial intelligence, AI)의 도입으로 약물 개발 과정이 더욱 저렴하고 더욱 효율적이 되었으며 궁극적으로 임상시험의 성공률이 향상되었다. 본 종설의 목적은 다양한 질병 환경의 탐색을 통해 자동화를 지원하고 특히 신약의 표적 확인 및 설계, 약물의 재포지셔닝, 생체표지자 확인 및 효과적인 환자 층화 부분에서 약물 개발의 성공을 향상시키는 AI 기술의 다양한 적용을 보여주고 비교하는 것이다. 또한 이러한 기술들이 임상으로 전환되는 방식에 대해서도 강조할 것이다. 이러한 패러다임 전환은 신약 개발 및 발견의 자동화 과정에서 AI의 통합을 더욱 크게 발전시켜 향후 정밀의학 및 맞춤 의학 달성 가능성을 높이고 그 실현을 가능하게 할 것이다.
抄録
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。}
}
@incollection{2021vii,
title = {Contents},
editor = {Alan Godfrey and Sam Stuart},
booktitle = {Digital Health},
publisher = {Academic Press},
pages = {vii-xiii},
year = {2021},
isbn = {978-0-12-818914-6},
doi = {https://doi.org/10.1016/B978-0-12-818914-6.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189146000314}
}
@article{WOSIAK20212422,
title = {Using semantic enrichment methods in expert search system for recruitment process in IT corporation},
journal = {Procedia Computer Science},
volume = {192},
pages = {2422-2431},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921017488},
author = {Agnieszka Wosiak},
keywords = {expert search system, semantic enrichment, text data analysis, preprocessing and outlier detection;},
abstract = {The problem of intelligent information retrieval and semantic enrichment becomes more and more popular due to the difficulty of searching and analyzing large text datasets. The common approach assumes user manual queries in natural language. Various semantic enrichment methods and intelligent text searching allow obtaining more accurate results leading to broader knowledge and user satisfaction. This research presents state-of-the-art methods of searching with enrichment and building rankings of results for the expert recruitment process in IT industry. The proposed model implements full-text search, semantic enrichment, and machine learning to match experts with job offers. Different data sources on expert competencies were used, including curricula vitae, historical data, and Internet resources. The testing results confirm an improvement in the search quality compared to the existing systems in the recruitment company.}
}
@article{VALENTINETTI2021549,
title = {Internet of things: Emerging impacts on digital reporting},
journal = {Journal of Business Research},
volume = {131},
pages = {549-562},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321000667},
author = {Diego Valentinetti and Francisco {Flores Muñoz}},
keywords = {Digital reporting, Internet of Things (IoT), Corporate communication, Media richness},
abstract = {This paper develops a future research agenda for fostering a resurged interest in digital reporting through the emergence of the Internet of Things (IoT). Drawing upon the media richness and corporate communication frameworks, we enquire the evolving stages of digital reporting and review the contemporary academic literature on IoT to discuss the opportunities and practical concerns for developing future advances in the digitisation of accounting information. Our analysis explores how the media richness-related features of IoT fit, challenge and enhance the dynamics that constitute corporate communication, i.e.: communicator, message, medium/channel, audience, relationship and conversation. This paper opens new directions of research on advances in digital reporting and sheds light on the innovative ways accounting information is co-produced and shared through the IoT.}
}
@incollection{MALIK20213,
title = {Chapter 1 - Advances in Machine Learning and Data Analytics},
editor = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
booktitle = {Intelligent Data-Analytics for Condition Monitoring},
publisher = {Academic Press},
pages = {3-29},
year = {2021},
isbn = {978-0-323-85510-5},
doi = {https://doi.org/10.1016/B978-0-323-85510-5.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323855105000016},
author = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
keywords = {feature extraction, feature selection, data preprocessing, visualization, condition monitoring, open access, software, dataset sources},
abstract = {Artificial intelligence (AI) is the intelligence demonstrated by the machines. AI is also the representation of a machine (like computer), which imitates cognitive behavior/functions associated to human mind for the purpose of learning and problem solving. Also, the subset of AI is machine learning (ML), which is improved automatically through experience. ML algorithms are developed based on the data without explicit information of the system behavior. Data statistics and computational analysis are the subset of ML. The data analytics is a process of analyzing, cleaning, transforming, and modeling the data with respect to the useful information. In this chapter, detailed information of data analytics of smart grid application, data analytics for business, condition monitoring, data and its relation, data preprocessing, feature extraction, feature selection, and different application areas are studied. A wide list of software and dataset’s digital library are also included.}
}
@article{OCONNOR2021102934,
title = {Integrating informatics into undergraduate nursing education: A case study using a spiral learning approach},
journal = {Nurse Education in Practice},
volume = {50},
pages = {102934},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2020.102934},
url = {https://www.sciencedirect.com/science/article/pii/S1471595320310209},
author = {Siobhan O'Connor and Elizabeth LaRue},
keywords = {Nursing education, Informatics, Technology, Digital health},
abstract = {A gap in informatics expertise amongst nursing students, practising staff and faculty has been noted globally, which reduces the potential for nurses to utilise technology to enhance patient care. National nursing education strategies and recommendations from professional associations have identified digital health as an area that needs investment. This case study describes how health informatics is being integrated into a Bachelor of Nursing programme in the United Kingdom. An international collaboration with a US-UK Fulbright Specialist Scholar enabled individual learning units corresponding to key health informatics competencies to be designed and incorporated into a pedagogic framework grounded in the spiral learning approach. This approach is proposed as one way to integrate informatics into nursing education, so students can become competent clinicians that are able to deliver technology enabled care in the health service.}
}
@article{ZHU2021102952,
title = {An improved convolution Merkle tree-based blockchain electronic medical record secure storage scheme},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102952},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102952},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001642},
author = {Hegui Zhu and Yujia Guo and Libo Zhang},
keywords = {Electronic medical record, Blockchain, Convolution operation, Improved convolution Merkle tree},
abstract = {Presently, more and more electronic medical records (EMR) are used to replace traditional recording methods. However, there has potential safety hazard in the transmission of EMR because of the personal privacy disclosure. So, how to store, transmit and share EMR effectively and securely has become a research hotspot. In this paper, we propose an improved Merkle tree based-blockchain EMR storage scheme. The hallmark of the proposed scheme is that we employ the convolutional layer structure to replace the original binary tree structure in the proposed convolution Merkle tree, which can improve the efficiency effectively. Experiments show that the number of stored nodes has decreased significantly with the same amount of input data, and the layers number of the improved convolution Merkle tree and hash calculated amount are all reduced dramatically. The security and efficiency analysis also illustrate that the proposed scheme can provide a reliable choice for the further development of data storage security in the future.}
}
@article{KADAR2021103113,
title = {Tourism flows in large-scale destination systems},
journal = {Annals of Tourism Research},
volume = {87},
pages = {103113},
year = {2021},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2020.103113},
url = {https://www.sciencedirect.com/science/article/pii/S0160738320302577},
author = {Bálint Kádár and Mátyás Gede},
keywords = {Tourism networks, Network analysis, Tourist flows, Large-scale destinations, Multi-destination trips, Danube, Flickr analysis},
abstract = {Large-scale destination systems, especially cross-border regions are less studied in literature as their size and transnational nature makes these hard to analyse with traditional methods. Tourism systems like the Danube Region are composed of several local and regional destinations, and even when these are branded together for tourists the integration of these into one system is often compromised by national boundaries and socio-economic differences. This study shows how the Danube region is composed of different clusters of destinations, and how national boundaries have a strong shielding effect in the interregional movements of tourists. A methodology based on network analysis with efficient clustering algorithms applied on large geotagged datasets from User Generated Content is proposed. Flickr data was used to map short time-interval visitor flows along the linear system of the river Danube. 18 regional clusters integrated into 3 strong, but separated destination systems were identified by modularity analysis. The central integrating effect of the large capital cities and the boundary-shielding effect impeding the total integration of this large-scale system were made measurable.}
}
@article{LISBOA2021709,
title = {Improve industrial performance based on systematic analyses of manufacturing data},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {709-716},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.083},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008260},
author = {M. Lisboa and E. Jesus and R. Seixas and P. Valle and F. Deschamps and C. Strobel},
keywords = {Industry 4.0, Industrial Internet of things (IIOT), Data Analytics, Predictive Maintenance},
abstract = {This Article was written based on a systematic review of the literature considering three reference axes, Industry 4.0, Data Analytics and Predictive Maintenance, including specific combination of search terms to ensure a reasonable quantity of articles keeping adherence to the topic that is decision making based on collection and analysis of relevant data on B2B (Business to Business). The study focuses on the area of predictive maintenance, which has been strongly highlighted by the fourth industrial revolution, due to the mechanical and mainly the electronic embedded systems complexity increases in the last decades, as well as network connectivity possibilities for equipment’s data acquisition, enabling technologies for predictive maintenance as a key factor for competitiveness, reducing costs, increasing equipment’s availability, or as a servitization strategy. The article is divided into five parts, starting with the research model and selection of articles, including the proposal for a data analysis framework and the application of systematic analysis of these data, concluding with the opening of a discussion and the indication of future directions.}
}
@article{POURBOZORGILANGROUDI2021119,
title = {Backward simulation of temperature changes of District Heating networks for enabling loading history in predictive maintenance},
journal = {Energy Reports},
volume = {7},
pages = {119-127},
year = {2021},
note = {The 17th International Symposium on District Heating and Cooling},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721008374},
author = {Pakdad {Pourbozorgi Langroudi} and Ingo Weidlich and Stefan Hay},
keywords = {Backward simulation, Loading history, Predictive maintenance, Machine learning, Asset management, Artificial neural networks, System reliability, District Heating},
abstract = {District Heating (DH) networks, like most of industries, are in transition to the fourth​ industry age and they are retrofitting themselves with different sensing and inspection technologies to enable cyber connectivity for different purposes, such as system optimization, failure detection, maintenance, etc. Since DH pipes show different ageing behaviour under different conditions and initially the pre-insulated bounded pipes had been designed for a minimum of 30 years life span, a long-term loading history is required for predictive maintenance (PdM) purposes and it is necessary to understand the ageing of the DH pipes. These historical temperature changes of the networks are not available for such a long period and they are usually limited to the past few years. To exploit the available implemented technologies for PdM , the missing data must become available to understand the ageing patterns and expand the ageing model to the pipes in use. In this research, various Machine Learning (ML) techniques such as Support Vector Machine (SVM), Random Forest algorithm (RF), Artificial Neural Networks (ANN) have been tested to train a model and backward simulate the temperature changes of the system based on recorded weather data. Various none-temperature variables have been used to enhance the prediction qualities to the real-world data. The historical temperature changes of the system shall be used for different ageing estimation such as fatigue cycles or remaining useful life of the polyurethane (PUR) foam.}
}
@incollection{OTT20211,
title = {1 - Information and Data Management},
editor = {Florence Ott},
booktitle = {Records Management At the Heart of Business Processes},
publisher = {ISTE},
pages = {1-50},
year = {2021},
isbn = {978-1-78548-043-0},
doi = {https://doi.org/10.1016/B978-1-78548-043-0.50001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480430500019},
author = {Florence Ott},
keywords = {Big data, Business processes, Data governance, Digital environment, Information, Open data, Protection of personal data, Records continuum, Three ages},
abstract = {Abstract:
While the digital world brings advantages by simplifying many processes, it also makes the context of records production more complex and difficult to understand according to traditional archival principles. The explosion in the volume of information leads to the multiplication of actors, the acceleration of exchanges, and the atomization and fragmentation of information with numerous digital files to replace what was formerly a paper document or the reproduction of several copies.}
}
@article{SCHRECKENBERG202131,
title = {Developing a maturity-based workflow for the implementation of ML-applications using the example of a demand forecast},
journal = {Procedia Manufacturing},
volume = {54},
pages = {31-38},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001396},
author = {Felix Schreckenberg and Nikolas Ulrich Moroff},
keywords = {artificial intelligenz, maturity-based workflow, challenges AI},
abstract = {The aim of the article is to present a guideline that has been developed in the form of a workflow to identify the capability of an organisation to implement machine learning (ML) applications on the one hand and, on the other hand, to describe a maturity-dependent procedure for the development of an ML application based on this knowledge. With the help of the guideline, application-specific requirements can be identified based on the phases of the development process of an ML application adapted to the corporate environment. The article begins with the motivation for using machine learning methods and presents the challenges in implementing these methods. Based on a literature review, a maturity-based approach is designed and the developed and adapted development phases from the literature are described in a more detailed way. The individual characteristics of certain phases are specified based on the maturity level. As well, the weighting of certain maturity dimensions of the respective phase is highlighted. The article ends with an outlook on the further development of the created guideline.}
}
@article{MA2021106972,
title = {Mining truck platooning patterns through massive trajectory data},
journal = {Knowledge-Based Systems},
volume = {221},
pages = {106972},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106972},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002355},
author = {Xiaolei Ma and Enze Huo and Haiyang Yu and Honghai Li},
keywords = {Energy consumption, Trajectory mining, Truck platooning, Spatial clustering, Association rule learning},
abstract = {Truck platooning refers to a series of trucks driving in close proximity via communication technologies, and it is considered one of the most implementable systems of connected and automated vehicles, bringing huge energy savings and safety improvements. Properly planning platoons and evaluating the potential of truck platooning are crucial to trucking companies and transportation authorities. This study proposes a series of data mining approaches to learn spontaneous truck platooning patterns from massive trajectories. An enhanced map matching algorithm is developed to identify truck headings by using digital map data, followed by an adaptive spatial clustering algorithm to detect trucks’ instantaneous co-moving sets. These sets are then aggregated to find the network-wide maximum platoon duration and size through frequent itemset mining for computational efficiency. The GPS data were collected from truck fleeting systems in Liaoning Province, China for platooning performance measures and spatiotemporal platooning distribution visualization. Results show that approximately 36% spontaneous truck platoons can be coordinated by speed adjustment without changing routes and schedules. The average platooning distance and duration ratios for these platooned trucks are 9.6% and 9.9%, respectively, leading to a 2.8% reduction in total fuel consumption. This study also distinguishes the optimal platooning periods and space headways for national freeways and trunk roads, and prioritize the road segments with high possibilities of truck platooning. The derived results are reproducible, providing useful policy implications and operational strategies for large-scale truck platoon planning and roadside infrastructure construction.}
}
@article{CHATTERJEE2021111051,
title = {Scientometric review of artificial intelligence for operations & maintenance of wind turbines: The past, present and future},
journal = {Renewable and Sustainable Energy Reviews},
volume = {144},
pages = {111051},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111051},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121003403},
author = {Joyjit Chatterjee and Nina Dethlefs},
keywords = {Wind turbines, Operations & maintenance, SCADA, Scientometric review, Artificial intelligence, Machine learning, Condition-based monitoring},
abstract = {Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.}
}
@article{GOYAL2021719,
title = {Internet of things: Architecture and enabling technologies},
journal = {Materials Today: Proceedings},
volume = {34},
pages = {719-735},
year = {2021},
note = {3rd International Conference on Science and Engineering in Materials},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.04.678},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320333253},
author = {Parul Goyal and Ashok Kumar Sahoo and Tarun Kumar Sharma},
keywords = {Enabling technologies, IoT, RFID, Sensors, ZigBee},
abstract = {Internet of Things is transforming real devices to smart intelligent virtual devices. In IoT day today devices of daily use are manufactured along with sensors which are capable for identification and sensing. They can be networked, are capable to process, can interact with other devices through Internet. IoT objective is to connect almost everything under a common infrastructure. This helps to control devices and will keep us informed about the status of devices. The paper aims to give Internet of Things overview, architectures, enabling technologies and their applications. It presents latest trends, current state, recent developments, challenges, security, privacy, applications of IoT and future research directions.}
}
@article{YIN2021106872,
title = {Knowledge discovery of geochemical patterns from a data-driven perspective},
journal = {Journal of Geochemical Exploration},
volume = {231},
pages = {106872},
year = {2021},
issn = {0375-6742},
doi = {https://doi.org/10.1016/j.gexplo.2021.106872},
url = {https://www.sciencedirect.com/science/article/pii/S0375674221001515},
author = {Bojun Yin and Renguang Zuo and Yihui Xiong and Yongsheng Li and Weigang Yang},
keywords = {Data-driven, Knowledge discovery, Data science, Geochemical exploration},
abstract = {We have entered the fourth research paradigm with the overwhelming availability of vast amounts of data. The processing and mining these data for a better understanding of earth systems and predicting mineral resources is challenging. This study discusses a data-driven knowledge discovery of geochemical patterns and presents a case study of geochemical data processing from a data-driven perspective. We employed local indicators of spatial association (LISA), principal component analysis (PCA), and deep autoencoder network (DAN) procedures to explore spatial association of geochemical patterns, extract elemental associations, and detect geochemical anomalies related to AuSb mineralization in the Daqiao district, Gansu Province, China. The results indicate the following: (1) both Au and Sb, and Pb and Zn have a close spatial correlation, indicating genetic connections among them; (2) the elemental association of Au, Sb, As, Hg and Ag can be adopted as a geochemical signature for the discovery of AuSb polymetallic mineralization in the study area; and (3) the geochemical anomalies identified by DAN exhibit a strong spatial relationship with locations of known mineral deposits and can provide a significant clue for further mineral exploration in this district. These findings indicate that data-driven procedures can help in the knowledge discovery of geochemical patterns in mineral exploration. Additional efforts are required for data-driven knowledge discovery in both geochemical prospecting and mineral exploration.}
}
@article{FLUKE2021104650,
title = {Child maltreatment data: A summary of progress, prospects and challenges},
journal = {Child Abuse & Neglect},
volume = {119},
pages = {104650},
year = {2021},
note = {30 Years of the Convention on the Rights of the Child: Challenges and progress in harm prevention},
issn = {0145-2134},
doi = {https://doi.org/10.1016/j.chiabu.2020.104650},
url = {https://www.sciencedirect.com/science/article/pii/S0145213420303057},
author = {John D. Fluke and Lil Tonmyr and Jenny Gray and Leonor {Bettencourt Rodrigues} and Flora Bolter and Scottye Cash and Andreas Jud and Franziska Meinck and Abigail {Casas Muñoz} and Melissa O’Donnell and Rhiannon Pilkington and Leemoy Weaver},
keywords = {Child maltreatment data, Linked data, Data collection, Data analysis, Self-report data, Administrative data, Sentinel data, Data collection ethics, International comparison, Child maltreatment data utilization, Evaluation, Decision-making},
abstract = {Background
In 1996, the ISPCAN Working Group on Child Maltreatment Data (ISPCAN-WGCMD) was established to provide an international forum in which individuals, who deal with child maltreatment data in their respective professional roles, can share concerns and solutions.
Objective
This commentary describes some of the key features and the status of child maltreatment related data collection addressed by the ISPCAN-WGCMD.
Methods
Different types of data collection methods including self-report, sentinel, and administrative data designs are described as well as how they address different needs for information to help understand child maltreatment and systems of prevention and intervention.
Results
While still lacking in many parts of the world, access to child maltreatment data has become much more widespread, and in many places a very sophisticated undertaking.
Conclusion
The ISPCAN-WGCMD has been an important forum for supporting the continued development and improvement in the global effort to understand and combat child maltreatment thus contributing to the long term goals of the UN Convention on the Rights of the Child. Nevertheless, based on what has been learned, even greater efforts are required to improve data in order to effectively combat child maltreatment.}
}
@article{REN2021101457,
title = {An intelligent charging scheme maximizing the utility for rechargeable network in smart city},
journal = {Pervasive and Mobile Computing},
volume = {77},
pages = {101457},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101457},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000961},
author = {Yingying Ren and Anfeng Liu and Xingliang Mao and Fangfang Li},
keywords = {Mobile charging, Wireless energy transfer, Mobile chargers, Quality utility, Workload balance},
abstract = {The mobile charging scheme is a promising solution to extending the lifetime of the network by replenishing the energy for the sensing nodes, which has attracted more and more attention from the researchers. However, due to the limitation of energy storage both for sensing nodes and mobile chargers, not all the sensing nodes can be recharged in time by mobile chargers. Therefore, how to select appropriate sensing nodes and design the path for the mobile charger are the key to improve the system utility. This paper proposes an Intelligent Charging scheme Maximizing the Quality Utility (ICMQU) to design the charging path for the mobile charger. Comparing to the previous studies, we consider not only the utility of the data collected from the environment, but also the impact of sensing nodes with different quality. Quality Utility is proposed to optimize the charging path design. Besides, ICMQU designs the charging scheme for a single mobile charger and multiple mobile chargers simultaneously. For the charging scheme with multiple mobile chargers, the workload balance among different mobile chargers is also considered as well as the utility of the system. Extensive simulation results are provided, which demonstrates the proposed ICMQU scheme can significantly improve the utility of the system.}
}
@article{AGUILAR2021111530,
title = {A systematic literature review on the use of artificial intelligence in energy self-management in smart buildings},
journal = {Renewable and Sustainable Energy Reviews},
volume = {151},
pages = {111530},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111530},
url = {https://www.sciencedirect.com/science/article/pii/S136403212100808X},
author = {J. Aguilar and A. Garces-Jimenez and M.D. R-Moreno and Rodrigo García},
keywords = {Energy management system, Autonomous management architecture, Smart building, Artificial intelligence, Systematic literature review, Smart grid},
abstract = {Buildings are one of the main consumers of energy in cities, which is why a lot of research has been generated around this problem. Especially, the buildings energy management systems must improve in the next years. Artificial intelligence techniques are playing and will play a fundamental role in these improvements. This work presents a systematic review of the literature on researches that have been done in recent years to improve energy management systems for smart building using artificial intelligence techniques. An originality of the work is that they are grouped according to the concept of “Autonomous Cycles of Data Analysis Tasks”, which defines that an autonomous management system requires specialized tasks, such as monitoring, analysis, and decision-making tasks for reaching objectives in the environment, like improve the energy efficiency. This organization of the work allows us to establish not only the positioning of the researches, but also, the visualization of the current challenges and opportunities in each domain. We have identified that many types of researches are in the domain of decision-making (a large majority on optimization and control tasks), and defined potential projects related to the development of autonomous cycles of data analysis tasks, feature engineering, or multi-agent systems, among others.}
}
@article{LAWSON2021100714,
title = {Detecting dirty data using SQL: Rigorous house insurance case},
journal = {Journal of Accounting Education},
volume = {55},
pages = {100714},
year = {2021},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100714},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000014},
author = {James G. Lawson and Daniel A. Street},
keywords = {Data analytics, Accounting education, Dirty data, Structured query language (“SQL”), Data integrity},
abstract = {Proficiency with data analytics is an increasingly important skill within in the accounting profession. However, successful data analysis requires clean source data (i.e., source data without errors) in order to draw reliable conclusions. Although users often assume clean source data, this assumption is frequently incorrect. Therefore, identifying and remediating “dirty data” is a prerequisite to effective data analysis. You, an accountant working at a firm that specializes in data analytics, have been hired by Rigorous House Insurance to analyze the company’s claim insurance data. In addition to investigating specific issues mentioned by the company’s controller, you are tasked with identifying any other data integrity issues that you encounter and providing preventative information system internal control suggestions to the client to mitigate these issues in the future.}
}
@article{WU2021107792,
title = {Sentiment classification using attention mechanism and bidirectional long short-term memory network},
journal = {Applied Soft Computing},
volume = {112},
pages = {107792},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107792},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621007134},
author = {Peng Wu and Xiaotong Li and Chen Ling and Shengchun Ding and Si Shen},
keywords = {Attention mechanism, Bidirectional long short-term memory network, Sentiment classification, Social media, Word embedding},
abstract = {We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.}
}
@incollection{PRADHAN2021285,
title = {Chapter 13 - The role of IoT in smart cities: Challenges of air quality mass sensor technology for sustainable solutions},
editor = {Sudhir Kumar Sharma and Bharat Bhushan and Narayan C. Debnath},
booktitle = {Security and Privacy Issues in IoT Devices and Sensor Networks},
publisher = {Academic Press},
pages = {285-307},
year = {2021},
series = {Advances in ubiquitous sensing applications for healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-821255-4.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212554000134},
author = {Alok Pradhan and Bhuvan Unhelkar},
keywords = {Air quality monitoring, Smart and sustainable cities, Air quality sensors, Citizen science, Air quality management},
abstract = {Holistic management of air quality in cities is crucial to set a reasonable standard of living for its citizens. With the growing number of large cities across the world, the Internet of Things (IoT) is a technology that holds the promise to make cities smart and sustainable. A primary objective of a smart and sustainable city is to ensure urban air quality is clean and sustainably managed for future generations. Current regulation-based ambient air quality monitoring technology in urban environments involves sparsely distributed bulky and expensive equipment, which requires continuous calibration and maintenance. IoT devices can help measure, monitor, and abet the impact of poor air quality. This is so because of the ubiquitous nature of the devices, available connectivity in smart cities, and opportunities to analyze data within reasonable time to take corrective actions. This chapter discusses various aspects of air quality monitoring for smart cities capitalizing on IoT devices. Starting with a discussion on the background of air quality monitoring in the context of smart cities, this chapter outlines the challenges and the various ways in which IoT and Cloud-based applications can work together to overcome those challenges. Overall, this chapter proposes a three-tiered approach for effectively applying data to achieve better urban air quality. The first tier is a practical approach, the second tier is a strategic approach, and the third and final tier is a legislative approach.}
}
@article{GRASSOTORO2021100306,
title = {Brief overview of the future of metrology},
journal = {Measurement: Sensors},
volume = {18},
pages = {100306},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100306},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421002695},
author = {Federico {Grasso Toro} and Hugo Lehmann},
keywords = {Digital metrology, Digitalization of metrology, Metrology of digitalization, Digitalization strategies, Ontology of knowledge},
abstract = {This position paper opens the discussion about the term digital metrology, the underlying digital trends and the activities related to the digitalization of metrology. Firstly, we present a clarification of terms between digitization and digitalization and the current digitalization strategies of two representative national metrology institutes, members of EURAMET. Subsequently, we describe the current METAS strategy towards digital metrology by its three main pillars and ongoing research and development efforts. Finally, we discuss the state of the digitalization of metrology, as well as presenting our suggestions for the next steps towards the digital transformation of metrology.}
}
@article{JOHANNA2021361,
title = {Predictive model for the identification of activities of daily living (ADL) in indoor environments using classification techniques based on Machine Learning},
journal = {Procedia Computer Science},
volume = {191},
pages = {361-366},
year = {2021},
note = {The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.069},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014721},
author = {García-Restrepo Johanna and Ariza-Colpas {Paola Patricia} and Oñate-Bowen {Alvaro Agustín} and Suarez-Brieva {Eydy del Carmen} and Urina-Triana Miguel and De-la-Hoz-Franco Emiro and Díaz-Martínez {Jorge Luis} and Butt {Shariq Aziz} and Molina_Estren Diego},
keywords = {HAR, Human Activity Recognition, Machine Learning, ADL, Activity Daily Living},
abstract = {AI-based techniques have included countless applications within the engineering field. These range from the automation of important procedures in Industry and companies, to the field of Process Control. Smart Home (SH) technology is designed to help house residents improve their daily activities and therefore enrich the quality of life while preserving their privacy. An SH system is usually equipped with a collection of software interrelated with hardware components to monitor the living space by capturing the behavior of the resident and their occupations. By doing so, the system can report risks, situations, and act on behalf of the resident to their satisfaction. This research article shows the experimentation carried out with the human activity recognition dataset, CASAS Kyoto, through preprocessing and cleaning processes of the data, showing the Vía Regression classifier as an excellent option to process this type of data with an accuracy 99.7% effective}
}
@article{WIERINGA2021915,
title = {Data analytics in a privacy-concerned world},
journal = {Journal of Business Research},
volume = {122},
pages = {915-925},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319303078},
author = {Jaap Wieringa and P.K. Kannan and Xiao Ma and Thomas Reutterer and Hans Risselada and Bernd Skiera},
abstract = {Data is considered the new oil of the economy, but privacy concerns limit their use, leading to a widespread sense that data analytics and privacy are contradictory. Yet such a view is too narrow, because firms can implement a wide range of methods that satisfy different degrees of privacy and still enable them to leverage varied data analytics methods. Therefore, the current study specifies different functions related to data analytics and privacy (i.e., data collection, storage, verification, analytics, and dissemination of insights), compares how these functions might be performed at different levels (consumer, intermediary, and firm), outlines how well different analytics methods address consumer privacy, and draws several conclusions, along with future research directions.}
}
@article{FRIEDERICH2021589,
title = {Towards Data-Driven Reliability Modeling for Cyber-Physical Production Systems},
journal = {Procedia Computer Science},
volume = {184},
pages = {589-596},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007067},
author = {Jonas Friederich and Sanja Lazarova-Molnar},
keywords = {Cyber-Physical Production Systems, Reliability Analysis, Data-Driven Reliability Modeling},
abstract = {Reliability is one of the most important performance indicators in contemporary production facilities. Increasing reliability of manufacturing systems results in their prolonged lifetimes, and reduced maintenance and repair costs. Reliability modeling is a common technique for deriving reliability measurements and illustrating relevant fault-dependencies. There is a significant body of research focusing on hardware- and software reliability models, such as Fault Trees, Petri Nets and Markov Chains. Up until now, development of reliability models has been a labor-intensive and expert-knowledge-driven process. To remedy that, through the prevalence of data stemming from the new and technologically advanced manufacturing systems, we propose that data generated in modern manufacturing lines could be used to either automate or at least to support development of reliability models. In this paper, we elaborate on the details of our proposed framework for data-driven reliability assessment of cyber-physical production systems. We, furthermore, introduce a case study that will aid the development and testing of the proposed novel data-driven approach.}
}
@article{DWIVEDI2021101994,
title = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
journal = {International Journal of Information Management},
volume = {57},
pages = {101994},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121930917X},
author = {Yogesh K. Dwivedi and Laurie Hughes and Elvira Ismagilova and Gert Aarts and Crispin Coombs and Tom Crick and Yanqing Duan and Rohita Dwivedi and John Edwards and Aled Eirug and Vassilis Galanos and P. Vigneswara Ilavarasan and Marijn Janssen and Paul Jones and Arpan Kumar Kar and Hatice Kizgin and Bianca Kronemann and Banita Lal and Biagio Lucini and Rony Medaglia and Kenneth {Le Meunier-FitzHugh} and Leslie Caroline {Le Meunier-FitzHugh} and Santosh Misra and Emmanuel Mogaji and Sujeet Kumar Sharma and Jang Bahadur Singh and Vishnupriya Raghavan and Ramakrishnan Raman and Nripendra P. Rana and Spyridon Samothrakis and Jak Spencer and Kuttimani Tamilmani and Annie Tubadji and Paul Walton and Michael D. Williams},
keywords = {Artificial intelligence, AI, Cognitive computing, Expert systems, Machine learning, Research agenda},
abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.}
}
@article{ZHOU2021104955,
title = {A heterogeneous key performance indicator metadata model for air quality monitoring in sustainable cities},
journal = {Environmental Modelling & Software},
volume = {136},
pages = {104955},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104955},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220310124},
author = {Lianjie Zhou and Qingquan Li and Wei Tu and Chisheng Wang},
keywords = {KPI, Meta-model, Formulization framework, Heterogeneity, Geospatial sensor web, SDGs},
abstract = {Due to heterogeneous and inconsistent key performance indicators (KPIs) for the quantitative evaluation of a sustainable city's operational status, it is a great challenge to share multidimensional, multi-source and heterogeneous indicators. We propose a heterogeneous KPI capability representation model (HKPM) in our study. Based on the Meta Object Facility architecture, a nine-tuple multi-hierarchical meta-model is formulated to define the metadata components. Nine specific representation element datasets for specific KPIs are proposed to represent the meta-model. Besides, the KPI classification based on Sustainable Development Goals (SDGs) has been accomplished to support HKPM instantiated in concrete application. Experiments are conducted with the multi-type KPIs to validate the feasibility of HKPM, as shown in public service and Air Quality Index KPI instantiation example. Furthermore, the KPIs can be characterized in different dimensions, which can be modelled in a stereoscopic manner, promoting a comprehensive perception of sustainable cities.}
}
@article{AHUJA20214,
title = {Evolving term “accessibility” in spatial systems: Contextual evaluation of indicators},
journal = {Transport Policy},
volume = {113},
pages = {4-11},
year = {2021},
note = {Contemporary national and regional transport policy and planning},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2021.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X21000688},
author = {Richa Ahuja and Geetam Tiwari},
keywords = {Accessibility, Evolution, Review, Measures, Indicator, Barriers},
abstract = {Access terminology is evolving since its inception by Hansen in 1959. Accounting accessibility is central to multiple disciplines such as geography, transportation, health, economics, social sciences, etc. Developing indicators to measure access is a common practice and usually favor specific dimensions of access based on application. Although measuring accessibility and developing related indicators is a common practice, there are missing links in the indicator development, planning process, its implementation and related policy-making. Due to many available indicators, each differing in context, the practicality of implementation and their transferability is generally lost. Current work focuses on extracting commonalities between indicators and understanding how the contextual focus of indicators’ have changed over time to measure access. Requirements for improved access related policies, developing realistic measures and future research directions based on gaps in the identified access measures are suggested.}
}
@article{BIMONTE2021101231,
title = {Collect and analysis of agro-biodiversity data in a participative context: A business intelligence framework},
journal = {Ecological Informatics},
volume = {61},
pages = {101231},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101231},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000224},
author = {Sandro Bimonte and Olivier Billaud and Benoît Fontaine and Thomy Martin and Frédéric Flouvat and Ali Hassan and Nora Rouillier and Lucile Sautot},
keywords = {Data warehouse, Data science, Biodiversity, Agriculture},
abstract = {In France and Europe, farmland represents a large fraction of land cover. The study and assessment of biodiversity in farmland is therefore a major challenge. To monitor biodiversity across wide areas, citizen science programs have demonstrated their effectiveness and relevance. The involvement of citizens in data collection offers a great opportunity to deploy extensive networks for biodiversity monitoring. But citizen science programs come with two issues: large amounts of data to manage and large numbers of participants with heterogeneous skills, needs and expectations about these data. In this article, we offer a solution to these issues, concretized by an information system. The study is based on a real life citizen science program tailored for farmers. This information system provides data and tools at several levels of complexity, to fit the needs and the skills of several users, from citizens with basic IT knowledge to scientists with strong statistical background. The proposed system is designed as follows. First, a data warehouse stores the data collected by citizens. This data warehouse is modelled depending on future data analysis. Secondly, associated with the data warehouse, a standard OLAP tool enables citizens and scientists to explore data. To complete the OLAP tool, we implement and compare four feature selection methods, in order to rank explanatory factors according to their relevance. Finally, for users with extended statistical skills, we use Generalized Linear Mixed Models to explore the temporal dynamics of invertebrate diversity in farmland ecosystems. The proposed system, a combination of business intelligence tools, data mining methods and advanced statistics, offers an example of complete exploitation of data by several user profiles. The proposition is supported by a real life citizen science program, and can be used as a guideline to design information systems in the same field.}
}
@article{DELSO2021112,
title = {How to Design AI-Driven Clinical Trials in Nuclear Medicine},
journal = {Seminars in Nuclear Medicine},
volume = {51},
number = {2},
pages = {112-119},
year = {2021},
note = {Artificial Intelligence in Nuclear Medicine},
issn = {0001-2998},
doi = {https://doi.org/10.1053/j.semnuclmed.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0001299820301082},
author = {Gaspar Delso and Davide Cirillo and Joshua D Kaggie and Alfonso Valencia and Ur Metser and Patrick Veit-Haibach},
abstract = {Artificial intelligence (AI) is an overarching term for a multitude of technologies which are currently being discussed and introduced in several areas of medicine and in medical imaging specifically. There is, however, limited literature and information about how AI techniques can be integrated into the design of clinical imaging trials. This article will present several aspects of AI being used in trials today and how imaging departments and especially nuclear medicine departments can prepare themselves to be at the forefront of AI-driven clinical trials. Beginning with some basic explanation on AI techniques currently being used and existing challenges of its implementation, it will also cover the logistical prerequisites which have to be in place in nuclear medicine departments to participate successfully in AI-driven clinical trials.}
}
@article{MAHAJAN2021102628,
title = {From Do-It-Yourself (DIY) to Do-It-Together (DIT): Reflections on designing a citizen-driven air quality monitoring framework in Taiwan},
journal = {Sustainable Cities and Society},
volume = {66},
pages = {102628},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102628},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308453},
author = {Sachit Mahajan and Cyuan-Heng Luo and Dong-Yi Wu and Ling-Jyh Chen},
keywords = {Air pollution monitoring, Particulate matter exposure, Citizen science, Resilient cities, Low-cost sensors},
abstract = {Air pollution is a serious problem and has caused public health concerns all over the world. Despite the evidence, the preparedness and response of citizens has been limited. This underlines the importance of having sustainable air quality monitoring solutions that foster inclusion and multi-stakeholder partnerships for social-scientific interventions. This study illustrates how AirBox project has emerged in Taiwan, where makers and citizens use the sensors to sense air quality and provide the public with actionable data about their environments. The AirBox project includes elements of technology-innovation and citizen science: (1) Participatory Sensing – Static and mobile air quality sensing, (2) Open Data – Open hardware, software and access to data, (3) Co-creation Citizen Science – Citizen-led campaigns and forums, and (4) Outreach – Knowledge sharing, trust building and multi-stakeholder collaboration. The project uses a wide range of sensors to provide extendable solutions and data at fine spatio-temporal resolution. The results are highlighted using five cases studies that show how integrating social dimensions in an air quality monitoring framework can lead to public awareness, data-driven applications and environmentally sustainable cities. The multi-faceted approach highlights the effects of a bottom-up citizen science approach that considers local culture, practices and problems at grassroots.}
}
@article{IBRAHIM20211355,
title = {Is poor quality non-melanoma skin cancer data affecting high quality research and patient care?},
journal = {Journal of Plastic, Reconstructive & Aesthetic Surgery},
volume = {74},
number = {6},
pages = {1355-1401},
year = {2021},
issn = {1748-6815},
doi = {https://doi.org/10.1016/j.bjps.2020.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S1748681520307117},
author = {Nader Ibrahim and John Gibson and Stephen Ali and Thomas Dobbs and Iain S. Whitaker}
}
@article{WANG2021126293,
title = {A hybrid deep learning model with 1DCNN-LSTM-Attention networks for short-term traffic flow prediction},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {583},
pages = {126293},
year = {2021},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126293},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121005665},
author = {Ke Wang and Changxi Ma and Yihuan Qiao and Xijin Lu and Weining Hao and Sheng Dong},
keywords = {Traffic flow prediction, Deep learning, One-dimensional convolutional neural network, Long short-term memory network, Attentional mechanism},
abstract = {With the rapid development of social economy, the traffic volume of urban roads has raised significantly, which has led to increasingly serious urban traffic congestion problems, and has caused much inconvenience to people’s travel. By focusing on the complexity and long-term dependence of traffic flow sequences on urban road, this paper considered the traffic flow data and weather conditions of the road section comprehensively, and proposed a short-term traffic flow prediction model based on the attention mechanism and the 1DCNN-LSTM network. The model combined the time expansion of the CNN and the advantages of the long-term memory of the LSTM. First, the model employs 1DCNN network to extract the spatial features in the road traffic flow data. Second, the output spatial features are considered as the input of LSTM neural network to extract the time features in road traffic flow data, and the long-term dependence characteristics of LSTM neural network are adopted to improve the prediction accuracy of traffic flow. Next, the spatio-temporal characteristics of road traffic flow were regarded as the input of the regression prediction layer, and the prediction results corresponding to the current input were calculated. Finally, the attention mechanism was introduced on the LSTM side to give enough attention to the key information, so that the model can focus on learning more important data features, and further improve the prediction performance. The experimental results showed that the prediction effect of the 1DCNN-LSTM-Attention model under the weather factor was better than that without considering the weather factor. At the same time, compared with traditional neural network models, the prediction effect of the proposed model revealed faster convergence speed and higher prediction accuracy. It can be found that for short-term traffic flow prediction on urban roads, the 1DCNN-LSTM network structure considering the attention mechanism provides superior features.}
}
@article{KRISHEN2021183,
title = {A broad overview of interactive digital marketing: A bibliometric network analysis},
journal = {Journal of Business Research},
volume = {131},
pages = {183-195},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002241},
author = {Anjala S. Krishen and Yogesh K. Dwivedi and N. Bindu and K. Satheesh Kumar},
keywords = {Digital marketing, Interactive marketing, Mobile marketing, e-marketing, e-advertising, e-Word-of-mouth},
abstract = {The widespread adoption of digital technologies and online social networks has revolutionized the way marketers engage with consumers. By deploying various digital platforms and information and communication technology (ICT) tools (e.g., smartphones, social media, mobile apps, electronic billboards, etc.), organizations can compete with more objective, relational, and interactive marketing techniques. The adoption of innovative devices and data-driven marketing, specifically in digital advertising, provides both a wide and efficient reach. Consequently, digital marketing (DM) triggered the creation of more informed, empowered, and connected groups of customers in both the real and virtual worlds. This paper tracks research dynamics in interactive digital marketing by identifying the stages of evolution of major topics, articles, citation and co-citation networks, using various computational techniques, including growth curve analysis and citation network analysis of bibliometric information. Finally, the study offers contributions to the field of interactive digital marketing as an international and interdisciplinary field of research.}
}
@article{CARVALHO2021499,
title = {On the relevance of data science for flight delay research: a systematic review},
journal = {Transport Reviews},
volume = {41},
number = {4},
pages = {499-528},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1861123},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000666},
author = {Leonardo Carvalho and Alice Sternberg and Leandro {Maia Gonçalves} and Ana {Beatriz Cruz} and Jorge A. Soares and Diego Brandão and Diego Carvalho and Eduardo Ogasawara},
keywords = {Flight delay, data science, data management, data analytics, systematic review},
abstract = {ABSTRACT
Flight delays are a significant problem for society as they evenly impair airlines, transport companies, air traffic controllers, facility managers, and passengers. Studying prior flight data is an essential activity for every player involved in the air transportation system. Besides, developing accurate prediction models for flight delays is a crucial component of the decision-making process. Prescribing actions to solve on-going delays is an even challenging task due to the air transportation system complexity. In this regard, this paper presents a thorough literature review of data science techniques used for investigating flight delays. This work proposes a taxonomy and compiles the initiatives used to address the flight delay studies. It also offers a systematic literature review that describes the trends of the field and methods to analyse the applicability of newly proposed methods.}
}
@article{YIN2021104274,
title = {Risk based arsenic rational sampling design for public and environmental health management},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {211},
pages = {104274},
year = {2021},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2021.104274},
url = {https://www.sciencedirect.com/science/article/pii/S0169743921000423},
author = {Lihao Yin and Huiyan Sang and Douglas J. Schnoebelen and Brian Wels and Don Simmons and Alyssa Mattson and Michael Schueller and Michael Pentella and Susie Y. Dai},
keywords = {Private well, Spatially clustered function model, Resource management},
abstract = {Groundwater contaminated with arsenic has been recognized as a global threat, which negatively impacts human health. Populations that rely on private wells for their drinking water are vulnerable to the potential arsenic-related health risks such as cancer and birth defects. Arsenic exposure through drinking water is among one of the primary arsenic exposure routes that can be effectively managed by active testing and water treatment. From the public and environmental health management perspective, it is critical to allocate the limited resources to establish an effective arsenic sampling and testing plan for health risk mitigation. We present a spatially adaptive sampling design approach based on an estimation of the spatially varying underlying contamination distribution. The method is different from traditional sampling design methods that often rely on a spatially constant or smoothly varying contamination distribution. In contrast, we propose a statistical regularization method to automatically detect spatial clusters of the underlying contamination risk from the currently available private well arsenic testing data in the USA, Iowa. This approach allows us to develop a sampling design method that is adaptive to the changes in the contamination risk across the identified clusters. We provide the spatially adaptive sample size calculation and sampling location determination at different acceptance precision and confidence levels for each cluster. The spatially adaptive sampling approach may effectively mitigate the arsenic risk from the resource management perspectives. The model presents a framework that can be widely used for other environmental contaminant monitoring and sampling for public and environmental health.}
}
@article{TAY2021101749,
title = {Industry 4.0: Current practice and challenges in Malaysian manufacturing firms},
journal = {Technology in Society},
volume = {67},
pages = {101749},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101749},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21002244},
author = {S.I. Tay and J. Alipal and T.C. Lee},
keywords = {Industry 4.0, Manufacturing firms, Industry 4.0 exploration, Internet of things, Policy, Data management},
abstract = {This research employed a qualitative approach to discuss the current practice and challenges of Malaysian manufacturing firms in the implementation of Industry 4.0. The study examined data from seven manufacturing companies pursuing Industry 4.0 initiatives to identify various options for their strategies. The study found that the implementation of Industry 4.0 in the manufacturing firms is still in the exploratory stage. The companies involved in this study were discovered to conduct exploration using an adaptive-like framework. That is, throughout the process, the majority of the subjects are 'trying and adding' Industry 4.0 to their operations. Their trial-and-error approach is based on what is feasible and effective in their manufacturing environment. Overall, the investigation determined that data management and integration, as well as personnel re-education, were the respondents' primary operational challenges.}
}
@article{HE2021116767,
title = {Development of the Leeuwin Current on the northwest shelf of Australia through the Pliocene-Pleistocene period},
journal = {Earth and Planetary Science Letters},
volume = {559},
pages = {116767},
year = {2021},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2021.116767},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X21000261},
author = {Yuxin He and Huanye Wang and Zhonghui Liu},
keywords = {Leeuwin Current, Indonesian Throughflow, northwest shelf of Australia, Pliocene-Pleistocene, temperatures, primary productivity},
abstract = {Although the Leeuwin Current (LC) is thought to play a pivotal role in climatic and oceanic systems of the western Australian region, how the LC developed through the Pliocene-Pleistocene period remains elusive. Here we used biomarker records to reconstruct variations of temperatures and primary productivity on the northwest shelf of Australia over the last 6 million years. Since ∼1.2 million years ago (Ma), our sea surface temperature record indicates progressive warming, with temperature values comparable to those in the Indo-Pacific Warm Pool, in contrast with the long-term global cooling trend. The regional surface warming was accompanied by suppressed primary productivity, together indicating prevailing warm, low-salinity, nutrient-deficient surface water, and thus a stronger LC since the Mid-Pleistocene Transition. During 4–1.2 Ma, greater surface temperature gradient between the Indo-Pacific Warm Pool and the northwest shelf of Australia and higher primary productivity seem to suggest a generally weaker LC. Warmer temperatures and lower productivity suggest a plausible existence of the LC during 6–4 Ma, but more work is required to confirm this. Impact of sea level and the Indonesian Throughflow on the LC strength may exist, but did not dominate through the Pliocene-Pleistocene period, considering different variation patterns among them. We propose the stronger LC after ∼1.2 Ma was more likely triggered by enhanced atmospheric circulation. Although the increased LC after ∼1.2 Ma may have potentially brought additional moisture to the Australian continent during the interglacial periods, it has not overturned the long-term drying trend through the Pliocene-Pleistocene period.}
}
@article{YANG2021103303,
title = {Freeway accident detection and classification based on the multi-vehicle trajectory data and deep learning model},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103303},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103303},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003120},
author = {Da Yang and Yuezhu Wu and Feng Sun and Jing Chen and Donghai Zhai and Chuanyun Fu},
keywords = {Freeway traffic accident, Vehicle trajectory, Deep Convolutional Neural Network, Accident detection and classification},
abstract = {The freeway accident detection and classification have attracted much attention of researchers in the past decades. With the popularity of Global Navigation Satellite System (GNSS) on mobile phones and onboard equipment, increasing amounts of real-time vehicle trajectory data can be obtained more and more easily, which provides a potential way to use the multi-vehicle trajectory data to detect and classify an accident on freeways. The data has the advantages of low cost, high penetration, high real-time performance, and being robust to the outdoor environment. Therefore, this paper proposes a new method for accident detection and classification based on the multi-vehicle trajectory data. Different from the existing methods using GNSS positioning data, the proposed method not only uses the position information of the related vehicles but also tries to capture the development tendencies of the trajectories of accident vehicles over a period of time. A Deep Convolutional Neural Network model is developed to recognize an accident from the normal driving of vehicles and also identify the type of the accident, and the six types of traffic accidents are considered in this study. To train and test the proposed model, the simulated trajectory data is generated based on PC-Crash, including the normal driving trajectories and the trajectories before, in, and after an accident. The results indicate that the detection accuracy of the proposed method can reach up to 100%, and the classification accuracy can reach up to 95%, which both outperform the existing methods using other data. In addition, to ensure the robustness of the detection accuracy, at least 1 s of duration and 5 Hz of frequency for the trajectory data should be adopted in practice. The study will help to accurately and fastly detect an accident, recognize the accident type, and future judge who is liable for the accident.}
}
@article{WU2021102594,
title = {Multi-label active learning from crowds for secure IIoT},
journal = {Ad Hoc Networks},
volume = {121},
pages = {102594},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102594},
url = {https://www.sciencedirect.com/science/article/pii/S157087052100130X},
author = {Ming Wu and Qianmu Li and Muhammad Bilal and Xiaolong Xu and Jing Zhang and Jun Hou},
keywords = {Crowdsourcing, Secure IIoT, Annotation consensus, Multi-label learning, Active learning},
abstract = {With the development of IIoT (Industrial Internet of Things), Artificial Intelligence technology is widely used in many research areas, such as image classification, speech recognition, and information retrieval. Traditional supervised machine learning obtains labels from high-quality oracles, which is high cost and time-consuming and does not consider security. Since multi-label active learning becomes a hot topic, it is more challenging to train efficient and secure classification models, and reduce the label cost in the field of IIoT. To address this issue, this research focuses on the secure multi-label active learning for IIoT using an economical and efficient strategy called crowdsourcing, which involves querying labels from multiple low-cost annotators with various expertise on crowdsourcing platforms rather than relying on a high-quality oracle. To eliminate the effects of annotation noise caused by imperfect annotators, we propose the Multi-label Active Learning from Crowds (MALC) method, which uses a probabilistic model to simultaneously compute the annotation consensus and estimate the classifier’s parameters while also taking instance similarity into account. Then, to actively choose the most informative instances and labels, as well as the most reliable annotators, an instance-label-annotator triplets selection technique is proposed. Experimental results on two real-world data sets show that the performance of MALC is superior to existing methods.}
}
@article{FADLELMOLA2021103900,
title = {Data Management Plans in the genomics research revolution of Africa: Challenges and recommendations},
journal = {Journal of Biomedical Informatics},
volume = {122},
pages = {103900},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103900},
url = {https://www.sciencedirect.com/science/article/pii/S153204642100229X},
author = {Faisal M. Fadlelmola and Lyndon Zass and Melek Chaouch and Chaimae Samtal and Verena Ras and Judit Kumuthini and Sumir Panji and Nicola Mulder},
keywords = {Data Management Plan, Africa, FAIR, Funders, Data sharing, Genomics data management},
abstract = {Drafting and writing a data management plan (DMP) is increasingly seen as a key part of the academic research process. A DMP is a document that describes how a researcher will collect, document, describe, share, and preserve the data that will be generated as part of a research project. The DMP illustrates the importance of utilizing best practices through all stages of working with data while ensuring accessibility, quality, and longevity of the data. The benefits of writing a DMP include compliance with funder and institutional mandates; making research more transparent (for reproduction and validation purposes); and FAIR (findable, accessible, interoperable, reusable); protecting data subjects and compliance with the General Data Protection Regulation (GDPR) and/or local data protection policies. In this review, we highlight the importance of a DMP in modern biomedical research, explaining both the rationale and current best practices associated with DMPs. In addition, we outline various funders’ requirements concerning DMPs and discuss open-source tools that facilitate the development and implementation of a DMP. Finally, we discuss DMPs in the context of African research, and the considerations that need to be made in this regard.}
}
@article{LIU2021144649,
title = {Regional characteristics of children's blood lead levels in China: A systematic synthesis of national and subnational population data},
journal = {Science of The Total Environment},
volume = {769},
pages = {144649},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144649},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720381808},
author = {Yang Liu and Feiyan Liu and Kylie Fei Dong and Yongning Wu and Xingfen Yang and Jintao Yang and Hong Tan and Xiaojun Niu and Xinyuan Zhao and Gexin Xiao and Shaoqi Zhou},
keywords = {Blood lead, Child health, Regional characteristics, China, Prefectures},
abstract = {The blood lead levels (BLLs) of children in China remain notably high in many areas. We aimed to summarise the relevant regional characteristics, identifying problematic areas and the causes of lead pollution. We searched the databases of PubMed, China National Knowledge Infrastructure (CNKI), and Wanfang Data, systematically reviewing 219 articles published from January 2010 to September 2020. In doing so, we assessed the BLLs noted in 220 prefectures across China. Data were organised using Geographic Information Systems (GIS) mapping. Out of a total of 629,627 children sampled, we found that the average blood lead level (BLL) of children included in our study is 50.61 ± 13.63 μg/L, which slightly exceeds the 50.00 μg/L US standard. Within the sample, 8.75% had BLLs higher than 100.00 μg/L. Children living in Liaoning, Hebei, Shanxi, Jiangxi, Anhui, Fujian, Guizhou, Yunnan, and Guangxi had notably high BLLs, at more than 60.00 μg/L. A total of 112 municipalities had an average children's BLL above 50.00 μg/L. Furthermore, Chenzhou, Linfen, Yuncheng, and Hechi had the highest children's BLLs, with average values above 100.00 μg/L. The leading contributors to lead pollution are lead mining, lead recovery and the smelting industry. Nonetheless, the lead-acid battery industry needs more attention. Although data suggest that BLLs are decreasing in China, many areas still have high BLLs that need to be monitored. Moreover, national standards must improve to decrease acceptable BLL thresholds for children.}
}
@incollection{CHAKRABORTY2021191,
title = {Chapter 9 - Industry sustainable supply chain management with data science},
editor = {Jennifer Dunn and Prasanna Balaprakash},
booktitle = {Data Science Applied to Sustainability Analysis},
publisher = {Elsevier},
pages = {191-202},
year = {2021},
isbn = {978-0-12-817976-5},
doi = {https://doi.org/10.1016/B978-0-12-817976-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179765000103},
author = {Deboleena Chakraborty and Richard K. Helling},
keywords = {Sustainability, Supply chain, Carbon footprints, Life cycle assessment (LCA), Big data analytics, Data quality, GHG reduction, Multi-objective optimization, Geospatial information},
abstract = {Life cycle assessment (LCA) is a quantitative tool to bring environmental insights into decisions, supplementing consideration of cost, performance and social impact. Calculation of “carbon footprints” or other metrics derived from LCA typically requires data and information drawn from many sources, both within an organization and externally. Knowing LCA metrics for all products and companies could help society understand the most significant opportunities and trade-offs in the quest for sustainability. A barrier to this state of knowledge is the ability to create, access, manage and verify the extensive data required. New information technology and analytics can allow LCA to get closer to the ideal state, reducing the time required to do an assessment and increasing the quality of the results.}
}
@article{KORU2021979,
title = {Bringing Quality Health Care Home via Technology Innovations},
journal = {Journal of the American Medical Directors Association},
volume = {22},
number = {5},
pages = {979-980},
year = {2021},
issn = {1525-8610},
doi = {https://doi.org/10.1016/j.jamda.2021.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1525861021003376},
author = {Güneş Koru}
}
@article{KRITTANAWONG2021780,
title = {Opportunities and challenges for artificial intelligence in clinical cardiovascular genetics},
journal = {Trends in Genetics},
volume = {37},
number = {9},
pages = {780-783},
year = {2021},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168952521000858},
author = {Chayakrit Krittanawong and Kipp W. Johnson and Benjamin S. Glicksberg},
abstract = {A combination of emerging genomic and artificial intelligence (AI) techniques may ultimately unlock a deeper understanding of heterogeneity and biological complexities in cardiovascular diseases (CVDs), leading to advances in prognostic guidance and personalized therapies. We discuss the state of AI in cardiovascular genetics, current applications, limitations, and future directions of the field.}
}
@article{MOURTZIS2021525,
title = {Equipment Design Optimization Based on Digital Twin Under the Framework of Zero-Defect Manufacturing},
journal = {Procedia Computer Science},
volume = {180},
pages = {525-533},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.271},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003203},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Digital Twin, Machine Design, Zero-Defect Manufacturing},
abstract = {A digitalized Smart Factory can be considered as a data island. Moreover, engineers have focused on the development of new technologies and techniques not only for transforming information to data but also to achieve efficient data utilization to further optimize manufacturing processes. However, the Zero-Defect Manufacturing concept has emerged, where the main goal is production optimization. The cornerstone in achieving the factories of the future is to further optimize the design of new assets so as they comply with the unique requirements of the customers. Therefore, this paper proposes the conceptualization, design, and initial development of a platform for the utilization of data derived from industrial environments for the optimization of the equipment design. The main aspects of the proposed framework are the data acquisition, data processing and the simulation. The applicability of the proposed framework has been tested in a laboratory-based machine shop utilizing data from a real-life industrial scenario.}
}
@article{BELFIELD2021104956,
title = {Determination of “fitness-for-purpose” of quantitative structure-activity relationship (QSAR) models to predict (eco-)toxicological endpoints for regulatory use},
journal = {Regulatory Toxicology and Pharmacology},
volume = {123},
pages = {104956},
year = {2021},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2021.104956},
url = {https://www.sciencedirect.com/science/article/pii/S0273230021000969},
author = {Samuel J. Belfield and Steven J. Enoch and James W. Firman and Judith C. Madden and Terry W. Schultz and Mark T.D. Cronin},
keywords = { models, QSAR, Toxicity prediction, Uncertainty, Regulatory use},
abstract = {In silico models are used to predict toxicity and molecular properties in chemical safety assessment, gaining widespread regulatory use under a number of legislations globally. This study has rationalised previously published criteria to evaluate quantitative structure-activity relationships (QSARs) in terms of their uncertainty, variability and potential areas of bias, into ten assessment components, or higher level groupings. The components have been mapped onto specific regulatory uses (i.e. data gap filling for risk assessment, classification and labelling, and screening and prioritisation) identifying different levels of uncertainty that may be acceptable for each. Twelve published QSARs were evaluated using the components, such that their potential use could be identified. High uncertainty was commonly observed with the presentation of data, mechanistic interpretability, incorporation of toxicokinetics and the relevance of the data for regulatory purposes. The assessment components help to guide strategies that can be implemented to improve acceptability of QSARs through the reduction of uncertainties. It is anticipated that model developers could apply the assessment components from the model design phase (e.g. through problem formulation) through to their documentation and use. The application of the components provides the possibility to assess QSARs in a meaningful manner and demonstrate their fitness-for-purpose against pre-defined criteria.}
}
@article{WANG2021119279,
title = {Generalized models to predict the lower heating value (LHV) of municipal solid waste (MSW)},
journal = {Energy},
volume = {216},
pages = {119279},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.119279},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220323860},
author = {Dan Wang and Yu-Ting Tang and Jun He and Fei Yang and Darren Robinson},
keywords = {LHV prediction, Physical composition of municipal solid waste, Multiple regression, Artificial neural network},
abstract = {Accurately and efficiently predicting the LHV of MSW is vital for designing and operating a waste-to-energy plant. However, previous prediction models possess limited geographical applicability. In this paper, we employ multiple linear regression and artificial neural network (ANN) techniques to predict LHV. These data-driven models utilize 151 globally distributed datasets identified during a systematic literature review, describing the wet physical composition of MSW and measured LHV. The results show that models built via both methods exhibited acceptable and compatible levels of performance in predicting LHV, based on the multiple statistical indicators. However, the ANN model proved to be more robust in handling of datasets of diverse quality. Models developed from both methods demonstrate clearly that the wet proportion of food waste has a negative impact on LHV. Supported by the strong and significant correlation between food waste and moisture content, we concluded that the negative impact of high moisture content in food waste on LHV outweighed its calorific value. Separating food waste or any other waste with high moisture content from the MSW for incineration can significantly improve energy recovery efficiency. Contrary to expectation, the models also reveal a higher contribution of paper waste to the LHV of MSW than plastic waste.}
}
@article{METCALF2021100507,
title = {Challenges in evaluating risks and policy options around endemic establishment or elimination of novel pathogens},
journal = {Epidemics},
volume = {37},
pages = {100507},
year = {2021},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2021.100507},
url = {https://www.sciencedirect.com/science/article/pii/S1755436521000566},
author = {C. Jessica E. Metcalf and Soa Fy Andriamandimby and Rachel E. Baker and Emma E. Glennon and Katie Hampson and T. Deirdre Hollingsworth and Petra Klepac and Amy Wesolowski},
keywords = {Endemic, Epidemic, Mathematical model, Elimination, Emergence},
abstract = {When a novel pathogen emerges there may be opportunities to eliminate transmission - locally or globally - whilst case numbers are low. However, the effort required to push a disease to elimination may come at a vast cost at a time when uncertainty is high. Models currently inform policy discussions on this question, but there are a number of open challenges, particularly given unknown aspects of the pathogen biology, the effectiveness and feasibility of interventions, and the intersecting political, economic, sociological and behavioural complexities for a novel pathogen. In this overview, we detail how models might identify directions for better leveraging or expanding the scope of data available on the pathogen trajectory, for bounding the theoretical context of emergence relative to prospects for elimination, and for framing the larger economic, behavioural and social context that will influence policy decisions and the pathogen’s outcome.}
}
@article{WANG20211,
title = {Knowledge transfer methods for expressing product design information and organization},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {1-15},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301965},
author = {Haishuo Wang and Ke Chen and Hongmei Zheng and Guojun Zhang and Rui Wu and Xiaopeng Yu},
keywords = {Product design, Information gap, Hypercycle theory, Information processing, Information carrier},
abstract = {Product design information represents not only the carrier of design but also the significant digital assets of businesses. At present, manufacturing is facing an environment with mass, fragmented, real-time, and multi-scene digital information in the process of product design. To improve the availability of information resources and the efficiency of information reuse as well as to achieve the sharing of production means, the expression of product design information should be optimized in information storage. In addition, a new ecosystem should be built for information increments, making the participants of every link in the process of product design become the contributors of information. Considering the unique aspects of design group individuals, this paper builds a knowledge transfer model that capitalizes on hypercycle theory and proposes the concept of modularizing information carriers and information processing methods. It comprehensively analyses the expression methodology and organizational attributes of product design information and proposes a knowledge transfer carrier model constructed via discretized fragmented semantic information, which is concretely implemented as informative product file labelling. By combining personnel, information carriers and information dissemination networks, this paper provides the functionality and architecture needed to build an information processing platform using social networking software (SNS). Some application scenarios are described by using this processing method for production information; the development process for an automotive air filter is shown as an example. The results of this study suggest that the proposed method is conducive to improving the expressions of product design information and the interactions among participants. The simplicity of the labelling process and the intuitive label content greatly reduce the usability threshold and the losses caused by information gaps, creating a precondition for many types of people to fully participate in the product design information knowledge transfer cycle.}
}
@article{STAVELIN2021101269,
title = {Applying object detection to marine data and exploring explainability of a fully convolutional neural network using principal component analysis},
journal = {Ecological Informatics},
volume = {62},
pages = {101269},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101269},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000601},
author = {Herman Stavelin and Adil Rasheed and Omer San and Arne Johan Hestnes},
keywords = {Neural networks, PCA, Object detection, XAI, Machine learning, YOLO},
abstract = {With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant.}
}
@article{FRITZSCHE2021683,
title = {Industrial Applications of Artificial Intelligence: From Grand Stories of Digital Disruption to Actual Progress},
journal = {Procedia CIRP},
volume = {104},
pages = {683-688},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.115},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010131},
author = {Albrecht Fritzsche and Philipp Gölzer},
keywords = {Artificial Intelligence, Data-Driven Operations Management, Digital Transformation Narratives, Industry 4.0},
abstract = {Data-driven operations management goes along with narratives of disruptive change and new potential for innovation. We study how these narratives are reflected in the outcomes of 82 implementation projects that took place during the last ten years. The analysis of the projects identifies varying focal points in different industrial sectors. Radical steps towards new forms of data-driven operations management have only been achieved in exceptional cases. For the most part, new technical solutions follow given organizational structures and preserve extant business processes. We describe typical implementation patterns, compare them across industries and discuss different interpretations of the findings.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@article{LABONTE2021101870,
title = {Tweets and transitions: Exploring Twitter-based political discourse regarding energy and electricity in Ontario, Canada},
journal = {Energy Research & Social Science},
volume = {72},
pages = {101870},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2020.101870},
url = {https://www.sciencedirect.com/science/article/pii/S221462962030445X},
author = {D. Labonte and I.H. Rowlands},
keywords = {Energy politics, Social media analysis, Twitter, Sustainability transitions, Ontario, Canada},
abstract = {The article explores how Twitter data can inform the study of the socio-political dimensions of sustainability transitions. Twitter is a widely used microblogging platform that allows users to share short comments, media, and links, and that offers researchers significant data collection opportunities. Twitter-based research has been growing in application in many disciplines but has not been prominently used in relation to sustainability transitions or sustainable energy research. This study aims to characterize the Twitter-based conversations regarding energy issues and politics in Ontario, Canada. The analysis in this article is based on 6946 tweets, from 2841 unique users, which were collected between September 2, 2017 and January 12, 2018. The Twitter-based discourse regarding energy issues in Ontario is described by a minority of very engaged users contributing disproportionately to the conversation, the most engaged users contributing different types of tweets to the conversation, and overall engagement that varies based on news events. Coding based on manual interpretation of the tweets by the most engaged users and those tweets that were highly retweeted identified a discourse that was highly partisan and often highlighted economic issues associated with electricity costs. Topics commonly associated with sustainable energy transitions were not prominent in the Twitter discourse. Additionally, the analysis suggests that users lacking traditional political empowerment can influence the political discourse on Twitter through high levels of retweets; however, savvy and strategic use of Twitter communication, rather than simply engagement with an issue, is important in generating consistent amplification from other users.}
}
@article{PLEY2021e739,
title = {Digital and technological innovation in vector-borne disease surveillance to predict, detect, and control climate-driven outbreaks},
journal = {The Lancet Planetary Health},
volume = {5},
number = {10},
pages = {e739-e745},
year = {2021},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(21)00141-8},
url = {https://www.sciencedirect.com/science/article/pii/S2542519621001418},
author = {Caitlin Pley and Megan Evans and Rachel Lowe and Hugh Montgomery and Sophie Yacoub},
abstract = {Summary
Vector-borne diseases are particularly sensitive to changes in weather and climate. Timely warnings from surveillance systems can help to detect and control outbreaks of infectious disease, facilitate effective management of finite resources, and contribute to knowledge generation, response planning, and resource prioritisation in the long term, which can mitigate future outbreaks. Technological and digital innovations have enabled the incorporation of climatic data into surveillance systems, enhancing their capacity to predict trends in outbreak prevalence and location. Advance notice of the risk of an outbreak empowers decision makers and communities to scale up prevention and preparedness interventions and redirect resources for outbreak responses. In this Viewpoint, we outline important considerations in the advent of new technologies in disease surveillance, including the sustainability of innovation in the long term and the fundamental obligation to ensure that the communities that are affected by the disease are involved in the design of the technology and directly benefit from its application.}
}
@article{BUSCHMANN2021313,
title = {Data-driven decision support for process quality improvements},
journal = {Procedia CIRP},
volume = {99},
pages = {313-318},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003218},
author = {Daniel Buschmann and Chrismarie Enslin and Hannes Elser and Daniel Lütticke and Robert H. Schmitt},
keywords = {Data-Driven Decisions, Machine Learning, Defect Detection, Mass Production Manufacturing, Statistical Process Control, Random Forests},
abstract = {This paper presents a data-driven approach for improving the process quality of production systems. Therefore, the product quality is detected during the production process. The worker is provided with reasonable parameter recommendations about the production process as decision support to improve the process quality. To achieve this, a cross-process data analysis of the process and quality data is carried out using decision trees. The results are visualized in a comprehensible form for the worker. Based on a case study from mass production, the approach is evaluated and its performance is demonstrated in comparison to classical statistical methods.}
}
@article{SIRCAR2021379,
title = {Application of machine learning and artificial intelligence in oil and gas industry},
journal = {Petroleum Research},
volume = {6},
number = {4},
pages = {379-391},
year = {2021},
issn = {2096-2495},
doi = {https://doi.org/10.1016/j.ptlrs.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096249521000429},
author = {Anirbid Sircar and Kriti Yadav and Kamakshi Rayavarapu and Namrata Bist and Hemangi Oza},
keywords = {Artificial intelligence, Machine learning, Upstream, Oil and gas industry, Petroleum exploration},
abstract = {Oil and gas industries are facing several challenges and issues in data processing and handling. Large amount of data bank is generated with various techniques and processes. The proper technical analysis of this database is to be carried out to improve performance of oil and gas industries. This paper provides a comprehensive state-of-art review in the field of machine learning and artificial intelligence to solve oil and gas industry problems. It also narrates the various types of machine learning and artificial intelligence techniques which can be used for data processing and interpretation in different sectors of upstream oil and gas industries. The achievements and developments promise the benefits of machine learning and artificial intelligence techniques towards large data storage capabilities and high efficiency of numerical calculations. In this paper a summary of various researchers work on machine learning and artificial intelligence applications and limitations is showcased for upstream and sectors of oil and gas industry. The existence of this extensive intelligent system could really eliminate the risk factor and cost of maintenance. The development and progress using this emerging technologies have become smart and makes the judgement procedure easy and straightforward. The study is useful to access intelligence of different machine learning methods to declare its application for distinct task in oil and gas sector.}
}
@article{MARMIER20211144,
title = {Towards a proactive vision of the training for the 4.0 Industry: From the required skills diagnostic to the training of employees},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {1144-1149},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.135},
url = {https://www.sciencedirect.com/science/article/pii/S240589632100896X},
author = {François Marmier and Ioana Deniaud and Ivana Rasovska and Jean-Louis Michalak},
keywords = {Active Learning, leaning path to 4.0, Engineering education, Human factor, Industry 4.0, Learning Factory, Skills},
abstract = {The digitalisation increase in industrial processes is perceived, by companies, as an opportunity to grow up their competitivity. Data are more and more accessible, potentially allowing making better decisions at all the level of the company. Then, job profiles and their required skills are changing. However, if competencies focused on software tools, programming, data analysis, simulation, virtual design, automatics and electronics becomes necessary, the initial trainings and continuous trainings are not changing as fast. Moreover, if new technologies are more available in companies, the workforce suffers of a lack of preparation. It generates risks of mistakes, improper use of tools and information, under performed activities, insufficiently informed decision. A global vision of how to train the whole industrial network is necessary to generate a progress of the whole industry. Workers must get the right skills for their activities in order to become a factor of efficiency for their workshop and consequently for the whole logistic chain. In that way, the role of the universities is to develop trainings for up-to-date needs as the industry 4.0. For this purpose, this paper introduced an overview of how to propose actual trainings on the topic of the Industry 4.0 both customized for the companies and for the learners. We detail more specifically in this paper 3 tools we develop at the University of Strasbourg: (1) a diagnostic tool to get the maturity level of companies and propose adapted learning paths. (2) a set of grids to design adapted learning path to the different work. (3) a Learning Factory to allow a learning by doing way.}
}
@article{MAK2021102868,
title = {Comparative assessments and insights of data openness of 50 smart cities in air quality aspects},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102868},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102868},
url = {https://www.sciencedirect.com/science/article/pii/S221067072100158X},
author = {Hugo Wai Leung Mak and Yun Fat Lam},
keywords = {Smart cities, Air quality, Open data policy, Environmental monitoring, Data sharing and privacy, Future development of air quality network},
abstract = {Data Openness is considered as an indispensable component for scientific innovation, community engagement and smart city development. In this study, a Data Openness in Air Quality (DOAQ) framework that consists of 3 tiers with a total of 23 open data principles was established to assess and monitor the status and development of data sharing, release and centralization of air quality information in the top 50 smart cities (Top50SC) around the world. The DOAQ utilizes additive formulas with predefined coefficients to obtain scores in each tier, thus reflecting the relative importance on data availability and visibility of different air quality data. The scores of DOAQ were compared with the smart cities scorings from Eden Strategy Institute and ONG&ONG Pte Ltd. (2018), and other socioeconomic attributes (i.e., social, political and humane) within the current study. Strong correlations (i.e., 0.4−0.6) among these indices implicate that the status of air quality reporting could be a good proxy to gauge the environmental data openness in a city. Lastly, good practices (e.g., apps and air quality forecasts), essential criteria and directions for future smart city development on air quality reporting were summarized, with the aim of laying down practical and efficient guidelines for individual smart city that desires to seek for improvements in air quality data openness.}
}
@article{NELSON202197,
title = {Crowdsourced data for bicycling research and practice},
journal = {Transport Reviews},
volume = {41},
number = {1},
pages = {97-114},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1806943},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000447},
author = {Trisalyn Nelson and Colin Ferster and Karen Laberee and Daniel Fuller and Meghan Winters},
keywords = {Crowdsourced, bicycling, exposure, safety, infrastructure, attitudes},
abstract = {ABSTRACT
Cities are promoting bicycling for transportation as an antidote to increased traffic congestion, obesity and related health issues, and air pollution. However, both research and practice have been stalled by lack of data on bicycling volumes, safety, infrastructure, and public attitudes. New technologies such as GPS-enabled smartphones, crowdsourcing tools, and social media are changing the potential sources for bicycling data. However, many of the developments are coming from data science and it can be difficult evaluate the strengths and limitations of crowdsourced data. In this narrative review we provide an overview and critique of crowdsourced data that are being used to fill gaps and advance bicycling behaviour and safety knowledge. We assess crowdsourced data used to map ridership (fitness, bike share, and GPS/accelerometer data), assess safety (web-map tools), map infrastructure (OpenStreetMap), and track attitudes (social media). For each category of data, we discuss the challenges and opportunities they offer for researchers and practitioners. Fitness app data can be used to model spatial variation in bicycling ridership volumes, and GPS/accelerometer data offer new potential to characterise route choice and origin-destination of bicycling trips; however, working with these data requires a high level of training in data science. New sources of safety and near miss data can be used to address underreporting and increase predictive capacity but require grassroots promotion and are often best used when combined with official reports. Crowdsourced bicycling infrastructure data can be timely and facilitate comparisons across multiple cities; however, such data must be assessed for consistency in route type labels. Using social media, it is possible to track reactions to bicycle policy and infrastructure changes, yet linking attitudes expressed on social media platforms with broader populations is a challenge. New data present opportunities for improving our understanding of bicycling and supporting decision making towards transportation options that are healthy and safe for all. However, there are challenges, such as who has data access and how data crowdsourced tools are funded, protection of individual privacy, representativeness of data and impact of biased data on equity in decision making, and stakeholder capacity to use data given the requirement for advanced data science skills. If cities are to benefit from these new data, methodological developments and tools and training for end-users will need to track with the momentum of crowdsourced data.}
}
@article{MANTOUKA2021266,
title = {Smartphone sensing for understanding driving behavior: Current practice and challenges},
journal = {International Journal of Transportation Science and Technology},
volume = {10},
number = {3},
pages = {266-282},
year = {2021},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2046043020300460},
author = {Eleni Mantouka and Emmanouil Barmpounakis and Eleni Vlahogianni and John Golias},
keywords = {Driving, Behavior, Analytics, Smartphones, Maxinum likelihood, Profiling},
abstract = {Understanding driving behavior – even in the rapid emergence of automation - remains in the spotlight, for decomposing complex driving dynamics, enabling the development of user-friendly and acceptable autonomous vehicles and ensuring the safe co-existence of autonomous and conventional vehicles on the road. Mobile crowdsensing has emerged as a means to understand and model driving behavior. Although the advantages of collecting data through smartphones are many (speed, accuracy, low cost etc.), the challenges including, but do not limited to, the preparation rate, the processing needs, as well as the methodological, legislative and security issues, are significant. The present paper aims to review the research dedicated to analyzing driving behavior based on smartphone sensors’ data streams. We first establish an inclusive stepwise framework to describe the path from data collection to informed decision making. Next, the existing literature is thoroughly analyzed and challenges in relation to data collection and data mining practices are critically discussed placing particular emphasis on the limitations and concerns regarding the use of mobile phones for driving data collection, as well as using crowd sensed data for feature extraction. Subsequently, modeling driving behavior practices and end-to-end solutions for driver assistance and recommendation systems are also reviewed. The paper ends with a discussion on the most critical challenges arising from the literature and future research steps.}
}
@article{CASTILLO2021342,
title = {The productivity impact of the digitally connected 5 – layer stack in manufacturing enterprises},
journal = {Procedia CIRP},
volume = {104},
pages = {342-350},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.058},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009562},
author = {Adolfo Crespo del Castillo and John Patsavellas and Konstantinos Salonitis and Christos Emmanouilidis},
keywords = {Industry 4.0, Industrial Internet of Things (IIoT), MQTT, Manufacturing Execution System (MES), SCADA, Enterprise Scalable Data Architecture (ESDA), Overall Equipment Efficiency (OEE), Self-Cleaning-Data},
abstract = {This paper investigates the application of modern industrial internet protocols and network architecture in manufacturing companies, from the perspectives of productivity and sustainability, framed in the fourth industrial revolution paradigm. This is achieved by delving into the existing information systems and devices, their inter-operability and interconnections using industrial internet of things protocols. The paper details a study generating a standard model of data architecture to better unify the different layers of the information systems that typify most manufacturing companies, leveraging their existing digital infrastructure to establish a solid base for further digitalization.}
}
@article{LI2021123618,
title = {A data-driven reversible framework for achieving Sustainable Smart product-service systems},
journal = {Journal of Cleaner Production},
volume = {279},
pages = {123618},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123618},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336635},
author = {Xinyu Li and Zuoxu Wang and Chun-Hsien Chen and Pai Zheng},
keywords = {Smart product-service system, Sustainability, Knowledge management, Reversible design, Context-awareness},
abstract = {Higher sustainability with extended product lifecycle is a tireless pursuit in companies’ product design/development endeavours. In this regard, two prevailing concepts, namely the smart circular system and smart product-service system (Smart PSS), have been introduced, respectively. However, most existing studies only focus on the sustainability of physical materials and components, without considering the cyber-physical resources as a whole, let alone an integrated strategy towards the so-called Sustainable Smart PSS. To fill the gap, this paper discusses the key features in Sustainable Smart PSS development from a broadened scope of cyber-physical resources management. A data-driven reversible framework is hereby proposed to sustainably exploit high-value and context-dependent information/knowledge in the development of Sustainable Smart PSS. A four-step context-aware process in the framework, including requirement elicitation, solution recommendation, solution evaluation, and knowledge evolvement, is further introduced to support the decision-making and optimization along the extended or circular lifecycle. An illustrative example is depicted in the sustainable development of a smart 3D printer, which validates the feasibility and advantages of the proposed framework. As an explorative study, it is hoped that this work provides useful insights for Smart PSS development with sustainability concerns in a cyber-physical environment.}
}
@article{KUANG20211,
title = {Application and development trend of artificial intelligence in petroleum exploration and development},
journal = {Petroleum Exploration and Development},
volume = {48},
number = {1},
pages = {1-14},
year = {2021},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(21)60001-0},
url = {https://www.sciencedirect.com/science/article/pii/S1876380421600010},
author = {Lichun KUANG and He LIU and Yili REN and Kai LUO and Mingyu SHI and Jian SU and Xin LI},
keywords = {artificial intelligence, logging interpretation, seismic exploration, reservoir engineering, drilling and completion, surface facility engineering},
abstract = {Aiming at the actual demands of petroleum exploration and development, this paper describes the research progress and application of artificial intelligence (AI) in petroleum exploration and development, and discusses the applications and development directions of AI in the future. Machine learning has been preliminarily applied in lithology identification, logging curve reconstruction, reservoir parameter estimation, and other logging processing and interpretation, exhibiting great potential. Computer vision is effective in picking of seismic first breaks, fault identification, and other seismic processing and interpretation. Deep learning and optimization technology have been applied to reservoir engineering, and realized the real-time optimization of waterflooding development and prediction of oil and gas production. The application of data mining in drilling, completion, and surface facility engineering etc. has resulted in intelligent equipment and integrated software. The potential development directions of artificial intelligence in petroleum exploration and development are intelligent production equipment, automatic processing and interpretation, and professional software platform. The highlights of development will be digital basins, fast intelligent imaging logging tools, intelligent seismic nodal acquisition systems, intelligent rotary-steering drilling, intelligent fracturing technology and equipment, real-time monitoring and control of zonal injection and production.}
}
@article{TAMOLARRIEUX2021105541,
title = {Decision-making by machines: Is the ‘Law of Everything’ enough?},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105541},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105541},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000145},
author = {Aurelia Tamò-Larrieux},
keywords = {Automated decision-making, Artificial intelligence, Data protection, Transparency, Fairness, Due process},
abstract = {Machines have moved from supporting decision-making processes of humans to making decisions for humans. This shift has been accompanied by concerns regarding the impact of decisions made by algorithms on individuals and society. Unsurprisingly, the delegation of important decisions to machines has therefore triggered a debate on how to regulate the automated decision-making practices. In Europe, policymakers have attempted to address these concerns through a combination of individual rights and due processes established in data protection law, which relies on other statutes, e.g., anti-discrimination law and restricting trade secret laws, to achieve certain goals. This article adds to the literature by disentangling the challenges arising from automated decision-making systems and focusing on ones arising without malevolence but merely as unwanted side-effects of increased automation. Such side-effects include ones arising from the internal processes leading to a decision, the impacts of decisions, as well as the responsibility for decisions and have consequences on an individual and societal level. Upon this basis the article discusses the redress mechanisms provided in data protection law. It shows that the approaches within data protection law complement one another, but do not fully remedy the identified side-effects. This is particularly true for side-effects that lead to systemic societal shifts. To that end, new paradigms to guide future policymaking discourse are being explored.}
}
@article{LI2021101232,
title = {The Spring Festival Effect: The change in NO2 column concentration in China caused by the migration of human activities},
journal = {Atmospheric Pollution Research},
volume = {12},
number = {12},
pages = {101232},
year = {2021},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2021.101232},
url = {https://www.sciencedirect.com/science/article/pii/S1309104221002956},
author = {Dongqing Li and Qizhong Wu and Hui Wang and Han Xiao and Qi Xu and Lizhi Wang and Jinming Feng and Xiaochun Yang and Huaqiong Cheng and Lanning Wang and Yiming Sun},
keywords = {Spring festival effect, Tropospheric NO column concentration, Megacities, Human activity},
abstract = {The Spring Festival is the most important holiday in China, and human activity and population mobility may contribute greatly to air quality. According to the satellite-based tropospheric nitrogen dioxide (NO2) column and ground-based observational concentration of NO2 in megacities from 2013 to 2018 around the Spring Festival, we found that NO2 concentration obviously decreases, presenting a “tide phenomenon”, particularly in the megacities, with the tropospheric NO2 column density decreasing by 31.8%–44.5%. The tropospheric NO2 column density in Beijing decreased by 41.6% and rebounded by 22.3% after the festival. Vehicle sources were among the important causes of NOx emissions in the megacities, and traffic intensity decreased significantly during the festival. As the coronavirus disease 2019 (COVID-19) pandemic progresses, the traffic intensity in urban areas is decreasing significantly, with the tropospheric NO2 column density decreasing by 56.2% and rebounding by only 6.8% in 2020, without the “tide phenomenon”.}
}
@article{BENFER20211269,
title = {A Framework for Digital Twins for Production Network Management},
journal = {Procedia CIRP},
volume = {104},
pages = {1269-1274},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.213},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011112},
author = {Martin Benfer and Sina Peukert and Gisela Lanza},
keywords = {Digital Twin, Production Network, Modeling},
abstract = {The dynamic and highly complex task of production network management requires decision support through quantitative models. In the industrial praxis, these models are specifically designed and implemented for particular management decisions, requiring significant one-time effort for model creation. This contribution utilizes the digital twin concept to facilitate production network models that are continuously synchronized with the examined production network to support several different management decisions. The approach structures data from existing information systems as a synchronized generic base model, which is used to create problem-specific executable models, thereby saving costs through repeated model use and quicker decision making.}
}
@article{MAASS2021101909,
title = {Pairing conceptual modeling with machine learning},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101909},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101909},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000367},
author = {Wolfgang Maass and Veda C. Storey},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence},
abstract = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.}
}
@article{LAROUI2021210,
title = {Edge and fog computing for IoT: A survey on current research activities & future directions},
journal = {Computer Communications},
volume = {180},
pages = {210-231},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003327},
author = {Mohammed Laroui and Boubakr Nour and Hassine Moungla and Moussa A. Cherif and Hossam Afifi and Mohsen Guizani},
keywords = {Internet of Things (IoT), Edge computing, Cloud computing},
abstract = {The Internet of Things (IoT) allows communication between devices, things, and any digital assets that send and receive data over a network without requiring interaction with a human. The main characteristic of IoT is the enormous quantity of data created by end-user’s devices that needs to be processed in a short time in the cloud. The current cloud-computing concept is not efficient to analyze very large data in a very short time and satisfy the users’ requirements. Analyzing the enormous quantity of data by the cloud will take a lot of time, which affects the quality of service (QoS) and negatively influences the IoT applications and the overall network performance. To overcome such challenges, a new architecture called edge computing — that allows to decentralize the process of data from the cloud to the network edge has been proposed to solve the problems occurred by using the cloud computing approach. Furthermore, edge computing supports IoT applications that require a short response time and consequently enhances the consumption of energy, resource utilization, etc. Motivated by the extensive research efforts in the edge computing and IoT applications, in this paper, we present a comprehensive review of edge and fog computing research in the IoT. We investigate the role of cloud, fog, and edge computing in the IoT environment. Subsequently, we cover in detail, different IoT use cases with edge and fog computing, the task scheduling in edge computing, the merger of software-defined networks (SDN) and network function virtualization (NFV) with edge computing, security and privacy efforts. Furthermore, the Blockchain in edge computing. Finally, we identify open research challenges and highlight future research directions.}
}
@article{CHANG2021101912,
title = {ArchNet: A data hiding design for distributed machine learning systems},
journal = {Journal of Systems Architecture},
volume = {114},
pages = {101912},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101912},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301764},
author = {Kaiyan Chang and Wei Jiang and Jinyu Zhan and Zicheng Gong and Weijia Pan},
keywords = {Distributed machine learning, Data hiding, Neural networks, Embedded systems, Encryption},
abstract = {Integrating idle embedded devices into cloud computing is a promising approach to support Distributed Machine Learning (DML). In this paper, we approach to address the data hiding problem in such DML systems. For the purpose of the data encryption in DML systems, we introduce the tripartite asymmetric encryption theorem to provide theoretical support. Based on the theorem, we design a general image encryption scheme (called ArchNet), which can encrypt original images via the encoder to resist against illegal users. ArchNet encrypts the dataset by a specific neural network, which is especially trained for encryption. The encrypted data can be easily recognized by deep learning model. However, the encrypted data cannot be recognized by human, which makes the illegal attacker difficult to steal the encrypted data. We use MNIST, Fashion-MNIST and Cifar-10 datasets to evaluate efficiency of our design. We deploy certain base models on the encrypted datasets and compare them with the RC4 algorithm and differential privacy policy. Our design can improve the accuracy on MNIST up to 97.26% compared with RC4. The accuracies on these three datasets encrypted by ArchNet are similar to the base model. ArchNet can be deployed on DML systems with embedded devices.}
}
@article{LI2021106095,
title = {Stated acceptance and behavioral responses of drivers towards innovative connected vehicle applications},
journal = {Accident Analysis & Prevention},
volume = {155},
pages = {106095},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106095},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521001263},
author = {Weixia Li and Guoyuan Wu and Danya Yao and Yi Zhang and Matthew J. Barth and Kanok Boriboonsomsin},
keywords = {Connected vehicle technology, Lane speed monitoring, High speed differential warning, Public acceptance, Behavioral response},
abstract = {This research is aimed at investigating drivers’ attitudes towards connected vehicle technology in general and two connected vehicle applications in particular—Lane Speed Monitoring and High Speed Differential Warning—which have been demonstrated via simulation to be effective in enhancing traffic mobility and safety, respectively. An online survey was sent to customers of an automobile manufacturer in the United States. Out of the 1453 survey responses that were received, 650 complete and valid responses were used to analyze the respondents’ stated acceptance of and expected behavioral responses to the two connected vehicle applications under a variety of scenarios. Statistical analyses were conducted to examine the influence of demographic and socioeconomic factors. The results reveal that the respondents express high willingness to use connected vehicle technology and the two applications under various circumstances, and the willingness is strongly associated with age, gender, education level, and income. Higher levels of acceptance are observed in older, male, higher-educated, or higher-income respondents, while the patterns of conditional acceptance and expected behavioral responses vary with specific scenarios. These results provide useful information for application developers, traffic operators, and policy makers to steer connected vehicle technology development and deployment in the direction that will benefit both the users and the society.}
}
@article{LI2021106888,
title = {Deep-learning-assisted power loss management for active distribution networks},
journal = {The Electricity Journal},
volume = {34},
number = {1},
pages = {106888},
year = {2021},
note = {Special Issue: Machine Learning Applications To Power System Planning And Operation},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2020.106888},
url = {https://www.sciencedirect.com/science/article/pii/S1040619020301809},
author = {Dairui Li and Jia Tan and Qun Yu and Zhiyi Li},
keywords = {Active distribution network, Deep learning, Data preprocessing, Power loss management, Cybersecurity},
abstract = {With widespread deployment of advanced information and communication techniques, power distribution networks have been undergoing a transition towards active distribution networks (ADNs). As ADNs feature multi-way interactions between power supply and demand as well as tight coupling between cyber and physical elements, conventional physical-driven energy management schemes are insufficient to tackle the challenges of increasingly complicated operating conditions. In this paper, a novel three-stage decision-making framework is proposed to manage the power losses of ADNs by taking advantage of state-of-the-art deep learning methods. The paper first sheds light on the opportunities and challenges brought by the immense amount of heterogeneous data in the context of ADNs. In particular, generative modeling methods such as generative adversarial networks are introduced to develop a data preprocessing scheme for fixing and enhancing data collected from heterogeneous field devices. The paper then employs recurrent neural networks such as long-short-term-memory networks to infer and formulate the intricate relations between preprocessed data with the objective of enhancing the observability of network topology and line losses. More specifically, an optimization-assisted deep learning method is proposed to facilitate the optimal decision making on power loss reduction under fast-changing operating conditions. Last, limitations of deep learning models such as cybersecurity challenges are discussed in depth in the context of ADNs.}
}
@article{ASHKOUTI20211,
title = {DI-Mondrian: Distributed improved Mondrian for satisfaction of the L-diversity privacy model using Apache Spark},
journal = {Information Sciences},
volume = {546},
pages = {1-24},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520307441},
author = {Farough Ashkouti and Keyhan khamforoosh and Amir Sheikhahmadi},
keywords = {Anonymization, PPDP, -anonymity, L-diversity, Information loss, Apache Spark, RDD},
abstract = {For the extraction of useful patterns, the collected data should be distributed to and shared with analyzers. This, however, creates problems and challenges for the individual with respect to their privacy and identity. In this paper, the Mondrian multidimensional anonymization method was developed and improved for satisfaction of the l-diversity privacy model, and it has been presented in a distributed fashion within the Apache Spark framework. Since one of the major challenges in data privacy is the tradeoff between privacy and data utility, the presented method focuses on information loss and classifier evaluation criteria. Therefore, the cut dimension was selected using the coefficient of variation and information gain criteria, and the cut points were chosen dynamically, which led to a decrease in the information loss parameter and an improvement in the classifier performance evaluation criteria such as accuracy and FMeasure compared to the previous algorithms in the literature. The processing speed is 100 times higher in Spark than in the Hadoop framework. Consequently, the proposed method was presented in a distributed fashion based on RDDs programming within Apache Spark framework. This will resolve the problem of speed in large-scale data anonymization as it exists in the previous Hadoop-based algorithms. The results of the experiments performed on the numerical datasets demonstrate the improvements made by the proposed method.}
}
@article{ITCHHAPORIA2021412,
title = {Navigating the Path to Digital Transformation},
journal = {Journal of the American College of Cardiology},
volume = {78},
number = {4},
pages = {412-414},
year = {2021},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2021.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0735109721053845},
author = {Dipti Itchhaporia}
}
@article{SCHUHMACHER20211,
title = {The present and future of project management in pharmaceutical R&D},
journal = {Drug Discovery Today},
volume = {26},
number = {1},
pages = {1-4},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620303007},
author = {Alexander Schuhmacher and Oliver Gassmann and Markus Hinder and Michael Kuss}
}
@article{LINKE2021579,
title = {Shared and Anxiety-Specific Pediatric Psychopathology Dimensions Manifest Distributed Neural Correlates},
journal = {Biological Psychiatry},
volume = {89},
number = {6},
pages = {579-587},
year = {2021},
note = {Early Developmental Antecedents of Mood Disorders},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2020.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0006322320320436},
author = {Julia O. Linke and Rany Abend and Katharina Kircanski and Michal Clayton and Caitlin Stavish and Brenda E. Benson and Melissa A. Brotman and Olivier Renaud and Stephen M. Smith and Thomas E. Nichols and Ellen Leibenluft and Anderson M. Winkler and Daniel S. Pine},
keywords = {Anxiety, Disruptive behavior, Intrinsic brain connectivity, Irritability, Joint canonical correlation and independent component analysis, Youth},
abstract = {Background
Imaging research has not yet delivered reliable psychiatric biomarkers. One challenge, particularly among youth, is high comorbidity. This challenge might be met through canonical correlation analysis designed to model mutual dependencies between symptom dimensions and neural measures. We mapped the multivariate associations that intrinsic functional connectivity manifests with pediatric symptoms of anxiety, irritability, and attention-deficit/hyperactivity disorder (ADHD) as common, impactful, co-occurring problems. We evaluate the replicability of such latent dimensions in an independent sample.
Methods
We obtained ratings of anxiety, irritability, and ADHD, and 10 minutes of resting-state functional magnetic resonance imaging data, from two independent cohorts. Both cohorts (discovery: n = 182; replication: n = 326) included treatment-seeking youth with anxiety disorders, with disruptive mood dysregulation disorder, with ADHD, or without psychopathology. Functional connectivity was modeled as partial correlations among 216 brain areas. Using canonical correlation analysis and independent component analysis jointly we sought maximally correlated, maximally interpretable latent dimensions of brain connectivity and clinical symptoms.
Results
We identified seven canonical variates in the discovery and five in the replication cohort. Of these canonical variates, three exhibited similarities across datasets: two variates consistently captured shared aspects of irritability, ADHD, and anxiety, while the third was specific to anxiety. Across cohorts, canonical variates did not relate to specific resting-state networks but comprised edges interconnecting established networks within and across both hemispheres.
Conclusions
Findings revealed two replicable types of clinical variates, one related to multiple symptom dimensions and a second relatively specific to anxiety. Both types involved a multitude of broadly distributed, weak brain connections as opposed to strong connections encompassing known resting-state networks.}
}
@article{DONTHU2021102307,
title = {Forty years of the International Journal of Information Management: A bibliometric analysis},
journal = {International Journal of Information Management},
volume = {57},
pages = {102307},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102307},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220315061},
author = {Naveen Donthu and Satish Kumar and Nitesh Pandey and Prashant Gupta},
keywords = {Bibliometric analysis, International Journal of Information Management, Performance analysis, Science mapping, Negative binomial regression, Citation analysis},
abstract = {In 2019, the International Journal of Information Management (IJIM) celebrated its 40th year of publication. This study commemorates this event by presenting a retrospect of the journal. Using a range of bibliometric tools, we find that the journal has grown impressively in terms of publication and citation. The contributions come from all over the world, but the majority are from Europe and the United States. The journal has mostly published empirical articles, with its authors dominantly using quantitative methodology. Further, the culture of collaboration has increased among authors over the years. The journal publishes on a number of including managing information systems, information technologies and their application in business, technology acceptance among consumers, using information systems for decision making, social perspectives on knowledge management, and information research from the social science perspective. Regression analysis reveals that article attributes such as article order, methodology, presence of authors from Europe, number of references, number of keywords, and abstract length have a significant association with the citations. Finally, we find that conceptual and review articles have a positive association with citations.}
}
@article{SHAO2021102320,
title = {Exploring potential roles of academic libraries in undergraduate data science education curriculum development},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {2},
pages = {102320},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2021.102320},
url = {https://www.sciencedirect.com/science/article/pii/S0099133321000112},
author = {Gang Shao and Jenny P. Quintana and Wei Zakharov and Senay Purzer and Eunhye Kim},
keywords = {Data science, Libraries, Curriculum, Education, Data management, College, Data ethics},
abstract = {Undergraduate data science education is receiving increasing interest in many higher education institutions in the U.S., with the proliferation of data and data related work and research. As an emerging interdisciplinary study field, data science curriculum is typically a collection of individual data science related courses from different schools and departments, most of which are teaching data science in a siloed fashion. Therefore, it is necessary to map the landscape of existing curricula and explore how academic libraries can collaborate and contribute to undergraduate data science education. In this study, we analyzed teaching content and topics of over 100 data science related courses at Purdue University to map the landscape and explore roles of academic libraries to support data science education curriculum. Our results indicate most existing courses focused on ‘hard-core’ scientific analytic principles, such as computer science, statistics, and domain-specific skills. Courses of data-oriented skills, such as data management, data ethics, and data communications were limited across disciplines. In addition, data science courses were more likely targeting STEM students at upper levels (3rd and 4th year students). Academic libraries can enrich data science education efforts, by supporting credit courses, certificate programs, and other co-curricular activities to provide learning opportunities to all students, particularly 1st and 2nd year students and non-STEM majors.}
}
@article{BROWNE2021e893,
title = {Global antibiotic consumption and usage in humans, 2000–18: a spatial modelling study},
journal = {The Lancet Planetary Health},
volume = {5},
number = {12},
pages = {e893-e904},
year = {2021},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(21)00280-1},
url = {https://www.sciencedirect.com/science/article/pii/S2542519621002801},
author = {Annie J Browne and Michael G Chipeta and Georgina Haines-Woodhouse and Emmanuelle P A Kumaran and Bahar H Kashef Hamadani and Sabra Zaraa and Nathaniel J Henry and Aniruddha Deshpande and Robert C Reiner and Nicholas P J Day and Alan D Lopez and Susanna Dunachie and Catrin E Moore and Andy Stergachis and Simon I Hay and Christiane Dolecek},
abstract = {Summary
Background
Antimicrobial resistance (AMR) is a serious threat to global public health. WHO emphasises the need for countries to monitor antibiotic consumption to combat AMR. Many low-income and middle-income countries (LMICs) lack surveillance capacity; we aimed to use multiple data sources and statistical models to estimate global antibiotic consumption.
Methods
In this spatial modelling study, we used individual-level data from household surveys to inform a Bayesian geostatistical model of antibiotic usage in children (aged <5 years) with lower respiratory tract infections in LMICs. Antibiotic consumption data were obtained from multiple sources, including IQVIA, WHO, and the European Surveillance of Antimicrobial Consumption Network (ESAC-Net). The estimates of the antibiotic usage model were used alongside sociodemographic and health covariates to inform a model of total antibiotic consumption in LMICs. This was combined with a single model of antibiotic consumption in high-income countries to produce estimates of antibiotic consumption covering 204 countries and 19 years.
Findings
We analysed 209 surveys done between 2000 and 2018, covering 284 045 children with lower respiratory tract infections. We identified large national and subnational variations of antibiotic usage in LMICs, with the lowest levels estimated in sub-Saharan Africa and the highest in eastern Europe and central Asia. We estimated a global antibiotic consumption rate of 14·3 (95% uncertainty interval 13·2–15·6) defined daily doses (DDD) per 1000 population per day in 2018 (40·2 [37·2–43·7] billion DDD), an increase of 46% from 9·8 (9·2–10·5) DDD per 1000 per day in 2000. We identified large spatial disparities, with antibiotic consumption rates varying from 5·0 (4·8–5·3) DDD per 1000 per day in the Philippines to 45·9 DDD per 1000 per day in Greece in 2018. Additionally, we present trends in consumption of different classes of antibiotics for selected Global Burden of Disease study regions using the IQVIA, WHO, and ESAC-net input data. We identified large increases in the consumption of fluoroquinolones and third-generation cephalosporins in North Africa and Middle East, and south Asia.
Interpretation
To our knowledge, this is the first study that incorporates antibiotic usage and consumption data and uses geostatistical modelling techniques to estimate antibiotic consumption for 204 countries from 2000 to 2018. Our analysis identifies both high rates of antibiotic consumption and a lack of access to antibiotics, providing a benchmark for future interventions.
Funding
Fleming Fund, UK Department of Health and Social Care; Wellcome Trust; and Bill & Melinda Gates Foundation.}
}
@article{BOKADE2021113885,
title = {A cross-disciplinary comparison of multimodal data fusion approaches and applications: Accelerating learning through trans-disciplinary information sharing},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113885},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113885},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306886},
author = {Rohit Bokade and Alfred Navato and Ruilin Ouyang and Xiaoning Jin and Chun-An Chou and Sarah Ostadabbas and Amy V. Mueller},
keywords = {Multimodal data fusion, Machine learning, Complex systems, Big data, Trans-disciplinary},
abstract = {Multimodal data fusion (MMDF) is the process of combining disparate data streams (of different dimensionality, resolution, type, etc.) to generate information in a form that is more understandable or usable. Despite the explosion of data availability in recent decades, as yet there is no well-developed theoretical basis for multimodal data fusion, i.e., no way to determine a priori which approach is best suited to combine an arbitrary set of available data to achieve a stated goal for a given application. This has resulted in exploration of a wide variety of approaches across numerous domains but as yet very little integration of conclusions at a meta (cross-disciplinary) level. In response, this manuscript poses the following questions: (1) How convergent (or divergent) are approaches within single disciplines? (2) How similar are the challenges posed across different disciplines, i.e., might there be opportunity for successes in MMDF achieved in one field to inform progress in other areas as well? and (3) Where are the outstanding gaps in MMDF research, and what does this imply as targets for high impact research in the coming years? To begin to answer these questions, an apples-to-apples comparison of the literature of nine stakeholder-centric engineering domains (civil engineering, transportation, energy, environmental engineering, food engineering, critical care (healthcare), neuroscience, manufacturing/automation, and robotics) was created by quantifying the numbers and dimensionalities of modalities and sensors in each published project and classifying the algorithms used and purposes for which they are used. Within disciplines, it is shown there is often a tendency for use of similar methodologies, both in choice of level of fusion and data algorithm class. Yet this analysis also reveals that many problem types (defined by data dimensionality, modality number and type, and fusion purpose) are shared across different domains and are approached differently in those domains, e.g., transportation problems have similar characteristics to critical care, food science, robotics, and civil engineering. Of the disciplines studied, most (>75%) share problem characteristics with 3–5 others; to support leveraging these resources, lookup tables indexed by data dimensions, number of modalities, etc. are provided as a starting point for cross-disciplinary MMDF literature searches for new applications. Critical gaps identified are (1) a drop off of the number of published studies with increasing number of distinct modalities and (2) a dearth of publications tackling challenges with high dimensionality inputs, especially time-series 2D and 3D data. These gaps may point to topics where algorithm development will be fruitful to enable future solutions as video and other high-dimensionality sensors decrease in price. Finally, the lack of a shared vocabulary across disciplines makes analyses like the one conducted here challenging, as does the often implicit incorporation of expert knowledge into design; therefore progress toward a better leveraging of the current state of knowledge and toward a theoretical MMDF framework depends critically on improved cross-disciplinary communication and coordination on this topic.}
}
@article{CORO2021101384,
title = {An Open Science approach to infer fishing activity pressure on stocks and biodiversity from vessel tracking data},
journal = {Ecological Informatics},
volume = {64},
pages = {101384},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101384},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121001758},
author = {Gianpaolo Coro and Anton Ellenbroek and Pasquale Pagano},
keywords = {Vessel transmitted information, Vessel tracking data, Automatic Identification System, Statistical analysis, e-Infrastructures, Open Science, Biodiversity, Integrated Environmental Assessment},
abstract = {Vessel tracking data help study the potential impact of fisheries on biodiversity and produce risk assessments. Existing workflows process vessel tracks to identify fishing activity and integrate information on species vulnerability. However, there are significant data integration challenges across the data sources needed for an integrated impact assessment due to heterogeneous nomenclatures, data accessibility issues, geographical and computational scalability of the processes, and confidentiality and transparency towards decision making authorities. This paper presents an Open Science data integration approach to use vessel tracking data in integrated impact assessments. Our approach combines heterogeneous knowledge sources from fisheries, biodiversity, and environmental observations to infer fishing activity and risks to potentially impacted species. An Open Science e-Infrastructure facilitates access to data sources and maximises the reproducibility of the results and the method's reusability across several application domains. Our method's quality is assessed through three case studies: The first demonstrates cross-dataset consistency by comparing the results obtained from two different vessel data sources. The second performs a temporal pattern analysis of fishing activity and potentially impacted species over time. The third assesses the potential impact of reduced fishing pressure on marine biodiversity and threatened species due to the 2020 COVID-19 lockdown in Italy. The method is meant to be integrated with other systems through its Open Science-oriented features and can rapidly use new sources of findable, accessible, interoperable, and reusable (FAIR) data. Other systems can use it to (i) classify vessel activity in data-limited scenarios, (ii) identify bycatch species (when catchability data are available), and (iii) study the effects of fisheries on habitats and populations’ growth.}
}