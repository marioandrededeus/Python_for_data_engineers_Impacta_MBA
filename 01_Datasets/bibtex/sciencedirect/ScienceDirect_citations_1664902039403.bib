@incollection{GRIMALDI2022111,
title = {Chapter 4 - Roadmap to develop a data-driven city},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {111-151},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000063},
author = {Didier Grimaldi and Jose M. Sallan and Josep Miquel {Piqué Huerta} and Jesús Soler Puebla and Kristi Shalla and Carlos Carrasco-Farré},
keywords = {Roadmap, Data-driven city, Smart city, Inception, Maturity, Smart index},
abstract = {Cities play a relevant role in economic and social development. According to data from the United Nations, 55% of the global human population lives in cities, amounting to 4.2 billion people. From the same source, we learn that this fraction amounts to 78.5% in more developed countries. On the other hand, cities are estimated to consume 2% of land, generate around 70% of global GDP, and use 60% of global energy. As much of the growth of urban population is coming from less developed cities, it is safe to predict that the weight of cities in the global social landscape will increase in the coming years.}
}
@article{MATTHEWS2022102495,
title = {Smart data and business analytics: A theoretical framework for managing rework risks in mega-projects},
journal = {International Journal of Information Management},
volume = {65},
pages = {102495},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2022.102495},
url = {https://www.sciencedirect.com/science/article/pii/S0268401222000263},
author = {Jane Matthews and Peter E.D. Love and Stuart R. Porter and Weili Fang},
keywords = {Business analytics, Machine learning, Rework, Risk, Smart data, Topic modelling},
abstract = {Within construction, we have become increasingly accustomed to relying on the benefits of digital technologies, such as Building Information Modelling, to improve the performance and productivity of projects. We have, however, overlooked the problems that technology is unable to redress. One such problem is rework, which has become so embedded in practice that technology adoption alone can not resolve the issue without fundamental changes in how information is managed for decision-making. Hence, the motivation of this paper is to bring to the fore the challenges of classifying and creating an ontology for rework that can be used to understand its patterns of occurrence and risks and provide a much-needed structure for decision-making in transport mega-projects. Using an exploratory case study approach, we examine ‘how’ rework information is currently being managed by an alliance that contributes significantly to delivering a multi-billion dollar mega-transport project. We reveal the challenges around location, format, structure, granularity and redundancy hindering the alliance’s ability to classify and manage rework data. We use the generative machine learning technique of Correlation Explanation to illustrate how we can make headway toward classifying and then creating an ontology for rework. We propose a theoretical framework utilising a smart data approach to generate an ontology that can effectively use business analytics (i.e., descriptive, predictive and prescriptive) to manage rework risks.}
}
@article{LIU2022100279,
title = {Geographic information science in the era of geospatial big data: A cyberspace perspective},
journal = {The Innovation},
volume = {3},
number = {5},
pages = {100279},
year = {2022},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2022.100279},
url = {https://www.sciencedirect.com/science/article/pii/S2666675822000753},
author = {Xintao Liu and Min Chen and Christophe Claramunt and Michael Batty and Mei-Po Kwan and Ahmad M. Senousi and Tao Cheng and Josef Strobl and Arzu Cöltekin and John Wilson and Temenoujka Bandrova and Milan Konecny and Paul M. Torrens and Fengyuan Zhang and Li He and Jinfeng Wang and Carlo Ratti and Olaf Kolditz and Alexander Klippel and Songnian Li and Hui Lin and Guonian Lü}
}
@incollection{CHOUVARDA2022151,
title = {Chapter 6 - Connected health technologies for knowledge extraction and knowledge-based medicine in cardiac care},
editor = {Anna Maria Bianchi and Jorge Henriques and Vicente {Traver Salcedo}},
booktitle = {Personalized Health Systems for Cardiovascular Disease},
publisher = {Academic Press},
pages = {151-175},
year = {2022},
isbn = {978-0-12-818950-4},
doi = {https://doi.org/10.1016/B978-0-12-818950-4.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818950400001X},
author = {Ioanna Chouvarda},
keywords = {Connected health technologies, personal health systems, enabling technologies, cardiovascular disease, knowledge-based medicine, learning health cycle},
abstract = {Connected health is a new model for health management. It puts the correct information in the correct hands at the correct time. It allows patients and clinicians to make better decisions and relies heavily on advanced technologies to do so. In the connected health era there is an opportunity to collect a multitude of data about a patient. These may include daily life and behavior, including compliance to interventions or psychological aspects, as well as daily fluctuations of physiological parameters and contextual factors that influence them and can be combined with clinical and biological data available for the patient at different time points. This multitude of data can offer the opportunity to move from evidence-based medicine to knowledge-based medicine, that is, medicine that relies on knowledge about the patient. To achieve this goal, it is worth investigating the path from new technological methods for data acquisition to analysis and new knowledge and the path from patient knowledge to better intervention, as aspects that have gradually been discovered within the connected health domain. Both points would contribute to the impact of wide adoption of connected health technologies (CHT) as parts of new healthcare models. This chapter discusses first the connected health technologies in cardiac care. Basic concepts and building blocks are discussed, such as point-of-care diagnostics and new sensors, big data analytics, health behavior informatics and systems medicine, interventions, and knowledge-based medicine. CHT as a part of an iterative learning cycle for knowledge acquisition and application is presented, and the relevant challenges are discussed.}
}
@incollection{DOGUC202295,
title = {Chapter 9 - Recent applications of data mining in medical diagnosis and prediction},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {95-109},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000066},
author = {Ozge Doguc and Zehra Nur Canbolat and Gokhan Silahtaroglu},
keywords = {Big data, Data mining, Health sector, Intelligent medical diagnosis systems},
abstract = {Big data has been used in the health sector to improve the quality of life, predict epidemics, cure diseases, and avoid preventable deaths, beyond increasing profits or reducing the burden of excess labor. Data sources in healthcare have become quite diversified and accessible to individuals, such as wearable and implantable devices, smartphones, and real-time sensors. When combined with existing health data, daily (even instantaneous) data from these devices can be used to predict future health conditions of individuals and to identify necessary intervention points. This chapter discusses a number of recent studies that introduces methods for using big data to create intelligent systems for patient diagnosis, triage, predicting lab results, and even detecting tumors. These studies open ways for researchers in the healthcare sector to improve the quality of services provided to the patients as well as reducing costs for the healthcare institutions.}
}
@article{PEI2022539,
title = {Monitoring and early warning model based on multi-dimensional structure of power grid data},
journal = {Energy Reports},
volume = {8},
pages = {539-545},
year = {2022},
note = {The 2021 7th International Conference on Advances in Energy Resources and Environment Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.03.147},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722007028},
author = {Caiyan Pei and Wei Zhao and Shuai Zhang and Yuqing Wei},
keywords = {Power monitoring system, Early warning data model, Complex network, Internet of Things technology},
abstract = {With the continuous improvement of information technology, the monitoring of various conditions on power system is under continuous construction. The multiple monitoring system caused by network disaster is unable to achieve complete data exchange, so it is easy to lead to early warning of persistent conditions and chain conditions. In this paper, a DQN-based power monitoring node identification method for complex networks is proposed, and a key node identification algorithm for complex networks based on the Internet of Things is proposed. The reward function is improved and designed to integrate the influence of edge weights and node attributes. By acting on the power multi-dimensional structure monitoring and early warning of conditions, the key monitoring data are preferentially provided to other monitoring networks for analysis and prompt. Experiments show that in undirected ARPA networks, compared with degree centrality, betweenness centrality, Page Rank algorithm and two comprehensive methods, the recognition results are verified by using the evaluation method based on network robustness. The evaluation based on network robustness is improved by 12.5% compared with other algorithms; the node evaluation based on dynamics is more reasonable from the global attribute, which verifies the application effect of this algorithm in power multi-dimensional condition monitoring network.}
}
@article{LI20221750,
title = {The land-sea interface mapping: China’s coastal land covers at 10 m for 2020},
journal = {Science Bulletin},
volume = {67},
number = {17},
pages = {1750-1754},
year = {2022},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2022.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S2095927322002961},
author = {Miao Li and Bin Chen and Chris Webster and Peng Gong and Bing Xu}
}
@incollection{GULERIA2022179,
title = {Chapter 15 - Predictions on diabetic patient datasets using big data analytics and machine learning techniques},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {179-199},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000182},
author = {Pratiyush Guleria},
keywords = {Analytics, Classification, Clustering, Decision, Diabetic, Healthcare},
abstract = {Big data analytics and machine learning are the promising fields of the present time and playing important role in the healthcare sector. Big data analytical techniques help in analyzing a huge volume of data which may be in structured, semistructured, or unstructured form, and extract meaningful information for effective decision-making. Machine learning techniques help in performing predictions with the trained models on the input datasets and perform classification, clustering of data. In this chapter, the author has performed data analysis on diabetic patients dataset categorical in nature using big data analytical techniques, i.e., MapReduce, Apache Pig, Apache Hive, Apache Spark, and their architectures are discussed. Apart from big data analytics, machine learning techniques, i.e., K-Nearest Neighbor, Decision Trees, Bagged Trees, are implemented on the female diabetic patient dataset which is categorical and numerical for performing predictions based on the attributes like Age, Body Mass Index, Glucose, Blood Pressure, etc. The sensitivity achieved by the decision tree is 61.2% which is higher compared to KNN and bagged tree, whereas the Specificity achieved by the KNN is 89.2% which is higher than the other two algorithms.}
}
@incollection{ZHANG20221681,
title = {Hashing-based just-in-time learning for big data quality prediction},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1681-1686},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50280-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502803},
author = {Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li},
keywords = {Virtual sensor, soft-sensor, big data quality prediction, hashing-based just-in-time modeling},
abstract = {In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.}
}
@article{PARK2022S457,
title = {Korean actions taken for implementation of measurement traceability in laboratory medicine},
journal = {Clinica Chimica Acta},
volume = {530},
pages = {S457-S458},
year = {2022},
note = {SI: IFCC WorldLab 2022},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2022.04.776},
url = {https://www.sciencedirect.com/science/article/pii/S0009898122009056},
author = {S. Park and H. Lee and J. Jeong}
}
@article{PLAZIER2022S198,
title = {PO048 / #665 THE BIG CHANGE IS COMMING: BIG DATA: E-POSTER VIEWING},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {25},
number = {7, Supplement },
pages = {S198},
year = {2022},
note = {INS 15th World Congress},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2022.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S1094715922009977},
author = {Mark Plazier and Vincent Raymaekers and Wim Duyvendak and Sacha Meeuws and Maarten Wissels and Steven Vanvolsem and Gert Roosen and Sven Bamps and Salah-Edine Achabar and Stefan Schu and Anna Keil and Björn Carsten Schultheis and Philipp Slotty and Dirk {De Ridder} and Jan Vesper}
}
@article{ZHAN202242,
title = {Evolutionary deep learning: A survey},
journal = {Neurocomputing},
volume = {483},
pages = {42-58},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.099},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222001345},
author = {Zhi-Hui Zhan and Jian-Yu Li and Jun Zhang},
keywords = {Deep learning, Evolutionary computation, Evolutionary algorithm, Swarm intelligence, Evolutionary deep learning, Artificial intelligence},
abstract = {As an advanced artificial intelligence technique for solving learning problems, deep learning (DL) has achieved great success in many real-world applications and attracted increasing attention in recent years. However, as the performance of DL depends on many factors such as the architecture and hyperparameters, how to optimize DL has become a hot research topic in the field of DL and artificial intelligence. Evolutionary computation (EC), including evolutionary algorithm and swarm intelligence, is a kind of efficient and intelligent optimization methodology inspired by the mechanisms of biological evolution and behaviors of swarm organisms. Therefore, a large number of researches have proposed EC algorithms to optimize DL, so called evolutionary deep learning (EDL), which have obtained promising results. Given the great progress and rapid development of EDL in recent years, it is quite necessary to review these developments in order to summarize previous research experiences and knowledge, as well as provide references to benefit the development of more researches and applications. For this aim, this paper categorizes existing works in a two-level taxonomy. The higher level includes four categories based on when the EC can be adopted in optimizing the DL, which are the four procedures of the whole DL lifetime, including data processing, model search, model training, and model evaluation and utilization. In the lower level, related works in each category are further classified according to the functionality and the aim of using EC in the corresponding DL procedure, i.e., why using EC in this DL procedure. As a result, the taxonomy can clearly show how an EC algorithm can be used to optimize and improve DL. Moreover, this survey also discusses the potential research directions to provide the prospect of EDL in the future.}
}
@incollection{SEBASTIANCOLEMAN202269,
title = {Chapter 4 - The Data Challenge: The Mechanics of Meaning},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {69-92},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000043},
author = {Laura Sebastian-Coleman},
keywords = {History of data, statistics, scientific data, organizational data, relational data, characteristics of data},
abstract = {This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.}
}
@incollection{YU2022177,
title = {Chapter 6 - Data-driven estimation for urban travel shareability},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {177-202},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000075},
author = {Qing Yu and Weifeng Li and Dongyuan Yang},
keywords = {Urban travel shareability, Geospatial big data, Agent-based model, Human mobility, Bicycle-sharing, Ride-sharing},
abstract = {The potential analysis of urban travel shareability is a fast-growing research topic, but it is still in its infancy. With the support of pervasively collected geospatial data generated by individuals, this chapter establishes the potential of the Agent-Based Model (ABM) towards enhancing sharing transportation modeling, data-driven estimation of shareability, and policy making. Our analysis of existing literature demonstrates that big geospatial data-driven ABM has had little mention to date within the application of urban travel shareability estimation. Therefore, we summarized the key technologies, the existing application, future opportunities, and challenges of ABM in travel shareability evaluation. As a potential application of ABM, this chapter also introduces an example of estimating the shareability of bicycle sharing in the case of dynamic electric fence planning. This is of significant importance due to the growing awareness of a need to make decisions based on the monitoring, simulation, and evaluation of shared transportation to deliver the next-generation smart city development.}
}
@incollection{KATARINA202283,
title = {Chapter 6 - Innovative technologies in precision healthcare},
editor = {Debmalya Barh},
booktitle = {Biotechnology in Healthcare},
publisher = {Academic Press},
pages = {83-102},
year = {2022},
isbn = {978-0-323-89837-9},
doi = {https://doi.org/10.1016/B978-0-323-89837-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898379000164},
author = {Šoltýs Katarína and Kľoc Marek and Rabajdová Miroslava and Mareková Mária},
keywords = {Precision healthcare, biotechnology, emerging technologies, big data},
abstract = {Precision medicine is the intersection of data science, analytics, and biomedicine in creating a healthy learning system that conducts research in the context of clinical care while optimizing the tools and information used to provide better outcomes for patients. Emerging technologies represent a novel, innovative, and fast-evolving trend within a particular field. Among the latest trends as virtual reality, robotics, wearable, and implantable sensors, and removable tattoos, together with nanotechnologies, 3D printing, and others are considered. In addition, new advanced computing technologies including artificial intelligence, machine learning (ML), big data mining, and cloud computing form an integral part of personalized healthcare. Personalized medicine is not necessarily the same as precision medicine. From the point of view of technology development, precision medicine is an intermediate step to personalized medicine, which will be much more complex and will require even more data. There is a big challenge to combine multiomics approaches in analysis, as we can see in bioinformatics that there are a lot of techniques in one area. Analysis of more than one area such as the genome, transcriptome even microbiome, starts exponentially grown on science field. The integration of multiomics data analysis and machine learning can have led to the discovery of new biomarkers, and improve of differential diagnostics of latent diseases. In this chapter, we describe the use of emerging technologies as well as bioengineering and ML for precision healthcare.}
}
@incollection{PARMAR2022401,
title = {18 - 5G-enabled deep learning-based framework for healthcare mining: State of the art and challenges},
editor = {Sudeep Tanwar},
booktitle = {Blockchain Applications for Healthcare Informatics},
publisher = {Academic Press},
pages = {401-420},
year = {2022},
isbn = {978-0-323-90615-9},
doi = {https://doi.org/10.1016/B978-0-323-90615-9.00016-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323906159000165},
author = {Rahil Parmar and Dhruval Patel and Naitik Panchal and Uttam Chauhan and Jitendra Bhatia},
keywords = {5G healthcare, Deep learning, Big data, Human disease prediction},
abstract = {The importance of healthcare technologies has been made clear in the current pandemic. Healthcare informatics plays an important role in facilitating healthcare and providing healthcare services in real time. Healthcare informatics has developed from Healthcare 1.0 to Healthcare 4.0 in the last few decades. The data generated from the various sources are stored as electronic health record. These data are collected in different forms and formats. The inconsistent data could be handled using various techniques of big data. The information obtained from big data analytics can be used for the prediction of diseases or conditions using artificial intelligence, machine learning, and deep learning techniques. 5G plays an important role in healthcare informatics by enabling real-time remote monitoring and improving augmented reality, virtual reality, and spatial computing. With 5G technologies, a large number of devices can be connected using high-performance computing over large distances to provide healthcare services. Blockchain is applied in healthcare for health record management, insurance claims, drug tracking, authentication, and ensuring the integrity of medical data. Deep learning techniques can be applied to ever-changing data for the detection and prevention of disease. For the classification challenge, deep convolutional neural networks using pictures of diseased regions are often utilized. In many research techniques, AlexNet and GoogLeNet have been utilized to identify plant diseases. This chapter discusses the state of the art for detecting human sickness as well as the associated 5G healthcare framework for improving it.}
}
@article{MATTIOLI2022453,
title = {Information Quality: the cornerstone for AI-based Industry 4.0},
journal = {Procedia Computer Science},
volume = {201},
pages = {453-460},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004720},
author = {Juliette Mattioli and Pierre-Olivier Robic and Emeric Jesson},
keywords = {Industry 4.0, Data-driven AI, Knowledge-based AI, Data Quality, Information Quality},
abstract = {AI becomes a key enabler for Industry 4.0. Data / information quality become a real cornerstone on the overall process from user expectation to products / systems / solutions in a consistent perspective in order to ensure quality of the manufacturing production. This paper highlights some key characteristics in terms of information quality required to implement an effective AI based monitoring framework, in order to achieve operational excellence in Industry.}
}
@incollection{BISWAS202263,
title = {Chapter 6 - Big data analytics in precision medicine},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {63-72},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000054},
author = {Saurabh Biswas and Yasha Hasija},
keywords = {Big data, EHR, Healthcare, Omics, Precision medicine},
abstract = {Precision medicine is a medical model that recommends custom-tailored products, techniques, treatments, and decisions for a subgroup of patients having the same biological basis of diseases. Due to the huge size and complexity of omics data and dataset of patient features, they cannot be analyzed directly by doctors. Big data is a term used for complex or large datasets that cannot be accurately processed or stored by traditional management tools. Omics and electronic health record (EHR) data are essential big biomedical data having a strong association with precision medicine. In this chapter, we review the importance of analyzing EHR and omics data in precision medicine. Big data analytics has been applied to healthcare in biomarker discovery and disease subtyping, drug repurposing, and integrating omics data into EHR. This will provide the most appropriate and efficient treatment to every patient on the basis of their subtyping data.}
}
@incollection{REIMER2022135,
title = {Chapter 6 - Data management in culture collections},
editor = {İpek Kurtböke},
booktitle = {Importance of Microbiology Teaching and Microbial Resource Management for Sustainable Futures},
publisher = {Academic Press},
pages = {135-155},
year = {2022},
isbn = {978-0-12-818272-7},
doi = {https://doi.org/10.1016/B978-0-12-818272-7.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182727000043},
author = {Lorenz Christian Reimer and Andrey Yurkov},
keywords = {Collection research, Data harmonisation, Data management, Databases, Literature, Microorganisms, Quality control},
abstract = {Culture collections preserve the living material and the associated information alike. The physical culture and its properties are both important for users. Strain characteristics can be inferred from a detailed description of environmental parameters and results of ex situ experiments. Some of these results will be published in the literature but others remain unpublished, such as strain tests performed by collection staff. Culture collections that did not restrict their holdings to a particular taxonomic or functional group of microorganisms consequently employed a large diversity of tests. In order to handle that heterogeneous information on a large scale, data management in culture collections is rapidly gaining importance. Data management includes several mobilisation and harmonisation steps. This chapter provides examples of data types routinely accumulated by culture collections, and how this information is unified, analysed and shared. Databases that accumulate and display records from collections worldwide become a window into modern big data research. In this chapter, we review different strategies for building up strain-related databases, point to important difficulties and name possible solutions.}
}
@article{MIRACOLO2022S206,
title = {POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review},
journal = {Value in Health},
volume = {25},
number = {1, Supplement },
pages = {S206},
year = {2022},
note = {Emerging Frontiers and Opportunities},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1002},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521027972},
author = {A Miracolo and M Mills and P Kanavos}
}
@article{FAKHRUDDIN2022100254,
title = {Harnessing risk-informed data for disaster and climate resilience},
journal = {Progress in Disaster Science},
pages = {100254},
year = {2022},
issn = {2590-0617},
doi = {https://doi.org/10.1016/j.pdisas.2022.100254},
url = {https://www.sciencedirect.com/science/article/pii/S2590061722000412},
author = {Bapon Fakhruddin and Jenty Kirsch-Wood and Dev Niyogi and Li Guoqing and Virginia Murray and Nina Frolova},
keywords = {Disaster risk reduction, Climate change, Data-driven, Risk management, FAIR data},
abstract = {Disaster and climate risks result from a complex interaction between hazard, exposure, and vulnerability in a broad context defined by socioeconomic, political, and ecological factors. To better understand the risk and manage it more effectively, we need to collect, store, analyse, and use risk-informed data. We identified challenges and opportunities for harnessing risk-informed data for disaster and climate resilience. The framework is inspired by the FAIR (findable, accessible, interoperable and reusable) and CARE (collective, authority to control, responsibility and ethics) principles to discuss opportunities how data could be available to inform risk-informed decision-making in climate and disaster risk management. Looking ahead, data could be developed and integrated with societal needs and participation. The use of data for risk management necessitates a common definition of risk to ensure a comparable research and development process. The world is shifting from a “for-profit” to a “for-benefit” operating model, which needs a Fifth Industrial Revolution driven by and for data for the benefit of society.}
}
@incollection{BATTINENI2022265,
title = {Chapter 20 - Opportunities and challenges in healthcare with the management of big biomedical data},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {265-275},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000170},
author = {Gopi Battineni},
keywords = {Big data challenges, Biomedical data, EHRs, Healthcare security, Medical imaging},
abstract = {It is a challenge to develop supervised models in clinical evaluation. Although the profundities of this challenge are frequently learned at that point, they failed to remember or willfully overlooked. This should be the situation, since dwelling too long on this limitation may bring about a skeptical viewpoint. Disregarding this challenge, we keep on using supervised model learning algorithms, and they perform better in practice. The timely translation of the machine learning model in clinical practice needs to be validated and accurately measured and the models with benefits for everyone are a big challenge. Number of factors, including the size of biomedical datasets and model fitting issues, with unbalanced datasets create unnecessary biases in medical practice. Therefore, robust clinical evaluation using simple datasets helps in simple model learning and also understands how much data is precisely needed to do performance calculations. In this chapter, the author explores the opportunities and medical challenges associated with large datasets in deep learning, including data streaming, model scalability, and distributed computing.}
}
@incollection{NASSEHI2022317,
title = {Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {317-348},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000026},
author = {Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu},
keywords = {Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0},
abstract = {With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.}
}
@incollection{SEBASTIANCOLEMAN20221,
title = {Section 1 - Data in Today’s Organizations},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {1-2},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00021-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000213},
author = {Laura Sebastian-Coleman}
}
@article{LEE2022100112,
title = {American Academy of Ophthalmology Intelligent Research in Sight (IRIS®) Registry and the IRIS Registry Analytic Center Consortium},
journal = {Ophthalmology Science},
volume = {2},
number = {1},
pages = {100112},
year = {2022},
issn = {2666-9145},
doi = {https://doi.org/10.1016/j.xops.2022.100112},
url = {https://www.sciencedirect.com/science/article/pii/S266691452200001X},
author = {Cecilia S. Lee and Marian Blazes and Alice Lorch and Suzann Pershing and Leslie Hyman and Allen C. Ho and Julia Haller and Joan W. Miller and Emily Y. Chew and Flora Lum and Aaron Y. Lee}
}
@incollection{SEBASTIANCOLEMAN202293,
title = {Chapter 5 - The Process Challenge: Managing for Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {93-117},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000055},
author = {Laura Sebastian-Coleman},
keywords = {Product quality management, quality by design, dimensions of quality, product life cycle, Juran Trilogy, data quality management, quality improvement methodology},
abstract = {This chapter presents foundational principles of quality management and applies these principles to data. Organizations do not produce high-quality products by accident. High-quality results depend on planning and commitment. Data is both a product of organizational processes and a resource required to execute those processes. Although data differs from other products and assets, with respect to its life cycle and the ways in which organizations can derive value from it, the product model of data nevertheless provides the foundational components on data quality management. It also allows us to see the connections between the process challenge and the other challenges at different points in the life cycle.}
}
@incollection{2022vii,
title = {In praise of Meeting the Challenges of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {vii-ix},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500016X}
}
@incollection{2022277,
title = {Index},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {277-281},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.09993-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546099931}
}
@incollection{SHANG2022203,
title = {Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {203-228},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000087},
author = {Wen-Long Shang and Haoran Zhang and Yi Sui},
keywords = {MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19},
abstract = {This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.}
}
@incollection{MANZELLA2022319,
title = {Chapter Six - How can ocean science observations contribute to humanity?},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {319-335},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000050},
author = {Giuseppe M.R. Manzella and William Emery},
keywords = {Data management, Interdisciplinarity, Knowledge, Mutual understanding, Ocean data science, Solution-oriented education},
abstract = {Experience is needed to prepare young ocean researchers to work in interdisciplinary environments and to teach them how to deal with complex processes. Such experience can be provided in courses/internships aimed at preparing qualified personnel to work on solution-oriented projects. These lessons are designed to deepen understanding in particular elements of ocean data science education: oceanography as a science in evolution, mutual understanding, the enrichment of data, and the process of moving from data to information. Such lessons combine the history of ocean science with ocean data methodologies and technologies, data quality elements, “fitness for use”/“fitness for purpose” and analyses. The approach consists of a significant mentoring program aimed at strengthening “thinking skills”—critical and creative thinking—and therefore the ability to solve complex problems.}
}
@incollection{NOH2022639,
title = {20 - Big data analysis for civil infrastructure sensing},
editor = {Jerome P. Lynch and Hoon Sohn and Ming L. Wang},
booktitle = {Sensor Technologies for Civil Infrastructures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {639-677},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-08-102706-6},
doi = {https://doi.org/10.1016/B978-0-08-102706-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081027066000076},
author = {Hae Young Noh and Jonathon Fagert},
keywords = {Big data, Civil infrastructure sensing, Damage diagnosis, Machine learning, Occupant monitoring, Smart infrastructure, Structural health monitoring},
abstract = {With the growing scale and complexity of city infrastructures, the need for data analysis and machine learning is becoming more and more prominent in the field of civil infrastructure sensing. This coupled with the explosion of available sensing data in smart cities and smart infrastructures has offered new opportunities like never before. Using big data tools at a structure level, we can understand important information about structural properties and damage states, city environmental and operational conditions, as well as an individual user or group patterns. In this chapter, we explore and provide guidance for big data analytics and its application to civil infrastructure problems. Furthermore, we discuss future directions and trends that will enable large-scale monitoring of civil infrastructure and smart cities.}
}
@incollection{LV2022247,
title = {Chapter 19 - Privacy security risks of big data processing in healthcare},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {247-263},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000200},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Cloud services, Healthcare, Privacy measures, Privacy security risk},
abstract = {This chapter aims to discuss healthcare's development in China and the privacy and security risk factors in medical data under big data. First, the development status of China's healthcare sector is analyzed. The questionnaire is made to analyze the privacy and security risk factors of healthcare big data (HBD) and protection measures are proposed according to the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions and medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all increase annually. In 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The questionnaire results reveal that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system are all greater than 0.8 when HBD is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to emphasize data privacy protection and grasp using digital medical data to provide decision support for subsequent medical data analysis.}
}
@incollection{ANGADI2022151,
title = {Chapter Seven - Role of big data analytic and machine learning in power system contingency analysis},
editor = {Rakesh Sehgal and Neeraj Gupta and Anuradha Tomar and Mukund Dutt Sharma and Vigna Kumaran},
booktitle = {Smart Electrical and Mechanical Systems},
publisher = {Academic Press},
pages = {151-184},
year = {2022},
isbn = {978-0-323-90789-7},
doi = {https://doi.org/10.1016/B978-0-323-90789-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390789700004X},
author = {Ravi V. Angadi and Suresh Babu Daram and P.S. Venkataramu},
keywords = {Big data, Classification, Contingency analysis, Critical, Data collection, Data mining, Data processing, Data visualization, Decision tree, Different load conditions, Energy systems, Machine learning, Non critical, Powersystem, Semi critical, Severity prediction, Structured data, Testing data set, Training data set, Voltage stability, Voltage stability index, Volume of data},
abstract = {This work discusses the use of big data and machine learning to predict the severity of a system breakdown caused by an n−1 transmission line condition. The contingency analysis is a key part of traditional energy management systems. The severity of the line is identified by computing the Line Voltage Stability Index The large amount of data handling will be involved during the contingency study. These data need to be processed and analyzed properly by data handling technique and the use of machine learning tools arrive required information in the system. The severity of transmission lines is predicted and compared using classification approaches. To create large data, MATLAB simulation results will be used and the machine learning tool, Weka is used to analyze the data and forecast transmission line. The standard IEEE 30 bus system is considered to understand the proposed methodology.}
}
@incollection{KOUL20223,
title = {Chapter 1 - Influence and implementation of Industry 4.0 in health care},
editor = {Aboul Ella Hassanien and Jyotir Moy Chatterjee and Vishal Jain},
booktitle = {Artificial Intelligence and Industry 4.0},
publisher = {Academic Press},
pages = {3-21},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-88468-6},
doi = {https://doi.org/10.1016/B978-0-323-88468-6.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884686000024},
author = {Sumit Koul},
keywords = {Industrial revolution, Healthcare 4.0, Blockchain technology, Internet of Things (IoT), Big data},
abstract = {With the advancement of technology, the world is changing and automating at a rapid pace. Digitization plays a major role in the automation of technology. In this context, Industry 4.0 shows how industrial production is developing along with the latest technology. In Industry 4.0, much manual work has been replaced by automated machines that can be controlled with developing technologies such as artificial intelligence (AI), big data, cloud computing, and so on. Various technologies have been introduced in the healthcare sector due to Industry 4.0. These include AI, three-dimensional printing, machine learning, cognitive systems, autonomous robots, autonomous vehicles, augmented reality, big data, Internet of Things (IoT), blockchain technology, and more. This chapter discusses the transformation of the healthcare industry in the context of Industry 4.0. It presents a detailed study on big data, IoT, and blockchain technology with different applications that can enhance what is known as Healthcare 4.0. The chapter includes three case studies that illustrate the use of innovation in Healthcare 4.0 to detect and diagnose disease using portable medical devices connected to the IoT.}
}
@incollection{KARACA202221,
title = {Chapter 3 - Multi-chaos, fractal and multi-fractional AI in different complex systems},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {21-54},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390032400016X},
author = {Yeliz Karaca},
keywords = {Chaotic systems, Complex order, Complexity, Computational complexity, Data ethics, Different complex systems, Dynamics of complex systems, Evolution, Fractional thinking, Nonlinearity and irregularity},
abstract = {Modern scientific thinking adopts the systemic properties and addresses them through revealing the spontaneous processes related to self-organization in a dynamical system in a state far from the equilibrium point and close to the disequilibrium point with no existence of external force acting upon the system. The modern way of thinking poses a challenge against the dichotomy between the natural world and social world, by taking into account the concepts around complexity, evolution and order. This study provides an overview encompassing multi-chaos, fractal, fractional and Artificial Intelligence (AI) way of thinking for the solution of the complex system problems concerned with natural and social sciences. Furthermore, ethical decision-making frameworks and strategies concerning big data and AI applications to provide assistance for the identification of the related problems in different settings and help thinking in a methodical manner with a deliberative compensating process so that tensions between conflicting aspects can be managed systematically. The values related to ethical issues, which are thorny in nature, point to being practical, flexible and problem-driven rather than purely theory-driven in order that dilemmas can be addressed and critical decision-making guided in a way beyond theoretical positions with a focus on applied aspects. Through the lenses of such transformative thinking along with mathematics-informed frameworks encompassing chaos, fractal and multi-fractional ways, the incorporation of technology, with Artificial Intelligence, as the most viable and far-reaching leg, is essentially required to be able to address and tackle complexity that has chaotic, nonlinear, and dynamic characteristics. Hence, optimized solutions can be conceived and implemented efficiently and in a facilitating way with some required degree of flexibility as well. Considering the impact and ubiquity of data technologies concerning all aspects of modern life, it becomes important to establish a balance between data use and ethical matters. Computational technologies in different complex systems based on mathematical-driven informed frameworks can enable the generation of more realistic and applicable adaptive models under transient, dynamic and ever-evolving conditions of different complex systems.}
}
@incollection{HOVENGA2022239,
title = {Chapter 10 - Guideline and knowledge management in a digital world},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {239-270},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000124},
author = {Evelyn Hovenga},
keywords = {Knowledge, Learning health systems, Ontology, Artificial intelligence, Decision support, Governance, Quality, Computing, Concept representation},
abstract = {Knowledge represents primary sources of truth, a crucial, valuable asset supporting the health system. The current digital health revolution has sped up the acquisition of new knowledge. This is far too much for any one person to process. Traditionally, this has taken too long to adopt as dissemination methods are failing to streamline timely access and use. Health-related knowledge needs to support decision-making at any level within the healthcare system. No clinician has the time to discover and wade through multiple PDF documents in order to access the best available or real-time evidence. Computers need to be enabled to deliver this by facilitating querying anytime, integrating and using clinical guidelines and protocols metadata to support automation in decision support systems. Electronic knowledge processing, governance and use, at points of decision-making, and the supporting standards and legislative requirements for a well-functioning digital health ecosystem to benefit the population at large are explored in this chapter.}
}
@incollection{SEBASTIANCOLEMAN2022119,
title = {Chapter 6 - The Technical Challenge: Data/Technology Balance},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {119-130},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000067},
author = {Laura Sebastian-Coleman},
keywords = {Technology hype, data quality tools, technology funding, data quality management},
abstract = {This chapter discusses the deep connection between the data we produce and the technology through which we create, collect, manage, access, and use it. Data brings value only when it is used. Without reliable, technical management of data, people cannot access and use data. Unfortunately, incorrect assumptions about the relationship between technology and data often result in poor-quality data. Organizations must manage their technology to support their data strategy, while avoiding the risk of being sucked into technology hype. Both data and technology must serve organizational goals.}
}
@incollection{FOTOPOULOS2022241,
title = {Chapter 8 - The edge-cloud continuum in wearable sensing for respiratory analysis},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {241-271},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000026},
author = {Anaxagoras Fotopoulos and Pantelis Z. Lappas and Alexis Melitsiotis},
keywords = {Artificial intelligence, Edge computing, Internet of Medical Things, Multisource fusion, P4 health care},
abstract = {Edge computing is seen as a set of remotely available computer system resources that drive the computing power at the source of data to improve energy efficiency and security, as well as decrease latency. Although the computation capability of biomedical wearables has increased extremely during the past decade, it is still challenging to perform sophisticated artificial intelligence (AI) algorithms in a resource-constrained environment for energy-efficiency and (near) real-time processing, along the edge-cloud continuum. The aim of this chapter is twofold. The first is to outline the role of edge computing on the Internet of Medical Things, in which wearable technologies are used as the sensory equipment for respiratory analysis, at the transition of patient monitoring from hospital to home. The second is to discuss the potential of explainable AI in the P4 health-care context for respiratory analysis, by highlighting computational intelligence and multisource fusion approaches to achieve continuous monitoring of respiratory analysis.}
}
@incollection{SUI2022113,
title = {Chapter 4 - Data fusion technologies for MaaS},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {113-142},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000051},
author = {Yi Sui and Haoran Zhang and Wenxiao Jiang and Rencheng Sun and Fengjing Shao},
keywords = {MaaS, Data fusion, Deep learning, Fusion strategies, Matrix factorization},
abstract = {Mobility as a Service (MaaS) connects passengers and drivers by providing online ride services through an internet-based platform, which has been popular around the world. In the MaaS system, a diversity of datasets from different sources are collected and processed for data mining tasks. However, data from different sources have multi-modal characteristics. How to effectively fuse them to support data mining tasks is uneasy work. This chapter provides a technical review of advanced data fusion models used in the MaaS applications. First, we summarize the data types in the MaaS system and their input formula for data fusion methods. Second, data fusion methods are classified into two categories: deep learning-based methods and decomposition-based methods. For deep learning-based methods, basic model units and widely used data fusion strategies are discussed. For decomposition-based methods, we introduce the basic mathematics tools and their applications. Finally, we discuss challenging problems and future study trends. We believe this review will facilitate future studies in the data fusion in the MaaS system.}
}
@incollection{SANNI202225,
title = {Chapter 3 - Advances in data-centric intelligent systems for air quality monitoring, assessment, and control},
editor = {Gonçalo Marques and Joshua O. Ighalo},
booktitle = {Current Trends and Advances in Computer-Aided Intelligent Environmental Data Engineering},
publisher = {Academic Press},
pages = {25-58},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-85597-6},
doi = {https://doi.org/10.1016/B978-0-323-85597-6.00021-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323855976000215},
author = {Samuel Eshorame Sanni and Emmanuel Emeka Okoro and Emmanuel Rotimi Sadiku and Babalola Aisosa Oni},
keywords = {Air pollution, air quality prediction, artificial intelligence, computer-aided algorithms, data-centric systems, machine learning, particle dispersion},
abstract = {Air pollution is currently an issue of great concern due to the increase in anthropogenic, economic, industrial, and social activities that release high concentrations of aerosols, NOx, CO2, and greenhouse gases into the environment. According to reports from the World Health Organization (2019), about 91% of the global population resides in areas affected by poor air quality, which results in poor health conditions, thus causing about 7 million deaths annually and the destruction of the ecosystem. Despite the myriad of proposals to curb these growing concerns caused by air pollutants, air pollution seems to know no bounds because their causatives in terms of the activities that lead to air pollution including manufacturing, incineration, combustion of coal etc. are necessary for human existence and sustenance. Strategic alteration of production patterns and the replacement of conventional heating systems have been proposed as air pollution control measures; however, due to the increased demands posed by basic necessities such as transportation, food, deforestation, and industrial processes, it has become necessary to use smart high-performance data-centric systems/artificial intelligence as air pollution forecasting tools that can examine the sources of these pollutants, predict their prevalence, determine their eventual consequences, as well as proffer informed decisions for contingency actions. In this chapter, topics covered include “deep- and machine-learning applications in air quality modeling, air quality prediction, modeling of particle dispersion/filters, heating, ventilation, and air-conditioning systems, industrial air quality control systems concerning data-centric intelligent systems, as well as previous and recent developments and application of these systems in air quality monitoring, assessment, and pollution control.”}
}
@incollection{SIDDIQUI2022169,
title = {Chapter 12 - Application of artificial intelligence and machine learning in blockchain technology},
editor = {Rajiv Pandey and Sunil Kumar Khatri and Neeraj kumar Singh and Parul Verma},
booktitle = {Artificial Intelligence and Machine Learning for EDGE Computing},
publisher = {Academic Press},
pages = {169-185},
year = {2022},
isbn = {978-0-12-824054-0},
doi = {https://doi.org/10.1016/B978-0-12-824054-0.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240540000010},
author = {Zeeshan Ali Siddiqui and Mohd Haroon},
keywords = {Blockchain, Machine learning, Artificial intelligence, Edge computing, Architecture},
abstract = {Blockchain technology is a distributed, decentralized, immutable ledger. It is used to store encrypted data; on the other hand, in artificial intelligence, human natural intelligence can be simulated. All the characteristics of the human brain may be programmed with artificial intelligence. Blockchain technology may be thought of as a body and artificial intelligence as its brain. In blockchain technology, if artificial intelligence approaches are used, then data collection, data enabling, data analytic, and decision-making of data collection can be improved. With both these technologies, we will be able to affect and enact upon data in different ways. Also, they may be proven as game-changers in the exploitation of data. At the same time, machine learning approaches may enhance the architecture of blockchain technology. Blockchain technology can make artificial intelligence further rational and comprehensible. We can trace the trail of decisions and determine why a particular decision was taken by a machine learning algorithm and on the other hand, blockchain technology records data and variables that go through in the decision-making process in machine learning. In this chapter, we have discussed key concepts of blockchain technology and applications of artificial intelligence and machine learning in the blockchain. This chapter also investigated edge computing as a potential use case of blockchain technology.}
}
@incollection{2022181,
title = {Index},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {181-184},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.09994-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012824475309994X}
}
@article{WANG2022663,
title = {A novel oscillation identification method for grid-connected renewable energy based on big data technology},
journal = {Energy Reports},
volume = {8},
pages = {663-671},
year = {2022},
note = {2021 International Conference on New Energy and Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722002682},
author = {Jian Wang},
keywords = {Oscillation identification, Big data, Evidence theory, Support vector machine},
abstract = {With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.}
}
@incollection{SEBASTIANCOLEMAN2022229,
title = {Chapter 10 - Dimensions of Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {229-256},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000109},
author = {Laura Sebastian-Coleman},
keywords = {Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling},
abstract = {This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.}
}
@article{SALEM2022e55,
title = {Unaccounted Confounders Limit the Ability to Draw Conclusions From Big Data Analysis Comparing Radiotherapy Fractionation Regimens in NSCLC},
journal = {Journal of Thoracic Oncology},
volume = {17},
number = {6},
pages = {e55-e56},
year = {2022},
issn = {1556-0864},
doi = {https://doi.org/10.1016/j.jtho.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556086422001381},
author = {Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn}
}
@article{MURTHY2022,
title = {Life sciences discovery and technology highlights},
journal = {SLAS Technology},
year = {2022},
issn = {2472-6303},
doi = {https://doi.org/10.1016/j.slast.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2472630322051743},
author = {Tal Murthy and Jamien Lim}
}
@incollection{CYCHOSZ20221,
title = {Chapter One - Using big data from long-form recordings to study development and optimize societal impact},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {1-36},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000434},
author = {Margaret Cychosz and Alejandrina Cristia},
keywords = {Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children},
abstract = {Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.}
}
@incollection{HOVENGA2022209,
title = {Chapter 9 - Quality data, design, implementation, and governance},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {209-237},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000136},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain},
abstract = {Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.}
}
@incollection{SEBASTIANCOLEMAN2022131,
title = {Chapter 7 - The People Challenge: Building Data Literacy},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {131-164},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000079},
author = {Laura Sebastian-Coleman},
keywords = {Data literacy, data visualization, analytics, metadata management, critical thinking, scientific thinking, data management},
abstract = {This chapter addresses the skills, knowledge, and experience people require to create, use, and interpret data. Data literacy is the ability to read, understand, interpret, and learn from data in different contexts and to communicate about data with other people. The people challenge is both a skills challenge and a knowledge challenge. No single individual can know everything about an organization’s data. But, together, people can solve more problems in better ways if they understand data as a construct, recognize the risks associated with data production and use, cultivate a level of skepticism about data, and develop skill in visualizing and interpreting data. They will solve even more problems if the organization supports these efforts through disciplined metadata management and data quality management.}
}
@incollection{LARKIN2022283,
title = {Chapter Five - Connecting marine data to society},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {283-317},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000037},
author = {Kate E. Larkin and Andrée-Anne Marsan and Nathalie Tonné and Nathalie {Van Isacker} and Tim Collart and Conor Delaney and Mickaël Vasquez and Eleonora Manca and Helen Lillis and Jan-Bart Calewaert},
keywords = {Big data, Climate change, Data visualization, Digital ocean, Ecosystems, FAIR, Hackathon, Knowledge broker, Marine data, Marine map, Ocean, Ocean literacy, Open data, Science communication, Seabed habitats},
abstract = {This chapter looks at connecting marine data to society, with a focus on key developments in Europe, set in a global context. It presents the European Marine Observation and Data Network-EMODnet as an exemplar in marine domain. EMODnet has significantly advanced European capability for Findable, Accessible, Interoperable, and Reusable marine knowledge, offering access to standardized and harmonized in-situ marine data and added value data products across seven marine environmental themes. Open and free data, products and associated metadata, are available for discovery and access through a wide range of web/data services. These ensure that the wealth of existing ocean observations and marine data collected in Europe and beyond can be easily discovered and used by a growing, and diversifying, user community. Interoperability with key services is crucial toward a pan-European and global approach. Key partnerships include the Copernicus Marine Environment Monitoring Service and international initiatives, e.g., the International Oceanographic Data and Information Exchange. Looking at societal tools and applications, the chapter provides a case study of the European Atlas of the Seas, a web-mapping tool that communicates marine and other open-source data and information in an attractive and interactive way. The EU Atlas is a key tool for the European ocean literacy initiative EU4Ocean, contributing to engage citizens and drive the societal change that is required for Europe to meet the ambitious targets to be climate neutral by 2050. The paper introduces examples of emerging tools for data visualization and presents hackathons, a powerful method to cocreate and innovate applications for society. Finally, the chapter looks toward the digital era and addresses the emerging challenges and opportunities of marine data, e.g., big data and plans for a digital twin of the Ocean, as tools to enable a step-change in societal connection, understanding, and action regarding the ocean.}
}
@incollection{REDMAN2022xxi,
title = {Foreword},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {xxi-xxii},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000195},
author = {Thomas C. Redman}
}
@article{CHINTAKINDI2022699,
title = {WAMS challenges and limitations in load modeling, voltage stability improvement, and controlled island protection—A review},
journal = {Energy Reports},
volume = {8},
pages = {699-709},
year = {2022},
note = {2021 The 8th International Conference on Power and Energy Systems Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.11.217},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721013615},
author = {Raju Chintakindi and Arghya Mitra},
keywords = {Wide-area monitoring, Phasor measurement unit, Voltage stability, Load modeling, Control-islanding, Renewable energy integration},
abstract = {Global energy demand pushes transmission lines to their limits, which causes voltage instability on the power grids. Solar and wind farms are being integrated with increased size and capacity to meet increased load demands, and this increased volumes of renewables, combined with their lower inertia and more sensitive voltage profiles, are complicating system operators’ roles. The execution of phasor measurement unit (PMU) with novel detection and protection schemes has been a focal point for power engineers. The real-time wide-area monitoring system (WAMS) helps grid operators to improve real-time voltage stability, situational awareness, fault detection, and grid planning. WAMS shows the path for developing substation predictions, improving system accuracy, and identifying control strategies to avoid power blackouts. It conducts post-event analysis and reduces the processing time of the power restoration. The novelty of this article is to describe load modeling, voltage stability, and islanding protection in real-time, as well as the technical challenges, limitations, opportunities, and solutions related to the same. Further, the paper explores and presents vital software and hardware resources, including HIL systems, for future use and research with WAMS technology.}
}
@incollection{KOLTAY2022ix,
title = {Introduction},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {ix-xi},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000072},
author = {Tibor Koltay}
}
@incollection{2022xvii,
title = {Preface},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {xvii-xx},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032391907400025X}
}
@article{EHWERHEMUEPHA2022108120,
title = {Cerner real-world data (CRWD) - A de-identified multicenter electronic health records database},
journal = {Data in Brief},
volume = {42},
pages = {108120},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108120},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922003304},
author = {Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster},
keywords = {Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records (EHR), HealtheIntent, HealtheDataLab™, Cerner learning Health Network (LHN)},
abstract = {Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter electronic health records. Cerner Corporation secured appropriate data use agreements and permissions from more than 100 health systems in the United States contributing to the database as of March 2022. A subset of the database was extracted to include data from only patients with SARS-CoV-2 infections and is referred to as the Cerner COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9 billion, 486 million, and 11.5 billion records in the condition, medication, procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19 Dataset consists of 130.1 million encounters from 3.8 million patients. The size and longitudinal nature of CRWD can be leveraged for advanced analytics and artificial intelligence in medical research across all specialties and is a rich source of novel discoveries on a wide range of conditions including but not limited to COVID-19.}
}
@article{REIS2022107675,
title = {Data-Driven Process System Engineering–Contributions to its consolidation following the path laid down by George Stephanopoulos},
journal = {Computers & Chemical Engineering},
volume = {159},
pages = {107675},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107675},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422000199},
author = {Marco S. Reis and Pedro M. Saraiva},
keywords = {Process Analytics, Process Systems Engineering 4.0, Data science, Industry 4.0, Artificial Intelligence, Data-Driven PSE},
abstract = {The number and diversity of Process Analytics applications is growing fast, impacting areas ranging from process operations to strategic planning or supply chain management. However, this field has not reached yet a maturity level characterized by a stable, organized and consolidated body of knowledge for handling the main classes of problems that need to be faced. Data-Driven Process Systems Engineering and Process Analytics only recently received wider recognition, becoming a regular presence in journals and conferences. As a tribute to the groundbreaking Process Analytics contributions of George Stephanopoulos, namely through his academic tree, to which we are proud to belong, this article aims to contribute to the systematization and consolidation of this field in the broad PSE scope, starting from a fundamental understanding of the key challenges facing it, and constructing from them a workflow that can flexibly be adapted to handle different problems, aimed at supporting value creation through good decision-making. In this path, we base our foresight and conceptual framework on the authors’ experience, as well as on contributions from other researchers that, across the world, have been collectively pushing forward Data-Driven Process Systems Engineering.}
}
@incollection{FARRE2022197,
title = {Chapter 7 - Data-driven policy evaluation},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {197-225},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000026},
author = {Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré},
keywords = {Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation},
abstract = {Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.}
}
@incollection{BAI202261,
title = {Chapter Three - Data-driven approaches: Use of digitized operational data in process safety},
editor = {Faisal Khan and Hans Pasman and Ming Yang},
series = {Methods in Chemical Process Safety},
publisher = {Elsevier},
volume = {6},
pages = {61-99},
year = {2022},
booktitle = {Methods to Assess and Manage Process Safety in Digitalized Process System},
issn = {2468-6514},
doi = {https://doi.org/10.1016/bs.mcps.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468651422000022},
author = {Yiming Bai and Shuaiyu Xiang and Zeheng Zhao and Borui Yang and Jinsong Zhao},
keywords = {Data-driven models, Process safety, Data preparation, Data cleaning, Statistical models, Artificial intelligence models, Process monitoring, Fault detection and diagnosis, Fault prognosis, Video monitoring},
abstract = {Process safety is playing an important role with the rapid development of industry. With the advent of the Big Data era, various and massive data from the Internet of Things can be used for process safety. In this chapter, we aim to provide the reader with a comprehensive understanding of rapidly growing data-driven process safety approaches in the chemical industry. Data-driven approaches primarily use past process data without a complex mechanism model of chemical properties or processes; hence, they have advantages in practical industrial applications. In this chapter, first, we describe the importance of data in process safety. Then, we briefly introduce the ideas and methods of data pre-processing. We follow this with a discussion on statistical-based and artificial intelligence-based data-driven approaches. Then, we elaborate on the application of data-driven methods in the field of chemical process safety. Finally, we provide a summary and outlook for advancing data-driven methods.}
}
@incollection{SPANAKI2022147,
title = {Chapter 9 - Digital architectures: frameworks for supply chain data and information governance},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {147-161},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000095},
author = {Konstantina Spanaki and Erisa Karafili and Stella Despoudi},
keywords = {Data access control, Data flows, Data quality, Data sharing, Information architecture, Information flows, Supply chain},
abstract = {Advances in digitalization present new and emerging Supply Chain (SC) Information Architectures that rely on data and information as vital resources. While the importance of data and information in SCs has long been understood, there is a dearth of research or understanding about the effective governance, control, or management of data ecosystems at the SC level. This chapter examines data architectures through a navigation of the background of database management and data quality research of previous decades. The chapter unfolds the critical architectural elements around data and information sharing in the SC regarding the context, systems, and infrastructure. A review of various frameworks and conceptual models is presented on data and information in SCs, as well as access control policies. The critical importance of data quality and the management of data in the cyber-physical systems are highlighted. Policies for data sharing agreements (DSAs) and access control are discussed and the importance of effective governance in the distributed environments of digitally enabled SCs is emphasized. We extend the concept of data sharing agreements to capture the interplay between the various SC stakeholders around data use. Research gaps and needs relevant to new and emerging SC data and information ecosystems are highlighted.}
}
@article{CHEN2022102401,
title = {Extreme events, energy security and equality through micro- and macro-levels: Concepts, challenges and methods},
journal = {Energy Research & Social Science},
volume = {85},
pages = {102401},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102401},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004886},
author = {Chien-fei Chen and Thomas Dietz and Nina H. Fefferman and Jamie Greig and Kristen Cetin and Caitlin Robinson and Laura Arpan and Marcel Schweiker and Bing Dong and Wenbo Wu and Yue Li and Hongyu Zhou and Jianzhong Wu and Jin Wen and Joshua S. Fu and Tianzhen Hong and Da Yan and Hannah Nelson and Yimin Zhu and Xueping Li and Le Xie and Rachel Fu},
keywords = {Disasters, COVID-19, Energy justice, Energy insecurity, Energy inequality, Resilience},
abstract = {Low-income households face long-standing challenges of energy insecurity and inequality (EII). During extreme events (e.g., disasters and pandemics) these challenges are especially severe for vulnerable populations reliant on energy for health, education, and well-being. However, many EII studies rarely incorporate the micro- and macro-perspectives of resilience and reliability of energy and internet infrastructure and social-psychological factors. To remedy this gap, we first address the impacts of extreme events on EII among vulnerable populations. Second, we evaluate the driving factors of EII and how they change during disasters. Third, we situate these inequalities within broader energy systems and pinpoint the importance of equitable infrastructure systems by examining infrastructure reliability and resilience and the role of renewable technologies. Then, we consider the factors influencing energy consumption, such as energy practices, socio-psychological factors, and internet access. Finally, we propose interdisciplinary research methods to study these issues during extreme events and provide recommendations.}
}
@incollection{BATHULA2022507,
title = {Chapter 22 - Digital healthcare data management using blockchain technology in genomics and COVID-19},
editor = {Arpana Parihar and Raju Khan and Ashok Kumar and Ajeet Kumar Kaushik and Hardik Gohel},
booktitle = {Computational Approaches for Novel Therapeutic and Diagnostic Designing to Mitigate SARS-CoV-2 Infection},
publisher = {Academic Press},
pages = {507-518},
year = {2022},
isbn = {978-0-323-91172-6},
doi = {https://doi.org/10.1016/B978-0-323-91172-6.00024-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911726000248},
author = {Sreenivas Reddy Bathula},
keywords = {COVID-19, insurance, telemedicine, point-of-care testing, big data, artificial intelligence, blockchain},
abstract = {From birth certificate until death certificate, a person accumulates healthcare data. Each time a person has contact with a healthcare professional some type of record is produced, be it paper, electronic, or both. In one year, a normal healthy American may visit the dentist twice, see a doctor for a wellness visit, call a physician for a bad cold, and refill a dozen allergy prescriptions. Each one of these encounters becomes a part of the many records stored somewhere in the archives of healthcare data. Much of the data is merely entered into a computer system so that the provider can receive payment, and then the data are added to a massive “warehouse.” Each one of these records can provide insight not only into an individual’s well-being but may, in fact, affect healthcare across similar groups based on age, gender, or location, and provide an analytic base to determine insurance rates, study disease trends, and modify treatment protocols. This administrative type of information claims and encounters eligibility and enrollment is still vastly underused. This is due in part to the massive amounts of administrative data, in part to their complexity, and in part to their misuse in the past. Studies of this type of data are only now beginning to receive proper time and attention due to Coronavirus disease-2019. The process of converting healthcare data into useful information is the focus of this book chapter.}
}
@article{HANSEN20222461,
title = {Finding the FAIRness in perovskite photovoltaics research},
journal = {Matter},
volume = {5},
number = {8},
pages = {2461-2464},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S259023852200296X},
author = {Kameron R. Hansen and Luisa Whittaker-Brooks},
abstract = {With thousands of publications per year, the volume of data published on perovskite solar cells since the spark of the “perovskite fever” in 2013 is enormous and far exceeds the amount that any individual researcher could digest. To tackle this issue, Jacobsson et al.1 have created The Perovskite Database, which is part of a larger trend to harness the power of big data and artificial intelligence to accelerate the commercialization of perovskite solar cells.}
}
@article{SIMONART2022956,
title = {Impact of human papillomavirus vaccine in reducing genital warts: A Google Trends analysis},
journal = {Journal of the American Academy of Dermatology},
volume = {86},
number = {4},
pages = {956-958},
year = {2022},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2021.03.091},
url = {https://www.sciencedirect.com/science/article/pii/S0190962221006563},
author = {Thierry Simonart and Xuân-Lan {Lam Hoai} and Viviane {De Maertelaer}}
}
@incollection{2022285,
title = {Glossary},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {285-297},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00024-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000249}
}
@incollection{KOLTAY2022145,
title = {Chapter 6 - Roles and education of information and data professionals},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {145-180},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000060},
author = {Tibor Koltay},
keywords = {Data librarian, Data scientist, Data science, Research data management, Data literacy, Social-constructionist approaches, Connectivism, Scholarship of teaching, Inquiry-based learning},
abstract = {This chapter is based on a nonexhaustive treatment of selected issues. The first subsection identifies the main professional groups involved in data-related activities in academic libraries and beyond. In the second subsection, educational issues are targeted with regard to the education of information professionals for research data management, as well as for data literacy. The third subsection examines similarities and convergences between the pedagogy of educating for literacies. Pedagogical approaches, including social-constructionist, cognitive, and connectivist approaches, as well as the Scholarship of Teaching, are characterized here briefly. Inquiry-based learning and the ideas for breaking out of silos are also portrayed, and the tasks of visualization are described as well.}
}
@incollection{PROVA2022111,
title = {Chapter 10 - Big medical data analytics for diagnosis},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {111-124},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000133},
author = {Omanin Siddiqua Prova and Faiza Ahmed and Jafrin Sultana and Md. Ashrafuzzaman},
keywords = {Algorithms, Analytics, Big medical data, Diagnosis},
abstract = {Big Medical Data Analytics is an intricate process of extracting information of medical interest from a massive amount of data. Unstructured or structured heterogeneous datasets gleaned from various sources are analyzed with data analytics approaches (e.g., artificial intelligence, machine learning, data mining, etc.) for excavating useful diagnostic information to predict diseases and suitable treatment models. Intensive research is needed to scrutinize the impact of big medical data analytics to diagnose cardiovascular diseases, neurological disorders, and early diagnosis of chronic and genetic diseases. Utilizing this analytics process not only shortens the decision-making time for the caregivers but initiates the development of cost-effective treatment modules, algorithms, or software-based devices. In this chapter, how big medical data analytics can be incorporated with already facilitating diagnosing diseases is discussed, as well as some analytical tools are highlighted that have higher accuracy rates compared to conventional procedures in diagnosing diseases at an early stage.}
}
@incollection{KOLTAY202249,
title = {Chapter 3 - Data quality, the essential “ingredient”},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {49-75},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000047},
author = {Tibor Koltay},
keywords = {Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance},
abstract = {This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.}
}
@article{GE20221739,
title = {Progress of big geodata},
journal = {Science Bulletin},
volume = {67},
number = {17},
pages = {1739-1742},
year = {2022},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2022.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S2095927322003206},
author = {Yong Ge and Ting Ma and Tao Pei and Huixian Weng and Xin Li and Xining Zhang}
}
@incollection{AMIN202225,
title = {Chapter Two - State-of-the-art in process safety and digital system},
editor = {Faisal Khan and Hans Pasman and Ming Yang},
series = {Methods in Chemical Process Safety},
publisher = {Elsevier},
volume = {6},
pages = {25-59},
year = {2022},
booktitle = {Methods to Assess and Manage Process Safety in Digitalized Process System},
issn = {2468-6514},
doi = {https://doi.org/10.1016/bs.mcps.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468651422000010},
author = {Md Tanjin Amin and Rajeevan Arunthavanathan and Md Alauddin and Faisal Khan},
keywords = {Process safety, Digitization, Digitalization, Digital transformation, Digital process},
abstract = {This chapter presents an overview of the current progress in digital process systems' safety. It starts with defining crucial terminologies associated with digital process systems, followed by a brief description of the stimulating factors behind the growth of digital process systems. Key opportunities and challenges of digital transformation in process industries are also discussed. A bibliometric analysis is performed to numerically understand digital process safety growth. It is found that the past decade has seen increasing attention from the research community due to the inauguration of the Industry 4.0 concept. Finally, a roadmap is provided for future developments in digital process safety.}
}
@incollection{HOVENGA202217,
title = {Chapter 2 - Global and national infrastructures supporting digital health ecosystems},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {17-33},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000082},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Ecosystem, Interoperability, Complex systems, Value propositions, Politics, Networks},
abstract = {Foundational infrastructure requirements in this chapter and lists of the form of studies that can be used to study any national, regional, or organisational digital health infrastructure are identified. Some criteria that may be used to evaluate overall performance are provided. Digital health foundational needs and adaptive complex digital health ecosystems are described. Examples of national digital health strategies adopted by Australia, the United Kingdom, New Zealand, and the United States are provided, noting missing foundational infrastructure components that continue the fragmentation of technical applications and data repositories. There is an urgent need for strong national non-hierarchical leadership promoting and supporting innovation, including greater clinical engagement. This chapter provides the rationale for the adoption of national roadmaps.}
}
@incollection{ZOHURI202259,
title = {Chapter 3 - Data warehousing, data mining, data modeling, and data analytics},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {59-86},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000015},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Big data, Data analytics and Data predective, Data modeling, Data warehousing},
abstract = {Data warehouses are constantly evolving to support new technologies and business requirements—and remain relevant when it comes to big data and analytics. Regardless of how new or sophisticated your data warehouse is, it likely needs modernization. Data warehousing, along with data modeling, and side by side with data analytic capability gives us the upper hand with our knowledge by collecting the right information at the right time with the right data coming from all directions, whether or not these data are structured or unstructured. We should be able to have proper tools in hand to be able to take this information and knowledge to be in a position of resilience based on predictive analysis driven by data. This chapter will discuss data warehousing, data modeling, and consequently, data analytics where, in combination, they all are variables functioning within the process of predictive analytic modeling. This process allows us to have the knowledge we are looking for. Getting reliable information from data warehouses is resource-intensive; missing one step can result in wasted processing time and/or bad data.}
}
@incollection{CHAKRABORTY202273,
title = {Chapter 7 - Recent advances in processing, interpreting, and managing biological data for therapeutic intervention of human infectious disease},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {73-82},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000091},
author = {Pritha Chakraborty and Parth Sarthi {Sen Gupta} and Shankar Dey and Nabarun {Chandra Das} and Ritwik Patra and Suprabhat Mukherjee},
keywords = {Big Data, Biomedical, Electronic health records, Epidemiology, Surveillance},
abstract = {Big data has been a buzzword for academics and the healthcare industry, describing data accumulation, processing, and interpretation using analytical tools and techniques. Hitherto, identification of state of the art in big data has become a question of research for academicians due to lack of addressable studies in this regard. The potential of big data is lagging, as compared to other fields. Herein, in this chapter, we have described the potential of big data in combating infectious diseases involving personalized, participatory, predictive, and preventive models based on biomedical data also called the “omics data” and electronic health records from different authenticated sources. Our study indicates the use of various tools and techniques for data accumulation and management, thus providing an insight toward the revolution of the healthcare industry as well as the research community. Though the application of big data is still in the preliminary stage, growing Research and Development investment with its successful implementation will show enormous potential of growth in coming years. Considering all together, there is a need for the development of advanced technologies with inclusion of transparent ethical values to provide acceptance and with a moral foundation.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@incollection{SEBASTIANCOLEMAN20223,
title = {Chapter 1 - The Importance of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {3-30},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000018},
author = {Laura Sebastian-Coleman},
keywords = {Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization},
abstract = {This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.}
}
@article{KRISHNAN202293,
title = {A “FAIR” approach to open research},
journal = {Journal of the World Federation of Orthodontists},
volume = {11},
number = {4},
pages = {93-94},
year = {2022},
issn = {2212-4438},
doi = {https://doi.org/10.1016/j.ejwf.2022.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212443822000388},
author = {Vinod Krishnan}
}
@incollection{SIMONCELLI2022197,
title = {Chapter Four - A collaborative framework among data producers, managers, and users},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {197-280},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000013},
author = {S. Simoncelli and Giuseppe M.R. Manzella and A. Storto and A. Pisano and M. Lipizer and A. Barth and V. Myroshnychenko and T. Boyer and C. Troupin and C. Coatanoan and A. Pititto and R. Schlitzer and Dick M.A. Schaap and S. Diggs},
keywords = {Data integration, Data management, Data products, Ocean decade, Ocean services, Quality elements},
abstract = {The needs of society and the emerging blue economy require access and integration of data and information for the construction of dedicated products. A “transparent and accessible ocean” is one of the key objectives of the Ocean Decade 2021–30. In this context, marine infrastructures become significant components of a global knowledge environment, enabling environmental assessment and providing the necessary data for scientifically valid actions to protect and restore ocean health, to use marine resources in a sustainable way. The data is collected, analyzed, organized, and used by people and their good use/reuse can be obtained with social practices, technological and physical agreements aimed at facilitating collaborative knowledge, decision-making, inference. The vision is a digital ocean data ecosystem made up of multiple, interoperable, and scalable components. The huge amount of data and the resulting products can drive the development of new knowledge as well as new applications and services. Predictive capabilities that derive from the digital ecosystem enable the implementation of services for real-time decision-making, multihazard warning systems, and advance marine space planning. The chapter develops following the progressive complexity and information content of products deriving from oceanic data: data cycle and data collections, data products, oceanic reanalysis. The chapter discusses the new challenges of data products and the complexity of deriving them.}
}
@incollection{NEALJOSHUA2022391,
title = {Chapter 20 - The use of digital technologies in the response to SARS-2 CoV2-19 in the public health sector},
editor = {Patricia Ordóñez {de Pablos} and Kwok Tai Chui and Miltiadis D. Lytras},
booktitle = {Digital Innovation for Healthcare in COVID-19 Pandemic},
publisher = {Academic Press},
pages = {391-418},
year = {2022},
series = {Information Technologies in Healthcare Industry},
isbn = {978-0-12-821318-6},
doi = {https://doi.org/10.1016/B978-0-12-821318-6.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213186000037},
author = {Eali Stephen {Neal Joshua} and Debnath Bhattacharyya and N. {Thirupathi Rao}},
keywords = {COVID-19, Digital, Artificial intelligence, Machine learning, Public healthcare, Natural language processing, Pandemic},
abstract = {In order to promote public health response to COVID-19, digital technologies are being used around the world. These include population surveillance, case identification, contact tracing, and intervention assessment based on mobility data and public communication. These rapid responses are made possible by the millions of mobile phones in use, massive online data sets, connected devices, low-cost computer resources and machines, and advances in natural language processing. To achieve this goal, a comprehensive review of digital innovations for COVID-19 response to public health around the world is being conducted, including a look at their limitations and implementation obstacles such as legal or ethical issues, privacy concerns, and organizational and personnel issues. We investigate the need for international strategies to improve pandemic control and future preparedness for COVID-19 and other infectious diseases through the regulation, assessment, and use of digital technologies, as well as the need for international strategies to regulate, assess, and use digital technology in pandemic management.}
}
@incollection{DEPAMPHILIS2022123,
title = {Chapter 5 - Implementation: search through closing: phases 3 to 10 of the acquisition process},
editor = {Donald M. DePamphilis},
booktitle = {Mergers, Acquisitions, and Other Restructuring Activities (Eleventh Edition)},
publisher = {Academic Press},
edition = {Eleventh Edition},
pages = {123-152},
year = {2022},
isbn = {978-0-12-819782-0},
doi = {https://doi.org/10.1016/B978-0-12-819782-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197820000058},
author = {Donald M. DePamphilis},
keywords = {acquisition process, merger process, search process, search and screening process, deal negotiation, integration planning, M&A evaluation, implementing M&A integration, due diligence, due diligence questions, negotiation, deal structuring, purchase price, total consideration, closing conditions, deal covenants, mergers, acquisitions, mergers and acquisitions, contacting the target, closing, financing plan, employee retention, data analytics, analytics, data protection, data privacy, privacy, artificial intelligence, smart contracting, machine learning, block chain networks, digital tokens, cryptocurrency, AI, augmented artificial intelligence, augmented AI},
abstract = {Big data, data analytics, artificial intelligence, block chains, and smart contracting are tools that are increasingly applied to the mergers and acquisitions (M&A) process. To what extent do they offer promising solutions to challenging problems? And to what extent are they overhyped? These are among the questions addressed in this chapter, which presumes that a firm has developed a viable business plan requiring an acquisition to implement its business strategy. Whereas the preceding chapter addressed the creation of business and acquisition plans (phases 1 and 2), this chapter focuses on phases 3 to 10 of the M&A process, including search, screening, first contact, negotiation, integration planning, closing, integration implementation, and evaluation. Search and screening potential targets focus on developing appropriate selection criteria, while first contact details strategies for discussing price and developing preliminary legal documents. The negotiation phase involves refining valuation, deal structuring, conducting due diligence, and developing a financing plan. Integration planning addresses the challenges of postacquisition integration of the target. Closing is about the transfer of ownership, resolving transition issues, and completing the merger agreement. Postclosing integration deals with developing communication plans, employee retention, resolving cultural issues, satisfying immediate cash flow requirements, and using best practices. And postclosing evaluation encompasses learning from previous mistakes.}
}
@incollection{SEBASTIANCOLEMAN2022187,
title = {Chapter 9 - Core Data Quality Management Capabilities},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {187-228},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000092},
author = {Laura Sebastian-Coleman},
keywords = {Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology},
abstract = {This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.}
}
@article{ALESSY20221060,
title = {Population health data in KSA: Status, challenges, and opportunities},
journal = {Journal of Taibah University Medical Sciences},
volume = {17},
number = {6},
pages = {1060-1064},
year = {2022},
issn = {1658-3612},
doi = {https://doi.org/10.1016/j.jtumed.2022.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1658361222001202},
author = {Saleh A. Alessy and Maha Alattas and Mahmoud A. Mahmoud and Ali Alqarni and Suliman Alghnam},
keywords = {Data, Epidemiology, Health research, KSA, Population health},
abstract = {Population dynamics and health risk factors keep changing in the KSA, requiring continuous research and quality data. We aimed to review the current status of population health data, outline the available opportunities for data utilization, and provide recommendations for population data-related improvement initiatives. We provide practical solutions to support the collection, linkage, quality assurance, and governance of population health data.}
}
@incollection{MUHAMADIBRAHIM202233,
title = {Chapter 4 - Towards big data framework in government public open data (GPOD) for health},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {33-45},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00024-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000248},
author = {Najhan {Muhamad Ibrahim} and Nur Hidayah Ilham {Ahmad Azri} and Norbik Bashah Idris},
keywords = {Big data, Big data framework, Government public open data, Open data, Public data for health},
abstract = {Today, data is one of the most valuable assets on the planet. As a valuable resource, data may be used to develop a wide range of data applications, all of which are driven by creativity and innovation. In order to obtain information and provide services, data is also a critical component. In the recent years, big data has become a popular topic in global discussion. Big data is a new technology and knowledge generation phenomenon, that record, capture, and execute a significant amount of data for the usage in a variety of domains such as research, education, business, investing, health, and so on. The proliferation of data inspired by new methods of data gatherings such as via social media, wireless sensors, and data from government agencies which makes big data management an ultimate challenge. This study includes a thorough evaluation of existing theories and practical approach to address the public sector open data issues for determining the determinants of government public open data (GPOD) development of big data. To investigate the revolution of GPOD for health, the framework was dominantly used over architecture, infrastructures, followed by theoretical and conceptual framework, according to the review. This study revealed that most of the existing frameworks still lack consideration of the requirement for public open data in health. There is less number of existing research works that have sophisticated big data frameworks in GPOD for health. There also is still a lack of investment and adoption of big data in the public sector. The findings of this chapter will help academicians to empirically study the revealed requirement and provide decision-makers a better knowledge of how to leverage GPOD adoption in health by taking appropriate actions.}
}
@incollection{KOLTAY20221,
title = {Chapter 1 - Information, data, text, document},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {1-14},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000035},
author = {Tibor Koltay},
keywords = {Information, Data, Text, Document, Big data, Information and data ecosystem, Debates},
abstract = {In this chapter, the most important concepts needed to clarify the subject of this book, such as information, data, and documents, are identified and described. Some of the relationships between them are also described. Many of these concepts are interdisciplinary by their nature, and thus are studied by varied disciplines and tied to a number of professions. Nonetheless, this book, and within it this chapter, focuses mainly on the approaches of library and information science (LIS). The difficulties related to identifying information, data, text, and documents are outlined. The treatment of these issues is detailed, but we did not strive to be exhaustive. Neither did we aspire to find final and definitive answers to all the questions that unavoidably arise surrounding these concepts.}
}
@article{GARCIAVELLO20221137,
title = {Preparing for future challenges in risk assessment in the European Union},
journal = {Trends in Biotechnology},
volume = {40},
number = {10},
pages = {1137-1140},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922001731},
author = {Pilar Garcia-Vello and Kiara Aiello and Nicola M. Smith and Julia Fabrega and Konstantinos Paraskevopoulos and Marta Hugas and Claudia Heppner},
keywords = {food safety, science communication, regulatory science, preparedness, science policy},
abstract = {In light of the new EU policy targets (e.g., Farm to Fork strategy) and the revised legal framework (Transparency Regulation), the European Food Safety Authority (EFSA) needs to invest further in preparedness in regulatory and communication science for food safety. To achieve this, EFSA has established a process of advancing selected scientific themes to anticipate future challenges.}
}
@incollection{VAIDYA2022409,
title = {Chapter 24 - Exploring performance and predictive analytics of agriculture data},
editor = {Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani},
booktitle = {AI, Edge and IoT-based Smart Agriculture},
publisher = {Academic Press},
pages = {409-436},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823694-9},
doi = {https://doi.org/10.1016/B978-0-12-823694-9.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823694900030X},
author = {Madhavi Vaidya and Shweta Katkar},
keywords = {Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction},
abstract = {The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.}
}
@incollection{2022v,
title = {Contents},
editor = {Dev Bukhsh Singh and Rajesh Kumar Pathak},
booktitle = {Bioinformatics},
publisher = {Academic Press},
pages = {v-xv},
year = {2022},
isbn = {978-0-323-89775-4},
doi = {https://doi.org/10.1016/B978-0-323-89775-4.00030-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897754000304}
}
@incollection{TONG20221,
title = {Chapter One - Introduction of medical genomics and clinical informatics integration for p-Health care},
editor = {David B. Teplow},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {190},
number = {1},
pages = {1-37},
year = {2022},
booktitle = {Precision Medicine},
issn = {1877-1173},
doi = {https://doi.org/10.1016/bs.pmbts.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S187711732200062X},
author = {Li Tong and Hang Wu and May D. Wang and Geoffrey Wang},
keywords = {p-Health, Data integration, Clinical informatics, Genomics, Machine learning and artificial intelligence, Biomedical big data analytics},
abstract = {Achieving predictive, precise, participatory, preventive, and personalized health (abbreviated as p-Health) requires comprehensive evaluations of an individual's conditions captured by various measurement technologies. Since the 1950s, analysis of care providers' and physicians' notes and measurement data by computers to improve healthcare delivery has been termed clinical informatics. Since the 2010s, wide adoptions of Electronic Health Records (EHRs) have greatly improved clinical informatics development with fast growing pervasive wearable technologies that continuously capture the human physiological profile in-clinic (EHRs) and out-of-clinic (PHRs or Personal Health Records) to bolster mobile health (mHealth). In addition, after the Human Genome Project in the 1990s, medical genomics has emerged to capture the high-throughput molecular profile of a person. As a result, integrated data analytics is becoming one of the fast-growing areas under Biomedical Big Data to improve human healthcare outcomes. In this chapter, we first introduce the scope of data integration and review applications, data sources, and tools for clinical informatics and medical genomics. We then describe the data integration analytics at the raw data level, feature level, and decision level with case studies, and the opportunity for research and translation using advanced artificial intelligence (AI), such as deep learning. Lastly, we summarize the opportunities in biomedical big data integration that can reshape healthcare toward p-health.}
}
@incollection{ZOHURI2022121,
title = {Chapter 5 - Mathematical modeling driven predication},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {121-163},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000052},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Data mining and data analytics, Forecasting and prediction, Modeling and mathematics},
abstract = {During the past decade, there has been a tremendous blast and progress in computation technology, and with it comes vast amounts of data in a variety of fields such as the economy, medicine, biology, banking services such as customer relation management and credit card fraud, finance, demographic population growth from a demographical point of view nationwide and worldwide, and the need for new lifestyles and growth in term of continuous renewable sources of energy and its production, as well as marketing are among the fields that can be mentioned. The challenge of understanding these data has led to the development of new tools such as predictive analytics in the field of statistics and spawned new areas such as data mining, machine learning, and bioinformatics to process these data and determine the integrity of their information for prediction analysis. Many of these tools have common underpinnings but are often expressed with different terminology. This chapter will summarize the important ideas in these areas in a common conceptual framework.}
}
@article{XU2022160,
title = {Editorial commentary: Artificial intelligence in structural heart disease interventions – The future is near},
journal = {Trends in Cardiovascular Medicine},
volume = {32},
number = {3},
pages = {160-162},
year = {2022},
issn = {1050-1738},
doi = {https://doi.org/10.1016/j.tcm.2021.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1050173821000281},
author = {Bo Xu and Saberio {Lo Presti Vega} and Reza Reyaldeen}
}
@incollection{THOMASIAN2022623,
title = {Chapter 12 - Conclusions},
editor = {Alexander Thomasian},
booktitle = {Storage Systems},
publisher = {Morgan Kaufmann},
pages = {623},
year = {2022},
isbn = {978-0-323-90796-5},
doi = {https://doi.org/10.1016/B978-0-32-390796-5.00021-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907965000218},
author = {Alexander Thomasian},
keywords = {Data preservation, provenance data cleansing},
abstract = {We discuss few topics not covered in the book to encourage readers to pursue them on their own.}
}
@incollection{2022311,
title = {Index},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {311-315},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00026-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000262}
}
@incollection{2022329,
title = {Index},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {329-334},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.20001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074200010}
}
@article{BASIRI2022100587,
title = {Missing data as data},
journal = {Patterns},
volume = {3},
number = {9},
pages = {100587},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100587},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002057},
author = {Anahid Basiri and Chris Brunsdon},
keywords = {missing data, big data paradox, under-representation, bias, crowdsourced data},
abstract = {Our “digified” lives have provided researchers with an unprecedented opportunity to study society at a much higher frequency and granularity. Such data can have a large sample size but can be sparse, biased, and exclusively contributed by the users of the technologies. We look at the increasing importance of missing data and under-representation and propose a new perspective that considers missing data as useful data to understand the underlying reasons for missingness and that provides a realistic view of the sample size of large but under-represented data.}
}
@incollection{2022281,
title = {Index},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {281-288},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.09999-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697099991}
}
@incollection{GRIMALDI202247,
title = {Chapter 2 - Governance, decision-making, and strategy for urban development},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {47-87},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000014},
author = {Didier Grimaldi and Eula Bianca Villar and Laurent Dupont and Jose M. Sallan and Carlos Carrasco-Farré},
keywords = {Bibliometric methodology, VOSviewer software, Sustainability, PDRF, Stakeholders, Multiple sectors, Urban development},
abstract = {This chapter starts with a systematic review of the academic state of art about the trending topics that deal with data-driven approaches in an urban environment. Then, we select a specific example and analyze recent progress in the use of data to improve the urban process, which is as strategic and a priority as the resilience process. In this vein, we then use the case of the Philippines and show their data-driven policy to cope with disasters that drastically affect their urban areas.}
}
@incollection{CHANG202221,
title = {Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {21-66},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000076},
author = {Victor Chang and Mohamed Aleem Ali and Alamgir Hossain},
keywords = {COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19},
abstract = {This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.}
}
@incollection{2022227,
title = {Index},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {227-232},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.09986-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229099863}
}
@article{ORR2022109735,
title = {Biodiversity data synthesis is critical for realizing a functional post-2020 framework},
journal = {Biological Conservation},
volume = {274},
pages = {109735},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109735},
url = {https://www.sciencedirect.com/science/article/pii/S0006320722002889},
author = {Michael C. Orr and Alice C. Hughes and Mark J. Costello and Huijie Qiao},
keywords = {Convention on biological diversity, Global biodiversity framework, Biodiversity mapping, Spatial ecology, IUCN, Data biases},
abstract = {The Post-2020 Global Biodiversity Framework currently is under development as part of the Convention of Biodiversity's aim to prevent global biodiversity losses by 2050, but targets can only be effectively developed and assessed if the data used for them are fit for purpose. The monitoring framework has been discussed at length and ensuring appropriate data use is critical to target effectiveness, enabling the monitoring of global biodiversity trends and assessment of target success. We outline a vision for how conservation data resources can be improved via automation and other routes to greatly enhance both ease of use and effectiveness for conservation. Synthesis across different types of data is urgently needed and could be enabled by a unified data system and automated workflows for cross-validation between data types, with downstream products such as grades for expert range maps that reflect their underlying bases and data quality and reliability to determine their fit for analysis, as well as automated preliminary IUCN assessments to expedite conservation. Capacity building and collaboration rooted in international agreements will be necessary for these initiatives to effectively function globally to enable new global targets to be achieved for effective conservation and targeted resource mobilization at all scales.}
}
@incollection{2022xi,
title = {Contents},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {xi-xvii},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000158}
}
@incollection{SEBASTIANCOLEMAN202231,
title = {Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {31-45},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500002X},
author = {Laura Sebastian-Coleman},
keywords = {Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy},
abstract = {This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.}
}
@incollection{ZOHURI202287,
title = {Chapter 4 - Structured and unstructured data processing},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {87-119},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000040},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Big data, Data analytics, Data productive, Structured data, Unstructured data},
abstract = {The amount of data that are being created and stored on a global level is almost inconceivable, and it just keeps growing. That means there is even more potential to glean key insights from business information—yet only a small percentage of data is actually analyzed. What does that mean for businesses? How can they make better use of the raw information that flows into their organizations every day? With respect to the mass categorization that is central to most computer operations, there are two types of relevant data, which affect the speed of assimilation as well as information recall: structured data and unstructured data. Smart robots need both types of data in order to sort and process these data as fast they receive them to the level of trusted degree for their processing procedure and set assignment, known as service-level agreement. Please note that with minor editing and manipulation, the materials presented in this chapter have been borrowed from the book published from Zohuri and Moghaddam2 with permission from both authors and publisher as well.}
}