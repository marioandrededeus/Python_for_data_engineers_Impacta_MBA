abstract;author;categories;citable_docs._(3years);cites_/_doc._(2years);country;coverage;doi;eigenfactor_score;h_index;isbn;issn;issn1;issn2;issn3;jcr_value;keywords;publisher_x;publisher_y;ref._/_doc.;region;scimago_value;sjr_best_quartile;sourceid;title_csv;title_bib;total_cites;total_cites_(3years);total_docs._(2020);total_docs._(3years);total_refs.;type;type_publication;year
A view from computational journalism.;Diakopoulos, Nicholas;Computer Science (miscellaneous) (Q1);475.0;351.0;United States;1958-2020;10.1145/2844110;0.0178;214.0;;00010782;15577317;00010782;15577317;4.654;;Association for Computing Machinery (ACM);Association for Computing Machinery;1053.0;Northern America;967.0;Q1;13675.0;Communications of the acm;Accountability in algorithmic decision making;20742.0;4350.0;266.0;849.0;2801.0;journal;article;2016
Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.;Bonnie L. Westra and Jessica J. Peterson;Medical and Surgical Nursing (Q2);475.0;25.0;United States;1963-2020;10.1016/j.aorn.2016.07.009;0.0013;43.0;;00012092;18780369;00012092;18780369;0.676;big data, perioperative nursing, quality care, nursing knowledge, nursing informatics;"John Wiley &amp; Sons Inc.";;855.0;Northern America;222.0;Q2;27052.0;Aorn journal;Big data and perioperative nursing;1553.0;276.0;288.0;682.0;2461.0;journal;article;2016
Artificial intelligence (AI) is an overarching term for a multitude of technologies which are currently being discussed and introduced in several areas of medicine and in medical imaging specifically. There is, however, limited literature and information about how AI techniques can be integrated into the design of clinical imaging trials. This article will present several aspects of AI being used in trials today and how imaging departments and especially nuclear medicine departments can prepare themselves to be at the forefront of AI-driven clinical trials. Beginning with some basic explanation on AI techniques currently being used and existing challenges of its implementation, it will also cover the logistical prerequisites which have to be in place in nuclear medicine departments to participate successfully in AI-driven clinical trials.;Gaspar Delso and Davide Cirillo and Joshua D Kaggie and Alfonso Valencia and Ur Metser and Patrick Veit-Haibach;Radiology, Nuclear Medicine and Imaging (Q1);142.0;339.0;United Kingdom;1971-2020;10.1053/j.semnuclmed.2020.09.003;0.00264;88.0;;00012998;15584623;00012998;15584623;4.446;;W.B. Saunders Ltd;;7532.0;Western Europe;984.0;Q1;17894.0;Seminars in nuclear medicine;How to design ai-driven clinical trials in nuclear medicine;2729.0;562.0;56.0;174.0;4218.0;journal;article;2021
The railroad industry plays a principal role in the transportation infrastructure and economic prosperity of the United States, and safety is of the utmost importance. Trespassing is the leading cause of rail-related fatalities and there has been little progress in reducing the trespassing frequency and deaths for the past ten years in the United States. Although the widespread deployment of surveillance cameras and vast amounts of video data in the railroad industry make witnessing these events achievable, it requires enormous labor-hours to monitor real-time videos or archival video data. To address this challenge and leverage this big data, this study develops a robust Artificial Intelligence (AI)-aided framework for the automatic detection of trespassing events. This deep learning-based tool automatically detects trespassing events, differentiates types of violators, generates video clips, and documents basic information of the trespassing events into one dataset. This study aims to provide the railroad industry with state-of-the-art AI tools to harness the untapped potential of video surveillance infrastructure through the risk analysis of their data feeds in specific locations. In the case study, the AI has analyzed over 1,600 h of archival video footage and detected around 3,000 trespassing events from one grade crossing in New Jersey. The data generated from these big video data will potentially help understand human factors in railroad safety research and contribute to specific trespassing proactive safety risk management initiatives and improve the safety of the train crew, rail passengers, and road users through engineering, education, and enforcement solutions to trespassing.;Zhipeng Zhang and Asim Zaman and Jinxuan Xu and Xiang Liu;"Human Factors and Ergonomics (Q1); Law (Q1); Public Health, Environmental and Occupational Health (Q1); Safety, Risk, Reliability and Quality (Q1)";1034.0;555.0;United Kingdom;1969-2020;10.1016/j.aap.2022.106594;0.0204;152.0;;00014575;18792057;00014575;18792057;4.993;Trespassing, Railroad safety, Artificial Intelligence, Computer vision, Risk management;Elsevier Ltd.;;5321.0;Western Europe;1816.0;Q1;19532.0;Accident analysis and prevention;Artificial intelligence-aided railroad trespassing detection and data analytics: methodology and a case study;25323.0;5984.0;385.0;1041.0;20484.0;journal;article;2022
"Job search is a demanding and often demotivating process, challenging job-seekers' self-regulation. Particularly, mature-aged job seekers face lower reemployment chances – and may benefit from strategies known from the lifespan literature. The current study examined whether and when the use of aging strategies (elective selection, loss-based selection, optimization, and compensation; SOC strategies) can support mature-aged job seekers in their self-regulated job search process (goal establishment and goal pursuit). We collected data from 659 mature-aged job seekers in three countries (Germany, United Kingdom, and United States) at four different times over two months. Results of multi-level modeling showed no support for gain-oriented strategies, namely elective selection (prioritizing one instead of multiple goals) and optimization (investing every effort to reach one's goal). In contrast, loss-oriented strategies, namely loss-based selection (prioritizing or selecting a new goal after a setback) and compensation (using new or previously unused means in the face of obstacles), supported mature-aged job seekers' goal establishment and goal pursuit. Moreover, with increasing age, mature-aged job seekers reported lower reemployment efficacy (the confidence to find a new job), which moderated the relation between compensation with goal pursuit. Compensation was particularly helpful for mature-aged job seekers' goal pursuit in weeks in which they reported lower (vs. higher) reemployment efficacy. These findings highlight the importance of loss-oriented aging strategies as beneficial coping strategies. With regard to practice, the present study speaks to the benefits of SOC strategies and points to the development of interventions targeted toward mature-aged job seekers.";Henriette Watermann and Ulrike Fasbender and Ute-Christine Klehe;"Applied Psychology (Q1); Education (Q1); Life-span and Life-course Studies (Q1); Organizational Behavior and Human Resource Management (Q1)";304.0;558.0;United States;1971-2020;10.1016/j.jvb.2021.103591;0.00945;151.0;;00018791;10959084;00018791;10959084;6.065;Aging, Job search, Mature-aged job seekers, Reemployment efficacy, Self-regulation, SOC strategies;Academic Press Inc.;;7958.0;Northern America;2607.0;Q1;12006.0;Journal of vocational behavior;Predicting the self-regulated job search of mature-aged job seekers: the use of elective selection, loss-based selection, optimization, and compensation strategies;17077.0;2051.0;96.0;310.0;7640.0;journal;article;2021
This white paper provides a summary of presentations, discussions and conclusions of a Thinktank entitled “The Role of Endpoint Adjudication in Medical Device Clinical Trials”. The think tank was cosponsored by the Cardiac Safety Research Committee, MDEpiNet and the US Food and Drug Administration (FDA) and was convened at the FDA's White Oak headquarters on March 11, 2016. Attention was focused on tailoring best practices for evaluation of endpoints in medical device clinical trials, practical issues in endpoint adjudication of therapeutic, diagnostic, biomarker and drug-device combinations, and the role of adjudication in regulatory and reimbursement issues throughout the device lifecycle. Attendees included representatives from medical device companies, the FDA, Centers for Medicare and Medicaid Services (CMS), end point adjudication specialist groups, clinical research organizations, and active, academically based adjudicators. The manuscript presents recommendations from the think tank regarding (1) rationale for when adjudication is appropriate, (2) best practices establishment and operation of a medical device adjudication committee and (3) the role of endpoint adjudication for post market evaluation in the emerging era of real world evidence.;Jonathan H. Seltzer and Ted Heise and Peter Carson and Daniel Canos and Jo Carol Hiatt and Pascal Vranckx and Thomas Christen and Donald E. Cutlip;Cardiology and Cardiovascular Medicine (Q1);592.0;377.0;United States;1925-2020;10.1016/j.ahj.2017.05.009;0.0235;187.0;;00028703;10976744;00028703;10976744;4.749;;Mosby Inc.;;3000.0;Northern America;2925.0;Q1;22414.0;American heart journal;Use of endpoint adjudication to improve the quality and validity of endpoint assessment for medical device development and post marketing evaluation: rationale and best practices. a report from the cardiac safety research consortium;22202.0;2605.0;252.0;685.0;7559.0;journal;article;2017
C-reactive protein (CRP) is a sensitive biomarker of chronic low-grade inflammation and is associated with multiple complex diseases. The genetic determinants of chronic inflammation remain largely unknown, and the causal role of CRP in several clinical outcomes is debated. We performed two genome-wide association studies (GWASs), on HapMap and 1000 Genomes imputed data, of circulating amounts of CRP by using data from 88 studies comprising 204,402 European individuals. Additionally, we performed in silico functional analyses and Mendelian randomization analyses with several clinical outcomes. The GWAS meta-analyses of CRP revealed 58 distinct genetic loci (p < 5 × 10−8). After adjustment for body mass index in the regression analysis, the associations at all except three loci remained. The lead variants at the distinct loci explained up to 7.0% of the variance in circulating amounts of CRP. We identified 66 gene sets that were organized in two substantially correlated clusters, one mainly composed of immune pathways and the other characterized by metabolic pathways in the liver. Mendelian randomization analyses revealed a causal protective effect of CRP on schizophrenia and a risk-increasing effect on bipolar disorder. Our findings provide further insights into the biology of inflammation and could lead to interventions for treating inflammation and its clinical consequences.;Symen Ligthart and Ahmad Vaez and Urmo Võsa and Maria G. Stathopoulou and Paul S. {de Vries} and Bram P. Prins and Peter J. {Van der Most} and Toshiko Tanaka and Elnaz Naderi and Lynda M. Rose and Ying Wu and Robert Karlsson and Maja Barbalic and Honghuang Lin and René Pool and Gu Zhu and Aurélien Macé and Carlo Sidore and Stella Trompet and Massimo Mangino and Maria Sabater-Lleal and John P. Kemp and Ali Abbasi and Tim Kacprowski and Niek Verweij and Albert V. Smith and Tao Huang and Carola Marzi and Mary F. Feitosa and Kurt K. Lohman and Marcus E. Kleber and Yuri Milaneschi and Christian Mueller and Mahmudul Huq and Efthymia Vlachopoulou and Leo-Pekka Lyytikäinen and Christopher Oldmeadow and Joris Deelen and Markus Perola and Jing Hua Zhao and Bjarke Feenstra and Behrooz Z. Alizadeh and H. Marike Boezen and Lude Franke and Pim {van der Harst} and Gerjan Navis and Marianne Rots and Harold Snieder and Morris Swertz and Bruce H.R. Wolffenbuttel and Cisca Wijmenga and Marzyeh Amini and Emelia Benjamin and Daniel I. Chasman and Abbas Dehghan and Tarunveer Singh Ahluwalia and James Meigs and Russell Tracy and Behrooz Z. Alizadeh and Symen Ligthart and Josh Bis and Gudny Eiriksdottir and Nathan Pankratz and Myron Gross and Alex Rainer and Harold Snieder and James G. Wilson and Bruce M. Psaty and Josee Dupuis and Bram Prins and Urmo Vaso and Maria Stathopoulou and Lude Franke and Terho Lehtimaki and Wolfgang Koenig and Yalda Jamshidi and Sophie Siest and Ali Abbasi and Andre G. Uitterlinden and Mohammadreza Abdollahi and Renate Schnabel and Ursula M. Schick and Ilja M. Nolte and Aldi Kraja and Yi-Hsiang Hsu and Daniel S. Tylee and Alyson Zwicker and Rudolf Uher and George Davey-Smith and Alanna C. Morrison and Andrew Hicks and Cornelia M. {van Duijn} and Cavin Ward-Caviness and Eric Boerwinkle and J. Rotter and Ken Rice and Leslie Lange and Markus Perola and Eco {de Geus} and Andrew P. Morris and Kari Matti Makela and David Stacey and Johan Eriksson and Tim M. Frayling and Eline P. Slagboom and Jari Lahti and Katharina E. Schraut and Myriam Fornage and Bhoom Suktitipat and Wei-Min Chen and Xiaohui Li and Teresa Nutile and Giovanni Malerba and Jian’an Luan and Tom Bak and Nicholas Schork and Fabiola {Del Greco M.} and Elisabeth Thiering and Anubha Mahajan and Riccardo E. Marioni and Evelin Mihailov and Joel Eriksson and Ayse Bilge Ozel and Weihua Zhang and Maria Nethander and Yu-Ching Cheng and Stella Aslibekyan and Wei Ang and Ilaria Gandin and Loïc Yengo and Laura Portas and Charles Kooperberg and Edith Hofer and Kumar B. Rajan and Claudia Schurmann and Wouter {den Hollander} and Tarunveer S. Ahluwalia and Jing Zhao and Harmen H.M. Draisma and Ian Ford and Nicholas Timpson and Alexander Teumer and Hongyan Huang and Simone Wahl and YongMei Liu and Jie Huang and Hae-Won Uh and Frank Geller and Peter K. Joshi and Lisa R. Yanek and Elisabetta Trabetti and Benjamin Lehne and Diego Vozzi and Marie Verbanck and Ginevra Biino and Yasaman Saba and Ingrid Meulenbelt and Jeff R. O’Connell and Markku Laakso and Franco Giulianini and Patrik K.E. Magnusson and Christie M. Ballantyne and Jouke Jan Hottenga and Grant W. Montgomery and Fernando Rivadineira and Rico Rueedi and Maristella Steri and Karl-Heinz Herzig and David J. Stott and Cristina Menni and Mattias Frånberg and Beate {St. Pourcain} and Stephan B. Felix and Tune H. Pers and Stephan J.L. Bakker and Peter Kraft and Annette Peters and Dhananjay Vaidya and Graciela Delgado and Johannes H. Smit and Vera Großmann and Juha Sinisalo and Ilkka Seppälä and Stephen R. Williams and Elizabeth G. Holliday and Matthijs Moed and Claudia Langenberg and Katri Räikkönen and Jingzhong Ding and Harry Campbell and Michele M. Sale and Yii-Der I. Chen and Alan L. James and Daniela Ruggiero and Nicole Soranzo and Catharina A. Hartman and Erin N. Smith and Gerald S. Berenson and Christian Fuchsberger and Dena Hernandez and Carla M.T. Tiesler and Vilmantas Giedraitis and David Liewald and Krista Fischer and Dan Mellström and Anders Larsson and Yunmei Wang and William R. Scott and Matthias Lorentzon and John Beilby and Kathleen A. Ryan and Craig E. Pennell and Dragana Vuckovic and Beverly Balkau and Maria Pina Concas and Reinhold Schmidt and Carlos F. {Mendes de Leon} and Erwin P. Bottinger and Margreet Kloppenburg and Lavinia Paternoster and Michael Boehnke and A.W. Musk and Gonneke Willemsen and David M. Evans and Pamela A.F. Madden and Mika Kähönen and Zoltán Kutalik and Magdalena Zoledziewska and Ville Karhunen and Stephen B. Kritchevsky and Naveed Sattar and Genevieve Lachance and Robert Clarke and Tamara B. Harris and Olli T. Raitakari and John R. Attia and Diana {van Heemst} and Eero Kajantie and Rossella Sorice and Giovanni Gambaro and Robert A. Scott and Andrew A. Hicks and Luigi Ferrucci and Marie Standl and Cecilia M. Lindgren and John M. Starr and Magnus Karlsson and Lars Lind and Jun Z. Li and John C. Chambers and Trevor A. Mori and Eco J.C.N. {de Geus} and Andrew C. Heath and Nicholas G. Martin and Juha Auvinen and Brendan M. Buckley and Anton J.M. {de Craen} and Melanie Waldenberger and Konstantin Strauch and Thomas Meitinger and Rodney J. Scott and Mark McEvoy and Marian Beekman and Cristina Bombieri and Paul M. Ridker and Karen L. Mohlke and Nancy L. Pedersen and Alanna C. Morrison and Dorret I. Boomsma and John B. Whitfield and David P. Strachan and Albert Hofman and Peter Vollenweider and Francesco Cucca and Marjo-Riitta Jarvelin and J. Wouter Jukema and Tim D. Spector and Anders Hamsten and Tanja Zeller and André G. Uitterlinden and Matthias Nauck and Vilmundur Gudnason and Lu Qi and Harald Grallert and Ingrid B. Borecki and Jerome I. Rotter and Winfried März and Philipp S. Wild and Marja-Liisa Lokki and Michael Boyle and Veikko Salomaa and Mads Melbye and Johan G. Eriksson and James F. Wilson and Brenda W.J.H. Penninx and Diane M. Becker and Bradford B. Worrall and Greg Gibson and Ronald M. Krauss and Marina Ciullo and Gianluigi Zaza and Nicholas J. Wareham and Albertine J. Oldehinkel and Lyle J. Palmer and Sarah S. Murray and Peter P. Pramstaller and Stefania Bandinelli and Joachim Heinrich and Erik Ingelsson and Ian J. Deary and Reedik Mägi and Liesbeth Vandenput and Pim {van der Harst} and Karl C. Desch and Jaspal S. Kooner and Claes Ohlsson and Caroline Hayward and Terho Lehtimäki and Alan R. Shuldiner and Donna K. Arnett and Lawrence J. Beilin and Antonietta Robino and Philippe Froguel and Mario Pirastu and Tine Jess and Wolfgang Koenig and Ruth J.F. Loos and Denis A. Evans and Helena Schmidt and George Davey Smith and P. Eline Slagboom and Gudny Eiriksdottir and Andrew P. Morris and Bruce M. Psaty and Russell P. Tracy and Ilja M. Nolte and Eric Boerwinkle and Sophie Visvikis-Siest and Alex P. Reiner and Myron Gross and Joshua C. Bis and Lude Franke and Oscar H. Franco and Emelia J. Benjamin and Daniel I. Chasman and Josée Dupuis and Harold Snieder and Abbas Dehghan and Behrooz Z. Alizadeh;"Genetics (Q1); Genetics (clinical) (Q1)";523.0;900.0;United States;1950-2020;10.1016/j.ajhg.2018.09.009;0.0542;302.0;;00029297;15376605;00029297;15376605;11.025;C-reactive protein, genome-wide association study, inflammation, Mendelian randomization, inflammatory disorders, DEPICT, coronary artery disease, schizophrenia, system biology;Cell Press;;4750.0;Northern America;6661.0;Q1;21677.0;American journal of human genetics;Genome analyses of >200,000 individuals identify 58 loci for chronic inflammation and highlight pathways that link inflammation and complex disorders;43477.0;5815.0;168.0;571.0;7980.0;journal;article;2018
"Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.";Anne Louise Coleman;Ophthalmology (Q1);848.0;373.0;United States;1918-2020;10.1016/j.ajo.2015.09.028;0.0263;186.0;;00029394;00029394;18791891;00029394;5.258;;Elsevier USA;;2799.0;Northern America;2704.0;Q1;13266.0;American journal of ophthalmology;How big data informs us about cataract surgery: the lxxii edward jackson memorial lecture;32566.0;4453.0;428.0;1146.0;11979.0;journal;article;2015
Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.;Ewa Szymańska;"Analytical Chemistry (Q1); Biochemistry (Q1); Environmental Chemistry (Q1); Spectroscopy (Q1)";2228.0;627.0;Netherlands;1947-2020;10.1016/j.aca.2018.05.038;0.03972;203.0;;00032670;00032670;18734324;00032670;6.558;Chemometrics, Data science, Big data, Chemical analytical data, Methodology;Elsevier;;5021.0;Western Europe;1403.0;Q1;23911.0;Analytica chimica acta;Modern data science for analytical chemical data – a comprehensive review;58170.0;14014.0;897.0;2235.0;45034.0;journal;article;2018
Despite the development of several tools for the analysis of the transcriptome data, non-availability of a standard pipeline for analyzing the low quality and fragmented mRNA samples pose a major challenge to the computational molecular biologist for effective interpretation of the data. Hence the present study aimed to establish a bioinformatics pipeline for analyzing the biologically fragmented sperm RNA. Sperm transcriptome data (2 x 75 PE sequencing) generated from bulls (n = 8) of high-fertile (n = 4) and low-fertile (n = 4) classified based on the fertility rate (41.52 ± 1.07 vs 36.04 ± 1.04%) were analyzed with different bioinformatics tools for alignment, quantitation, and differential gene expression studies. TopHat2 was effectual compared to HISAT2 and STAR for sperm mRNA due to the higher exonic (6% vs 2%) mapping percentage and quantitating the low expressed genes. TopHat2 also had significantly strong correlation with STAR (0.871, p = 0.05) and HISAT2 (0.933, p = 0.01). TopHat2 and Cufflinks combo quantitated the number of genes higher than the other combinations. Among the tools (Cuffdiff, DESeq, DESeq2, edgeR, and limma) used for the differential gene expression analysis, edgeR and limma identified the largest number of significantly differentially expressed genes (p < 0.05) with biological relevance. The concordance analysis concurred that edgeR had an edge over the other tools. It also identified a higher number (9.5%) of fertility-related genes to be differentially expressed between the two groups. The present study established that TopHat2, Cufflinks, and edgeR as a suitable pipeline for the analysis of fragmented mRNA from bovine spermatozoa.;Laxman Ramya and Divakar Swathi and Santhanahalli Siddalingappa Archana and Maharajan Lavanya and Sivashanmugam Parthipan and Sellappan Selvaraju;"Biophysics (Q2); Biochemistry (Q3); Cell Biology (Q3); Molecular Biology (Q3)";901.0;319.0;United States;1960-2020;10.1016/j.ab.2021.114141;0.010029999999999999;190.0;;00032697;10960309;00032697;10960309;3.365;Bioinformatics pipeline, Fragmented transcripts, Transcriptomics, Differential gene expression, Bovine spermatozoa;Academic Press Inc.;;4080.0;Northern America;633.0;Q2;16789.0;Analytical biochemistry;Establishment of bioinformatics pipeline for deciphering the biological complexities of fragmented sperm transcriptome;42950.0;2942.0;334.0;908.0;13628.0;journal;article;2021
;Felix G. Fernandez;"Cardiology and Cardiovascular Medicine (Q1); Pulmonary and Respiratory Medicine (Q1); Surgery (Q1)";2269.0;213.0;United States;1965-2020;10.1016/j.athoracsur.2019.11.003;0.0351;197.0;;00034975;15526259;00034975;15526259;4.330;;Elsevier USA;;1370.0;Northern America;1130.0;Q1;20489.0;Annals of thoracic surgery;The future is now: the 2020 evolution of the society of thoracic surgeons national database;41620.0;6373.0;1111.0;2981.0;15218.0;journal;article;2020
We describe an arrangement for simultaneous recording of speech and vocal tract geometry in patients undergoing surgery involving this area. Experimental design is considered from an articulatory phonetic point of view. The speech signals are recorded with an acoustic-electrical arrangement. The vocal tract is simultaneously imaged with MRI. A MATLAB-based system controls the timing of speech recording and MR image acquisition. The speech signals are cleaned from acoustic MRI noise by an adaptive signal processing algorithm. Finally, a vowel data set from pilot experiments is qualitatively compared both with validation data from the anechoic chamber and with Helmholtz resonances of the vocal tract volume, obtained using FEM.;Daniel Aalto and Olli Aaltonen and Risto-Pekka Happonen and Päivi Jääsaari and Atle Kivelä and Juha Kuortti and Jean-Marc Luukinen and Jarmo Malinen and Tiina Murtola and Riitta Parkkola and Jani Saunavaara and Tero Soukka and Martti Vainio;Acoustics and Ultrasonics (Q1);1062.0;316.0;United Kingdom;1968-2021;10.1016/j.apacoust.2014.03.003;0.00853;82.0;;0003682X;0003682X;1872910X;0003682X;2.639;Speech production, Speech recording, MRI, Noise reduction, Formant analysis, Vocal tract resonance;Elsevier Ltd.;;3452.0;Western Europe;767.0;Q1;12946.0;Applied acoustics;Large scale data acquisition of simultaneous mri and speech;9042.0;3608.0;456.0;1068.0;15741.0;journal;article;2014
Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.;Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk;"Engineering (miscellaneous) (Q1); Human Factors and Ergonomics (Q1); Physical Therapy, Sports Therapy and Rehabilitation (Q1); Safety, Risk, Reliability and Quality (Q1)";675.0;417.0;United Kingdom;1969-2021;10.1016/j.apergo.2022.103795;0.00851;98.0;;00036870;18729126;00036870;18729126;3.661;;Elsevier Ltd.;;4877.0;Western Europe;1093.0;Q1;13929.0;Applied ergonomics;At the limit? using operational data to estimate train driver human reliability;9523.0;2995.0;232.0;682.0;11314.0;journal;article;2022
"Objectives
To understand how health status preceding traumatic brain injury (TBI) affects relative functional gain after inpatient rehabilitation using a data mining approach.
Design
Population-based, sex-stratified, retrospective cohort study using health administrative data from Ontario, Canada (39% of the Canadian population).
Setting
Inpatient rehabilitation.
Participants
Patients 14 years or older (N=5802; 63.4% male) admitted to inpatient rehabilitation within 1 year of a TBI-related acute care discharge between April 1, 2008, and March 31, 2015.
Interventions
Not applicable.
Main Outcome Measures
Relative functional gain (RFG) in percentage, calculated as ([discharge FIM−admission FIM]/[126−admission FIM]×100). Health status prior to TBI was identified and internally validated using a data mining approach that categorized all International Classification of Diseases, 10th revision, codes for each patient.
Results
The average RFG was 52.8%±27.6% among male patients and 51.6%±27.1% among female patients. Sex-specific Bonferroni adjusted multivariable linear regressions identified 10 factors of preinjury health status related to neurology, emergency medicine, cardiology, psychiatry, geriatrics, and gastroenterology that were significantly associated with reduced RFG in FIM for male patients. Only 1 preinjury health status category, geriatrics, was significantly associated with RFG in female patients.
Conclusions
Comorbid health conditions present up to 5 years preceding the TBI event were significantly associated with RFG. These findings should be considered when planning and executing interventions to maximize functional gain and to support an interdisciplinary approach. Best practices guidelines and clinical interventions for older male and female patients with TBI should be developed given the increasingly aging population with TBI.";Vincy Chan and Mitchell Sutton and Tatyana Mollayeva and Michael D. Escobar and Mackenzie Hurst and Angela Colantonio;"Physical Therapy, Sports Therapy and Rehabilitation (Q1); Rehabilitation (Q1); Sports Science (Q1)";829.0;329.0;United Kingdom;1945-2020;10.1016/j.apmr.2020.05.017;0.01786;188.0;;00039993;1532821X;00039993;1532821X;3.966;Brain injuries, Comorbidity, Data mining, International Classification of Diseases, Rehabilitation;W.B. Saunders Ltd;;3866.0;Western Europe;1305.0;Q1;16270.0;Archives of physical medicine and rehabilitation;Data mining to understand how health status preceding traumatic brain injury affects functional outcome: a population-based sex-stratified study;30690.0;3351.0;289.0;917.0;11172.0;journal;article;2020
Self-regulation is a broad construct representing the general ability to recruit cognitive, motivational and emotional resources to achieve long-term goals. This construct has been implicated in a host of health-risk behaviors, and is a promising target for fostering beneficial behavior change. Despite its clear importance, the behavioral, psychological and neural components of self-regulation remain poorly understood, which contributes to theoretical inconsistencies and hinders maximally effective intervention development. We outline a research program that seeks to define a neuropsychological ontology of self-regulation, articulating the cognitive components that compose self-regulation, their relationships, and their associated measurements. The ontology will be informed by two large-scale approaches to assessing individual differences: first purely behaviorally using data collected via Amazon's Mechanical Turk, then coupled with neuroimaging data collected from a separate population. To validate the ontology and demonstrate its utility, we will then use it to contextualize health risk behaviors in two exemplar behavioral groups: overweight/obese adults who binge eat and smokers. After identifying ontological targets that precipitate maladaptive behavior, we will craft interventions that engage these targets. If successful, this work will provide a structured, holistic account of self-regulation in the form of an explicit ontology, which will better clarify the pattern of deficits related to maladaptive health behavior, and provide direction for more effective behavior change interventions.;Ian W. Eisenberg and Patrick G. Bissett and Jessica R. Canning and Jesse Dallery and A. Zeynep Enkavi and Susan Whitfield-Gabrieli and Oscar Gonzalez and Alan I. Green and Mary Ann Greene and Michaela Kiernan and Sunny Jung Kim and Jamie Li and Michael R. Lowe and Gina L. Mazza and Stephen A. Metcalf and Lisa Onken and Sadev S. Parikh and Ellen Peters and Judith J. Prochaska and Emily A. Scherer and Luke E. Stoeckel and Matthew J. Valente and Jialing Wu and Haiyi Xie and David P. MacKinnon and Lisa A. Marsch and Russell A. Poldrack;"Clinical Psychology (Q1); Developmental and Educational Psychology (Q1); Experimental and Cognitive Psychology (Q1); Psychiatry and Mental Health (Q1)";413.0;413.0;United Kingdom;1963-2020;10.1016/j.brat.2017.09.014;0.014169999999999999;185.0;;00057967;00057967;1873622X;00057967;4.473;Self-regulation, Ontology, Neuroimaging, Intervention, Obesity, Smoking;Elsevier Ltd.;;6426.0;Western Europe;2506.0;Q1;12127.0;Behaviour research and therapy;Applying novel technologies and methods to inform the ontology of self-regulation;22841.0;2304.0;143.0;423.0;9189.0;journal;article;2018
Soil salinization is an important factor that restricts crop quality and yield and causes an enormous toll to human beings. Salt stress and abscisic acid (ABA) stress will occur in the process of soil salinization. In this study, transcriptome sequencing of tobacco leaves under salt and ABA stress in order to further study the resistance mechanism of tobacco. Compared with controlled groups, 1654 and 3306 DEGs were obtained in salt and ABA stress, respectively. The genes function enrichment analysis showed that the up-regulated genes in salt stress were mainly concentrated in transcription factor WRKY family and PAR1 resistance gene family, while the up-regulated genes were mainly concentrated on bHLH transcription factor, Kunitz-type protease inhibitor, dehydrin (Xero1) gene and CAT (Catalase) family protein genes in ABA stress. Tobacco MAPK cascade triggered stress response through up-regulation of gene expression in signal transduction. The expression products of these up-regulated genes can improve the abiotic stress resistance of plants. These results have an important implication for further understanding the mechanism of salinity tolerance in plants.;Hui Wu and Huayang Li and Wenhui Zhang and Heng Tang and Long Yang;"Biophysics (Q1); Biochemistry (Q2); Molecular Biology (Q2); Cell Biology (Q3)";6438.0;331.0;United States;1959-2020;10.1016/j.bbrc.2021.07.011;0.06934;263.0;;0006291X;0006291X;10902104;0006291X;3.575;Tobacco, Transcriptome, MAPK, Soil salinization mechanism;Academic Press Inc.;;3030.0;Northern America;998.0;Q1;16845.0;Biochemical and biophysical research communications;Transcriptional regulation and functional analysis of nicotiana tabacum under salt and aba stress;105148.0;21713.0;1943.0;6445.0;58869.0;journal;article;2021
The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.;John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko;"Biochemistry (Q1); Pharmacology (Q1)";983.0;529.0;United States;1958-2020;10.1016/j.bcp.2018.03.014;0.01736;198.0;;00062952;18732968;00062952;18732968;5.858;Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome;Elsevier Inc.;;7833.0;Northern America;1595.0;Q1;20063.0;Biochemical pharmacology;Integrity, standards, and qc-related issues with big data in pre-clinical drug discovery;33633.0;5594.0;513.0;993.0;40185.0;journal;article;2018
Humans have transformed most landscapes across the globe, forcing other species to adapt in order to persist in increasingly anthropogenic landscapes. Wide-ranging solitary species, such as wild felids, struggle particularly in such landscapes. Conservation planning and management for their long-term persistence critically depends on understanding what determine survival and what are the main mortality risks. We carried out the first study on annual survival and cause-specific mortality of the European wildcat with a large and unique dataset of 211 tracked individuals from 22 study areas across Europe. Furthermore, we tested the effect of environmental and human disturbance variables on the survival probability. Our results show that mortalities were mainly human-caused, with roadkill and poaching representing 57% and 22% of the total annual mortality, respectively. The annual survival probability of wildcat was 0.92 (95% CI = 0.87–0.98) for females and 0.84 (95% CI = 0.75–0.94) for males. Road density strongly impacted wildcat annual survival, whereby an increase in the road density of motorways and primary roads by 1 km/km2 in wildcat home-ranges increased mortality risk ninefold. Low-traffic roads, such as secondary and tertiary roads, did not significantly affect wildcat's annual survival. Our results deliver key input parameters for population viability analyses, provide planning-relevant information to maintain subcritical road densities in key wildcat habitats, and identify conditions under which wildcat-proof fences and wildlife crossing structures should be installed to decrease wildcat mortality.;Matteo Luca Bastianelli and Joseph Premier and Mathias Herrmann and Stefano Anile and Pedro Monterroso and Tobias Kuemmerle and Carsten F. Dormann and Sabrina Streif and Saskia Jerosch and Malte Götz and Olaf Simon and Marcos Moleón and José María Gil-Sánchez and Zsolt Biró and Jasja Dekker and Analena Severon and Axel Krannich and Karsten Hupe and Estelle Germain and Dominique Pontier and René Janssen and Pablo Ferreras and Francisco Díaz-Ruiz and José María López-Martín and Fermín Urra and Lolita Bizzarri and Elena Bertos-Martín and Markus Dietz and Manfred Trinzen and Elena Ballesteros-Duperón and José Miguel Barea-Azcón and Andrea Sforzi and Marie-Lazarine Poulle and Marco Heurich;"Ecology, Evolution, Behavior and Systematics (Q1); Nature and Landscape Conservation (Q1)";1170.0;523.0;Netherlands;1968-2020;10.1016/j.biocon.2021.109239;0.033389999999999996;199.0;;00063207;00063207;00063207;00063207;5.990;Anthropogenic landscapes, European wildcat, Survival, Human-caused mortality, Roadkill, Road density;Elsevier BV;;6829.0;Western Europe;2227.0;Q1;17244.0;Biological conservation;Survival and cause-specific mortality of european wildcat (felis silvestris) across europe;39669.0;7021.0;433.0;1251.0;29571.0;journal;article;2021
"Background
Imaging research has not yet delivered reliable psychiatric biomarkers. One challenge, particularly among youth, is high comorbidity. This challenge might be met through canonical correlation analysis designed to model mutual dependencies between symptom dimensions and neural measures. We mapped the multivariate associations that intrinsic functional connectivity manifests with pediatric symptoms of anxiety, irritability, and attention-deficit/hyperactivity disorder (ADHD) as common, impactful, co-occurring problems. We evaluate the replicability of such latent dimensions in an independent sample.
Methods
We obtained ratings of anxiety, irritability, and ADHD, and 10 minutes of resting-state functional magnetic resonance imaging data, from two independent cohorts. Both cohorts (discovery: n = 182; replication: n = 326) included treatment-seeking youth with anxiety disorders, with disruptive mood dysregulation disorder, with ADHD, or without psychopathology. Functional connectivity was modeled as partial correlations among 216 brain areas. Using canonical correlation analysis and independent component analysis jointly we sought maximally correlated, maximally interpretable latent dimensions of brain connectivity and clinical symptoms.
Results
We identified seven canonical variates in the discovery and five in the replication cohort. Of these canonical variates, three exhibited similarities across datasets: two variates consistently captured shared aspects of irritability, ADHD, and anxiety, while the third was specific to anxiety. Across cohorts, canonical variates did not relate to specific resting-state networks but comprised edges interconnecting established networks within and across both hemispheres.
Conclusions
Findings revealed two replicable types of clinical variates, one related to multiple symptom dimensions and a second relatively specific to anxiety. Both types involved a multitude of broadly distributed, weak brain connections as opposed to strong connections encompassing known resting-state networks.";Julia O. Linke and Rany Abend and Katharina Kircanski and Michal Clayton and Caitlin Stavish and Brenda E. Benson and Melissa A. Brotman and Olivier Renaud and Stephen M. Smith and Thomas E. Nichols and Ellen Leibenluft and Anderson M. Winkler and Daniel S. Pine;Biological Psychiatry (Q1);528.0;704.0;United States;1969-2020;10.1016/j.biopsych.2020.10.018;0.04554;319.0;;00063223;00063223;18732402;00063223;13.382;Anxiety, Disruptive behavior, Intrinsic brain connectivity, Irritability, Joint canonical correlation and independent component analysis, Youth;Elsevier USA;;5051.0;Northern America;5335.0;Q1;14308.0;Biological psychiatry;Shared and anxiety-specific pediatric psychopathology dimensions manifest distributed neural correlates;50155.0;6464.0;294.0;886.0;14850.0;journal;article;2021
"Complete blood count (CBC) analysis is one of the most commonly ordered laboratory tests and is a critical first step in patients' clinical evaluation. However, CBC analyzers are limited in their ability to positively identify several types of white blood cells (WBC), and cells with substantial clinical significance, such as immature granulocytes or blasts, are merely marked as flags. Also, CBC analyzers fall short of recognizing informative red blood cell (RBC) morphology, such as schistocytes, and often provide inaccurate platelets count. Flags and clinically non-sufficient CBC-derived data reflex to generation of blood smear (BS), and BS review comprises a substantial portion of the workload in routine hematology laboratories. For accurate identification and classification of WBC, BS analysis (BSA) requires detailed observation of cells with high-magnification objective (60-100X), which provides a relatively narrow Field of View (FOV). This physical limitation restricts current BSA to either low resolution/wide FOV or to high resolution/narrow FOV data generation (Fig. 1A). Hence, key issues of BSA such as the effects of the smearing process on the distribution of blood components, the effects of cells distribution on their morphology and further classification, as well as many other attributes, are addressed only qualitatively or empirically, leaving the real topology of the BS obscure. The computational imaging microscopy system presented herein uses a low resolution and wide FOV objective, and records a plurality of images under different illumination conditions, of the same sample area (Fig. 1B). An algorithm reconstructs a high resolution and aberration free image of whole specimens, as can be observed in the attached link (https://tinyurl.com/Scopio-Labs-X100-ASH-2020). High resolution images are critical not only for manual BSA, but also for artificial intelligence (AI)-derived BSA, since data quality is of prime importance for deep-learning processes, and to a large extent determine their outcome. Thus, the combination of high resolution/wide FOV turns each BS into a big data analytic field, rendering the measurement of yet undetermined cell characteristics. In order to elucidate the basic topology, 60 normal BS (28 females, 32 males) were subjected to analysis utilizing this novel computational imaging microscopy. For convenience of analysis and comparison with current BSA methodology, BS were segmented into strips according to RBC density (Fig. 1C, D). The average length of smear from females (F) was higher by nearly 28% compared with smear from males (M), and the presence of acute inflammation (A) resulted in a significant 33% increase in overall smear length compared to normal (N) average (Fig. 1E). As expected, RBC density formed a linear gradient (Fig. 1C) along the axis of sample smearing, however, RBC morphology was affected by location within the BS. For example, strips 4-5 contained RBC with the appearance of spherocytes (Fig. 1F; arrows), while in strips with increased RBC density, cells aggregated resembling rouleaux formation (Fig. 1F; arrowheads). Platelets distribution was non-linear, with only a few of them reaching the feathered edge of the smear (Fig. 1G). Since the variance of both RBC/FOV and platelets/FOV concentrations drops starting with strip 4, BS-derived platelets number estimates should not be performed in strips 1-3. On average, a normal BS contains 890+399 WBC in the scanned area (strips 1-8). Similar to RBC, the location of individual WBC throughout the BS may affect their morphology, and hence their classification. WBC in the feathered edge (strips 1-3) are generally more stretched, and often squeezed between RBC, rendering their classification by AI-based tools challenging (Fig. 1H). In strips 4-7, WBC morphology is optimal for a classification task, enabling favorable outcomes for either manual or AI cell analysis (Fig. 1H). These data indicate that BSA can be taken to a sensitivity level of at least 10-3 of WBC analysis, provided that a large portion of the BS is scanned. Our system provides a novel combination of computational imaging microscopy and AI-based classification tools to unravel the complex topology of blood smears, and upgrade the data obtained in BSA. This approach enables the establishment of quantitative rules to scientifically direct the objective analysis of cellular blood components both manually, and by AI-tools. Figure
Disclosures
Katz: Scopio Labs: Consultancy.";Ben Zion Katz and Irit Avivi and Dan Benisty and Shahar Karni and Hadar Shimoni and Omri Grooper and Olga Pozdnyakova;"Biochemistry (Q1); Cell Biology (Q1); Hematology (Q1); Immunology (Q1)";2041.0;741.0;United States;1946-2020;10.1182/blood-2020-134903;0.18725;465.0;;00064971;15280020;00064971;15280020;22.113;;American Society of Hematology;;3106.0;Northern America;5515.0;Q1;25454.0;Blood;A novel approach to blood smear analysis based on specimen topology: implications for human and artificial intelligence decision making;200027.0;22558.0;853.0;2755.0;26498.0;journal;article;2020
Recent preclinical advances highlight the therapeutic potential of treatments aimed at boosting regeneration and plasticity of spinal circuitry damaged by spinal cord injury (SCI). With several promising candidates being considered for translation into clinical trials, the SCI community has called for a non-human primate model as a crucial validation step to test efficacy and validity of these therapies prior to human testing. The present paper reviews the previous and ongoing efforts of the California Spinal Cord Consortium (CSCC), a multidisciplinary team of experts from 5 University of California medical and research centers, to develop this crucial translational SCI model. We focus on the growing volumes of high resolution data collected by the CSCC, and our efforts to develop a biomedical informatics framework aimed at leveraging multidimensional data to monitor plasticity and repair targeting recovery of hand and arm function. Although the main focus of many researchers is the restoration of voluntary motor control, we also describe our ongoing efforts to add assessments of sensory function, including pain, vital signs during surgery, and recovery of bladder and bowel function. By pooling our multidimensional data resources and building a unified database infrastructure for this clinically relevant translational model of SCI, we are now in a unique position to test promising therapeutic strategies׳ efficacy on the entire syndrome of SCI. We review analyses highlighting the intersection between motor, sensory, autonomic and pathological contributions to the overall restoration of function. This article is part of a Special Issue entitled SI: Spinal cord injury.;Jessica L. Nielson and Jenny Haefeli and Ernesto A. Salegio and Aiwen W. Liu and Cristian F. Guandique and Ellen D. Stück and Stephanie Hawbecker and Rod Moseanko and Sarah C. Strand and Sharon Zdunowski and John H. Brock and Roland R. Roy and Ephron S. Rosenzweig and Yvette S. Nout-Lomas and Gregoire Courtine and Leif A Havton and Oswald Steward and V. {Reggie Edgerton} and Mark H. Tuszynski and Michael S. Beattie and Jacqueline C. Bresnahan and Adam R. Ferguson;"Developmental Biology (Q2); Molecular Biology (Q2); Neurology (clinical) (Q2); Neuroscience (miscellaneous) (Q2)";1281.0;295.0;Netherlands;1966-2020;10.1016/j.brainres.2014.10.048;0.0236;204.0;;00068993;18726240;00068993;18726240;3.252;Non-human primate, Spinal cord injury, Bioinformatics, Big-data, Syndromics, Statistics, Translation, Plasticity, Recovery, Motor function, Sensory function, Autonomic function;Elsevier;;6717.0;Western Europe;1037.0;Q2;14346.0;Brain research;Leveraging biomedical informatics for assessing plasticity and repair in primate spinal cord injury;58190.0;3970.0;442.0;1289.0;29689.0;journal;article;2015
;J.P. Thompson and T.J. Coats and M.R. Sims;Anesthesiology and Pain Medicine (Q1);684.0;433.0;United Kingdom;1923-2020;10.1093/bja/aev097;0.02901;181.0;;00070912;00070912;14716771;00070912;9.166;;Elsevier Ltd.;;2600.0;Western Europe;2589.0;Q1;21858.0;British journal of anaesthesia;Known knowns, known unknowns, and unknown unknowns: can systems medicine provide a new approach to sepsis?;27510.0;5321.0;486.0;1260.0;12638.0;journal;article;2015
With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.;Vijay Khatri;"Business and International Management (Q1); Marketing (Q1)";227.0;668.0;United Kingdom;1957-2020;10.1016/j.bushor.2016.06.001;0.00571;87.0;;00076813;00076813;00076813;00076813;6.361;Analytics, Big data, Managerial decision making, Managerial work, Digital universe;Elsevier Ltd.;;3867.0;Western Europe;2174.0;Q1;20209.0;Business horizons;Managerial work in the realm of the digital universe: the role of the data triad;7443.0;2066.0;64.0;278.0;2475.0;journal;article;2016
X-ray computed tomography (XCT) is increasingly being used for evaluating quality and conformance of complex products, including assemblies and additively manufactured parts. The metrological performance and traceability of XCT nevertheless remains an important research area that is reviewed in this paper. The error sources influencing XCT measurement results are discussed, along with related qualification, calibration and optimization procedures. Moreover, progress on performance verification testing and on the determination of task-specific measurement uncertainty is covered. Results of interlaboratory comparisons are summarized and performance in various dimensional measurement fields is illustrated. Conclusions and an outlook for future research activities are also provided.;Wim Dewulf and Harald Bosse and Simone Carmignato and Richard Leach;"Industrial and Manufacturing Engineering (Q1); Mechanical Engineering (Q1)";472.0;527.0;United States;1969-2020;10.1016/j.cirp.2022.05.001;;155.0;;00078506;00078506;17260604;00078506;;X-ray, Metrology, Traceability;Elsevier USA;;3110.0;Northern America;2370.0;Q1;19804.0;Cirp annals - manufacturing technology;Advances in the metrological traceability and performance of x-ray computed tomography;;3073.0;144.0;472.0;4478.0;journal;article;2022
Just-in-time (JIT) and Relevant vector machine (RVM) are two of commonly used models for soft-sensors modeling, the efficiency of which is governed by few critical parameters and hyper-parameters significantly. These parameters are routinely selected by trial and error or experience, thus leading to over- or under-fitting for the prediction. Adaptive differential evolution with optional external archive (JADE) has been used to optimize the parameters of JIT and RVM in this paper. The resulted JADE-JIT and JADE-RVM based soft-sensors are further enhanced into an adaptive format by the moving window (WM) technique. The proposed methodologies are applied to prediction of hard-to-measured variables in the wastewater treatment plants (WWTPs) and successful results are obtained.;Yiqi Liu;"Applied Mathematics (Q1); Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1)";2073.0;438.0;United Kingdom;1951-2021;10.1016/j.ces.2017.07.006;0.02743;186.0;;00092509;00092509;00092509;00092509;4.311;Soft-sensors, Just-in-time, Relevant vector machine, Differential evolution, Parameters;Elsevier BV;;4958.0;Western Europe;1022.0;Q1;16413.0;Chemical engineering science;Adaptive just-in-time and relevant vector machine based soft-sensors with adaptive differential evolution algorithms for parameter optimization;49734.0;9203.0;629.0;2092.0;31186.0;journal;article;2017
In recent years, various AI-based methods have been developed in order to uncover chemico-biological interactions associated with DNA damage and oxidative stress. Various decision trees, bayesian networks, random forests, logistic regression models, support vector machines as well as deep learning tools, have great potential in the area of molecular biology and toxicology, and it is estimated that in the future, they will greatly contribute to our understanding of molecular and cellular mechanisms associated with DNA damage and repair. In this concise review, we discuss recent attempts to build machine learning tools for assessment of radiation – induced DNA damage as well as algorithms that can analyze the data from the most frequently used DNA damage assays in molecular biology. We also review recent works on the detection of antioxidant proteins with machine learning, and the use of AI-related methods for prediction and evaluation of noncoding DNA sequences. Finally, we discuss previously published research on the potential application of machine learning tools in aging research.;Lazar M. Davidovic and Darko Laketic and Jelena Cumic and Elena Jordanova and Igor Pantic;"Medicine (miscellaneous) (Q2); Toxicology (Q2)";950.0;479.0;Ireland;1969-2020;10.1016/j.cbi.2021.109533;0.01195;120.0;;00092797;00092797;00092797;00092797;5.192;Reactive oxygen species, Radiation, Machine learning, Non-coding DNA, Aging;Elsevier Ireland Ltd;;5466.0;Western Europe;943.0;Q2;24652.0;Chemico-biological interactions;Application of artificial intelligence for detection of chemico-biological interactions associated with oxidative stress and dna damage;16208.0;4486.0;282.0;961.0;15415.0;journal;article;2021
Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.;Dominik C. Hezel and Markus Harak and Guy Libourel;"Geophysics (Q1); Geochemistry and Petrology (Q2)";118.0;249.0;Germany;1978-1990, 1993-2020;10.1016/j.chemer.2017.05.003;;53.0;;00092819;00092819;00092819;00092819;;Chondrules, Matrix, Elemental composition, ChondritedDB, Database;Elsevier GmbH;;7944.0;Western Europe;988.0;Q1;25163.0;Chemie der erde;What we know about elemental bulk chondrule and matrix compositions: presenting the chondritedb database;;371.0;96.0;120.0;7626.0;journal;article;2018
"This paper is a review of the standardisation required to achieve interoperability for pathology test requesting and reporting. Interoperability is the ability of two parties, either human or machine, to exchange data or information in a manner that preserves shared meaning. This is needed to make healthcare safer, more efficient and more effective. Interoperability requires standardisation around: transmission of data; identification policies; information structures; common terminology; common understanding; and behavioural agreement. It is dependent on consensus. Each of these aspects is considered from the perspective of pathology requesting and reporting concluding that while much has been done, much remains to be done.";Michael Legg;"Biochemistry (Q2); Biochemistry (medical) (Q2); Clinical Biochemistry (Q2); Medicine (miscellaneous) (Q2)";1202.0;335.0;Netherlands;1956-2020;10.1016/j.cca.2013.12.007;0.01641;142.0;;00098981;00098981;18733492;00098981;3.786;Pathology, Interoperability, Request, Report, Standardisation, Electronic health record;Elsevier;;4771.0;Western Europe;924.0;Q2;29286.0;Clinica chimica acta;Standardisation of test requesting and reporting for the electronic health record;22229.0;4130.0;555.0;1269.0;26479.0;journal;article;2014
The hype over artificial intelligence (AI) has spawned claims that clinicians (particularly radiologists) will become redundant. It is still moot as to whether AI will replace radiologists in day-to-day clinical practice, but more AI applications are expected to be incorporated into the workflows in the foreseeable future. These applications could produce significant ethical and legal issues in healthcare if they cause abrupt disruptions to its contextual integrity and relational dynamics. Sustaining trust and trustworthiness is a key goal of governance, which is necessary to promote collaboration among all stakeholders and to ensure the responsible development and implementation of AI in radiology and other areas of clinical work. In this paper, the nature of AI governance in biomedicine is discussed along with its limitations. It is argued that radiologists must assume a more active role in propelling medicine into the digital age. In this respect, professional responsibilities include inquiring into the clinical and social value of AI, alleviating deficiencies in technical knowledge in order to facilitate ethical evaluation, supporting the recognition, and removal of biases, engaging the “black box” obstacle, and brokering a new social contract on informational use and security. In essence, a much closer integration of ethics, laws, and good practices is needed to ensure that AI governance achieves its normative goals.;C.W.L. Ho and D. Soon and K. Caals and J. Kapur;"Medicine (miscellaneous) (Q2); Radiology, Nuclear Medicine and Imaging (Q2)";715.0;214.0;United Kingdom;1960-2020;10.1016/j.crad.2019.02.005;0.007390000000000001;90.0;;00099260;1365229X;00099260;1365229X;2.350;;W.B. Saunders Ltd;;2851.0;Western Europe;778.0;Q2;16616.0;Clinical radiology;Governance of automated image analysis and artificial intelligence analytics in healthcare;8240.0;1830.0;291.0;790.0;8295.0;journal;article;2019
Celiac disease (CD) has been on the rise in the world and a large part of it remains undiagnosed. Novel methods are required to address the gaps in prompt detection and management. Artificial intelligence (AI) has seen an exponential surge in the last decade worldwide. With the advent of big data and powerful computational ability, we now have self-driving cars and smart devices in our daily lives. Huge databases in the form of electronic medical records and images have rendered healthcare a lucrative sector where AI can prove revolutionary. It is being used extensively to overcome the barriers in clinical workflows. From the perspective of a disease, it can be deployed in multiple steps i.e. screening tools, diagnosis, developing novel therapeutic agents, proposing management plans, and defining prognostic indicators, etc. We review the areas where it may augment physicians in the delivery of better healthcare by summarizing current literature on the use of AI in healthcare using CD as a model. We further outline major barriers to its large-scale implementations and prospects from the healthcare point of view.;Muhammad Khawar Sana and Zeshan M. Hussain and Pir Ahmad Shah and Muhammad Haisum Maqsood;"Computer Science Applications (Q1); Health Informatics (Q2)";923.0;559.0;United Kingdom;1970-2020;10.1016/j.compbiomed.2020.103996;0.011859999999999999;94.0;;00104825;00104825;18790534;00104825;4.589;Artificial intelligence, Machine learning, Deep learning, Celiac disease, Systematic review;Elsevier Ltd.;;5216.0;Western Europe;884.0;Q1;17957.0;Computers in biology and medicine;Artificial intelligence in celiac disease;9751.0;5223.0;383.0;936.0;19977.0;journal;article;2020
;David M. Liebovitz and John Fahrenbach;"Cardiology and Cardiovascular Medicine (Q1); Critical Care and Intensive Care Medicine (Q1); Pulmonary and Respiratory Medicine (Q1)";938.0;340.0;United States;1970-2020;10.1016/j.chest.2018.01.033;0.04591;289.0;;00123692;00123692;19313543;00123692;9.410;;American College of Chest Physicians;;2286.0;Northern America;2647.0;Q1;18429.0;Chest;Rebuttal from drs liebovitz and fahrenbach;58778.0;6472.0;854.0;1687.0;19519.0;journal;article;2018
"Coseismic slip distributions of repeated earthquakes are important data used to forecast magnitude and recurrence characteristics of future earthquakes. Many studies infer coseismic slip of past individual paleoearthquakes from frequency histograms of geomorphic offsets. Such histograms, often expressed as a COPD (cumulative offset probability distribution), commonly show multiple peaks with similar displacements, which are interpreted to be incremental slip associated with successive paleoearthquakes. However, assumptions in linking COPD peaks to individual earthquakes have not been fully explored. Here, we combine statistical modeling with global historical coseismic surface rupture data to evaluate the conditions required for the approach. The results show that only coseismic slip with low variability allow more than one event identification; for example, even low CoVs (coefficient of offset variation) of 0.25 and 0.35 permit identification of only 2 to 3 events. In contrast, the mean CoV value for all historical ruptures is 0.58 to 0.66. Furthermore, interpreting the first COPD peak as a single-event slip distribution could result in artificially low variation and flatter distribution of offset than observed in historical earthquakes. This study cautions that robust COPD interpretation on large strike-slip faults requires low offset variability, a sufficient number of offset measurements, and additional constraints like historical coseismic slip data or paleoseismic event sequence data.";Zhou Lin and Jing Liu-Zeng and Ray J. Weldon and Jing Tian and Chao Ding and Yu Du;"Earth and Planetary Sciences (miscellaneous) (Q1); Geochemistry and Petrology (Q1); Geophysics (Q1); Space and Planetary Science (Q1)";1673.0;522.0;Netherlands;1966-2020;10.1016/j.epsl.2020.116313;0.06548999999999999;248.0;;0012821X;0012821X;0012821X;0012821X;5.255;geomorphic offset,  (coefficient of offset variation), COPD (cumulative offset probability distribution), global historical coseismic surface ruptures, strike-slip fault, earthquake recurrence models;Elsevier BV;;5457.0;Western Europe;2829.0;Q1;22566.0;Earth and planetary science letters;Modeling repeated coseismic slip to identify and characterize individual earthquakes from geomorphic offsets on strike-slip faults;74499.0;9373.0;543.0;1692.0;29632.0;journal;article;2020
The element carbon plays a central role in climate and life on Earth. It is capable of moving among the geosphere, cryosphere, atmosphere, biosphere and hydrosphere. This flow of carbon is referred to as the Earth's carbon cycle. It is also intimately linked to the cycling of other elements and compounds. The ocean plays a fundamental role in Earth's carbon cycle, helping to regulate atmospheric CO2 concentration. The ocean biological carbon pump (OBCP), defined as a set of processes that transfer organic carbon from the surface to the deep ocean, is at the heart of the ocean carbon cycle. Monitoring the OBCP is critical to understanding how the Earth's carbon cycle is changing. At present, satellite remote sensing is the only tool available for viewing the entire surface ocean at high temporal and spatial scales. In this paper, we review methods for monitoring the OBCP with a focus on satellites. We begin by providing an overview of the OBCP, defining and describing the pools of carbon in the ocean, and the processes controlling fluxes of carbon between the pools, from the surface to the deep ocean, and among ocean, land and atmosphere. We then examine how field measurements, from ship and autonomous platforms, complement satellite observations, provide validation points for satellite products and lead to a more complete view of the OBCP than would be possible from satellite observations alone. A thorough analysis is then provided on methods used for monitoring the OBCP from satellite platforms, covering current capabilities, concepts and gaps, and the requirement for uncertainties in satellite products. We finish by discussing the potential for producing a satellite-based carbon budget for the oceans, the advantages of integrating satellite-based observations with ecosystem models and field measurements, and future opportunities in space, all with a view towards bringing satellite observations into the limelight of ocean carbon research.;Robert J.W. Brewin and Shubha Sathyendranath and Trevor Platt and Heather Bouman and Stefano Ciavatta and Giorgio Dall'Olmo and James Dingle and Steve Groom and Bror Jönsson and Tihomir S. Kostadinov and Gemma Kulk and Marko Laine and Victor Martínez-Vicente and Stella Psarra and Dionysios E. Raitsos and Katherine Richardson and Marie-Hélène Rio and Cécile S. Rousseaux and Joe Salisbury and Jamie D. Shutler and Peter Walker;Earth and Planetary Sciences (miscellaneous) (Q1);573.0;1182.0;Netherlands;1966-2020;10.1016/j.earscirev.2021.103604;0.0329;192.0;;00128252;00128252;00128252;00128252;12.413;Ocean, Carbon cycle, Satellite, Biology;Elsevier;;17999.0;Western Europe;3893.0;Q1;22580.0;Earth-science reviews;Sensing the ocean biological carbon pump from space: a review of capabilities, concepts, research gaps and future developments;27304.0;7358.0;339.0;602.0;61016.0;journal;article;2021
Five large and many small landslides are developed in Jurassic strata along the lower reaches of Xiangxi River, where interbedded weak and hard bedrock layers foster the development of landslides with a “stair-step” sliding surface. The paper investigates the evolution characteristics of these landslides and presents a novel forecasting model for their displacements. The distribution characteristics and behavior of landslides developed along Xiangxi River is revealed by the database of landslides in the larger Zigui basin, of which this area is part. Most landslides occur at rather low elevations of <300 m and in areas of moderate rainfall. The geological evolution of landslides in the Xiangxi River valley can be divided into four stages, beginning with anticline formation, followed by valley incision, then by weathering and erosion, and culminating in formation of the colluvial landslides. The accumulative displacement curves of landslides with a stair-step sliding surface in Xiangxi River region also present obvious, step-like characteristics. A novel GA-CEEMD-RF algorithm was developed to predict the displacement of these stair-step landslides, which helps to define the combination of induced factors and weak stableness of prediction results using a single displacement prediction model and the multi-field monitoring data.;Changdong Li and Robert E. Criss and Zhiyong Fu and Jingjing Long and Qinwen Tan;"Geology (Q1); Geotechnical Engineering and Engineering Geology (Q1)";870.0;667.0;Netherlands;1965-2020;10.1016/j.enggeo.2020.105961;0.019790000000000002;136.0;;00137952;00137952;00137952;00137952;6.755;Reservoir landslide, Evolution process, Multi-step sliding surface, Displacement forecasting model, Lower reaches of Xiangxi River;Elsevier;;5115.0;Western Europe;2441.0;Q1;15648.0;Engineering geology;Evolution characteristics and displacement forecasting model of landslides with stair-step sliding surface along the xiangxi river, three gorges reservoir region, china;25097.0;6177.0;425.0;881.0;21738.0;journal;article;2021
"Introduction
The extent of the biological impact of passive smoke exposure is unclear. We sought to investigate the association between passive smoke exposure and DNA methylation, which could serve as a biomarker of health risk.
Materials and methods
We derived passive smoke exposure from self-reported questionnaire data among smoking and non-smoking partners of participants enrolled in the UK Household Longitudinal Study ‘Understanding Society’ (n=769). We performed an epigenome-wide association study (EWAS) of passive smoke exposure with DNA methylation in peripheral blood measured using the Illumina Infinium Methylation EPIC array.
Results
No CpG sites surpassed the epigenome-wide significance threshold of p<5.97 × 10−8 in relation to partner smoking, compared with 10 CpG sites identified in relation to own smoking. However, 10 CpG sites surpassed a less stringent threshold of p<1 × 10−5 in a model of partner smoking adjusted for own smoking (model 1), 7 CpG sites in a model of partner smoking restricted to non-smokers (model 2) and 16 CpGs in a model restricted to regular smokers (model 3). In addition, there was evidence for an interaction between own smoking status and partners’ smoking status on DNA methylation levels at the majority of CpG sites identified in models 2 and 3. There was a clear lack of enrichment for previously identified smoking signals in the EWAS of passive smoke exposure compared with the EWAS of own smoking.
Conclusion
The DNA methylation signature associated with passive smoke exposure is much less pronounced than that of own smoking, with no positive findings for ‘expected’ signals. It is unlikely that changes to DNA methylation serve as an important mechanism underlying the health risks of passive smoke exposure.";Paige M. Hulls and Frank {de Vocht} and Yanchun Bao and Caroline L. Relton and Richard M. Martin and Rebecca C. Richmond;"Biochemistry (Q1); Environmental Science (miscellaneous) (Q1)";1650.0;628.0;United States;1967-2020;10.1016/j.envres.2020.109971;0.034260000000000006;136.0;;00139351;10960953;00139351;10960953;6.498;DNA methylation, EWAS, Understanding society, Passive smoke exposure;Academic Press Inc.;;5969.0;Northern America;1460.0;Q1;21524.0;Environmental research;Dna methylation signature of passive smoke exposure is less pronounced than active smoking: the understanding society study;28576.0;11076.0;1355.0;1688.0;80881.0;journal;article;2020
Studies have indicated that detection of circulating tumor DNA (ctDNA) prior to treatment is a negative prognostic marker in non-small cell lung cancer (NSCLC). ctDNA is currently identified by detection of tumor mutations. Commercial next-generation sequencing (NGS) assays for mutation analysis of ctDNA for routine practice usually include small gene panels and are not suitable for general mutation analysis. In this study, we investigated whether mutation analysis of cfDNA could be performed using a commercially available comprehensive NGS gene panel and bioinformatics workflow. Tumor DNA, plasma DNA and peripheral blood leukocyte DNA from 30 NSCLC patients were sequenced. In two patients (7%), tumor mutations in cfDNA were immediately called by the bioinformatic workflow. In 13 patients (43%), tumor mutations were not called, but were present in ctDNA and were identified based on the known tumor mutation profile. In the remaining 15 patients (50%), no concordant mutations were detected. In conclusion, we were able to identify tumor mutations in ctDNA from 57% of NSCLC patients using a comprehensive gene panel. We demonstrated that sequencing paired tumor DNA was helpful to interpret data and confirm ctDNA, and thus increased the ratio of patients with detectable ctDNA. This approach might be feasible for mutation analysis of ctDNA in routine diagnostic practice, especially in case of suboptimal plasma quality and quantity.;Anine Larsen Ottestad and Sissel G.F. Wahl and Bjørn Henning Grønberg and Frank Skorpen and Hong Yan Dai;"Clinical Biochemistry (Q2); Pathology and Forensic Medicine (Q2); Molecular Biology (Q3)";327.0;306.0;United States;1962-1995, 1997-2020;10.1016/j.yexmp.2019.104347;0.0036899999999999997;68.0;;00144800;10960945;00144800;10960945;3.362;Non-small cell lung cancer (NSCLC), Next-generation sequencing (NGS), Circulating tumor DNA (ctDNA);Academic Press Inc.;;4805.0;Northern America;791.0;Q2;13247.0;Experimental and molecular pathology;The relevance of tumor mutation profiling in interpretation of ngs data from cell-free dna in non-small cell lung cancer patients;4792.0;986.0;170.0;329.0;8168.0;journal;article;2020
"The rapid growth in data sharing presents new opportunities across the spectrum of biomedical research. Global efforts are underway to develop practical guidance for implementation of data sharing and open data resources. These include the recent recommendation of ‘FAIR Data Principles’, which assert that if data is to have broad scientific value, then digital representations of that data should be Findable, Accessible, Interoperable and Reusable (FAIR). The spinal cord injury (SCI) research field has a long history of collaborative initiatives that include sharing of preclinical research models and outcome measures. In addition, new tools and resources are being developed by the SCI research community to enhance opportunities for data sharing and access. With this in mind, the National Institute of Neurological Disorders and Stroke (NINDS) at the National Institutes of Health (NIH) hosted a workshop on October 5–6, 2016 in Bethesda, MD, in collaboration with the Open Data Commons for Spinal Cord Injury (ODC-SCI) titled “Preclinical SCI Data: Creating a FAIR Share Community”. Workshop invitees were nominated by the workshop steering committee (co-chairs: ARF and VPL; members: AC, KDA, MSB, KF, LBJ, PGP, JMS), to bring together junior and senior level experts including preclinical and basic SCI researchers from academia and industry, data science and bioinformatics experts, investigators with expertise in other neurological disease fields, clinical researchers, members of the SCI community, and program staff representing federal and private funding agencies. The workshop and ODC-SCI efforts were sponsored by the International Spinal Research Trust (ISRT), the Rick Hansen Institute, Wings for Life, the Craig H. Neilsen Foundation and NINDS. The number of attendees was limited to ensure active participation and feedback in small groups. The goals were to examine the current landscape for data sharing in SCI research and provide a path to its future. Below are highlights from the workshop, including perspectives on the value of data sharing in SCI research, workshop participant perspectives and concerns, descriptions of existing resources and actionable directions for further engaging the SCI research community in a model that may be applicable to many other areas of neuroscience. This manuscript is intended to share these initial findings with the broader research community, and to provide talking points for continued feedback from the SCI field, as it continues to move forward in the age of data sharing.";Alison Callahan and Kim D. Anderson and Michael S. Beattie and John L. Bixby and Adam R. Ferguson and Karim Fouad and Lyn B. Jakeman and Jessica L. Nielson and Phillip G. Popovich and Jan M. Schwab and Vance P. Lemmon;"Developmental Neuroscience (Q1); Neurology (Q1)";639.0;490.0;United States;1959-2020;10.1016/j.expneurol.2017.05.012;0.01565;186.0;;00144886;00144886;10902430;00144886;5.330;FAIR data principles, Reproducibility, Neuroscience, Informatics, Workshop proceedings, Open Data Commons;Academic Press Inc.;;8040.0;Northern America;1779.0;Q1;15598.0;Experimental neurology;Developing a data sharing community for spinal cord injury research;24022.0;3200.0;283.0;654.0;22752.0;journal;article;2017
"Objective
To report the utilization, effectiveness, and safety of practices in assisted reproductive technology (ART) globally in 2013 and assess global trends over time.
Design
Retrospective, cross-sectional survey on the utilization, effectiveness, and safety of ART procedures performed globally during 2013.
Setting
Seventy-five countries and 2,639 ART clinics.
Patient(s)
Women and men undergoing ART procedures.
Intervention(s)
All ART.
Main Outcome Measure(s)
The ART cycles and outcomes on country-by-country, regional, and global levels. Aggregate country data were processed and analyzed based on methods developed by the International Committee for Monitoring Assisted Reproductive Technology (ICMART).
Result(s)
A total of 1,858,500 ART cycles were conducted for the treatment year 2013 across 2,639 clinics in 75 participating countries with a global participation rate of 73.6%. Reported and estimated data suggest 1,160,474 embryo transfers (ETs) were performed resulting in >344,317 babies. From 2012 to 2013, the number of reported aspiration and frozen ET cycles increased by 3% and 16.4%, respectively. The proportion of women aged >40 years undergoing nondonor ART increased from 25.2% in 2012 to 26.3% in 2013. As a percentage of nondonor aspiration cycles, intracytoplasmic sperm injection (ICSI) was similar to results for 2012. The in vitro fertilization (IVF)/ICSI combined delivery rates per fresh aspiration and frozen ET cycles were 24.2% and 22.8%, respectively. In fresh nondonor cycles, single ET increased from 33.7% in 2012 to 36.5% in 2013, whereas the average number of transferred embryos was 1.81—again with wide country variation. The rate of twin deliveries after fresh nondonor transfers was 17.9%; the triplet rate was 0.7%. In frozen ET cycles performed in 2013, single ET was used in 57.6%, with an average of 1.49 embryos transferred and twin and triplet rates of 10.8% and 0.4%, respectively. The cumulative delivery rate per aspiration was 30.4%, similar to that in 2012. Perinatal mortality rate per 1,000 births was 22.2% after fresh IVF/ICSI and 16.8% after frozen ET. The data presented depended on the quality and completeness of the data submitted by individual countries. This report covers approximately two-thirds of world ART activity. Continued efforts to improve the quality and consistency of reporting ART data by registries are still needed.
Conclusion(s)
Reported ART cycles, effectiveness, and safety increased between 2012 and 2013 with adoption of a better method for estimating unreported cycles.
Comité Internacional para la monitorización de las Tecnologías de Reproducción Asistida (ICMART): informe mundial.
Objetivo
Informar sobre la utilización, la eficacia y la seguridad de las prácticas de la tecnología de reproducción asistida (TRA) a nivel mundial en 2013 y evaluar las tendencias mundiales a lo largo del tiempo.
Diseño
Encuesta retrospectiva y transversal sobre la utilización, la eficacia y la seguridad de los procedimientos de TRA realizados a nivel mundial durante el año 2013.
Entorno
Setenta y cinco países y 2639 clínicas de TRA.
Pacientes
Mujeres y hombres sometidos a procedimientos de TRA.
Intervención(es)
Todas las TRA.
Medida(s) principal(es) del resultado
Los ciclos de TRA y los resultados a nivel de país, regional y mundial. Los datos agregados de los países se procesaron y analizaron según los métodos desarrollados por el Comité Internacional para la Vigilancia de las Tecnologías de Reproducción Asistida (ICMART). Tecnología de Reproducción Asistida (ICMART).
Resultados
En el año de tratamiento 2013 se realizaron un total de 1.858.500 ciclos de TRA en 2.639 clínicas de 75 países participantes con una tasa de participación global del 73,6%. Los datos informados y estimados sugieren que se realizaron 1.160.474 transferencias de embriones (TE) que dieron lugar a >344.317 bebés. De 2012 a 2013, el número de ciclos de aspiración y de TE congelados notificados aumentó un 3% y un 16,4%, respectivamente. La proporción de mujeres de >40 años que se sometieron a TRA autóloga aumentó del 25,2% en 2012 al 26,3% en 2013. Como porcentaje de ciclos de aspiración autóloga, la inyección intracitoplasmática de espermatozoides (ICSI) fue similar a los resultados de 2012. Las tasas de parto combinadas de fecundación in vitro (FIV)/ICSI por ciclos de aspiración en fresco y TE congelada fueron del 24,2% y el 22,8%, respectivamente. En los ciclos autólogos en fresco,la TE única aumentó del 33,7% en 2012 al 36,5% en 2013, mientras que el número medio de embriones transferidos fue de 1,81 -de nuevo, con una amplia variación entre países. La tasa de partos gemelares tras transferencias autólogas en fresco fue del 17,9%; la tasa de trillizos fue del 0,7%. En los ciclos de TE de congelados realizados en 2013, se utilizó la TE simple en el 57,6%, con una media de 1,49 embriones transferidos y unas tasas de gemelos y trillizos del 10,8% y 0,4%, respectivamente. La tasa acumulada de partos por aspiración fue del 30,4%, similar a la de 2012. La tasa de mortalidad perinatal por cada 1.000 nacimientos fue del 22,2% tras la FIV/ICSI en fresco y del 16,8% tras la TE de congelados. Los datos presentados dependían de la calidad y la exhaustividad de los datos presentados por cada país. Este informe abarca aproximadamente dos tercios de la actividad mundial de TRA. Los esfuerzos continuos para mejorar la calidad y la coherencia de los datos presentados por los registros sobre la TRA deben seguir siendo objeto de esfuerzos continuos.
Conclusión(es)
Los ciclos de TRA notificados, la eficacia y la seguridad aumentaron entre 2012 y 2013 con la adopción de un mejor método para estimar los ciclos no reportados.";Manish Banker and Silke Dyer and Georgina M. Chambers and Osamu Ishihara and Markus Kupka and Jacques {de Mouzon} and Fernando Zegers-Hochschild and G. David Adamson;"Obstetrics and Gynecology (Q1); Reproductive Medicine (Q1)";883.0;371.0;United States;1950-2020;10.1016/j.fertnstert.2021.03.039;0.03305;208.0;;00150282;15565653;00150282;15565653;7.329;Assisted reproductive technology, IVF/ICSI outcome, frozen embryo transfer, ICMART, cumulative live birth rate, registry;Elsevier Inc.;;2386.0;Northern America;2272.0;Q1;12705.0;Fertility and sterility;International committee for monitoring assisted reproductive technologies (icmart): world report on assisted reproductive technologies, 2013;45818.0;5714.0;502.0;1277.0;11976.0;journal;article;2021
With the gradual aggravation of energy shortage, automobile energy saving has gained widespread attention from scholars. However, due to the lack of a high-accuracy practical fuel consumption model, it is difficult to estimate transient fuel consumption and evaluate the actual effect of real-time fuel consumption control strategies. Therefore, it is necessary to establish a more accurate and practical model according to the transient motion characteristics of the vehicles. To ensure the accuracy of the model, an integrated structure of the steady-state base module and the transient correction module is determined as the overall structure of the model. Based on the steady-state fuel consumption data, the steady-state base module is established. Then, based on the easily obtained vehicle and engine state parameters, principal component analysis and cluster analysis are used to reasonably classify different driving conditions of the vehicles. Following that, the distance correlation analysis is applied to find the combination of state parameters with the strongest correlation with the estimation error of the steady-state module, and a transient correction module is established according to the optimal state parameter combination obtained. After that, the optimal transient correction module structure is determined based on the Bayesian criterion. Finally, the model is tested, and the results show that the mean absolute percentage error (MAPE) of the fuel consumption estimation of the new model is about 15%, while that of the classical VT-Micro model and the VT-CPFM model are about 28% and 20%, respectively. It can be seen that the new model has a higher accuracy. On the other hand, compared with structured physical fuel consumption models such as VT-CPEM model, the new model has a simpler structure, a shorter computation time, and a higher computational speed. In addition, the new model has high practicability due to its clear structure and easy access to parameters.;Ding Rui and Jin Hui;"Chemical Engineering (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Organic Chemistry (Q1)";5139.0;688.0;Netherlands;1922, 1970-2021;10.1016/j.fuel.2022.123927;0.08033;213.0;;00162361;00162361;18737153;00162361;6.609;Transient fuel consumption model, Model optimization, Distance correlation analysis;Elsevier BV;;4999.0;Western Europe;1560.0;Q1;16313.0;Fuel;High-accuracy transient fuel consumption model based on distance correlation analysis;98202.0;35895.0;2396.0;5152.0;119785.0;journal;article;2022
The world economy relies on access to industrial metals, oil and gas for maintaining its critical industrial infrastructure. Although demand is likely to remain high, the most accessible deposits have been depleted. Future capacity growth will be facilitated through further technological developments. Russia as a leading producer is paying great attention to strengthening its competitive edge in global markets. This paper reports on a large-scale technology foresight study of the Russian extractive sector (including oil and gas), which combined expert-based foresight activities with statistical analyses and text-mining techniques based on artificial intelligence and machine learning technologies. The presented methodology helped to link the technologies to dominant discussions (e.g. climate change vs rural development) and to flag key trends. Furthermore, quantitative estimates can be identified quickly. The study’s methodology should function as an example for similar studies to support policy planning and investment decisions based on text-mining techniques.;Leonid Gokhberg and Ilya Kuzminov and Elena Khabirova and Thomas Thurner;"Business and International Management (Q1); Development (Q1); Sociology and Political Science (Q1)";297.0;333.0;United Kingdom;1968-2020;10.1016/j.futures.2019.102476;0.00418;79.0;;00163287;00163287;00163287;00163287;3.073;Extractive industries, Mineral sector, New technologies, Technology foresight, Horizon scanning, Text-mining, Science, Technology and innovation, Russian Federation, Critical technologies;Elsevier Ltd.;;6629.0;Western Europe;1210.0;Q1;25561.0;Futures;Advanced text-mining for trend analysis of russia’s extractive industries;5060.0;1163.0;90.0;303.0;5966.0;journal;article;2020
"ABSTRACT
Background and aims
Publicly available databases containing colonoscopic imaging data are valuable resources for artificial intelligence (AI) research. Currently, little is known regarding the available number and content of these databases. This review aimed to describe the availability, accessibility and usability of publicly available colonoscopic imaging databases, focusing on polyp detection, polyp characterization and quality of colonoscopy.
Methods
A systematic literature search was performed in MEDLINE and Embase to identify AI-studies describing publicly available colonoscopic imaging datasets published after 2010. Second, a targeted search using Google’s Dataset Search, Google Search, GitHub and Figshare was done to identify datasets directly. Datasets were included if they contained data about polyp detection, polyp characterization or quality of colonoscopy. To assess accessibility of datasets the following categories were defined: open access, open access with barriers and regulated access. To assess the potential usability of the included datasets, essential details of each dataset were extracted using a checklist derived from the CLAIM-checklist.
Results
We identified 22 datasets with open access, 3 datasets open access with barriers and 15 datasets with regulated access. The 22 open access databases containing 19,463 images and 952 videos. Nineteen of these databases focused on polyp detection, localization and/or segmentation, six on polyp characterization and three on quality of colonoscopy. Only half of these databases have been used by other researcher to develop, train or benchmark their AI-system. Although technical details were in general well-reported, important details such as polyp and patient demographics and the annotation process were underreported in almost all databases.
Conclusion
This review provides greater insight on public availability of colonoscopic imaging databases for AI-research. Incomplete reporting of important details limits the ability of researchers to assess the usability of the current databases.";Britt B.S.L. Houwen and Karlijn J. Nass and Jasper L.A. Vleugels and Paul Fockens and Yark Hazewinkel and Evelien Dekker;"Gastroenterology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";963.0;347.0;United States;1965-2020;10.1016/j.gie.2022.08.043;0.030869999999999998;200.0;;00165107;10976779;00165107;10976779;9.427;artificial intelligence, machine learning, colonoscopy, colorectal cancer, colorectal polyps;Mosby Inc.;;1731.0;Northern America;2365.0;Q1;28350.0;Gastrointestinal endoscopy;Comprehensive review of publicly available colonoscopic imaging databases for artificial intelligence research: availability, accessibility and usability;28955.0;5062.0;580.0;1496.0;10041.0;journal;article;2022
Understanding the processes that control mass and energy exchanges between soil, plants and the atmosphere plays a critical role for understanding the root zone system, but it is also beneficial for practical applications such as sustainable agriculture and geotechnics. Improved process understanding demands fast, minimally invasive and cost-effective methods of monitoring the shallow subsurface. Geoelectrical monitoring methods fulfil these criteria and have therefore become of increasing interest to soil scientists. Such methods are particularly sensitive to variations in soil moisture and the presence of root material, both of which are essential drivers for processes and mechanisms in soil and root zone systems. This review analyses the recent use of geoelectrical methods in the soil sciences, and highlights their main achievements in focal areas such as estimating hydraulic properties and delineating root architecture. We discuss the specific advantages and limitations of geoelectrical monitoring in this context. Standing out amongst the latter are the non-uniqueness of inverse model solution and the appropriate choice of pedotransfer functions between electrical parameters and soil properties. The relationship between geoelectrical monitoring and alternative characterization methodologies is also examined. Finally, we advocate for future interdisciplinary research combining models of root hydrology and geoelectrical measurements. This includes the development of more appropriate analogue root electrical models, careful separation between different root zone contributors to the electrical response and integrating spatial and temporal geophysical measurements into plant hydrological models to improve the prediction of root zone development and hydraulic parameters.;Mihai Octavian Cimpoiaşu and Oliver Kuras and Tony Pridmore and Sacha J. Mooney;Soil Science (Q1);1460.0;613.0;Netherlands;1967-2021;10.1016/j.geoderma.2020.114232;0.02441;165.0;;00167061;18726259;00167061;18726259;6.114;Electrical-properties, Root zone soil moisture, Root system monitoring, Root zone structure, Root detection, modelling, Geophysics, Geoelectrical methods;Elsevier;;6319.0;Western Europe;1846.0;Q1;39112.0;Geoderma;Potential of geoelectrical methods to monitor root zone processes and structure: a review;33708.0;9006.0;543.0;1472.0;34310.0;journal;article;2020
Most farm and farmer typologies focus on specific aspects and use standard structural and socio-economic indicators. Regional assessments of agricultural diversity based on farming systems are rarely done, as detailed and representative information is difficult to collect. The discipline of comparative agriculture addresses these challenges but remains little known, and seldom applied to broadacre situations. This study demonstrates in Western Australia the value of its mixed methods and multi-disciplinary concepts to determine the level and nature of regional farming system heterogeneity. The typology built comprised six farming systems based on 36 farms that represented half the farming population of a 4000 km2 area (broadacre rainfed systems dominated by winter cereals and sheep, Mediterranean climate). The farm groups corresponding to these farming systems differed across 36 variables representing biophysical, technical, and social aspects at varied spatial and temporal scales. Results were compared with five sets of farm clusters produced through multivariate clustering procedures commonly employed to build typologies. These farm clusters differed across fewer variables than the farm groups of the comparative agriculture typology. The analytical, methodological and conceptual tools used in comparative agriculture to solve the challenges associated with the holistic study of farming system heterogeneity are discussed. These included basing data collection and analysis on an empirical approach that assessed groups of farms rather than individuals, solving data scarcity through a range of qualitative techniques, and progressively informing the choice of typology criteria. Comparative agriculture thus provides an alternative to standard typology paradigms that deserves wider application.;Myrtille Lacoste and Roger Lawes and Olivier Ducourtieux and Ken Flower;Sociology and Political Science (Q1);725.0;354.0;United Kingdom;1970-2020;10.1016/j.geoforum.2018.01.017;0.0159;116.0;;00167185;00167185;00167185;00167185;3.901;Farming systems, Agrarian systems, Farm typology, Farmer practices, Mixed methods, Multivariate cluster analysis;Elsevier BV;;7275.0;Western Europe;1584.0;Q1;28611.0;Geoforum;Assessing regional farming system diversity using a mixed methods typology: the value of comparative agriculture tested in broadacre australia;11684.0;3032.0;284.0;773.0;20662.0;journal;article;2018
A rotary kiln is core equipment in cement calcination. Significant time delay, time-varying, and nonlinear characteristics cause challenges in the advance process control and operational optimization of the rotary kiln. However, the traditional mechanism model with many assumptions cannot accurately represent the dynamic kiln process because kinetic parameters are difficult to obtain. This paper proposes a novel hybrid strategy to develop a dynamic model of a rotary kiln by combining a process mechanism and a recurrent neural network to address this issue. A time delay mechanism is used to estimate the kiln’s residence time to compensate for the time delay. A long short-term memory model that combines an attention mechanism and an ordinary differential equation solver is proposed to capture the time-varying and nonlinear behaviors of the kiln process. Case studies from two real-world cement plants with different processing loads are used to verify the effectiveness of the proposed hybrid modeling strategy. The results show that the proposed method has better accuracy and robustness than the traditional methods. The sensitivity analysis of the proposed model also makes it practical for t control system design and real-time optimization. ;Jinquan Zheng and Liang Zhao and Wenli Du;"Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1)";884.0;608.0;United States;1968-2020;10.1016/j.isatra.2022.02.018;0.012830000000000001;79.0;;00190578;00190578;00190578;00190578;5.468;Hybrid model, Rotary kiln, Attention mechanism, Recurrent neural network;ISA - Instrumentation, Systems, and Automation Society;;4036.0;Northern America;1147.0;Q1;29805.0;Isa transactions;Hybrid model of a cement rotary kiln using an improved attention-based recurrent neural network;10483.0;5669.0;559.0;892.0;22562.0;journal;article;2022
The rise of sharing economy has accelerated the growth of marketing analytics to match demand and supply in industrial markets. However, the conceptualization of marketing analytics remains unclear in the sharing economy. Theorizing market turbulence as the dark side of the sharing economy, this study presents a marketing analytics capability model using dynamic capabilities and contingency theories to advance thought and practice in industrial marketing research. Using a thematic analysis and a survey-based empirical study on B2B cloud sharing platforms (n = 252), the findings present pattern identification, real-time solutions and data governance as the antecedents of marketing analytics capability with its holistic effects on marketing agility and marketing effectiveness. The empirical findings further support the mediating role of marketing agility and the moderating impact of market turbulence on marketing analytics-effectiveness and marketing agility-effectiveness chain. Overall, our results contribute toward a more nuanced understanding of the dark side of market turbulence on marketing analytics capability dynamics in the sharing economy.;Shahriar Akter and Umme Hani and Yogesh K. Dwivedi and Anuj Sharma;Marketing (Q1);450.0;686.0;United States;1971-2020;10.1016/j.indmarman.2022.04.008;0.009009999999999999;136.0;;00198501;00198501;00198501;00198501;6.960;Marketing analytics capability, Sharing economy, Marketing agility, Marketing effectiveness, Market turbulence;Elsevier Inc.;;8914.0;Northern America;2022.0;Q1;22792.0;Industrial marketing management;The future of marketing analytics in the sharing economy;16291.0;3787.0;314.0;465.0;27989.0;journal;article;2022
The efficiency of attribute reduction is one of the important challenges being faced in the field of Big Data processing. Although many quick attribute reduction algorithms have been proposed, they are tightly coupled with their corresponding indiscernibility relations, and it is difficult to extend specific acceleration policies to other reduction models. In this paper, we propose a generalized indiscernibility reduction model(GIRM) and a concept of the granular structure in GIRM, which is a quantitative measurement induced from multiple indiscernibility relations and which can be used to represent the computation cost of varied models. Then, we prove that our GIRM is compatible with three typical reduction models. Based on the proposed GIRM, we present a generalized attribute reduction algorithm and a generalized positive region computing algorithm. We perform a quantitative analysis of the computation complexities of two algorithms using the granular structure. For the generalized attribute reduction, we present systematic acceleration policies that can reduce the computational domain and optimize the computation of the positive region. Based on the granular structure, we propose acceleration policies for the computation of the generalized positive region, and we also propose fast positive region computation approaches for three typical reduction models. Experimental results for various datasets prove the efficiency of our acceleration policies in those three typical reduction models.;Fan Jing and Jiang Yunliang and Liu Yong;"Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)";2168.0;789.0;United States;1968-2021;10.1016/j.ins.2017.02.032;0.04908;184.0;;00200255;00200255;00200255;00200255;6.795;Generalized indiscernibility relation, Attribute reduction, Granular structure, Acceleration policy;Elsevier Inc.;;4311.0;Northern America;1524.0;Q1;15134.0;Information sciences;Quick attribute reduction with generalized indiscernibility models;44038.0;17554.0;928.0;2184.0;40007.0;journal;article;2017
ObjectivesL There is a need for monitoring dental health and healthcare, as support for quality development, allocation of resources and long-term planning of dental care. The aim of this paper is to describe the concept and implementation of the Swedish Quality Registry for Caries and Periodontal Diseases (SKaPa). Materials and methods: The SKaPa receives information by automatic transfer of data daily from electronic patient dental records via secure connections from affiliated dental care organisations (DCOs). The registry stores information about DCOs, dental professionals and patients. Information on a patient level includes personal identifier, gender, age, living area, dental status, risk assessments for caries and periodontitis, and dental care provided. In addition, data generated from a global question on patient-perceived oral health are uploaded. In total, more than 400 variables are transferred to the registry and updated daily. Results: In 2018, all of the 21 public DCOs and the largest private DCO in Sweden were affiliated to SKaPa, representing a total of 1,089 public and 234 private dental clinics. The accumulated amount of information on dental healthcare covers 6.9 million individuals out of the total Swedish population of 10 million. SKaPa produces reports on de-identified data, both cross-sectional and longitudinal. Conclusion: As a nationwide registry based on automatic retrieval of data directly from patient records, SKaPa offers the basis for a new era of systematic evaluation of oral health and quality of dental care. The registry supports clinical and epidemiological research, data mining and external validation of results from randomised controlled trials;Inger {von Bültzingslöwen} and Hans Östholm and Lars Gahnberg and Dan Ericson and Jan L. Wennström and Jörgen Paulander;Dentistry (miscellaneous) (Q1);176.0;216.0;United States;1960-1962, 1965-2020;10.1111/idj.12481;0.0016699999999999998;64.0;;00206539;00206539;00206539;00206539;2.512;Odontology, epidemiology, oral health, big data, quality registry;Wiley-Blackwell;;3749.0;Northern America;840.0;Q1;25609.0;International dental journal;Swedish quality registry for caries and periodontal diseases – a framework for quality development in dentistry;3063.0;512.0;81.0;204.0;3037.0;journal;article;2019
"The process of cleaning motion capture data of aberrant points has been described as “the bane of motion capture operators”. Yet, managing the high volume kinematic data generated through in-home neurogames requires data quality control that, executed insufficiently, jeopardizes accuracy of outcomes. To begin to address this issue at the intersection of biomechanics and “big data”, we performed a secondary analysis of a neurogame, evaluating gesture count as well as shoulder and elbow joint angle outcomes calculated from kinematic data in which valid gestures were identified through 3 methods: visual review of regions of interest by an expert (BP); manufacturer-recommended data smoothing (MS); and automated methods (AI). We hypothesized that upper extremity kinematic outcomes from BP would be matched by AI but not MS methods. From one person with post-stroke hemiparesis, upper-extremity kinematic data were collected for 6 days over 2 weeks using a Microsoft Kinect™-based neurogame. We calculated gesture count, shoulder angle, and elbow angle outcomes from data managed using BP, MS, and AI methods. BP identified 1929 valid gestures total over 6 days which was different than the other two methods (p = 0.0015). In contrast, the AI algorithm with best precision identified 4372 and MS identified 4459 valid gestures. Furthermore, angle outcomes calculated from AI and MS methods resulted in different values than BP (p < 0.001 for 5 of 6 variables). More research is needed to automate treatment of high volume, low quality motion data to support investigation of motion associated with in-home rehabilitation neurogames.";Lise C. Worthen-Chaudhari and Michael P. McNally and Akshay Deshpande and Vivek Bakaraju;"Rehabilitation (Q1); Biomedical Engineering (Q2); Biophysics (Q2); Orthopedics and Sports Medicine (Q2); Sports Science (Q2)";1361.0;266.0;United Kingdom;1968-2020;10.1016/j.jbiomech.2020.109726;0.018619999999999998;199.0;;00219290;18732380;00219290;18732380;2.712;;Elsevier Ltd.;;3796.0;Western Europe;826.0;Q1;15846.0;Journal of biomechanics;In-home neurogaming: demonstrating the impact of valid gesture recognition method on high volume kinematic outcomes;35320.0;3948.0;510.0;1383.0;19358.0;journal;article;2020
This article is based on the 20th Rossini Lecture delivered on 28 July 2014 at the opening of the 23rd International Conference on Chemical Thermodynamics in Durban, South Africa. In the last several decades, enormous progress in material and computer sciences has led, in many scientific disciplines, to fundamental improvements in experimental measurement technologies. That, in combination with new communication technologies and gradually increasing societal commitment to support public scientific research, has resulted in an unprecedented growth in the “production” of the reported experimental results. These trends together with dramatically growing demand for thermophysical and thermochemical property data related to new chemical processes and products necessitated development of dynamic methods of critical data evaluation within a framework of the concept of Global Information Systems in Science which was developed and implemented for the field of thermodynamics at the Thermodynamics Research Center (TRC) of the US National Institute of Standards and Technology (NIST). Principal advantages of this approach in comparison with the traditional static methods of critical data evaluation are illustrated. Major components of the developed system and its impact on such areas of human activities as information delivery, data publication, chemical process design, chemical product design, experiment planning, and education are discussed. The systems and software tools designed for global validation of experimental values are outlined. A variety of experimental data-driven technologies for thermophysical property prediction developed with the use of the elements of the Global Information System in Thermodynamics, including those based on surrogate mixture models, group contributions, QSPR, UNIFAC, and Monte Carlo molecular simulation, are described.;Michael Frenkel;"Atomic and Molecular Physics, and Optics (Q2); Materials Science (miscellaneous) (Q2); Physical and Theoretical Chemistry (Q2)";1078.0;303.0;United States;1969-2021;10.1016/j.jct.2014.12.016;0.00852;88.0;;00219614;10963626;00219614;10963626;3.178;Dynamic data evaluation, Global Information Systems, Thermophysical property data, Data-driven technologies, Experimental data quality;Academic Press Inc.;;4449.0;Northern America;810.0;Q2;26949.0;Journal of chemical thermodynamics;A never-ending search for the truth: thermodynamics in the uncertain era of the internet;12927.0;3154.0;278.0;1085.0;12369.0;journal;article;2015
It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.;Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen;"Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Networks and Communications (Q2); Theoretical Computer Science (Q2)";245.0;187.0;United States;1967-2021;10.1016/j.jcss.2014.04.022;0.00268;99.0;;00220000;10902724;00220000;10902724;1.023;Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling;Academic Press Inc.;;3535.0;Northern America;573.0;Q2;12370.0;Journal of computer and system sciences;A spatiotemporal compression based approach for efficient big data processing on cloud;4516.0;568.0;46.0;252.0;1626.0;journal;article;2014
"ABSTRACT
The transition from pregnancy to lactation is the most challenging period for high-producing dairy cows. The liver plays a key role in biological adaptation during the peripartum. Prior works have demonstrated that hepatic glucose synthesis, cholesterol metabolism, lipogenesis, and inﬂammatory response are increased or activated during the peripartum in dairy cows; however, those works were limited by a low number of animals used or by the use of microarray technology, or both. To overcome such limitations, an RNA sequencing analysis was performed on liver biopsies from 20 Holstein cows at 7 ± 5d before (Pre-P) and 16 ± 2d after calving (Post-P). We found 1,475 upregulated and 1,199 downregulated differently expressed genes (DEG) with a false discovery rate adjusted P-value < 0.01 between Pre-P and Post-P. Bioinformatic analysis revealed an activation of the metabolism, especially lipid, glucose, and amino acid metabolism, with increased importance of the mitochondria and a key role of several signaling pathways, chiefly peroxisome proliferators-activated receptor (PPAR) and adipocytokines signaling. Fatty acid oxidation and gluconeogenesis, with a likely increase in amino acid utilization to produce glucose, were among the most important functions revealed by the transcriptomic adaptation to lactation in the liver. Although gluconeogenesis was induced, data indicated decrease in expression of glucose transporters. The analysis also revealed high activation of cell proliferation but inhibition of xenobiotic metabolism, likely due to the liver response to inflammatory-like conditions. Co-expression network analysis disclosed a tight connection and coordination among genes driving biological processes associated with protein synthesis, energy and lipid metabolism, and cell proliferation. Our data confirmed the importance of metabolic adaptation to lipid and glucose metabolism in the liver of early Post-P cows, with a pivotal role of PPAR and adipocytokines.";S.T. Gao and D.D. Girma and M. Bionaz and L. Ma and D.P. Bu;"Animal Science and Zoology (Q1); Food Science (Q1); Genetics (Q1)";2938.0;391.0;United States;1917-2020;10.3168/jds.2020-19101;0.03123;191.0;;00220302;15253198;00220302;15253198;4.034;RNA sequencing, peripartum cow, metabolic adaptation, hepatic transcriptome;Elsevier Ltd.;;4563.0;Northern America;1483.0;Q1;32795.0;Journal of dairy science;Hepatic transcriptomic adaptation from prepartum to postpartum in dairy cows;66689.0;12522.0;1042.0;2955.0;47550.0;journal;article;2021
Crises provide an opportunity for the field to take stock, as do the articles in this special issue. Constructive advice for 21st century publication standards includes appropriate theory, internal validity, and external validity. First, well-grounded theory can produce a priori plausibility, testable logic, and a focus on the ideas involved, all cumulatively informed by meta-analysis across studies. Second, internal validity benefits from both exploratory work and confirmatory analyses on well-powered samples that require systematic detection and principled decisions about data quality. Inferences benefit from manipulated mediation analysis and from careful interpretation without over-claiming. Finally, external validity profits from a variety of exact and conceptual replications, best evaluated by meta-analysis.;Susan T. Fiske;"Social Psychology (Q1); Sociology and Political Science (Q1)";435.0;337.0;United States;1965-2020;10.1016/j.jesp.2016.01.006;0.01439;142.0;;00221031;00221031;10960465;00221031;3.603;Experiments, Replicability, Internal validity, External validity, Theory;Academic Press Inc.;;6022.0;Northern America;2401.0;Q1;15499.0;Journal of experimental social psychology;How to publish rigorous experiments in the 21st century;15958.0;1800.0;110.0;437.0;6624.0;journal;article;2016
Urban waterlogging often causes urban disasters, and the rapid early warning and comprehensive analysis of the urban waterlogging can help disaster defenses. However, the warning of waterlogging through the monitoring data cannot give grid distribution and the forecast of hydrological models cannot ensure rapid early warning. To obtain a grid rapid early warning result for a region, like an urban area, a method needs to be proposed which can meet the above problems. In this research, AutoML (automatic machine learning based on genetic algorithm) was recommended to construct the rapid early warning and comprehensive analysis models for urban waterlogging by compared with the other three machine learning algorithms, CatBoost (Categorical Boosting), XGBoost (eXtreme Gradient Boosting), and BPDNN (Back Propagation Deep Learning Neural Network). In the models, the forecast and historical precipitation obtained from the Integrated Nowcasting through Comprehensive analysis system (INCA), the difference of elevation, and the urban waterlogging risk maps provided by Tianjin Meteorological Administration were employed as the input sources. The input precipitation duration was determined as 12 h based on the sensitivity analysis of the influence of various precipitation duration on waterlogging depths. Due to the non-digital (discrete dataset) features, the urban waterlogging risk maps were transformed to the weight of each corresponding risk level according to the area of each risk level and the number of samples falling in each risk level. The difference of elevation was characterized by the average elevations of various distances from the points of concern. The output waterlogging depths were compared with the waterlogging depths monitored in Tianjin, China, whose quality was controlled by eliminating the records of the waterlogging depths lasting for a long time after the end of rainfall. The comparison of the models constructed by different methods demonstrated that the AutoML performed better (NSE and R2 > 0.92, CC > 0.95, RMSE1.1–1.9 cm) than the other three models. The forecast waterlogging depths by AutoML was also coherent with the monitoring waterlogging depths (NSE and R2 ≥ 0.9, CC ≥ 0.95, RMSE 1.7–2.2 cm). For that local topography and waterlogging risk are considered, the AutoML models can be used in the area without the monitoring of water level, quickly predict waterlogging depths and give spatial grid results for rapidly early warning.;Yuchen Guo and Lihong Quan and Lili Song and Hao Liang;Water Science and Technology (Q1);2563.0;576.0;Netherlands;1949, 1963-2020;10.1016/j.jhydrol.2021.127367;0.04972;226.0;;00221694;00221694;00221694;00221694;5.722;Urban waterlogging, Automatic machine learning algorithm based on genetic algorithms, Rapid early warning, XGBoost, CatBoost, BPDNN, Comprehensive analysis;Elsevier;;6550.0;Western Europe;1684.0;Q1;50089.0;Journal of hydrology;Construction of rapid early warning and comprehensive analysis models for urban waterlogging based on automl and comparison of the other three machine learning algorithms;73620.0;15250.0;1336.0;2587.0;87508.0;journal;article;2022
The application of high-density DNA array technology to monitor gene transcription has been responsible for a real paradigm shift in biology. The majority of research groups now have the ability to measure the expression of a significant proportion of the human genome in a single experiment, resulting in an unprecedented volume of data being made available to the scientific community. As a consequence of this, the storage, analysis and interpretation of this information present a major challenge. In the field of immunology the analysis of gene expression profiles has opened new areas of investigation. The study of cellular responses has revealed that cells respond to an activation signal with waves of co-ordinated gene expression profiles and that the components of these responses are the key to understanding the specific mechanisms which lead to phenotypic differentiation. The discovery of ‘cell type specific’ gene expression signatures have also helped the interpretation of the mechanisms leading to disease progression. Here we review the principles behind the most commonly used data analysis methods and discuss the approaches that have been employed in immunological research.;Joaquin Dopazo and Edward Zanders and Ilaria Dragoni and Gillian Amphlett and Francesco Falciani;"Immunology (Q3); Immunology and Allergy (Q3)";414.0;217.0;Netherlands;1971-2020;10.1016/S0022-1759(01)00307-6;0.0063100000000000005;133.0;;00221759;00221759;18727905;00221759;2.303;Immunological research, Data analysis, Human genome;Elsevier;;2785.0;Western Europe;870.0;Q3;21274.0;Journal of immunological methods;Methods and approaches in the analysis of gene expression data;12982.0;884.0;105.0;419.0;2924.0;journal;article;2001
Dupilumab, an IL-4/IL-13 receptor blocker, has been linked to emergent seronegative inflammatory arthritis and psoriasis that form part of the spondyloarthropathy spectrum. We systematically investigated patterns of immune disorders, including predominantly T helper 17‒(spondyloarthropathy pattern) and T helper 2‒mediated disorders and humoral autoimmune pattern diseases, using VigiBase, the World Health Organization’s global pharmacovigilance of adverse drug reactions. Several bioinformatics databases and repositories were mined to couple dupilumab-related immunopharmacovigilance with molecular cascades relevant to reported findings. A total of 37,848 dupilumab adverse drug reaction cases were reported, with skin, eye, and musculoskeletal systems most affected. Seronegative arthritis (OR = 9.61), psoriasis (OR = 1.48), enthesitis/enthesopathy (OR = 12.65), and iridocyclitis (OR = 3.77) were highly associated. However, ankylosing spondylitis and inflammatory bowel disease were not conclusively associated. Overall, classic polygenic humoral‒mediated autoimmune diseases such as rheumatoid arthritis and systemic lupus erythematosus were not associated with dupilumab use. Pathway analysis identified several biological pathways potentially involved in dupilumab‒associated adverse drug reactions, including the fibroblast GF receptor (in particular, FGFR2) pathway. MicroRNAs analysis revealed the potential involvement of hsa-miR-21-5p and hsa-miR-335-5p. In conclusion, IL-4/IL-13 blockers are not unexpectedly protective against humoral autoimmune diseases but dynamically skew immune responses toward some IL-23/IL-17 cytokine pathway‒related diseases. IL-4/13 axis also plays a role in homeostatic tissue repair and we noted evidence for a link with ocular and arterial pathology.;Charlie Bridgewood and Miriam Wittmann and Tom Macleod and Abdulla Watad and Darren Newton and Kanchan Bhan and Howard Amital and Giovanni Damiani and Sami Giryes and Nicola Luigi Bragazzi and Dennis McGonagle;"Biochemistry (Q1); Cell Biology (Q1); Dermatology (Q1); Molecular Biology (Q1)";1090.0;430.0;United Kingdom;1945-2020;10.1016/j.jid.2022.03.013;0.029110000000000004;201.0;;0022202X;0022202X;0022202X;0022202X;8.551;;Nature Publishing Group;;3412.0;Western Europe;1951.0;Q1;24868.0;Journal of investigative dermatology;T helper 2 il-4/il-13 dual blockade with dupilumab is linked to some emergent t helper 17‒type diseases, including seronegative arthritis and enthesitis/enthesopathy, but not to humoral autoimmune diseases;37630.0;5516.0;419.0;1249.0;14296.0;journal;article;2022
"Following the resurgence of the COVID-19 epidemic in the UK in late 2020 and the emergence of the alpha (also known as B117) variant of the SARS-CoV-2 virus, a third national lockdown was imposed from January 4, 2021. Following the decline of COVID-19 cases over the remainder of January 2021, the question of when and how to reopen schools became an increasingly pressing one in early 2021. This study models the impact of a partial national lockdown with social distancing measures enacted in communities and workplaces under different strategies of reopening schools from March 8, 2021 and compares it to the impact of continual full national lockdown remaining until April 19, 2021. We used our previously published agent-based model, Covasim, to model the emergence of the alpha variant over September 1, 2020 to January 31, 2021 in presence of Test, Trace and Isolate (TTI) strategies. We extended the model to incorporate the impacts of the roll-out of a two-dose vaccine against COVID-19, with 200,000 daily vaccine doses prioritised by age starting with people 75 years or older, assuming vaccination offers a 95% reduction in disease acquisition risk and a 30% reduction in transmission risk. We used the model, calibrated until January 25, 2021, to simulate the impact of a full national lockdown (FNL) with schools closed until April 19, 2021 versus four different partial national lockdown (PNL) scenarios with different elements of schooling open: 1) staggered PNL with primary schools and exam-entry years (years 11 and 13) returning on March 8, 2021 and the rest of the schools years on March 15, 2020; 2) full-return PNL with both primary and secondary schools returning on March 8, 2021; 3) primary-only PNL with primary schools and exam critical years (years 11 and 13) going back only on March 8, 2021 with the rest of the secondary schools back on April 19, 2021 and 4) part-rota PNL with both primary and secondary schools returning on March 8, 2021 with primary schools remaining open continuously but secondary schools on a two-weekly rota-system with years alternating between a fortnight of face-to-face and remote learning until April 19, 2021. Across all scenarios, we projected the number of new daily cases, cumulative deaths and effective reproduction number R until April 30, 2021. Our calibration across different scenarios is consistent with alpha variant being around 60% more transmissible than the wild type. We find that strict social distancing measures, i.e. national lockdowns, were essential in containing the spread of the virus and controlling hospitalisations and deaths during January and February 2021. We estimated that a national lockdown over January and February 2021 would reduce the number of cases by early March to levels similar to those seen in October 2020, with R also falling and remaining below 1 over this period. We estimated that infections would start to increase when schools reopened, but found that if other parts of society remain closed, this resurgence would not be sufficient to bring R above 1. Reopening primary schools and exam critical years only or having primary schools open continuously with secondary schools on rotas was estimated to lead to lower increases in cases and R than if all schools opened. Without an increase in vaccination above the levels seen in January and February, we estimate that R could have increased above 1 following the reopening of society, simulated here from April 19, 2021. Our findings suggest that stringent measures were integral in mitigating the increase in cases and bringing R below 1 over January and February 2021. We found that it was plausible that a PNL with schools partially open from March 8, 2021 and the rest of the society remaining closed until April 19, 2021 would keep R below 1, with some increase evident in infections compared to continual FNL until April 19, 2021. Reopening society in mid-April, without an increase in vaccination levels, could push R above 1 and induce a surge in infections, but the effect of vaccination may be able to control this in future depending on the transmission blocking properties of the vaccines.";J. Panovska-Griffiths and R.M. Stuart and C.C. Kerr and K. Rosenfield and D. Mistry and W. Waites and D.J. Klein and C. Bonell and R.M. Viner;"Applied Mathematics (Q1); Analysis (Q2)";2828.0;198.0;United States;1960-2021;10.1016/j.jmaa.2022.126050;0.03795;142.0;;0022247X;10960813;0022247X;10960813;1.583;COVID-19, National lockdown, Reopening schools and society, Mathematical modelling;Academic Press Inc.;;2718.0;Northern America;951.0;Q1;23935.0;Journal of mathematical analysis and applications;Modelling the impact of reopening schools in the uk in early 2021 in the presence of the alpha variant and with roll-out of vaccination against sars-cov-2;29202.0;5578.0;838.0;2831.0;22774.0;journal;article;2022
Single-cell RNA sequencing (scRNA-seq), a method of transcriptome sequencing at the single-cell level, has recently emerged as a revolutionary technology in the field of biomedical research. Compared to conventional gene expression profiling in bulk, scRNA-seq resolves biological differences among individual cells and enables the identification of rare cell populations that are easily overlooked. This review introduces the method of scRNA-seq, summarizes its applications in the field of cardiovascular disease research, and discusses existing limitations and prospects for future applications.;Chen Yifan and Yang Fan and Pu Jun;"Cardiology and Cardiovascular Medicine (Q1); Molecular Biology (Q2)";568.0;427.0;United States;1970-2020;10.1016/j.yjmcc.2020.03.005;0.01569;159.0;;00222828;00222828;10958584;00222828;5.000;Atherosclerosis, Vulnerable plaques, Macrophage polarization, Melatonin, Single-cell sequencing analysis, Multiomics analysis;Academic Press Inc.;;5705.0;Northern America;1645.0;Q1;23882.0;Journal of molecular and cellular cardiology;Visualization of cardiovascular development, physiology and disease at the single-cell level: opportunities and future challenges;17399.0;2631.0;177.0;589.0;10098.0;journal;article;2020
Cryo-electron tomography is a powerful technique that can faithfully image the native cellular environment at nanometer resolution. Unlike many other imaging approaches, cryo-electron tomography provides a label-free method of detecting biological structures, relying on the intrinsic contrast of frozen cellular material for direct identification of macromolecules. Recent advances in sample preparation, detector technology, and phase plate imaging have enabled the structural characterization of protein complexes within intact cells. Here, we review these technical developments and outline a detailed computational workflow for in situ structural analysis. Two recent studies are described to illustrate how this workflow can be adapted to examine both known and unknown cellular complexes. The stage is now set to realize the promise of visual proteomics—a complete structural description of the cell's native molecular landscape.;Shoh Asano and Benjamin D. Engel and Wolfgang Baumeister;"Biophysics (Q1); Molecular Biology (Q1); Structural Biology (Q1)";973.0;513.0;United States;1959-2020;10.1016/j.jmb.2015.09.030;0.0384;269.0;;00222836;00222836;10898638;00222836;5.469;cryo-EM, tomography, template matching, visual proteomics, subtomogram averaging;Academic Press Inc.;;8483.0;Northern America;3189.0;Q1;17618.0;Journal of molecular biology;In situ cryo-electron tomography: a post-reductionist approach to structural biology;65163.0;5286.0;446.0;1012.0;37835.0;journal;article;2016
Arguably one of the most important factors in the fast deployment of advanced nuclear reactors, with major improvements in safety, is the development and qualification of radiation and corrosion tolerant materials, that serve as the structural components in reactor cores. However, the discovery, improvement, and assessment of materials resistant to radiation and corrosion in the advanced reactors’ extreme environments is quite demanding, time-consuming, and costly, which represents a significant barrier to materials innovation and qualification for nuclear energy. This short review highlights a novel, integrated, high-throughput (HTP) research framework to develop understanding and predictive models for irradiation at high doses and molten salt corrosion responses of structural Compositionally Complex Alloys (CCAs), with the objective to accelerate materials discovery for high-temperature nuclear structural applications. Using a novel in situ alloying technique, arrays of additively manufactured bulk CCAs are processed, heat-treated, and characterized, while still attached to the build plate. Leveraging recent development in automation of heavy-ion irradiation experiments at the University of Wisconsin Ion Beam Laboratory, arrays of CCAs can be rapidly irradiated to hundreds of dpa up to 800 °C. An innovative droplet corrosion method is also used to test molten salt corrosion behavior of CCAs arrays. Automated and rapid characterization methods are used to assess the irradiation and molten salt corrosion resistance of CCAs. Finally, a brief discussion of the results is presented considering future use of machine-learning-based methods to develop useful trends and highlight features of importance. Using this novel HTP approach, a robust and reliable database containing literally hundreds of data points for irradiation and corrosion responses of CCAs can be established within a year, which is considered a significant increase in the pace of nuclear structural materials research and discovery.;Adrien Couet;"Materials Science (miscellaneous) (Q1); Nuclear and High Energy Physics (Q1); Nuclear Energy and Engineering (Q1)";1843.0;300.0;Netherlands;1959-2021;10.1016/j.jnucmat.2021.153425;0.02189;141.0;;00223115;00223115;00223115;00223115;2.936;;Elsevier;;4528.0;Western Europe;1123.0;Q1;29021.0;Journal of nuclear materials;Integrated high-throughput research in extreme environments targeted toward nuclear structural materials discovery;32651.0;5611.0;555.0;1854.0;25132.0;journal;article;2022
;Alan H. Jobe;Pediatrics, Perinatology and Child Health (Q1);2111.0;238.0;United States;1932-2020;10.1016/j.jpeds.2019.01.038;0.04055;205.0;;00223476;00223476;10976833;00223476;4.406;off-label, neonatology, surfactant;Mosby Inc.;;2131.0;Northern America;1227.0;Q1;15116.0;Journal of pediatrics;Off-label drugs in neonatology: analyses using large data bases;39713.0;6724.0;809.0;2516.0;17242.0;journal;article;2019
"Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.";Kim S. Betts and Steve Kisely and Rosa Alati;"Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)";755.0;441.0;United Kingdom;1961-1982, 1984-2020;10.1016/j.jpsychires.2020.07.002;0.020030000000000003;136.0;;00223956;18791379;00223956;18791379;4.791;Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning;Elsevier Ltd.;;5173.0;Western Europe;1875.0;Q1;16812.0;Journal of psychiatric research;Predicting postpartum psychiatric admission using a machine learning approach;20371.0;3624.0;357.0;776.0;18469.0;journal;article;2020
The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.;Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti;Marketing (Q1);87.0;451.0;United Kingdom;1993-2020;10.1016/j.jretai.2016.12.004;0.00458;136.0;;00224359;00224359;00224359;00224359;5.245;Big data, Predictive analytics, Retailing, Pricing;Elsevier BV;;6532.0;Western Europe;3184.0;Q1;22990.0;Journal of retailing;The role of big data and predictive analytics in retailing;10594.0;839.0;66.0;98.0;4311.0;journal;article;2017
"Problem
The increasing use of smartphones and low cost GPS have provided new sources for collecting data and using them to explain travel behavior. This study aims to use data collected from a smartphone application (CyclePhilly) to explain wrong-way riding behavior of cyclists on one-way segments to help better identify the demographic and network factors influencing the wrong-way riding decision making.
Methods
The data used in this study consist of two different sources: (a) Route trips data downloaded from the CyclePhilly Website contained trips detailed up to segment level, collected from May 2014 to April 2016 (12,202 trips by 300 unique users); and (b) Open Street Maps (OSM). Using ArcGIS, we calculate detour routes for each wrong way segment. We then built a mixed logistic regression model to identify the trip and riders' characteristics affecting wrong-way riding behavior. Next, we explore the characteristics of road facilities associated with wrong-way riding behavior.
Results and discussion
Only 2.7% of travel distance is wrong-way, yet 42% of trips include a wrong-way segment. Commute trips have a higher chance of wrong-way riding. The longer the trips also include more wrong-way riding. Segments with higher detour ratios (ratio of distance with a detour to the wrong-way distance) are found to be associated with more wrong-way behavior. Compared to roads with no bike lane, roads with sharrow markings and buffered bike lane discourage wrong way riding.
Practical applications
This study proposes new methods that can be adapted to use naturalistic and probe data and analyze city-wide aberrant riders' behavior. These help planners and engineers choose between various types of bike infrastructure. Wrong-way riding is one application that can be investigated, but probe bicycle datasets provide unprecedented resolution and volume of data that will allow for more sophisticated safety and planning analyses.";Nirbesh Dhakal and Christopher R. Cherry and Ziwen Ling and Mojdeh Azad;Safety, Risk, Reliability and Quality (Q1);282.0;369.0;United Kingdom;1969-1980, 1982-2020;10.1016/j.jsr.2018.10.004;0.00469;85.0;;00224375;00224375;00224375;00224375;3.487;Cycling behavior, Naturalistic data, Smartphones, Wrong-way riding, Bicycle safety;Elsevier Ltd.;;4209.0;Western Europe;972.0;Q1;29284.0;Journal of safety research;Using cyclephilly data to assess wrong-way riding of cyclists in philadelphia;4855.0;1119.0;130.0;286.0;5472.0;journal;article;2018
As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.;Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo;Surgery (Q2);1614.0;196.0;United States;1961-2021;10.1016/j.jss.2019.09.053;0.01875;108.0;;00224804;10958673;00224804;10958673;2.192;Electronic health record, Big data, Drug safety, Health care database, Cancer risk;Academic Press Inc.;;2850.0;Northern America;780.0;Q2;21728.0;Journal of surgical research;What can we learn about drug safety and other effects in the era of electronic health records and big data that we would not be able to learn from classic epidemiology?;17062.0;3583.0;688.0;1666.0;19607.0;journal;article;2020
"Shales are a class of multiscale, multiphase, hybrid inorganic-organic composite materials exhibiting both frictional and cohesive behavior, and it is very challenging to characterize and interpret their complex mechanical properties. A statistical nanoindentation approach with pertinent viable data analytics was developed to probe the mechanical properties of shales across different length scales. Grid nanoindentation experiments with continuous stiffness measurement performed on shales to relatively large depths of 6–8 µm obtained massive data, which were processed by the new data analytics: segmentation at selected depths of a great number (e.g., >500) of continuous Young's modulus versus indentation depth curves obtained from unknown constituent phases yielded multiple discretized sub-datasets that were processed to extract individual phases’ elastic moduli at respective segmentation depths via probability density function (PDF)-based deconvolution; these depth-dependent Young's moduli of each phase were then fitted by a newly proposed surround effect model, leading to determination of the properties of both individual phases at the nano/micro-scales (i.e., virtually infinitesimal depths) and the bulk rock at the macroscale (i.e., ~10–100 µm depths). A significant advantage of this massive data-based indentation approach is that the mechanical properties of composite materials such as shales can be probed across different scales by a single measurement technique. In addition, a new criterion, termed Bin Size Index, was formulated for selecting depth-dependent, rational, optimized bin sizes for PDF construction. For the studied shales, results show that five mechanically-distinct phases are discerned, including a virtual interface phase between hard and soft constituents accounting for a majority of indents. Coincidently, the Young's modulus of the bulk rock is nearly the same as that of the interface phase, suggesting that the macroscopic properties of similar composites may be estimated from measurements on the interface of two phases with contrasting mechanical properties. Finally, this approach can guide the selection of appropriate indentation depths to probe the mechanical properties of both highly heterogeneous bulk materials at the macroscale and their individual constituent phases at the nano/micro-scale.";Shengmin Luo and Yunhu Lu and Yongkang Wu and Jinliang Song and Don J. DeGroot and Yan Jin and Guoping Zhang;"Condensed Matter Physics (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1)";725.0;616.0;United Kingdom;1952-2020;10.1016/j.jmps.2020.103945;0.01754;173.0;;00225096;00225096;00225096;00225096;5.471;Data analytics, Nanoindentation, Shale, Surround effect, Young's modulus;Elsevier Ltd.;;5399.0;Western Europe;1857.0;Q1;14428.0;Journal of the mechanics and physics of solids;Cross-scale characterization of the elasticity of shales: statistical nanoindentation and data analytics;24119.0;4551.0;315.0;727.0;17008.0;journal;article;2020
Obstructive renal injury and drug-induced nephrotoxicity are the two most common causes of renal fibrosis diseases. However, whether these two different pathogeny induced same pathological outcomes contain common genetic targets or signaling pathway, the current research has not paid great attention. GSE121190 and GSE35257 were downloaded from the Gene Expression Omnibus (GEO) database. While GSE121190 represents a differential expression profile in kidney of mice with unilateral ureteral obstruction (UUO) model, GSE35257 represents cisplatin nephrotoxicity model. By using GEO2R, 965 differential expression genes (DEGs) in GSE121190 and 930 DEGs in GSE35257 were identified. 43 co-DEGs were shared and were extracted for protein-protein interaction (PPI) analysis. Subsequently, three shared pathways including glycolysis/gluconeogenesis, fatty acid degradation and pathways in cancer were involved in two models with Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis. We reconfirmed that these three pathways have relatively high scores by using Gene Set Enrichment Analysis (GSEA) software. Additionally, further bioinformatic analysis showed that Aldehyde dehydrogenase-2 (Aldh2) involved in the progression of renal fibrosis by mediating glycolysis pathway. Then real-time PCR and western blotting were performed to validate the expression of Aldh2 in kidney tissue after three different etiologies that caused renal fibrosis. Basically consistent with our bioinformatics results, our experiment showed that the expression of Aldh2 is the most significantly decreased in the UUO model, followed by ischemia-reperfusion injury (IRI) model and finally the cisplatin-induced model. Thus, Aldh2 can act as a common potential genetic target for different renal fibrosis diseases.;Simin Tang and Teng Huang and Huan Jing and Zhenxing Huang and Hongtao Chen and Youling Fan and Jiying Zhong and Jun Zhou;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Medicine (miscellaneous) (Q1); Pharmacology, Toxicology and Pharmaceutics (miscellaneous) (Q1)";1883.0;466.0;United States;1962-2020;10.1016/j.lfs.2019.117015;0.02358;164.0;;00243205;18790631;00243205;18790631;5.037;Unilateral ureteral obstruction, Cisplatin, Ischemia-reperfusion injury, Renal fibrosis, Bioinformatics, Aldh2, Glycolysis pathway;Elsevier Inc.;;6196.0;Northern America;1131.0;Q1;20473.0;Life sciences;Aldehyde dehydrogenase-2 acts as a potential genetic target for renal fibrosis;33802.0;8645.0;1283.0;1885.0;79494.0;journal;article;2019
Seabed sediment classification has significance for the utilization of marine resources and marine scientific research. Currently, the multibeam echo sounder (MBES) is increasingly becoming the tool of choice for large-scale seabed sediment classification. To further explore the technology of seabed sediment classification, this paper proposes a new classification method. In addition to backscatter mosaic, the method also integrates three other different types of features, including texture features of backscatter mosaic, MBES bathymetry features, and backscatter angular response (AR) features, which are given different weights in the classification process. First, geographically weighted regression (GWR) analysis is performed between different types of features and seabed sediment types, and the normalized coefficient of determination (R2) is employed as the weight coefficient for the different types of features. Second, the backscatter mosaic is combined with features from different types to predict the seabed sediment types using a deep neural network (DNN) classifier. Third, the classification residuals of the features from these three different types are acquired through the above classification results. Last, the classification residuals of features from different types are added to the classification results of the backscatter mosaic according to the weights, thereby achieving seabed sediment classification based on MBES multifeatures with different weights. The results show that the overall classification accuracy of the seabed sediments can be significantly improved from 88.98%/85.14% to 93.43% when using the DNN classification model based on MBES multifeatures with different weights compared with the other two models (DNN classification model based on MBES multifeatures with equal weights and DNN classification model based on principal component analysis (PCA) dimensionality reduction). The kappa coefficient can also be significantly improved from approximately 0.85/0.80 to 0.91. Via analysis, the proposed method can reasonably assign the weights of the different features and take advantage of integrating MBES multifeatures for seabed sediment classification. This approach also provides an important reference for future research on seabed sediment classification.;Zhengren Zhu and Xiaodong Cui and Kai Zhang and Bo Ai and Bo Shi and Fanlin Yang;"Geochemistry and Petrology (Q1); Geology (Q1); Oceanography (Q1)";479.0;332.0;Netherlands;1964-2020;10.1016/j.margeo.2021.106519;0.00799;134.0;;00253227;00253227;00253227;00253227;3.548;MBES, Backscatter mosaic, Angular response, Seabed sediment classification, Deep neural networks, Weight coefficient;Elsevier;;7917.0;Western Europe;1236.0;Q1;27847.0;Marine geology;Dnn-based seabed classification using differently weighted mbes multifeatures;16994.0;1843.0;217.0;490.0;17180.0;journal;article;2021
There is considerable scientific and societal concern about plastic pollution, which has resulted in citizen science projects to study the scale of the issue. Citizen science is a cost-effective way to gather data over a large geographical range while simultaneously raising public awareness on the problem. Because the experiences of researchers involved in these projects are not yet adequately covered, this paper presents the findings from ten semi-structured qualitative interviews with researchers leading a citizen science project on micro- or macroplastics. Our results show it is important to specify the goal(s) of the project and that expertise on communication and data science is needed. Furthermore, simple protocols, quality control, and engagement with volunteers and the public are key elements for successful projects. From these results, a framework with recommendations was drafted, which can be used by anyone who wants to develop or improve citizen science projects.;Liselotte Rambonnet and Suzanne C. Vink and Anne M. Land-Zandstra and Thijs Bosker;"Aquatic Science (Q1); Oceanography (Q1); Pollution (Q1)";2512.0;554.0;United Kingdom;1970-2020;10.1016/j.marpolbul.2019.05.056;0.03696;179.0;;0025326X;0025326X;18793363;0025326X;5.553;Citizen science, Microplastics, Macroplastics, Global, Plastic pollution;Elsevier Inc.;;6041.0;Western Europe;1548.0;Q1;24024.0;Marine pollution bulletin;Making citizen science count: best practices and challenges of citizen science projects on plastics in aquatic environments;48810.0;14671.0;1039.0;2537.0;62770.0;journal;article;2019
With the wide adoption of functional magnetic resonance imaging (fMRI) by cognitive neuroscience researchers, large volumes of brain imaging data have been accumulated in recent years. Aggregating these data to derive scientific insights often faces the challenge that fMRI data are high-dimensional, heterogeneous across people, and noisy. These challenges demand the development of computational tools that are tailored both for the neuroscience questions and for the properties of the data. We review a few recently developed algorithms in various domains of fMRI research: fMRI in naturalistic tasks, analyzing full-brain functional connectivity, pattern classification, inferring representational similarity and modeling structured residuals. These algorithms all tackle the challenges in fMRI similarly: they start by making clear statements of assumptions about neural data and existing domain knowledge, incorporate those assumptions and domain knowledge into probabilistic graphical models, and use those models to estimate properties of interest or latent structures in the data. Such approaches can avoid erroneous findings, reduce the impact of noise, better utilize known properties of the data, and better aggregate data across groups of subjects. With these successful cases, we advocate wider adoption of explicit model construction in cognitive neuroscience. Although we focus on fMRI, the principle illustrated here is generally applicable to brain data of other modalities.;Ming Bo Cai and Michael Shvartsman and Anqi Wu and Hejia Zhang and Xia Zhu;"Behavioral Neuroscience (Q1); Cognitive Neuroscience (Q1); Experimental and Cognitive Psychology (Q1)";1023.0;303.0;United Kingdom;1963-2020;10.1016/j.neuropsychologia.2020.107500;0.02238;206.0;;00283932;18733514;00283932;18733514;3.139;Probabilistic graphical model, Bayesian, fMRI, Cognitive neuroscience, Big data, Factor model, Matrix normal;Elsevier Ltd.;;8150.0;Western Europe;1439.0;Q1;18009.0;Neuropsychologia;Incorporating structured assumptions with probabilistic graphical models in fmri data analysis;28496.0;3257.0;333.0;1036.0;27141.0;journal;article;2020
"Background
Two decades ago, findings from an Institute of Medicine (IOM) report sparked the urgent need for evidence supporting relationships between nurse staffing and patient outcomes.
Purpose
This article provides an overview of nurse staffing, practice environment, and patient outcomes research, with an emphasis on findings from military studies. Lessons learned also are enumerated.
Method
This study is a review of the entire Military Nursing Outcomes Database (MilNOD) program of research.
Discussion
The MilNOD, in combination with evidence from other health care studies, provides nurses and leaders with information about the associations between staffing, patient outcomes, and the professional practice environment of nursing in the military. Leaders, therefore, have useful empirical evidence to make data-driven decisions. The MilNOD studies are the basis for the current Army nursing dashboard, and care delivery framework, called the Patent CaringTouch System.
Conclusion
Future research is needed to identify ideal staffing based on workload demands, and provide leaders with factors to consider when operationalizing staffing recommendations.";Patricia A. Patrician and Lori A. Loan and Mary S. McCarthy and Pauline Swiger and Sara Breckenridge-Sproat and Laura Ruse Brosch and Bonnie Mowinski Jennings;Nursing (miscellaneous) (Q1);240.0;237.0;United States;1953-2020;10.1016/j.outlook.2017.06.015;0.00323;57.0;;00296554;15283968;00296554;15283968;3.250;Nursing sensitive indicators, Outcomes, Practice environment, Staffing;Mosby Inc.;;3423.0;Northern America;953.0;Q1;29253.0;Nursing outlook;Twenty years of staffing, practice environment, and outcomes research in military nursing;2751.0;747.0;119.0;315.0;4073.0;journal;article;2017
In the maritime industry, more accurate predictions of fuel oil consumption (FOC) could yield multidimensional results including more precise bunker calculations, emission reductions, more informed planning and limiting operational costs. However, models often require sophisticated data that may be partially unavailable to operators beforehand. The present research aims to develop accurate main engine FOC forecasting models that utilize exclusively data from sensors and simple weather data readily available in operational practice. Commonly available sensor data from a Very Large Crude Oil Carrier (VLCC) were used, comprising speed through water, relative wind direction, relative wind speed, mean draft, trim, days since last drydock and laden or ballast vessel state. Multivariate Polynomial Regression (MPR), Artificial Neural Networks (ANNs) and eXtreme Gradient Boosting (XGBoost) regression models were developed and evaluated based on their predictive accuracy for VLCC FOC. Results indicated that XGBoost had the best performance, yielding predictions within 5% of the true value more than 86% of the total cases, followed by MPR and ANN. In addition, accurate aggregate FOC forecasting was conducted with XGBoost for a laden voyage and a ballast voyage of a VLCC.;Christos Papandreou and Apostolos Ziakopoulos;"Environmental Engineering (Q1); Ocean Engineering (Q1)";2469.0;431.0;United Kingdom;1968-2020;10.1016/j.oceaneng.2021.110321;0.0247;100.0;;00298018;00298018;00298018;00298018;3.795;Fuel oil consumption prediction, VLCC sensor Data, Machine learning, Polynomial regression, Artificial neural network, XGBoost regression;Elsevier BV;;4209.0;Western Europe;1321.0;Q1;28339.0;Ocean engineering;Predicting vlcc fuel consumption with machine learning using operationally available sensor data;23463.0;10979.0;1216.0;2475.0;51177.0;journal;article;2022
;Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh;Orthopedics and Sports Medicine (Q1);153.0;201.0;United Kingdom;1970-2020;10.1016/j.ocl.2016.05.009;0.0027300000000000002;86.0;;00305898;00305898;15581373;00305898;2.472;Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database;W.B. Saunders Ltd;;3751.0;Western Europe;1177.0;Q1;12469.0;Orthopedic clinics of north america;Big data, big problems: incorporating mission, values, and culture in provider affiliations;3693.0;437.0;61.0;186.0;2288.0;journal;article;2016
Referring relationship aims at localizing subject and object entities in an image, according to a triple text <subject, predicate, object>. Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.;Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou;"Artificial Intelligence (Q1); Computer Vision and Pattern Recognition (Q1); Signal Processing (Q1); Software (Q1)";1168.0;1035.0;United Kingdom;1968-2021;10.1016/j.patcog.2022.109044;0.03271;210.0;;00313203;00313203;00313203;00313203;7.740;Referring relationship, Multimodal learning, Image and text, Visual grounding, Deep learning;Elsevier Ltd.;;4738.0;Western Europe;1492.0;Q1;24823.0;Pattern recognition;Rethinking referring relationships from a perspective of mask-level relational reasoning;33363.0;12476.0;412.0;1180.0;19522.0;journal;article;2023
;Scott M. Sutherland and David C. Kaelber and N. Lance Downing and Veena V. Goel and Christopher A. Longhurst;Pediatrics, Perinatology and Child Health (Q1);253.0;227.0;United Kingdom;1953-2020;10.1016/j.pcl.2015.12.002;0.0047;85.0;;00313955;00313955;15578240;00313955;3.278;EHR, EMR, Electronic health record, Electronic medical record, Research, Clinical discovery, Children;W.B. Saunders Ltd;;4243.0;Western Europe;853.0;Q1;15719.0;Pediatric clinics of north america;Electronic health record–enabled research in children using the electronic health record for clinical discovery;4678.0;770.0;109.0;325.0;4625.0;journal;article;2016
I engage the philosophy of geology, and the philosophy of planetary geomorphology in particular, offering a perspective from the standpoint of a Mars geographer who is acquainted with both experimental and social research. I discuss the nature of geological reasoning and probe its somewhat paradoxical commitment to both a physical realism and to interpretative narrative construction, engaging particularly with how the interplay of these two factors are addressed in the works of the philosopher-geologist Victor Baker. I also draw attention to specific characteristics of knowledge construction in Martian geomorphology which systemically limit the attachment of our inferences to a physical reality. For instance I highlight a geospatial-geochemical dimorphism in the semantic content of available planetary data and several non-scientific controls on the knowledge construction process, including mission safety constraints and sociopolitical factors. Lastly, I find that physical geography has grown apart from planetary geomorphology in the decades since planetary geomorphology assumed its largely geological modern form. An academic siloing effect has followed suit. An immediate implication is that this contours the typical pathways of knowledge construction in our discipline, and I try to call attention to a few examples of this effect. As an effort to bridge this gap, I highlight the recent developments of the physical geographical subdisciplines of GIScience and critical physical geography. These disciplines offer opportunities for insight which might be fruitful for planetary geomorphologists, but their lines of inquiry have largely passed beneath our community’s radar. Specifically, in offering examples of what discourse in a critical planetary geology might look like, I hope to encourage the informal philosophical discussions already circulating in our community to pass into a formal space of debate.;Andrew G. Siwabessy;"Astronomy and Astrophysics (Q2); Space and Planetary Science (Q2)";464.0;235.0;United Kingdom;1959-2020;10.1016/j.pss.2020.105121;0.0069099999999999995;92.0;;00320633;00320633;00320633;00320633;2.030;Philosophy of geology, Philosophy of geography, Sociology of geology, Epistemology of geology, Critical planetary geology;Elsevier Ltd.;;5154.0;Western Europe;696.0;Q2;27773.0;Planetary and space science;Physical reality in planetary geomorphological inference and the pathway to a critical planetary geology;8285.0;1158.0;258.0;474.0;13297.0;journal;article;2021
"ABSTRACT
In broiler production, there is a continuous effort to breed feed efficient chickens. Residual feed intake (RFI) is an accurate indicator that has been accepted as an alternative measure of the conventional feed conversion ratio. This study conducted a duodenal transcriptome survey to explore the molecular basis of broiler RFI. Results showed that there are 599 genes that were differentially expressed (DE) in the duodenum between high RFI and low RFI (LRFI) broilers. Functional analysis showed that RFI can be explained by differences in the regulation of the immune system process, complement activation, nutrient digestion, and absorption pathways. Among those processes, the glutathione S-transferase family and serpin family are involved in glutathione metabolism and TGF-β signaling. These genes are involved in complement and coagulation cascade pathways that constitute a new regulatory network to reduce oxidative stress and inflammatory reaction, as well as to improve the defense capability in LRFI broilers. Ten DE genes related to the digestive tract health and digestive function, CCK, MPEG1, EPHB2, SERPINH1, VANGL2, CYFIP2, PCDH19, TGFBI, SCUBE3and CATHL1, were identified as candidate genes related to RFI. In conclusion, the results indicate that there is less oxidative stress, less inflammatory reactions, and better digestion and absorption in the duodenum of the LRFI broilers, which might result in improved intestinal health and contribute to an increase in the efficiency of feed conversion.";Ranran Liu and Jie Liu and Guiping Zhao and Wei Li and Maiqing Zheng and Jie Wang and Qinghe Li and Huanxian Cui and Jie Wen;"Animal Science and Zoology (Q1); Medicine (miscellaneous) (Q1)";1796.0;320.0;United States;1965-2020;10.3382/ps/pey506;0.01403;141.0;;00325791;15253171;00325791;15253171;3.352;RFI, broiler, intestinal health, transcriptome;Elsevier Inc.;;4357.0;Northern America;1072.0;Q1;36431.0;Poultry science;Relevance of the intestinal health-related pathways to broiler residual feed intake revealed by duodenal transcriptome profiling;30896.0;6073.0;773.0;1804.0;33681.0;journal;article;2019
This paper presents a cost-effective method to optimize hydrocyclones used for particle separation. It integrates a mechanistic model for data generation with data-driven models for prediction and optimization. The mechanistic model is based on a validated two-fluid model (TFM), and the data-driven models are the artificial neural network (ANN) and non-dominated sorting genetic algorithm II (NSGA-II). In this integration, the response surface methodology (RSM), coupled with the steepest ascent, is used to design the numerical experiments based on the TFM, aiming to achieve reliable prediction through limited numerical experiments or training data. The applicability of the proposed method is demonstrated by multi-variable and multi-objective optimization of hydrocyclone geometry to achieve low pressure drop and accurate separation, especially for fine particles. The optimization result is elucidated using the multiphase flows predicted by the TFM.;Qing Ye and Peibo Duan and Shibo Kuang and Li Ji and Ruiping Zou and Aibing Yu;Chemical Engineering (miscellaneous) (Q1);2706.0;513.0;Netherlands;1967-2021;10.1016/j.powtec.2022.117674;0.03085;137.0;;00325910;1873328X;00325910;1873328X;5.134;Hydrocyclone, Two-fluid model, Steepest ascent, Artificial neural network, Multi-objective optimization;Elsevier;;4520.0;Western Europe;1079.0;Q1;13717.0;Powder technology;Multi-objective optimization of hydrocyclone by combining mechanistic and data-driven models;42052.0;14082.0;1100.0;2716.0;49721.0;journal;article;2022
;Chayakrit Krittanawong and Scott Kaplin and W.H. Wilson Tang and Hani Jneid and Salim S. Virani and Franz H. Messerli;Cardiology and Cardiovascular Medicine (Q1);227.0;587.0;United Kingdom;1958-2020;10.1016/j.pcad.2021.07.006;0.00687;100.0;;00330620;18731740;00330620;15328643;8.194;Social media, DASH, DASH diet, Social media analysis, Facebook, Twitter;W.B. Saunders Ltd;;6112.0;Western Europe;1929.0;Q1;20552.0;Progress in cardiovascular diseases;Social media and predictive analysis regarding dietary approaches to stop hypertension;5799.0;1484.0;109.0;258.0;6662.0;journal;article;2021
;P. Mackie and F. Sim and C. Johnman;"Medicine (miscellaneous) (Q2); Public Health, Environmental and Occupational Health (Q2)";788.0;210.0;Netherlands;1888-1913, 1915-2020;10.1016/j.puhe.2015.02.013;0.01222;75.0;;00333506;00333506;14765616;00333506;2.427;;Elsevier;;2746.0;Western Europe;826.0;Q2;17697.0;Public health;Big data! big deal?;7603.0;2120.0;433.0;874.0;11891.0;journal;article;2015
The upcoming Fluorescence Explorer (FLEX) satellite mission aims to provide high quality radiometric measurements for subsequent retrieval of sun-induced chlorophyll fluorescence (SIF). The combination of SIF with other observations stemming from the FLEX/Sentinel-3 tandem mission holds the potential to assess complex ecosystem processes. The calibration and validation (cal/val) of these radiometric measurements and derived products are central but challenging components of the mission. This contribution outlines strategies for the assessment of in situ radiometric measurements and retrieved SIF. We demonstrate how in situ spectrometer measurements can be analysed in terms of radiometric, spectral and spatial uncertainties. The analysis of more than 200 k spectra yields an average bias between two radiometric measurements by two individual spectrometers of 8%, with a larger variability in measurements of downwelling radiance (25%) compared to upwelling radiance (6%). Spectral shifts in the spectrometer relevant for SIF retrievals are consistently below 1 spectral pixel (up to 0.75). Found spectral shifts appear to be mostly dependent on temperature (as measured by a temperature probe in the instrument). Retrieved SIF shows a low variability of 1.8% compared with a noise reduced SIF estimate based on APAR. A combination of airborne imaging and in situ non-imaging fluorescence spectroscopy highlights the importance of a homogenous sampling surface and holds the potential to further uncover SIF retrieval issues as here shown for early evening acquisitions. Our experiments clearly indicate the need for careful site selection, measurement protocols, as well as the need for harmonized processing. This work thus contributes to guiding cal/val activities for the upcoming FLEX mission.;Bastian Buman and Andreas Hueni and Roberto Colombo and Sergio Cogliati and Marco Celesti and Tommaso Julitta and Andreas Burkart and Bastian Siegmann and Uwe Rascher and Matthias Drusch and Alexander Damm;"Computers in Earth Sciences (Q1); Geology (Q1); Soil Science (Q1)";1374.0;1069.0;United States;1969-2020;10.1016/j.rse.2022.112984;0.0534;281.0;;00344257;00344257;00344257;00344257;10.164;Sun-induced chlorophyll fluorescence, Spectroradiometer, Uncertainty, Bias, Measurement variability, Spectral shift, FLEX, FloX;Elsevier Inc.;;7598.0;Northern America;3611.0;Q1;12503.0;Remote sensing of environment;Towards consistent assessments of in situ radiometric measurements for the validation of fluorescence satellite missions;73581.0;16246.0;496.0;1374.0;37684.0;journal;article;2022
Petrographic analysis based on microfacies identification in thin sections is widely used in sedimentary environment interpretation and paleoecological reconstruction. Fossil recognition from microfacies is an essential procedure for petrographers to complete this task. Distinguishing the morphological and microstructural diversity of skeletal fragments requires extensive prior knowledge of fossil morphotypes in microfacies and long training sessions under the microscope. This requirement engenders certain challenges for sedimentologists and paleontologists, especially novices. However, a machine classifier can help address this challenge. In this study, we collected a microfacies image dataset comprising both public data from 1133 references and our own materials (including a total of 30,815 images of 22 fossil and abiotic grain groups). We employed a high-performance workstation to implement four classic deep convolutional neural networks, which have proven to be highly efficient in computer vision. Our framework uses a transfer learning technique, which reuses the pre-trained parameters that are trained on a larger ImageNet dataset as initialization for the network to achieve high accuracy with low computing costs. We obtained up to 95% of the top one and 99% of the top three test accuracies in the Inception ResNet v2 architecture. The machine classifier exhibited 0.99 precision on minerals such as dolomite and pyrite. Although it had some difficulty on samples having similar morphologies, such as bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88 precision. Our machine learning framework demonstrates high accuracy with reproducibility and bias avoidance that is comparable to those of human classifiers. Its application can thus eliminate much of the tedious, manually intensive efforts by human experts conducting routine identification.;Xiaokang Liu and Haijun Song;"Geology (Q1); Stratigraphy (Q1)";401.0;338.0;Netherlands;1967-2020;10.1016/j.sedgeo.2020.105790;0.00845;113.0;;00370738;00370738;00370738;00370738;3.397;Microfossils, Minerals, Sedimentary structures, Machine learning, Transfer learning;Elsevier;;9285.0;Western Europe;1234.0;Q1;26319.0;Sedimentary geology;Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks;14860.0;1466.0;127.0;403.0;11792.0;journal;article;2020
The high-speed rail (HSR) of China has developed and expanded rapidly and made great achievements in the past twenty years. The ongoing HSR plan is expected to have a significant impact on the urban economy and spatial structure in China. However, relevant data-driven research is still lacking. Traditional data collection approaches such as field surveys are costly to assure the accuracy of materials. In this study, a new remote sensing perspective of night-time light (NTL) was adopted to observe the long-term impact of the HSR on cities along the rail. More specifically, we investigated the impact of the Beijing–Guangzhou High-Speed Railway (BGHSR) on urban economic development by using night-time light data from 2002 to 2018. Such a line connects the capital (located in the north of China) and southern China and lies on the most important geographic axis of the country. Our results find that the construction of BGHSR line has a considerable positive impact on economies of first-tier cities (e.g., Beijing and Guangzhou) and new-first-tier cities (e.g., Zhengzhou, Wuhan, and Changsha), but also hurt some second-tier and third-tier cities such as Baoding and Handan. Generally, the spatial economic pattern of cities along the BGHSR line has been rapidly reshaped with the change of the transportation system. Each city needs to reconsider its role and value in the coming regionalization process to adapt to the national strategy.;Yunxiang Guo and Wenhao Yu and Zhanlong Chen and Renwei Zou;"Geography, Planning and Development (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1); Economics and Econometrics (Q2); Statistics, Probability and Uncertainty (Q2)";137.0;509.0;United Kingdom;1967-2020;10.1016/j.seps.2020.100905;0.00155;52.0;;00380121;00380121;00380121;00380121;4.923;High-speed rail, Urban economic development, Nighttime light image, Urban agglomeration, China;Elsevier Ltd.;;5721.0;Western Europe;1020.0;Q1;18291.0;Socio-economic planning sciences;Impact of high-speed rail on urban economic development: an observation from the beijing-guangzhou line based on night-time light images;2347.0;706.0;200.0;141.0;11441.0;journal;article;2020
"Text mining is an emerging topic that advances the review of academic literature. This paper presents a preliminary study on how to review solar irradiance and photovoltaic (PV) power forecasting (both topics combined as “solar forecasting” for short) using text mining, which serves as the first part of a forthcoming series of text mining applications in solar forecasting. This study contains three main contributions: (1) establishing the technological infrastructure (authors, journals & conferences, publications, and organizations) of solar forecasting via the top 1000 papers returned by a Google Scholar search; (2) consolidating the frequently-used abbreviations in solar forecasting by mining the full texts of 249 ScienceDirect publications; and (3) identifying key innovations in recent advances in solar forecasting (e.g., shadow camera, forecast reconciliation). As most of the steps involved in the above analysis are automated via an application programming interface, the presented method can be transferred to other solar engineering topics, or any other scientific domain, by means of changing the search word. The authors acknowledge that text mining, at its present stage, serves as a complement to, but not a replacement of, conventional review papers.";Dazhi Yang and Jan Kleissl and Christian A. Gueymard and Hugo T.C. Pedro and Carlos F.M. Coimbra;"Materials Science (miscellaneous) (Q1); Renewable Energy, Sustainability and the Environment (Q1)";2963.0;595.0;United Kingdom;1957-2020;10.1016/j.solener.2017.11.023;0.03982;181.0;;0038092X;0038092X;0038092X;0038092X;5.742;Text mining, Solar forecasting, Review, Photovoltaics;Elsevier Ltd.;;5101.0;Western Europe;1337.0;Q1;13333.0;Solar energy;History and trends in solar irradiance and pv power forecasting: a preliminary assessment and review using text mining;47228.0;17672.0;1161.0;2977.0;59226.0;journal;article;2018
Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.;Robert Northcott;"History (Q1); History and Philosophy of Science (Q1)";134.0;122.0;United Kingdom;1970-1971, 1974-1978, 1980-1981, 1983-1986, 1988, 1990-2020;10.1016/j.shpsa.2019.09.002;;37.0;;00393681;00393681;18792510;00393681;;Big data, Prediction, Case studies, Explanation, Elections, Weather;Elsevier Ltd.;;5317.0;Western Europe;615.0;Q1;11600154632.0;Studies in history and philosophy of science part a;Big data and prediction: four case studies;;229.0;95.0;150.0;5051.0;journal;article;2020
"The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.";Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo;Surgery (Q1);1108.0;240.0;United States;1937-2020;10.1016/j.surg.2018.06.022;0.02493;162.0;;00396060;15327361;00396060;15327361;3.982;;Mosby Inc.;;2205.0;Northern America;1532.0;Q1;22305.0;Surgery;Big data: more than big data sets;25223.0;4267.0;487.0;1520.0;10740.0;journal;article;2018
Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.;Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens;Ophthalmology (Q1);199.0;475.0;United States;1956-1970, 1972-2020;10.1016/j.survophthal.2016.01.003;0.00487;132.0;;00396257;00396257;18793304;00396257;6.048;data linkage, clinical registry, health services research, ophthalmic epidemiology, big data;Elsevier USA;;10132.0;Northern America;2131.0;Q1;20107.0;Survey of ophthalmology;Big data and ophthalmic research;7296.0;1131.0;69.0;235.0;6991.0;journal;article;2016
Standardisation is often linked to the very existence of an industry and its evolutionary dynamics especially in sectors where activities are particularly reliant on network economies. This paper investigates the interests of the stakeholders of the materials industry in the effort of setting the basis for standard-compliant formats for engineering materials test data used in materials test certificates (TCs). Test certificates provide a guarantee of origin, characteristics and materials specifications. A transition to standardised electronic-test certificates (E-TCs) is thought to offer advantages over the static alternatives currently in use. This work highlights that the stakeholders involved in the process have a general need for a standard compliant data model yet their particular interests might not be completely aligned. The choice of an open, non-proprietary, data format and compatibility of the standard with legacy technological solutions prove to be critical for the implementation of standardised E-TCs.;Dimitri Gagliardi;"Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)";1061.0;901.0;United States;1970-2020;10.1016/j.techfore.2015.09.015;0.02416;117.0;;00401625;00401625;00401625;00401625;8.593;Innovation in materials data models, Standardisation, Materials data, Engineering materials;Elsevier Inc.;;7942.0;Northern America;2226.0;Q1;14704.0;Technological forecasting and social change;Material data matter — standard data format for engineering materials;21116.0;10127.0;448.0;1099.0;35581.0;journal;article;2015
"Summary
The important medical and social burden of nervous system diseases contrasts with the currently limited therapeutic armamentarium and with the difficulty encountered in developing new therapeutic options. These failures can be explained by the conjunction of various phenomena related to the limitations of animal models, the narrow focus of research on precise pathophysiological mechanisms, and methodological issues in clinical trials. It is perhaps the paradigm itself of the way research is conducted that may be the real reason for our incapacity to find effective strategies. The purpose of this workshop was to define overall lines of research that could lead to the development of effective novel therapeutic solutions. Research has long focused on diseases per se rather than on cognitive and behavioural dimensions common to several diseases. Their expression is often partial and variable, but can today be well-characterised using neurophysiological or imaging methods. This dimensional or syndromic vision should enable a new insight to the question, taking a transnosographic approach to re-position research and to propose: translational models exploring the same functions in animal models and in humans; identification of homogeneous groups of patients defined according to the clinical, anatomico-functional and molecular characteristics; and preclinical and clinical developments enriched by the use of cognitive-behavioural, biological neurological, and imaging biomarkers. For this mutation to be successful, it must be accompanied by synchronised action from the public authorities and by ad hoc measures from the regulatory agencies.";Jean-Christophe Corvol and Sylvia Goni and Régis Bordet and Carole Azuar and Olivier Blin and Frédéric Checler and Denis J. David and Franck Durif and Pierre-Olivier Fernagut and Julien Dupouey and Lisa Otten and Raphaël Gaillard and Marie-Louise Kemel and Joëlle Micallef and Marie-Christine Perault-Pochat and Anne-Lise Pitel and Philippe Truffinet;Pharmacology (medical) (Q3);192.0;140.0;France;1947-1948, 1950-2020;10.1016/j.therap.2016.01.001;0.0010199999999999999;35.0;;00405957;00405957;19585578;00405957;2.070;Neurology, Psychiatrics, Cognition, Behaviour, Pharmacology, Medications, Medical devices;Elsevier Masson SAS;;2999.0;Western Europe;387.0;Q3;20132.0;Therapie;Translational research on cognitive and behavioural disorders in neurological and psychiatric diseases;1069.0;338.0;117.0;261.0;3509.0;journal;article;2016
Many studies of nanomaterials make non-systematic alterations of nanoparticle physicochemical properties. Given the immense size of the property space for nanomaterials, such approaches are not very useful in elucidating fundamental relationships between inherent physicochemical properties of these materials and their interactions with, and effects on, biological systems. Data driven artificial intelligence methods such as machine learning algorithms have proven highly effective in generating models with good predictivity and some degree of interpretability. They can provide a viable method of reducing or eliminating animal testing. However, careful experimental design with the modelling of the results in mind is a proven and efficient way of exploring large materials spaces. This approach, coupled with high speed automated experimental synthesis and characterization technologies now appearing, is the fastest route to developing models that regulatory bodies may find useful. We advocate greatly increased focus on systematic modification of physicochemical properties of nanoparticles combined with comprehensive biological evaluation and computational analysis. This is essential to obtain better mechanistic understanding of nano-bio interactions, and to derive quantitatively predictive and robust models for the properties of nanomaterials that have useful domains of applicability.;Xue Bai and Fang Liu and Yin Liu and Cong Li and Shenqing Wang and Hongyu Zhou and Wenyi Wang and Hao Zhu and David A. Winkler and Bing Yan;"Toxicology (Q1); Pharmacology (Q2)";928.0;391.0;United States;1959-2020;10.1016/j.taap.2017.03.011;0.0119;170.0;;0041008X;10960333;0041008X;10960333;4.219;;Academic Press Inc.;;5420.0;Northern America;1017.0;Q1;25222.0;Toxicology and applied pharmacology;Toward a systematic exploration of nano-bio interactions;23426.0;3701.0;302.0;932.0;16368.0;journal;article;2017
Instrumentation, control and automation (ICA) are currently applied throughout the urban water system at water treatment plants, in water distribution networks, in sewer networks, and at wastewater treatment plants. However, researchers and practitioners specialising in respective urban water sub-systems do not frequently interact, and in most cases to date the application of ICA has been achieved in silo. Here, we review start-of-the-art ICA throughout these sub-systems, and discuss the benefits achieved in terms of performance improvement, cost reduction, and more importantly, the enhanced capacity of the existing infrastructure to cope with increased service demand caused by population growth and continued urbanisation. We emphasise the importance of integrated control within each of the sub-systems, and also across the entire urban water system. System-wide ICA will have increasing importance with the growing complexity of the urban water environment in cities of the future.;Zhiguo Yuan and Gustaf Olsson and Rachel Cardell-Oliver and Kim {van Schagen} and Angela Marchi and Ana Deletic and Christian Urich and Wolfgang Rauch and Yanchen Liu and Guangming Jiang;"Civil and Structural Engineering (Q1); Ecological Modeling (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1); Water Science and Technology (Q1)";2588.0;1132.0;United Kingdom;1967-2020;10.1016/j.watres.2019.02.034;0.07877999999999999;303.0;;00431354;00431354;18792448;00431354;11.236;Distribution, ICA, IUWM, Sewer, Water supply, WWTP;Elsevier Ltd.;;6141.0;Western Europe;3099.0;Q1;18795.0;Water research;Sweating the assets – the role of instrumentation, control and automation in urban water systems;120695.0;29848.0;1142.0;2603.0;70129.0;journal;article;2019
The scientific research on aquaculture efficiency and productivity has been increasing over the years. This study aims to identify the publication trends and growth potential of aquaculture efficiency and productivity studies. A bibliometric analysis was employed for a sample of 85 scientific articles published during the 1998–2020 period. The findings show that authors and institutions have close groups in collaboration networks. Through the citation analysis, three clusters were obtained that were related to the use of stochastic frontier analysis in an empirical application, Norwegian salmon aquaculture, and efficiency studies associated to freshwater aquaculture. For the temporal evolution of the keywords, earlier existing studies adopted a stochastic translog production function to assess the technical efficiency of aquaculture production, whereas later studies used data envelopment analysis, which focused on more diverse research objectives. The farms or subsectors of aquaculture in Norway, Bangladesh, and Vietnam have been analyzed in-depth for the efficiency and productivity in the existing studies. Education, experience, and age of farmers are often used as determinants to explain the variations in technical efficiency. The present study concludes that aquaculture efficiency and productivity research is not moving toward a mature stage. Several of the discovered issues are only focused on specific countries, and there is still room for methodological improvement in assessing aquaculture efficiency and productivity. Nevertheless, research collaborations are growing, and new research trends that are related to environmental regulation and pollution show great interest in aquaculture efficiency and productivity. This study provides a clear roadmap for researchers and practitioners to understand the publication patterns and hotspots in the research field.;Kok Fong See and Rabiatul Adawiyah Ibrahim and Kim Huat Goh;Aquatic Science (Q1);2099.0;409.0;Netherlands;1972-2021;10.1016/j.aquaculture.2021.736881;0.02071;176.0;;00448486;00448486;00448486;00448486;4.242;Aquaculture, Bibliometric analysis, Data envelopment analysis, Efficiency, Productivity, Stochastic frontier analysis;Elsevier;;5649.0;Western Europe;1066.0;Q1;29419.0;Aquaculture;Aquaculture efficiency and productivity: a comprehensive review and bibliometric analysis;52908.0;9000.0;1084.0;2108.0;61237.0;journal;article;2021
"It is well known benzene negatively impacts human health. This study is the first to predict spatial-temporal variations in benzene concentrations for the entirety of Taiwan by using a mixed spatial prediction model integrating multiple machine learning algorithms and predictor variables selected by Land-use Regression (LUR). Monthly benzene concentrations from 2003 to 2019 were utilized for model development, and monthly benzene concentration data from 2020, as well as mobile monitoring vehicle data from 2009 to 2019, served as external data for verifying model reliability. Benzene concentrations were estimated by running six LUR-based machine learning algorithms; these algorithms, which include random forest (RF), deep neural network (DNN), gradient boosting (GBoost), light gradient boosting (LightGBM), CatBoost, extreme gradient boosting (XGBoost), and ensemble algorithms (a combination of the three best performing models), can capture how nonlinear observations and predictions are related. The results indicated conventional LUR captured 79% of the variability in benzene concentrations. Notably, the LUR with ensemble algorithm (GBoost, CatBoost, and XGBoost) surpassed all other integrated methods, increasing the explanatory power to 92%. This study establishes the value of the proposed ensemble-based model for estimating spatiotemporal variation in benzene exposure.";Chin-Yu Hsu and Hong-Xin Xie and Pei-Yi Wong and Yu-Cheng Chen and Pau-Chung Chen and Chih-Da Wu;"Chemistry (miscellaneous) (Q1); Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Public Health, Environmental and Occupational Health (Q1)";6428.0;704.0;United Kingdom;1972-2021;10.1016/j.chemosphere.2022.134758;0.0979;248.0;;00456535;00456535;00456535;00456535;7.086;Benzene, Land-use regression (LUR), Machine learning, Mixed spatial prediction;Elsevier Ltd.;;5709.0;Western Europe;1632.0;Q1;24657.0;Chemosphere;A mixed spatial prediction model in estimating spatiotemporal variations in benzene concentrations in taiwan;127067.0;46391.0;3039.0;6460.0;173496.0;journal;article;2022
We formulate a Data Driven Computing paradigm, termed max-ent Data Driven Computing, that generalizes distance-minimizing Data Driven Computing and is robust with respect to outliers. Robustness is achieved by means of clustering analysis. Specifically, we assign data points a variable relevance depending on distance to the solution and on maximum-entropy estimation. The resulting scheme consists of the minimization of a suitably-defined free energy over phase space subject to compatibility and equilibrium constraints. Distance-minimizing Data Driven schemes are recovered in the limit of zero temperature. We present selected numerical tests that establish the convergence properties of the max-ent Data Driven solvers and solutions.;T. Kirchdoerfer and M. Ortiz;"Computational Mechanics (Q1); Computer Science Applications (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1); Physics and Astronomy (miscellaneous) (Q1)";1512.0;756.0;Netherlands;1972-2020;10.1016/j.cma.2017.07.039;0.04197;198.0;;00457825;00457825;03742830;00457825;6.756;Data science, Big data, Approximation theory, Scientific computing;Elsevier;;5559.0;Western Europe;2530.0;Q1;18158.0;Computer methods in applied mechanics and engineering;Data driven computing with noisy material data sets;40986.0;11793.0;685.0;1516.0;38081.0;journal;article;2017
;Spyros Sioutas and Yannis Velegrakis and Valia Kordoni;"Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)";979.0;479.0;United Kingdom;1973-1984, 1986-2020;10.1016/j.compeleceng.2018.07.033;;64.0;;00457906;00457906;00457906;00457906;;;Elsevier Ltd.;;2505.0;Western Europe;630.0;Q1;18159.0;Computers and electrical engineering;Introduction to the special section on new trends in humanistic informatics: implementations and applications;;4626.0;298.0;1040.0;7464.0;journal;article;2018
Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components – (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-timescale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the ‘mixture of experts using GP’, an efficient framework that exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.;S. Chakraborty and S. Adhikari;"Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Materials Science (miscellaneous) (Q1); Mechanical Engineering (Q1); Modeling and Simulation (Q1)";465.0;487.0;United Kingdom;1971-2021;10.1016/j.compstruc.2020.106410;;138.0;;00457949;00457949;00457949;00457949;;Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process, Frequency;Elsevier Ltd.;;4954.0;Western Europe;1450.0;Q1;18171.0;Computers and structures;Machine learning based digital twin for dynamical systems with multiple time-scales;;2256.0;135.0;469.0;6688.0;journal;article;2021
"Summary
Evidence-based pathology advocates using a combination of best available data (“evidence”) from the literature and personal experience for the diagnosis, estimation of prognosis, and assessment of other variables that impact individual patient care. Evidence-based pathology relies on systematic reviews of the literature, evaluation of the quality of evidence as categorized by evidence levels and statistical tools such as meta-analyses, estimates of probabilities and odds, and others. However, it is well known that previously “statistically significant” information usually does not accurately forecast the future for individual patients. There is great interest in “cognitive computing” in which “data mining” is combined with “predictive analytics” designed to forecast future events and estimate the strength of those predictions. This study demonstrates the use of IBM Watson Analytics software to evaluate and predict the prognosis of 101 patients with typical and atypical pulmonary carcinoid tumors in which Ki-67 indices have been determined. The results obtained with this system are compared with those previously reported using “routine” statistical software and the help of a professional statistician. IBM Watson Analytics interactively provides statistical results that are comparable to those obtained with routine statistical tools but much more rapidly, with considerably less effort and with interactive graphics that are intuitively easy to apply. It also enables analysis of natural language variables and yields detailed survival predictions for patient subgroups selected by the user. Potential applications of this tool and basic concepts of cognitive computing are discussed.";Alberto M. Marchevsky and Ann E. Walts and Mark R. Wick;Pathology and Forensic Medicine (Q1);838.0;293.0;United Kingdom;1970-2020;10.1016/j.humpath.2016.09.002;0.01257;139.0;;00468177;15328392;00468177;15328392;3.466;Evidence-based, Pathology, Cognitive computing, Predictive analytics, IBM Watson;W.B. Saunders Ltd;;4179.0;Western Europe;1213.0;Q1;14024.0;Human pathology;Evidence-based pathology in its second decade: toward probabilistic cognitive computing;15964.0;2727.0;137.0;929.0;5725.0;journal;article;2017
"Big data, high dimensional data, sparse data, large scale data, and imaging data are all becoming new frontiers of statistics. Changing technologies have created this flood and have led to a real hunger for new modeling strategies and data analysis by scientists. In many cases data are not Euclidean; for example, in molecular biology, the data sit on manifolds. Even in a simple non-Euclidean manifold (circle), to summarize angles by the arithmetic average cannot make sense and so more care is needed. Thus non-Euclidean settings throw up many major challenges, both mathematical and statistical. This paper will focus on the PCA and clustering methods for some manifolds. Of course, the PCA and clustering methods in multivariate analysis are one of the core topics. We basically deal with two key manifolds from a practical point of view, namely spheres and tori. It is well known that dimension reduction on non-Euclidean manifolds with PCA-like methods has been a challenging task for quite some time but recently there has been some breakthrough. One of them is the idea of nested spheres and another is transforming a torus into a sphere effectively and subsequently use the technology of nested spheres PCA. We also provide a new method of clustering for multivariate analysis which has a fundamental property required for molecular biology that penalizes wrong assignments to avoid chemically no go areas. We give various examples to illustrate these methods. One of the important examples includes dealing with COVID-19 data.";Kanti V. Mardia and Henrik Wiechers and Benjamin Eltzner and Stephan F. Huckemann;"Numerical Analysis (Q1); Statistics and Probability (Q1); Statistics, Probability and Uncertainty (Q1)";378.0;190.0;United States;1971-2021;10.1016/j.jmva.2021.104862;0.00836;80.0;;0047259X;0047259X;10957243;0047259X;1.473;Adaptive linkage clustering, Circular mode hunting, Dimension reduction, Multivariate wrapped normal, SARS-CoV-2 geometry, Stratified spheres, Torus PCA;Academic Press Inc.;;3264.0;Northern America;1283.0;Q1;23947.0;Journal of multivariate analysis;Principal component analysis and clustering on manifolds;6401.0;786.0;84.0;382.0;2742.0;journal;article;2022
The interrogation of established, large-scale datasets presents great opportunities in health data science for the linkage and mining of potentially disparate resources to create new knowledge in a fast and cost-efficient manner. The number of datasets that can be queried in the field of multimorbidity is vast, ranging from national administrative and audit datasets, large clinical, technical and biological cohorts, through to more bespoke data collections made available by individual organisations and laboratories. However, with these opportunities also come technical and regulatory challenges that require an informed approach. In this review, we outline the potential benefits of using previously collected data as a vehicle for research activity. We illustrate the added value of combining potentially disparate datasets to find answers to novel questions in the field. We focus on the legal, governance and logistical considerations required to hold and analyse data acquired from disparate sources and outline some of the solutions to these challenges. We discuss the infrastructure resources required and the essential considerations in data curation and informatics management, and briefly discuss some of the analysis approaches currently used.;Christopher Boulton and J. Mark Wilkinson;"Aging (Q1); Developmental Biology (Q2)";269.0;501.0;Ireland;1972-2020;10.1016/j.mad.2020.111310;0.00451;117.0;;00476374;18726216;00476374;18726216;5.432;Public datasets, Legislation, Information governance, Data curation, Multimorbidity;Elsevier Ireland Ltd;;9974.0;Western Europe;1534.0;Q1;29640.0;Mechanisms of ageing and development;Use of public datasets in the examination of multimorbidity: opportunities and challenges;7302.0;1383.0;144.0;279.0;14363.0;journal;article;2020
;Maryann Feldman and Martin Kenney and Francesco Lissoni;"Engineering (miscellaneous) (Q1); Management of Technology and Innovation (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1)";483.0;829.0;Netherlands;1971-2021;10.1016/j.respol.2015.02.007;0.02172;238.0;;00487333;00487333;00487333;00487333;8.110;Digital data, Administrative records, Social science research;Elsevier;;8275.0;Western Europe;3666.0;Q1;22900.0;Research policy;The new data frontier: special issue of research policy;32584.0;4408.0;168.0;491.0;13902.0;journal;article;2015
Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.;Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia;"Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)";13125.0;796.0;Netherlands;1970, 1972-2021;10.1016/j.scitotenv.2018.12.276;0.23081999999999997;244.0;;00489697;00489697;18791026;00489697;7.963;Points of interest, Population, Random forests, Nighttime light, China;Elsevier;;6581.0;Western Europe;1795.0;Q1;25349.0;Science of the total environment;Improved population mapping for china using remotely sensed and points-of-interest data within a random forests model;210143.0;106837.0;6929.0;13278.0;455970.0;journal;article;2019
The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.;Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben;"Education (Q1); Sociology and Political Science (Q1)";398.0;228.0;United States;1972-2020;10.1016/j.ssresearch.2016.04.015;0.00899;89.0;;0049089X;10960317;0049089X;10960317;2.322;Big data, Administrative data, Data management, Data quality, Data access;Academic Press Inc.;;7303.0;Northern America;1042.0;Q1;26441.0;Social science research;The role of administrative data in the big data revolution in social science research;6976.0;1123.0;80.0;398.0;5842.0;journal;article;2016
Head-mounted eye tracking is a new method that allows researchers to catch a glimpse of what infants and children see during naturalistic activities. In this chapter, we review how mobile, wearable eye trackers improve the construct validity of important developmental constructs, such as visual object experiences and social attention, in ways that would be impossible using screen-based eye tracking. Head-mounted eye tracking improves ecological validity by allowing researchers to present more realistic and complex visual scenes, create more interactive experimental situations, and examine how the body influences what infants and children see. As with any new method, there are difficulties to overcome. Accordingly, we identify what aspects of head-mounted eye-tracking study design affect the measurement quality, interpretability of the results, and efficiency of gathering data. Moreover, we provide a summary of best practices aimed at allowing researchers to make well-informed decisions about whether and how to apply head-mounted eye tracking to their own research questions.;John M. Franchak and Chen Yu;"Developmental and Educational Psychology (Q2); Pediatrics, Perinatology and Child Health (Q2); Behavioral Neuroscience (Q3)";8.0;181.0;United States;1964-1965, 1967, 1969-1976, 1978-1980, 1982, 1984-1985, 1987, 1989, 1991, 1993-1994, 1996, 1999, 2001-2020;10.1016/bs.acdb.2021.11.001;0.0011300000000000001;41.0;;00652407;00652407;00652407;00652407;2.182;Eye movements, Head-mounted eye tracking, Mobile eye tracking, Ecological validity, Perceptual-motor development, Joint attention, Language development, Computer vision, Social attention;Academic Press Inc.;JAI;9344.0;Northern America;767.0;Q2;29439.0;Advances in child development and behavior;Chapter three - beyond screen time: using head-mounted eye tracking to study natural behavior;1055.0;106.0;18.0;55.0;1682.0;journal;incollection;2022
In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.;Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos;"Chemistry (miscellaneous) (Q1); Clinical Biochemistry (Q1)";18.0;445.0;United States;1958-1973, 1975-1978, 1980-1981, 1983, 1985-1987, 1989-1990, 1992-1994, 1996, 1998-2001, 2003-2020;10.1016/bs.acc.2020.08.002;0.0021899999999999997;46.0;;00652423;00652423;00652423;00652423;5.394;Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials;Academic Press Inc.;Elsevier;17469.0;Northern America;1330.0;Q1;16759.0;Advances in clinical chemistry;Chapter four - translational biomarkers in the era of precision medicine;1701.0;495.0;59.0;118.0;10307.0;journal;incollection;2021
With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.;Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam;Computer Science (miscellaneous) (Q1);3.0;458.0;United States;1960-1962, 1964, 1966-1967, 1969-1973, 1975-2000, 2002-2020;10.1016/bs.adcom.2021.09.006;0.0005200000000000001;33.0;;00652458;00652458;00652458;00652458;2.655;NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification;Academic Press Inc.;Elsevier;3092.0;Northern America;1236.0;Q1;23080.0;Advances in computers;Bivariate, cluster, and suitability analysis of nosql solutions for big graph applications;598.0;290.0;36.0;79.0;1113.0;book series;incollection;2022
Biological invasions exert multiple pervasive effects on ecosystems, potentially disrupting species interactions and global ecological processes. Our ability to successfully predict and manage the ecosystem-level impacts of biological invasions is strongly dependent on our capacity to empirically characterize complex biological interactions and their spatiotemporal dynamics. In this chapter, we argue that the comprehensive integration of multiple complementary tools within the explicit context of ecological networks is essential for providing mechanistic insight into invasion processes and their impact across organizational levels. We provide an overview of traditional (stable isotopes, populations genetics) and emerging (metabarcoding, citizen science) techniques and methods, and their practical implementation in the context of biological invasions. We also present several currently available models and machine-learning approaches that could be used for predicting novel or undocumented interactions, thus allowing a more robust and cost-effective forecast of network and ecosystem stability. Finally, we discuss the importance of methodological advancements on the emergence of scientific and societal challenges for investigating local and global species histories with several skill sets.;S. Kamenova and T.J. Bartley and D.A. Bohan and J.R. Boutain and R.I. Colautti and I. Domaizon and C. Fontaine and A. Lemainque and I. {Le Viol} and G. Mollot and M.-E. Perga and V. Ravigné and F. Massol;"Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)";1.0;511.0;United States;1962, 1964, 1966-1969, 1971, 1974-1975, 1977, 1980, 1982-1984, 1986-1995, 1997, 1999-2001, 2003-2006, 2008-2020;10.1016/bs.aecr.2016.10.009;0.00198;62.0;;00652504;00652504;00652504;00652504;7.429;Invasive species, Palaeogenetics, Long-term monitoring, Citizen science, Interaction network, Stable isotopes, eDNA, Metabarcoding, High-throughput sequencing, Trophic interactions, Population genetics, Predictive models, Machine learning, MinION™;Academic Press Inc.;Academic Press;15075.0;Northern America;739.0;Q2;22230.0;Advances in ecological research;Chapter three - invasions toolkit: current methods for tracking the spread and impact of invasive species;2777.0;231.0;20.0;40.0;3015.0;book series;incollection;2017
Complex and dynamic networks of molecules are involved in human diseases. High-throughput technologies enable omics studies interrogating thousands to millions of makers with similar biochemical properties (eg, transcriptomics for RNA transcripts). However, a single layer of “omics” can only provide limited insights into the biological mechanisms of a disease. In the case of genome-wide association studies, although thousands of single nucleotide polymorphisms have been identified for complex diseases and traits, the functional implications and mechanisms of the associated loci are largely unknown. Additionally, the genomic variants alone are not able to explain the changing disease risk across the life span. DNA, RNA, protein, and metabolite often have complementary roles to jointly perform a certain biological function. Such complementary effects and synergistic interactions between omic layers in the life course can only be captured by integrative study of multiple molecular layers. Building upon the success in single-omics discovery research, population studies started adopting the multi-omics approach to better understanding the molecular function and disease etiology. Multi-omics approaches integrate data obtained from different omic levels to understand their interrelation and combined influence on the disease processes. Here, we summarize major omics approaches available in population research, and review integrative approaches and methodologies interrogating multiple omic layers, which enhance the gene discovery and functional analysis of human diseases. We seek to provide analytical recommendations for different types of multi-omics data and study designs to guide the emerging multi-omic research, and to suggest improvement of the existing analytical methods.;Yan V. Sun and Yi-Juan Hu;"Medicine (miscellaneous) (Q3); Genetics (Q4)";18.0;180.0;United States;1947-1948, 1950-1951, 1953-1956, 1958, 1961, 1963-1965, 1968, 1970-1971, 1973, 1976-1977, 1979, 1982, 1984-1985, 1987-1992, 1994-1999, 2001-2020;10.1016/bs.adgen.2015.11.004;0.00131;67.0;;00652660;00652660;00652660;00652660;1.944;DNA methylation, Epigenome, Gene expression, Genomic epidemiology, GWAS, Integrative genomics, Metabolome, Proteome, Quantitative trait loci, Transcriptome;Academic Press Inc.;Academic Press;9024.0;Northern America;458.0;Q3;21656.0;Advances in genetics;Chapter three - integrative analysis of multi-omics data for discovery and functional studies of complex human diseases;1773.0;99.0;21.0;41.0;1895.0;journal;incollection;2016
"Long-term mechanistic research and monitoring provides integral science support for ecosystem-based management (EBM) of resources, activities and services. Decades of oceanographic and ecological research by Bill Peterson and colleagues along the Newport Hydrographic Line (NH Line) provides essential context for understanding and managing Pacific salmon Oncorhynchus spp. and other marine resources in the California Current ecosystem. This research program helped federal scientists convey the significance of the northeast Pacific marine heatwave (2013–2016) to fisheries managers and stakeholders. Particularly illustrative were shifts in the composition of the copepod community, which reflected feeding conditions for Pacific salmon and other consumers. We identify six traits of the dataset produced by Peterson and colleagues that have made it especially valuable for informing management: (i) it has generated robust ecosystem indicators; (ii) it is long-term; (iii) it is associated with meaningful ecological mechanisms; (iv) it relates to ecosystem components of high societal value; (v) it can represent processes and lags at meaningful temporal scales; and (vi) it is part of a broader, integrative science effort. This research effort underscores the importance of developing and sustaining long-term mechanistic research and monitoring along the U.S. West Coast and elsewhere in the world.";Chris J. Harvey and Jennifer L. Fisher and Jameal F. Samhouri and Gregory D. Williams and Tessa B. Francis and Kym C. Jacobson and Yvonne L. deReynier and Mary E. Hunsicker and Newell Garfield;"Aquatic Science (Q1); Geology (Q1)";441.0;381.0;United Kingdom;1963-1965, 1969, 1973, 1976-2020;10.1016/j.pocean.2020.102418;0.01183;132.0;;00796611;00796611;00796611;00796611;4.080;Ecosystem-based fisheries management, Ecosystem indicators, California Current large marine ecosystem, Copepods, Salmon, Food web ecology, Climate, Long-term monitoring;Elsevier Ltd.;;8280.0;Western Europe;1487.0;Q1;28409.0;Progress in oceanography;The importance of long-term ecological time series for integrated ecosystem assessment and ecosystem-based management;10811.0;1822.0;161.0;448.0;13330.0;journal;article;2020
Registries are essential for health infrastructure planning, benchmarking, continuous quality improvement, hypothesis generation, and real-world trials. To date, data from these registries have predominantly been analyzed in isolated “silos,” hampering efforts to analyze “big data” at the international level, an approach that provides wide-ranging benefits, including enhanced statistical power, an ability to conduct international comparisons, and greater capacity to study rare diseases. This review serves as a valuable resource to clinicians, researchers, and policymakers, by comprehensively describing kidney failure registries active in 2021, before proposing approaches for inter-registry research under current conditions, as well as solutions to enhance global capacity for data collaboration. We identified 79 kidney-failure registries spanning 77 countries worldwide. International Society of Nephrology exemplar initiatives, including the Global Kidney Health Atlas and Sharing Expertise to support the set-up of Renal Registries (SharE-RR), continue to raise awareness regarding international healthcare disparities and support the development of universal kidney-disease registries. Current barriers to inter-registry collaboration include underrepresentation of lower-income countries, poor syntactic and semantic interoperability, absence of clear consensus guidelines for healthcare data sharing, and limited researcher incentives. This review represents a call to action for international stakeholders to enact systemic change that will harmonize the current fragmented approaches to kidney-failure registry data collection and research.;Monica S.Y. Ng and Vivek Charu and David W. Johnson and Michelle M. O’Shaughnessy and Andrew J. Mallett;Nephrology (Q1);749.0;485.0;United States;1972-2020;10.1016/j.kint.2021.09.024;0.03902;276.0;;00852538;00852538;15231755;00852538;10.612;data sharing, dialysis, inter-registry collaboration, kidney failure, registry, transplantation;Elsevier Inc.;;2681.0;Northern America;3499.0;Q1;19992.0;Kidney international;National and international kidney failure registries: characteristics, commonalities, and contrasts;52616.0;6461.0;470.0;1251.0;12600.0;journal;article;2022
Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.;Guillaume Taglang and David B. Jackson;"Obstetrics and Gynecology (Q1); Oncology (Q1)";1087.0;441.0;United States;1972-2020;10.1016/j.ygyno.2016.02.022;0.02767;162.0;;00908258;00908258;10956859;00908258;5.482;Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers;Academic Press Inc.;;3224.0;Northern America;2105.0;Q1;27467.0;Gynecologic oncology;Use of “big data” in drug discovery and clinical trials;29012.0;5359.0;480.0;1150.0;15477.0;journal;article;2016
"Background
Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.
Objective
Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.
Methods
We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.
Results
The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.
Conclusion
We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.";Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Renée Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian;"Immunology (Q1); Immunology and Allergy (Q1)";1480.0;622.0;United States;1963-1965, 1971-2020;10.1016/j.jaci.2020.11.045;0.06998;292.0;;00916749;10976825;00916749;10976825;10.793;Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics;Mosby Inc.;;3565.0;Northern America;3281.0;Q1;21267.0;Journal of allergy and clinical immunology;Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records;63614.0;12487.0;472.0;1828.0;16826.0;journal;article;2021
This paper outlines a commentary response to an article published by Young and colleagues in Preventive Medicine that evaluated the feasibility of using Twitter as a surveillance and monitoring took for HIV. We draw upon the broader literature on disease surveillance and public health prevention using social media and broader considerations of epidemiological and surveillance methods to provide readers with necessary considerations for using social media in epidemiology and surveillance.;Mark A. Stoové and Alisa E. Pedrana;"Public Health, Environmental and Occupational Health (Q1); Epidemiology (Q2)";971.0;345.0;United States;1946, 1972-2020;10.1016/j.ypmed.2014.03.008;0.028980000000000002;169.0;;00917435;10960260;00917435;10960260;4.018;Epidemiology, Surveillance, Social media, Twitter, Prevention, HIV;Academic Press Inc.;;4021.0;Northern America;1628.0;Q1;17665.0;Preventive medicine;Making the most of a brave new world: opportunities and considerations for using twitter as a public health monitoring tool;20705.0;4091.0;307.0;1042.0;12343.0;journal;article;2014
"Summary
Past human genetic diversity and migration between southern China and Southeast Asia have not been well characterized, in part due to poor preservation of ancient DNA in hot and humid regions. We sequenced 31 ancient genomes from southern China (Guangxi and Fujian), including two ∼12,000- to 10,000-year-old individuals representing the oldest humans sequenced from southern China. We discovered a deeply diverged East Asian ancestry in the Guangxi region that persisted until at least 6,000 years ago. We found that ∼9,000- to 6,000-year-old Guangxi populations were a mixture of local ancestry, southern ancestry previously sampled in Fujian, and deep Asian ancestry related to Southeast Asian Hòabìnhian hunter-gatherers, showing broad admixture in the region predating the appearance of farming. Historical Guangxi populations dating to ∼1,500 to 500 years ago are closely related to Tai-Kadai and Hmong-Mien speakers. Our results show heavy interactions among three distinct ancestries at the crossroads of East and Southeast Asia.";Tianyi Wang and Wei Wang and Guangmao Xie and Zhen Li and Xuechun Fan and Qingping Yang and Xichao Wu and Peng Cao and Yichen Liu and Ruowei Yang and Feng Liu and Qingyan Dai and Xiaotian Feng and Xiaohong Wu and Ling Qin and Fajun Li and Wanjing Ping and Lizhao Zhang and Ming Zhang and Yalin Liu and Xiaoshan Chen and Dongju Zhang and Zhenyu Zhou and Yun Wu and Hassan Shafiey and Xing Gao and Darren Curnoe and Xiaowei Mao and E. Andrew Bennett and Xueping Ji and Melinda A. Yang and Qiaomei Fu;Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1);1563.0;2845.0;United States;1974-2020;10.1016/j.cell.2021.05.018;0.52696;776.0;;00928674;00928674;10974172;00928674;41.582;ancient DNA, 12,000-year-old humans, deeply diverged ancestry, pre-farming, cross-interactions, admixture;Cell Press;;6179.0;Northern America;26304.0;Q1;18434.0;Cell;Human population history at the crossroads of east and southeast asia since 11,000 years ago;320407.0;52644.0;572.0;1690.0;35345.0;journal;article;2021
Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.;Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum;"Hematology (Q1); Oncology (Q1)";124.0;421.0;United Kingdom;1974-2020;10.1053/j.seminoncol.2020.02.006;0.00455;133.0;;00937754;00937754;15328708;00937754;4.929;Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics;W.B. Saunders Ltd;;5822.0;Western Europe;1812.0;Q1;13120.0;Seminars in oncology;Using big data in pediatric oncology: current applications and future directions;5713.0;690.0;51.0;150.0;2969.0;journal;article;2020
Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.;Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel;"Computer Networks and Communications (Q1); Management Information Systems (Q1)";76.0;170.0;United States;1969-2020;10.1145/3353401.3353406;0.0005200000000000001;57.0;;00950033;00950033;15320936;00950033;1.828;social media, data analytics., expertise location;Association for Computing Machinery (ACM);Association for Computing Machinery;6808.0;Northern America;797.0;Q1;4900152206.0;Data base for advances in information systems;Recognizing experts on social media: a heuristics-based approach;772.0;193.0;26.0;87.0;1770.0;journal;article;2019
The number and diversity of Process Analytics applications is growing fast, impacting areas ranging from process operations to strategic planning or supply chain management. However, this field has not reached yet a maturity level characterized by a stable, organized and consolidated body of knowledge for handling the main classes of problems that need to be faced. Data-Driven Process Systems Engineering and Process Analytics only recently received wider recognition, becoming a regular presence in journals and conferences. As a tribute to the groundbreaking Process Analytics contributions of George Stephanopoulos, namely through his academic tree, to which we are proud to belong, this article aims to contribute to the systematization and consolidation of this field in the broad PSE scope, starting from a fundamental understanding of the key challenges facing it, and constructing from them a workflow that can flexibly be adapted to handle different problems, aimed at supporting value creation through good decision-making. In this path, we base our foresight and conceptual framework on the authors’ experience, as well as on contributions from other researchers that, across the world, have been collectively pushing forward Data-Driven Process Systems Engineering.;Marco S. Reis and Pedro M. Saraiva;"Chemical Engineering (miscellaneous) (Q1); Computer Science Applications (Q1)";1001.0;433.0;United Kingdom;1977-2020;10.1016/j.compchemeng.2022.107675;;139.0;;00981354;00981354;00981354;00981354;;Process Analytics, Process Systems Engineering 4.0, Data science, Industry 4.0, Artificial Intelligence, Data-Driven PSE;Elsevier BV;;4771.0;Western Europe;1017.0;Q1;24600.0;Computers and chemical engineering;Data-driven process system engineering–contributions to its consolidation following the path laid down by george stephanopoulos;;4474.0;399.0;1010.0;19037.0;journal;article;2022
The human gut microbiome is a complex and dynamic microbial entity that interacts with the environment and other parts of the body including the brain, heart, liver, and immune system. These multisystem interactions are highly conserved from invertebrates to humans, however the complexity and diversity of human microbiota compositions often yield a context that is unique to each individual. Yet commonalities remain across species, where a healthy gut microbiome will be rich in symbiotic commensal biota while an unhealthy gut microbiota will be experiencing abnormal blooms of pathobiont bacteria. In this review we discuss how omics technologies can be applied in a personalized approach to understand the microbial crosstalk and microbial-host interactions that affect the delicate balance between eubiosis and dysbiosis in an individual gut microbiome. We further highlight the strengths of model organisms in identifying and characterizing these conserved synergistic and/or pathogenic host-microbe interactions. And finally, we touch upon the growing area of personalized therapeutic interventions targeting gut microbiome.;Cristina Matthewman and Alexandra Narin and Hannah Huston and Christopher Edward Hopkins;"Biochemistry (Q1); Clinical Biochemistry (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1); Molecular Medicine (Q1)";157.0;1230.0;United Kingdom;1976-1985, 1987-2020;10.1016/j.mam.2022.101115;0.006640000000000001;134.0;;00982997;00982997;18729452;00982997;14.235;;Elsevier Ltd.;;15759.0;Western Europe;3492.0;Q1;19084.0;Molecular aspects of medicine;Systems to model the personalized aspects of microbiome health and gut dysbiosis;8136.0;1909.0;54.0;167.0;8510.0;journal;article;2022
Diverse observational and simulation datasets are needed to understand and predict complex ecosystem behavior over seasonal to decadal and century time-scales. Integration of these datasets poses a major barrier towards advancing environmental science, particularly due to differences in the structure and formats of data provided by various sources. Here, we describe BASIN-3D (Broker for Assimilation, Synthesis and Integration of eNvironmental Diverse, Distributed Datasets), a data integration framework designed to dynamically retrieve and transform heterogeneous data from different sources into a common format to provide an integrated view. BASIN-3D enables users to adopt a standardized approach for data retrieval and avoid customizations for the data type or source. We demonstrate the value of BASIN-3D with two use cases that require integration of data from regional to watershed spatial scales. The first application uses the BASIN-3D Python library to integrate time-series hydrological and meteorological data to provide standardized inputs to analytical and machine learning codes in order to predict the impacts of hydrological disturbances on large river corridors of the United States. The second application uses the BASIN-3D Django framework to integrate diverse time-series data in a mountainous watershed in East River, Colorado, United States to enable scientific researchers to explore and download data through an interactive web portal. Thus, BASIN-3D can be used to support data integration for both web-based tools, as well as data analytics using Python scripting and extensions like Jupyter notebooks. The framework is expected to be transferable to and useful for many other field and modeling studies.;Charuleka Varadharajan and Valerie C. Hendrix and Danielle S. Christianson and Madison Burrus and Catherine Wong and Susan S. Hubbard and Deborah A. Agarwal;"Computers in Earth Sciences (Q1); Information Systems (Q1)";491.0;362.0;United Kingdom;1975-2020;10.1016/j.cageo.2021.105024;;123.0;;00983004;00983004;00983004;00983004;;Data integration, Multiscale diverse data, Synthesis, Environmental data;Elsevier Ltd.;;4543.0;Western Europe;936.0;Q1;110327.0;Computers and geosciences;Basin-3d: a brokering framework to integrate diverse environmental data;;1923.0;143.0;498.0;6497.0;journal;article;2022
;Katy DiVittorio;Library and Information Sciences (Q3);130.0;34.0;United Kingdom;1973, 1975-2020;10.1016/j.serrev.2013.10.016;0.00023999999999999998;23.0;;00987913;00987913;00987913;00987913;0.324;;Taylor and Francis Ltd.;;1641.0;Western Europe;263.0;Q3;16611.0;Serials review;2013 north american serials interest group (nasig) 28th annual conference: art of information, architecture of knowledge;232.0;57.0;46.0;139.0;755.0;journal;article;2013
"Abstract
A positional quality control method based on the application of the International Standard ISO 2859 is proposed. This entails a common framework for dealing with the control of all other spatial data quality components (e.g., completeness, consistency, etc.). We propose a relationship between the parameters “acceptable quality level” and “limiting quality” of the international standard and positional quality by means of observed error models. This proposal does not require any assumption for positional errors (e.g., normality), which means that the application is universal. It can be applied to any type of positional and geometric controls (points, line-strings), to any dimension (1D, 2D, 3D, etc.) and with parametric or non-parametric error models (e.g., lidar). This paper introduces ISO 2859, presents the statistical bases of the proposal and develops two examples of application, the first dealing with a lot-by-lot control and the second, isolated lot control.";F.J. Ariza-López and J. Rodríguez-Avi;Computers in Earth Sciences (Q3);178.0;110.0;United States;1975-2020;10.14358/PERS.81.8.657;0.00235;127.0;;00991112;00991112;00991112;00991112;1.083;;American Society for Photogrammetry and Remote Sensing;;2815.0;Northern America;483.0;Q3;12366.0;Photogrammetric engineering and remote sensing;Using international standards to control the positional quality of spatial data;7415.0;294.0;13.0;240.0;366.0;journal;article;2015
Undergraduate data science education is receiving increasing interest in many higher education institutions in the U.S., with the proliferation of data and data related work and research. As an emerging interdisciplinary study field, data science curriculum is typically a collection of individual data science related courses from different schools and departments, most of which are teaching data science in a siloed fashion. Therefore, it is necessary to map the landscape of existing curricula and explore how academic libraries can collaborate and contribute to undergraduate data science education. In this study, we analyzed teaching content and topics of over 100 data science related courses at Purdue University to map the landscape and explore roles of academic libraries to support data science education curriculum. Our results indicate most existing courses focused on ‘hard-core’ scientific analytic principles, such as computer science, statistics, and domain-specific skills. Courses of data-oriented skills, such as data management, data ethics, and data communications were limited across disciplines. In addition, data science courses were more likely targeting STEM students at upper levels (3rd and 4th year students). Academic libraries can enrich data science education efforts, by supporting credit courses, certificate programs, and other co-curricular activities to provide learning opportunities to all students, particularly 1st and 2nd year students and non-STEM majors.;Gang Shao and Jenny P. Quintana and Wei Zakharov and Senay Purzer and Eunhye Kim;"Education (Q1); Library and Information Sciences (Q1)";249.0;165.0;United Kingdom;1988-2020;10.1016/j.acalib.2021.102320;0.00237;58.0;;00991333;00991333;00991333;00991333;1.533;Data science, Libraries, Curriculum, Education, Data management, College, Data ethics;Elsevier BV;;4143.0;Western Europe;889.0;Q1;12791.0;Journal of academic librarianship;Exploring potential roles of academic libraries in undergraduate data science education curriculum development;1990.0;501.0;112.0;252.0;4640.0;journal;article;2021
;Kenrick Cato;Emergency Nursing (Q3);337.0;68.0;United States;1975-2020;10.1016/j.jen.2021.08.008;0.00137;47.0;;00991767;15272966;00991767;15272966;1.836;;Mosby Inc.;;2121.0;Northern America;325.0;Q3;28206.0;Journal of emergency nursing;The accuracy of medication administration data in the emergency department: why does it matter?;1861.0;329.0;145.0;502.0;3076.0;journal;article;2021
In this paper, we discuss the state of the art of (mobile) multi-hop ad hoc networking with the aim to present the current status of the research activities and identify the consolidated research areas, with limited research opportunities, and the hot and emerging research areas for which further research is required. We start by briefly discussing the MANET paradigm, and why the research on MANET protocols is now a cold research topic. Then we analyze the active research areas. Specifically, after discussing the wireless-network technologies, we analyze four successful ad hoc networking paradigms, mesh networks, opportunistic networks, vehicular networks, and sensor networks that emerged from the MANET world. We also present an emerging research direction in the multi-hop ad hoc networking field: people centric networking, triggered by the increasing penetration of the smartphones in everyday life, which is generating a people-centric revolution in computing and communications.;Marco Conti and Chiara Boldrini and Salil S. Kanhere and Enzo Mingozzi and Elena Pagani and Pedro M. Ruiz and Mohamed Younis;Computer Networks and Communications (Q1);591.0;408.0;Netherlands;1978-2020;10.1016/j.comcom.2015.09.007;0.00513;105.0;;01403664;01403664;01403664;01403664;3.167;MANET, Mesh networks, Opportunistic networks, Vehicular networks, Sensor networks;Elsevier;;4052.0;Western Europe;627.0;Q1;13681.0;Computer communications;From manet to people-centric networking: milestones and open research challenges;6725.0;2424.0;616.0;599.0;24961.0;journal;article;2015
"Summary
Background
18% of the world's population lives in India, and many states of India have populations similar to those of large countries. Action to effectively improve population health in India requires availability of reliable and comprehensive state-level estimates of disease burden and risk factors over time. Such comprehensive estimates have not been available so far for all major diseases and risk factors. Thus, we aimed to estimate the disease burden and risk factors in every state of India as part of the Global Burden of Disease (GBD) Study 2016.
Methods
Using all available data sources, the India State-Level Disease Burden Initiative estimated burden (metrics were deaths, disability-adjusted life-years [DALYs], prevalence, incidence, and life expectancy) from 333 disease conditions and injuries and 84 risk factors for each state of India from 1990 to 2016 as part of GBD 2016. We divided the states of India into four epidemiological transition level (ETL) groups on the basis of the ratio of DALYs from communicable, maternal, neonatal, and nutritional diseases (CMNNDs) to those from non-communicable diseases (NCDs) and injuries combined in 2016. We assessed variations in the burden of diseases and risk factors between ETL state groups and between states to inform a more specific health-system response in the states and for India as a whole.
Findings
DALYs due to NCDs and injuries exceeded those due to CMNNDs in 2003 for India, but this transition had a range of 24 years for the four ETL state groups. The age-standardised DALY rate dropped by 36·2% in India from 1990 to 2016. The numbers of DALYs and DALY rates dropped substantially for most CMNNDs between 1990 and 2016 across all ETL groups, but rates of reduction for CMNNDs were slowest in the low ETL state group. By contrast, numbers of DALYs increased substantially for NCDs in all ETL state groups, and increased significantly for injuries in all ETL state groups except the highest. The all-age prevalence of most leading NCDs increased substantially in India from 1990 to 2016, and a modest decrease was recorded in the age-standardised NCD DALY rates. The major risk factors for NCDs, including high systolic blood pressure, high fasting plasma glucose, high total cholesterol, and high body-mass index, increased from 1990 to 2016, with generally higher levels in higher ETL states; ambient air pollution also increased and was highest in the low ETL group. The incidence rate of the leading causes of injuries also increased from 1990 to 2016. The five leading individual causes of DALYs in India in 2016 were ischaemic heart disease, chronic obstructive pulmonary disease, diarrhoeal diseases, lower respiratory infections, and cerebrovascular disease; and the five leading risk factors for DALYs in 2016 were child and maternal malnutrition, air pollution, dietary risks, high systolic blood pressure, and high fasting plasma glucose. Behind these broad trends many variations existed between the ETL state groups and between states within the ETL groups. Of the ten leading causes of disease burden in India in 2016, five causes had at least a five-times difference between the highest and lowest state-specific DALY rates for individual causes.
Interpretation
Per capita disease burden measured as DALY rate has dropped by about a third in India over the past 26 years. However, the magnitude and causes of disease burden and the risk factors vary greatly between the states. The change to dominance of NCDs and injuries over CMNNDs occurred about a quarter century apart in the four ETL state groups. Nevertheless, the burden of some of the leading CMNNDs continues to be very high, especially in the lowest ETL states. This comprehensive mapping of inequalities in disease burden and its causes across the states of India can be a crucial input for more specific health planning for each state as is envisioned by the Government of India's premier think tank, the National Institution for Transforming India, and the National Health Policy 2017.
Funding
Bill & Melinda Gates Foundation; Indian Council of Medical Research, Department of Health Research, Ministry of Health and Family Welfare, Government of India; and World Bank";Lalit Dandona and Rakhi Dandona and G Anil Kumar and D K Shukla and Vinod K Paul and Kalpana Balakrishnan and Dorairaj Prabhakaran and Nikhil Tandon and Sundeep Salvi and A P Dash and A Nandakumar and Vikram Patel and Sanjay K Agarwal and Prakash C Gupta and R S Dhaliwal and Prashant Mathur and Avula Laxmaiah and Preet K Dhillon and Subhojit Dey and Manu R Mathur and Ashkan Afshin and Christina Fitzmaurice and Emmanuela Gakidou and Peter Gething and Simon I Hay and Nicholas J Kassebaum and Hmwe Kyu and Stephen S Lim and Mohsen Naghavi and Gregory A Roth and Jeffrey D Stanaway and Harvey Whiteford and Vineet K Chadha and Sunil D Khaparde and Raghuram Rao and Kirankumar Rade and Puneet Dewan and Melissa Furtado and Eliza Dutta and Chris M Varghese and Ravi Mehrotra and P Jambulingam and Tanvir Kaur and Meenakshi Sharma and Shalini Singh and Rashmi Arora and Reeta Rasaily and Ranjit M Anjana and Viswanathan Mohan and Anurag Agrawal and Arvind Chopra and Ashish J Mathew and Deeksha Bhardwaj and Pallavi Muraleedharan and Parul Mutreja and Kelly Bienhoff and Scott Glenn and Rizwan S Abdulkader and Ashutosh N Aggarwal and Rakesh Aggarwal and Sandra Albert and Atul Ambekar and Monika Arora and Damodar Bachani and Ashish Bavdekar and Gufran Beig and Anil Bhansali and Anurag Bhargava and Eesh Bhatia and Bilali Camara and D J Christopher and Siddharth K Das and Paresh V Dave and Sagnik Dey and Aloke G Ghoshal and N Gopalakrishnan and Randeep Guleria and Rajeev Gupta and Subodh S Gupta and Tarun Gupta and M D Gupte and G Gururaj and Sivadasanpillai Harikrishnan and Veena Iyer and Sudhir K Jain and Panniyamamkal Jeemon and Vasna Joshua and Rajni Kant and Anita Kar and Amal C Kataki and Kiran Katoch and Tripti Khanna and Ajay Khera and Sanjay Kinra and Parvaiz A Koul and Anand Krishnan and Avdhesh Kumar and Raman K Kumar and Rashmi Kumar and Anura Kurpad and Laishram Ladusingh and Rakesh Lodha and P A Mahesh and Rajesh Malhotra and Matthews Mathai and Dileep Mavalankar and Murali {Mohan BV} and Satinath Mukhopadhyay and Manoj Murhekar and G V S Murthy and Sanjeev Nair and Sreenivas A Nair and Lipika Nanda and Romi S Nongmaithem and Anu M Oommen and Jeyaraj D Pandian and Sapan Pandya and Sreejith Parameswaran and Sanghamitra Pati and Kameshwar Prasad and Narayan Prasad and Manorama Purwar and Asma Rahim and Sreebhushan Raju and Siddarth Ramji and Thara Rangaswamy and Goura K Rath and Ambuj Roy and Yogesh Sabde and K S Sachdeva and Harsiddha Sadhu and Rajesh Sagar and Mari J Sankar and Rajendra Sharma and Anita Shet and Shreya Shirude and Rajan Shukla and Sharvari R Shukla and Gagandeep Singh and Narinder P Singh and Virendra Singh and Anju Sinha and Dhirendra N Sinha and R K Srivastava and A Srividya and Vanita Suri and Rajaraman Swaminathan and P N Sylaja and Babasaheb Tandale and J S Thakur and Kavumpurathu R Thankappan and Nihal Thomas and Srikanth Tripathy and Mathew Varghese and Santosh Varughese and S Venkatesh and K Venugopal and Lakshmi Vijayakumar and Denis Xavier and Chittaranjan S Yajnik and Geevar Zachariah and Sanjay Zodpey and J V R Prasada Rao and Theo Vos and K Srinath Reddy and Christopher J L Murray and Soumya Swaminathan;Medicine (miscellaneous) (Q1);1227.0;945.0;United Kingdom;1823-2020;10.1016/S0140-6736(17)32804-0;;762.0;;01406736;01406736;1474547X;01406736;;;Elsevier Ltd.;;1114.0;Western Europe;13103.0;Q1;16590.0;Lancet, the;Nations within a nation: variations in epidemiological transition across the states of india, 1990–2016 in the global burden of disease study;;45581.0;1488.0;4593.0;16580.0;journal;article;2017
This paper presents a hybrid deep forest approach for outlier detection and fault diagnosis. Isolation forest algorithm is combined with Pearson's correlation coefficient for outlier detection. The physical significance of outliers detected by the proposed algorithm is explained by origin analysis, which is rarely mentioned in existing studies. In addition, a novel non-neural network deep learning model-cascade forest model is proposed to fault diagnosis of HVAC system for the first time to achieve high precision accuracy in low-dimensional features. The proposed approach is validated with the refrigerant charge fault of VRF system. The results show that the isolation forest algorithm can improve the performance of fault diagnosis model and the mainly outliers of VRF system are defrosting data. The IF-CF model has short operation time, and high accuracy in low-dimensional features. When the dimension drops to 6, the accuracy of the IF-CF model is 94.16%, which is 5.26%, 10.02%, 5.87% and 3.34% higher than the IF-MLP, IF-BPNN, IF-SVM and IF-LSTM models, respectively. Moreover, IF-CF model does not require complex hyper-parameter optimization strategy because its maximum accuracy difference in different hyper-parameters is 2.04%. This study is enlightening which may inspire the potential of outlier detection technology and deep learning in HVAC field.;Yuke Zeng and Huanxin Chen and Chengliang Xu and Yahao Cheng and Qijian Gong;"Building and Construction (Q1); Mechanical Engineering (Q1)";1061.0;385.0;United Kingdom;1978-2020;10.1016/j.ijrefrig.2020.08.014;;116.0;;01407007;01407007;01407007;01407007;;Variable refrigerant flow system, Outlier detection, Fault diagnosis, Deep forest model, Système à débit de frigorigène variable, Méthode de détection des valeurs aberrantes, Diagnostic des défaillances, Modèle de forêt neuronale profonde;Elsevier Ltd.;;3476.0;Western Europe;1497.0;Q1;16113.0;International journal of refrigeration;A hybrid deep forest approach for outlier detection and fault diagnosis of variable refrigerant flow system;;4287.0;371.0;1104.0;12895.0;journal;article;2020
Globalization has integrated nations into a world economy. Based on the world input-output database (WIOD), this paper explored the energy use of the world economy under a household-consumption-based MRIO (multi-region input-output) accounting scheme. Pertaining to normative economics, the household-consumption-based MRIO accounting scheme corresponds to the value judgement of household consumption being the ultimate driver of the economy, which complements existing accounting methods based on different viewpoints. The energy use associated with the internationally traded products is calculated to be around one-fifth of the global total energy consumption. For China as the largest exporter and also the biggest deficit economy in terms of energy use, its trade imbalance is nearly the summation of that of the United States, the United Kingdom, Japan and Germany. Energy self-sufficiency rates by supply and by demand are respectively proposed. While the United States economy as the largest importer maintains the majority of the energy welfare denoted by the onsite energy use at home, China exports large quantities of energy use abroad. For economies like Germany, South Korea and Taiwan, they could be regarded as hubs that export a considerable amount of energy use abroad and absorb massive energy use from outside simultaneously. For sustainable use of energy resources, economies are suggested to carefully identify their roles in the global trading network of energy use.;G.Q. Chen and X.D. Wu and Jinlan Guo and Jing Meng and Chaohui Li;"Economics and Econometrics (Q1); Energy (miscellaneous) (Q1)";1072.0;710.0;Netherlands;1979-2019;10.1016/j.eneco.2019.05.019;0.02456;152.0;;01409883;01409883;18736181;01409883;7.042;World economy, Household-consumption-based MRIO accounting, Energy use, Trade imbalance, Hubs;Elsevier;;5818.0;Western Europe;2500.0;Q1;29374.0;Energy economics;Global overview for energy use of the world economy: household-consumption-based accounting based on the world input-output database (wiod);26186.0;8195.0;378.0;1081.0;21992.0;journal;article;2019
A comprehensive database containing data on approximately 2700 buildings and 6000 full-scale measured period samples was constructed through massive literature searching and stringent data filtering. The newly emerged maximal information coefficient method, which is suitable for large data set statistical analysis, was adopted in conjunction with Kruskal–Wallis analysis of variance to identify factors that significantly affect a building’s fundamental period. It was quantitatively verified that height, predominant structural material, and lateral-force resisting system are the three most important influencing factors. Subsequently, height was used as the dominant regression variable, and material and lateral-force resisting system were used as categorical variables, predictive models in combination with confidence intervals of the fundamental period are provided for multi-factors, including four material types and three structural types. In addition, multi-level empirical formulas of the natural period in other five modes (two translational and three torsional) are provided on the basis of the regression results of the fundamental period. All these predictive models can effectively reflect the tendency of the median and the rational scope of variability of the natural period of buildings.;Zetao Wang and Jun Chen and Jiaxu Shen;Civil and Structural Engineering (Q1);3259.0;477.0;United Kingdom;1970, 1978-2020;10.1016/j.engstruct.2021.112622;0.0431;141.0;;01410296;18737323;01410296;18737323;4.471;Natural period of building, Database, Maximal information coefficient, Kruskal–Wallis ANOVA, Empirical formula, Confidence interval, Rational scope;Elsevier BV;;4387.0;Western Europe;1567.0;Q1;15652.0;Engineering structures;Multi-factor and multi-level predictive models of building natural period;43208.0;16421.0;1262.0;3267.0;55358.0;journal;article;2021
Mobile short video-based product sales sharing sites like YouTube and Tudor have many established user content for creating and distributing shares. The increasing number of mobile devices for product sales leads to the upcoming new 5G technology roadmap for embedded systems and 5G network connectivity. As these are the main sources of 5G information and online activities for consumers, mobile phone short films are rapidly being replaced by embedded systems. As the demand for more embedded system devices and applications continues to grow, supported bandwidth is also essential to meet this growing connection demand. The existing system does not allocate the product sales data upload bandwidth size. The system proposed here focuses on user upload bandwidth allocation, one of the basic resources of a short video sharing system with product details. Its allocation upload bandwidth Recurrent Neural Network (RNN) algorithm is proposed in a centralized or decentralized way and evaluated for balancing widely used strategies (equal allocation) and a mobile short video. Embedded systems are responsible for running professional product sales and control applications consistently and predictably. Development while using the microprocessor is also important. It increases the need to process product sales to handle the bandwidth, latency requirements, product sales data and data generated from multiple connected devices. It's a big challenge for the industry to data and data capabilities.;Lu Zheng and Shikun Liu;"Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3)";423.0;234.0;Netherlands;1978-2020;10.1016/j.micpro.2021.103831;0.00207;38.0;;01419331;01419331;01419331;01419331;1.525;Recurrent Neural Network (RNN), Mobile short video, Product sales data upload bandwidth size, Embedded systems, 5G technology;Elsevier;;2771.0;Western Europe;323.0;Q3;15552.0;Microprocessors and microsystems;Research on the strategy of mobile short video in product sales based on 5g network and embedded system;1490.0;962.0;462.0;428.0;12804.0;journal;article;2021
Cyber-physical power systems (CPPS) are interconnected architectures that interact with the physical power system environment by utilizing communication, control, and computing resources. Enterprise, industries, and critical infrastructure rely on CPPS. As a result of their critical importance, they are prime targets for cyberattacks aimed at disrupting their operations. CPPS are mission-critical, attacks on them can have disastrous consequences. CPPS security can be improved by utilizing testbed capabilities to replicate power system operations, discover threats, develop sustainable cybersecurity solutions, and evaluate grid operation in physical fault-induced or cyberattack scenarios. This Part-II paper reviews the CPPS testbeds in the view of the testbed type, targeted research area, CPPS domain, and communication infrastructure with the fusion of physical and cyber systems. Furthermore, this review gives an outline, framework, and application-based assessment of various industry and institutional testbeds. The design process of CPPS testbeds is reviewed first in this paper(Part-II), followed by description of the classification of CPPS testbeds for cyberattacks and sustainable cybersecurity analysis in CPPS. A brief overview of CPPS testbeds in various academic institutions, R&D laboratories, and industries are provided and each testbed is evaluated using the eleven proposed assessment criteria. Finally, the current issues and future research directions for CPPS testbeds to develop a secure and sustainable electric power grid are discussed. The Part-I paper reviewed the major research areas in CPPS, the importance of cyberattack testbeds, and long-term cybersecurity analysis in CPPS, as well as the NIST framework for CPS, CPPS domains and research areas.;Rajaa Vikhram Yohanandhan and Rajvikram Madurai Elavarasan and Rishi Pugazhendhi and Manoharan Premkumar and Lucian Mihet-Popa and Vladimir Terzija;"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1)";1355.0;562.0;United Kingdom;1970, 1979-2021;10.1016/j.ijepes.2021.107721;;130.0;;01420615;01420615;01420615;01420615;;Critical infrastructure protection, Cyber attack, Cyber-physical power system testbed, Cyber security, Real-time testbed, Smart grid, Sustainability;Elsevier Ltd.;;3679.0;Western Europe;1050.0;Q1;17985.0;International journal of electrical power and energy systems;A holistic review on cyber-physical power system (cpps) testbeds for secure and sustainable electric power grid – part – ii: classification, overview and assessment of cpps testbeds;;7494.0;839.0;1355.0;30866.0;journal;article;2022
Modern Chinese cities are defined from the administrative view and classified into several administrative categories, which makes it inconsistent between Chinese cities and their counterparts in western countries. Without easy access to fine-scale data, researchers have to rely heavily on statistical and aggregated indicators available in officially released yearbooks, to understand Chinese city system. Not to mention the data quality of yearbooks, it is problematic that a large number of towns or downtown areas of counties are not addressed in yearbooks. To address this issue, as a following study of Long et al. (2016), we have redefined the Chinese city system, using percolation theory in the light of newly emerging big/open data. In this paper, we propose our alternative definition of a city with road/street junctions, and present the methodology for extracting city system for the whole country with national wide road junctions. A city is defined as “a spatial cluster with a minimum of 100 road/street junctions within a 300 m distance threshold”. Totally we identify 4629 redefined cities with a total urban area of 64,144 km2 for the whole China. We observe total city number increases from 2273 in 2009 to 4629 in 2014. We find that expanded urban area during 2009 and 2014, comparing with urban areas in 2009 are associated with 73.3% road junction density, 25.3% POI density and 5.5% online comment density. In addition, we benchmark our results with the conventional Chinese city system by using yearbooks.;Ying Long;"Environmental Science (miscellaneous) (Q1); Forestry (Q1); Geography, Planning and Development (Q1); Tourism, Leisure and Hospitality Management (Q1)";495.0;440.0;Netherlands;1980-2020;10.1016/j.apgeog.2016.08.002;0.00938;99.0;;01436228;01436228;01436228;01436228;4.240;Urban morphology, Urban function, Human activity, Street network, City evolution;Elsevier BV;;6422.0;Western Europe;1165.0;Q1;27471.0;Applied geography;Redefining chinese city system with emerging new data;10205.0;2353.0;156.0;497.0;10019.0;journal;article;2016
"ABSTRACT
Citizen observatories that incorporate participatory sensing can complement traditional and automated data collection methods for mobility planning and increase the level of participation of citizens in transport planning. The process of developing such an online environment is not only time-consuming and costly, but it would also require an extensive knowledge of computer programming. This is one of the main barriers to the proliferation of citizen observatories. Therefore, this paper develops a conceptual framework of a citizen observatory platform that does not require special skills or resources. It would enable the collection, analysis and exchange of quantitative and qualitative mobility-related data by citizens. We have reviewed 69 participatory sensing applications in the field of mobility to derive the essential building stones of such an observatory. We identified the requirements considering eight criteria: campaign management, objective, context, data types, sensing technology, motivation of data collectors, validation and representativeness, visualisation and reporting. Some concerns regarding representativeness of data, motivation of data collectors, accuracy of sensors and validated algorithms for indicators are also raised.";Imre Keseru and Nils Wuytens and Cathy Macharis;Transportation (Q1);106.0;934.0;United Kingdom;1981-2020;10.1080/01441647.2018.1536089;0.00476;82.0;;01441647;01441647;14645327;01441647;9.643;participatory sensing, citizen observatory, mobility surveys, smartphones, citizen science, citizen participation;Routledge;;7381.0;Western Europe;3046.0;Q1;98982.0;Transport reviews;Citizen observatory for mobility: a conceptual framework;4598.0;1233.0;52.0;124.0;3838.0;journal;article;2019
The ecology driving the remodeling of gut microbial consortia with dietary fiber intervention remains incomplete. We investigated the short-term dynamics of the gut microbiota and metabolic function during inulin fermentation with distinct microbiota from two swine breeds using an in vitro fermentation model. Different gut microbiota at a transient temporal time displayed a similar response to inulin intervention such as the similar fermentation stage and a rapid response followed by gradual stabilization of microbial diversity. Inulin-induced bacterial succession and individual metabolic change were determined by the original microbial compositions, in particular the α-diversity. Levels of short-chain fatty acids (SCFAs) were predictable with the key bacteria by the regression model, especially butyrate was associated with the abundance and ecological interactions of Lactobacillus delbrueckii, Bifidobacterium thermophilum and Megasphaera elsdenii. This study emphasizes the importance of complex ecology to understand fiber-induced microbiome and metabolic changes, thus providing a reference for predictable dietary responses.;Siyu Wei and Cheng Wang and Qifan Zhang and Hui Yang and Edward C. Deehan and Xin Zong and Yizhen Wang and Mingliang Jin;"Materials Chemistry (Q1); Organic Chemistry (Q1); Polymers and Plastics (Q1)";3787.0;915.0;United Kingdom;1981-2021;10.1016/j.carbpol.2022.120057;0.06756000000000001;208.0;;01448617;18791344;01448617;18791344;9.381;Dietary fiber, Gut microbiota, Individual variability, Butyrate, Swine;Elsevier Ltd.;;5411.0;Western Europe;1639.0;Q1;25801.0;Carbohydrate polymers;Dynamics of microbial communities during inulin fermentation associated with the temporal response in scfa production;104569.0;33899.0;1448.0;3791.0;78352.0;journal;article;2022
"Background
In 1996, the ISPCAN Working Group on Child Maltreatment Data (ISPCAN-WGCMD) was established to provide an international forum in which individuals, who deal with child maltreatment data in their respective professional roles, can share concerns and solutions.
Objective
This commentary describes some of the key features and the status of child maltreatment related data collection addressed by the ISPCAN-WGCMD.
Methods
Different types of data collection methods including self-report, sentinel, and administrative data designs are described as well as how they address different needs for information to help understand child maltreatment and systems of prevention and intervention.
Results
While still lacking in many parts of the world, access to child maltreatment data has become much more widespread, and in many places a very sophisticated undertaking.
Conclusion
The ISPCAN-WGCMD has been an important forum for supporting the continued development and improvement in the global effort to understand and combat child maltreatment thus contributing to the long term goals of the UN Convention on the Rights of the Child. Nevertheless, based on what has been learned, even greater efforts are required to improve data in order to effectively combat child maltreatment.";John D. Fluke and Lil Tonmyr and Jenny Gray and Leonor {Bettencourt Rodrigues} and Flora Bolter and Scottye Cash and Andreas Jud and Franziska Meinck and Abigail {Casas Muñoz} and Melissa O’Donnell and Rhiannon Pilkington and Leemoy Weaver;"Developmental and Educational Psychology (Q1); Pediatrics, Perinatology and Child Health (Q1); Psychiatry and Mental Health (Q1); Social Work (Q1)";870.0;364.0;United Kingdom;1977-2020;10.1016/j.chiabu.2020.104650;;144.0;;01452134;01452134;01452134;01452134;;Child maltreatment data, Linked data, Data collection, Data analysis, Self-report data, Administrative data, Sentinel data, Data collection ethics, International comparison, Child maltreatment data utilization, Evaluation, Decision-making;Elsevier Ltd.;;5539.0;Western Europe;1552.0;Q1;24451.0;Child abuse and neglect;Child maltreatment data: a summary of progress, prospects and challenges;;3737.0;401.0;901.0;22211.0;journal;article;2021
"The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of ""ethics-by-design"" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.";Kenneally, Erin and Bailey, Michael;"Computer Networks and Communications (Q2); Software (Q2)";121.0;213.0;United States;1995-2020;10.1145/2602204.2602217;;170.0;;01464833;19435819;01464833;19435819;;trust, network measurement, cyber security, law, ethics;Association for Computing Machinery (ACM);Association for Computing Machinery;3153.0;Northern America;542.0;Q2;13683.0;Computer communication review;"Cyber-security research ethics dialogue &amp; strategy workshop";;362.0;36.0;128.0;1135.0;journal;article;2014
The HERMES collaboration has collected many millions of deep-inelastic lepton-nucleon scattering events during the years 1995–2000, i. e. the first phase of the experiment. Longitudinally polarised electron or positron beams in the HERA electron storage ring were incident on longitudinally polarised internal atomic gas targets as well as several nuclear gas targets. The primary goal of the HERMES experiment is the study of the spin structure of the nucleon. High precision measurements of double-spin asymmetries in inclusive and semi-inclusive scattering from undiluted polarised atomic hydrogen, deuterium and 3He gas targets are presented. These data represent the world's most precise experimental determination to date of the separate contributions of the spin of up, down and strange quarks to the spin of the nucleon, and the first direct indication of a positive gluon polarisation. The observation of single-spin asymmetries in the azimuthal distributions of hadrons in semi-inclusive deep-inelastic scattering from longitudinally polarised targets is also presented. These single-spin asymmetries can be related to transversity, the only unmeasured of the three leading twist parton distribution functions. Moreover, first measurements of single-spin asymmetries in hard exclusive production of real photons and of pions are reported. Such data can be interpreted in the framework of generalised parton distributions, which is also used when discussing new data on hard exclusive electro-production of the vector mesons ϱ, ω and φ. Finally some results from measurement of longitudinal spin transfer in electro-production of Λ hyperons, and of transverse Λ polarisation in quasi-real photo-production are discussed.;K. Rith;Nuclear and High Energy Physics (Q1);96.0;2905.0;Netherlands;1978-2020;10.1016/S0146-6410(02)00157-6;0.0072900000000000005;128.0;;01466410;01466410;01466410;01466410;16.281;;Elsevier;;26002.0;Western Europe;7159.0;Q1;29488.0;Progress in particle and nuclear physics;Spin asymmetries in deep-inelastic electron-nucleon scatering - selected hermes results;4714.0;2579.0;41.0;96.0;10661.0;journal;article;2002
;Kevin E. Fisher and Geoffrey H. Smith and Stewart G. Neill and Michael R. Rossi;"Cancer Research (Q2); Oncology (Q2)";159.0;275.0;United States;1976-2020;10.1016/j.currproblcancer.2014.08.004;0.0019399999999999999;37.0;;01470272;01470272;15356345;01470272;3.187;;Mosby Inc.;;3211.0;Northern America;1146.0;Q2;29325.0;Current problems in cancer;Section i: integrating laboratory medicine with tissue specimens;1111.0;534.0;84.0;170.0;2697.0;journal;article;2014
Data is considered the new oil of the economy, but privacy concerns limit their use, leading to a widespread sense that data analytics and privacy are contradictory. Yet such a view is too narrow, because firms can implement a wide range of methods that satisfy different degrees of privacy and still enable them to leverage varied data analytics methods. Therefore, the current study specifies different functions related to data analytics and privacy (i.e., data collection, storage, verification, analytics, and dissemination of insights), compares how these functions might be performed at different levels (consumer, intermediary, and firm), outlines how well different analytics methods address consumer privacy, and draws several conclusions, along with future research directions.;Jaap Wieringa and P.K. Kannan and Xiao Ma and Thomas Reutterer and Hans Risselada and Bernd Skiera;Marketing (Q1);1315.0;738.0;United States;1973-2021;10.1016/j.jbusres.2019.05.005;0.035230000000000004;195.0;;01482963;01482963;01482963;01482963;7.550;;Elsevier Inc.;;8340.0;Northern America;2049.0;Q1;20550.0;Journal of business research;Data analytics in a privacy-concerned world;46935.0;11507.0;878.0;1342.0;73221.0;journal;article;2021
The purpose of this paper is to develop an insight and review the effect of FinTech development against the broader environment in financial technology. We further aim to offer various perspectives in order to aid the understanding of the disruptive potential of FinTech, and its implications for the wider financial ecosystem. By drawing upon very recent and highly topical research on this area this study examines the implications for financial institutions, and regulation especially when technology poses a challenge to the global banking and regulatory system. It is driven by a wide-ranging overview of the development, the current state, and possible future of fintech. This paper attempts to connect practitioner-led and academic research. While it draws on academic research, the perspective it takes is also practice-oriented. It relies on the current academic literature as well as insights from industry sources, action research and other publicly available commentaries. It also draws on professional practitioners’ roundtable discussions, and think-tanks in which the author has been an active participant. We attempt to interpret banking, and regulatory issues from a behavioural perspective. The last crisis exposed significant failures in regulation and supervision. It has made the Financial Market Law and Compliance a key topic on the current agenda. Disruptive technological change also seems to be important in investigating regulatory compliance followed by change. We contribute to the current literature review on financial and digital innovation by new entrants where this has also practical implications. We also provide for an updated review of the current regulatory issues addressing the contextual root causes of disruption within the financial services domain. The aim here is to assist market participants to improve effectiveness and collaboration. The difficulties arising from extensive regulation may suggest a more liberal and principled approach to financial regulation. Disruptive innovation has the potential for welfare outcomes for consumers, regulatory, and supervisory gains as well as reputational gains for the financial services industry. It becomes even more important as the financial services industry evolves. For example, the preparedness of the regulators to instil culture change and harmonise technological advancements with regulation could likely achieve many desired outcomes. Such results range from achieving an orderly market growth, further aiding systemic stability and restoring trust and confidence in the financial system. Our action-led research results have implications for both research and practice. These should be of interest to regulatory standard setters, investors, international organisations and other academics who are researching regulatory and competition issues, and their manifestation within the financial and social contexts. As a perspective on a social construct, this study appeals to regulators and law makers, entrepreneurs, and investors who participate in technology applied within the innovative financial services domain. It is also of interest to bankers who might consider FinTech and strategic partnerships as a prospective, future strategic direction.11We thank two anonymous referees for their very thoughtful and constructing comments who took a keen interest in reviewing our research and in this way contributed materially to the development of the paper. All other omissions and/or errors remain our own.;Ioannis Anagnostopoulos;"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q2)";89.0;394.0;United States;1978, 1980, 1982-2020;10.1016/j.jeconbus.2018.07.003;;50.0;;01486195;01486195;01486195;01486195;;FinTech, RegTech, Business models, Regulation, Financial services, Future research direction;Elsevier Inc.;;4451.0;Northern America;636.0;Q1;24378.0;Journal of economics and business;Fintech and regtech: impact on regulators and banks;;293.0;35.0;91.0;1558.0;journal;article;2018
"Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.";Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell;"Pharmacology (Q2); Pharmacology (medical) (Q2)";587.0;233.0;United States;1977-2020;10.1016/j.clinthera.2022.01.012;0.009640000000000001;134.0;;01492918;1879114X;01492918;1879114X;3.393;Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making;Elsevier Inc.;;4049.0;Northern America;925.0;Q2;29712.0;Clinical therapeutics;Real-world evidence for regulatory decision-making: guidance from around the world;9397.0;1766.0;223.0;672.0;9030.0;journal;article;2022
Use of administrative data to inform decision making is now commonplace throughout the public sector, including program and policy evaluation. While reuse of these data can reduce costs, improve methodologies, and shorten timelines, challenges remain. This article informs evaluators about the growing field of Integrated Data Systems (IDS), and how to leverage cross-sector administrative data in evaluation work. This article is informed by three sources: a survey of current data integration efforts in the United States (U.S.) (N=63), informational interviews with experts, and internal knowledge cultivated through Actionable Intelligence for Social Policy’s (AISP) 12+ years of work in the field. A brief discussion of the U.S. data integration context and history is provided, followed by discussion of tangible recommendations for evaluators, examples of evaluations relying on integrated data, and a list of U.S. IDS sites with publicly available processes for external data requests. Despite the challenges associated with reusing administrative data for program evaluation, IDS offer evaluators a new set of tools for leveraging data across institutional silos.;Sharon Zanti and Emily Berkowitz and Matthew Katz and Amy Hawn Nelson and T.C. Burnett and Dennis Culhane and Yixi Zhou;"Business and International Management (Q2); Geography, Planning and Development (Q2); Public Health, Environmental and Occupational Health (Q2); Social Psychology (Q2); Strategy and Management (Q2)";350.0;193.0;United Kingdom;1978-2020;10.1016/j.evalprogplan.2022.102093;0.0031;62.0;;01497189;01497189;18737870;01497189;1.849;Cross-sector data linkage, Integrated data systems, Administrative data reuse;Elsevier Ltd.;;4804.0;Western Europe;555.0;Q2;26602.0;Evaluation and program planning;Leveraging integrated data for program evaluation: recommendations from the field;3211.0;753.0;107.0;353.0;5140.0;journal;article;2022
Resting-state functional magnetic resonance imaging (RFMRI) enables researchers to monitor fluctuations in the spontaneous brain activities of thousands of regions in the human brain simultaneously, representing a popular tool for macro-scale functional connectomics to characterize normal brain function, mind-brain associations, and the various disorders. However, the test-retest reliability of RFMRI remains largely unknown. We review previously published papers on the test-retest reliability of voxel-wise metrics and conduct a meta-summary reliability analysis of seven common brain networks. This analysis revealed that the heteromodal associative (default, control, and attention) networks were mostly reliable across the seven networks. Regarding examined metrics, independent component analysis with dual regression, local functional homogeneity and functional homotopic connectivity were the three mostly reliable RFMRI metrics. These observations can guide the use of reliable metrics and further improvement of test-retest reliability for other metics in functional connectomics. We discuss the main issues with low reliability related to sub-optimal design and the choice of data processing options. Future research should use large-sample test-retest data to rectify both the within-subject and between-subject variability of RFMRI measurements and accelerate the application of functional connectomics.;Xi-Nian Zuo and Xiu-Xia Xing;"Behavioral Neuroscience (Q1); Cognitive Neuroscience (Q1); Neuropsychology and Physiological Psychology (Q1)";883.0;790.0;United Kingdom;1978-2020;10.1016/j.neubiorev.2014.05.009;0.04897;240.0;;01497634;18737528;01497634;18737528;8.989;Test-retest reliability, Reproducibility, Resting state fMRI, Brain connectome, Functional connectomics;Elsevier Ltd.;;16320.0;Western Europe;3590.0;Q1;18045.0;Neuroscience and biobehavioral reviews;Test-retest reliabilities of resting-state fmri measurements in human brain functional connectomics: a systems neuroscience perspective;36525.0;8529.0;388.0;943.0;63323.0;journal;article;2014
;Audrey S. Paterson and Melanie J. Wilson;"Accounting (Q1); Finance (Q1)";68.0;293.0;United Kingdom;2004-2020;10.1016/j.accfor.2018.06.001;0.00074;45.0;;01559982;01559982;14676303;01559982;2.875;;Taylor and Francis Ltd.;;7562.0;Western Europe;942.0;Q1;145078.0;Accounting forum;;1522.0;228.0;21.0;73.0;1588.0;journal;article;2018
Numerous studies have shown that the increasing trend of respiratory diseases have been closely associated with the endogenous toxic chemicals (polycyclic aromatic hydrocarbons, heavy metal ions, etc.) in PM10. In the present study, we aim to determine the strong correlations between the chemicals in PM10 and the adverse consequences. We used the ChemView DB, the ToxRef DB and a comprehensive literature analysis to collect, identify, and evaluate the chemicals in PM10 and their adverse effects on respiratory system, and then used the ToxCast DB to analyze their bioactivity and key targets through 1192 molecular targets and cell characteristic endpoints. Meanwhile, the bioinformatics analysis were carried out on the molecular targets to screen out prevention and treatment targets. A total of 310 chemicals related to the respiratory system were identified. An unsupervised two-directional heatmap was constructed based on hierarchical clustering of 227 chemicals by their effect scores. A subset of 253 chemicals with respiratory system toxicity had in vitro bioactivity on 318 molecular targets that could be described, clustered and annotated in the heatmap and bipartite network, which were analyzed based on the protein information in UniProt KB database and the software of GO, STRING, and KEGG. These results showed that the chemicals in PM10 have strong correlation with different types of respiratory system injury. The main pathways of respiratory system injury caused by PM10 are the Calcium signaling pathway, MAPK signaling pathway, and PI3K-AKT signaling pathway, and the core proteins in which are likely to be the molecular targets for the prevention and treatment of damage caused by PM10.;Junyu Wang and Yixia Zhang and Zhijun Zhang and Wancong Yu and Ang Li and Xin Gao and Danyu Lv and Huaize Zheng and Xiaohong Kou and Zhaohui Xue;Environmental Science (miscellaneous) (Q1);1698.0;933.0;United Kingdom;1976-2020;10.1016/j.envint.2021.107040;;191.0;;01604120;18736750;01604120;18736750;;PM, Respiratory system injury, Bioinformatics analysis, Molecular targets, Signaling pathway;Elsevier Ltd.;;6526.0;Western Europe;2582.0;Q1;20912.0;Environmental international;Toxicology of respiratory system: profiling chemicals in pm10 for molecular targets and adverse outcomes;;16963.0;885.0;1729.0;57755.0;journal;article;2022
This study reviews 211 key papers published between 1968 and 2018, for a better understanding of how the methods of tourism demand forecasting have evolved over time. The key findings, drawn from comparisons of method-performance profiles over time, are that forecasting models have grown more diversified, that these models have been combined, and that the accuracy of forecasting has been improved. Given the complexity of determining tourism demand, there is no single method that performs well for all situations, and the evolution of forecasting methods is still ongoing. This article also launches the Annals of Tourism Research Curated Collection on tourism demand forecasting, which contains past and hot off the press work on the topic and will continue to grow as new articles on the topic appear in Annals.;Haiyan Song and Richard T.R. Qiu and Jinah Park;"Development (Q1); Tourism, Leisure and Hospitality Management (Q1)";335.0;670.0;United Kingdom;1973-2020;10.1016/j.annals.2018.12.001;0.00748;171.0;;01607383;01607383;01607383;01607383;9.011;Tourism demand, Time series, Econometric model, Forecast combination, Artificial intelligence model, Judgment forecasts;Elsevier Ltd.;;5610.0;Western Europe;2159.0;Q1;30718.0;Annals of tourism research;A review of research on tourism demand forecasting: launching the annals of tourism research curated collection on tourism demand forecasting;19981.0;2483.0;230.0;339.0;12904.0;journal;article;2019
Existing studies on task recommendation in crowdsourcing systems provide additional insights into the field from their perspectives, methodologies, frameworks, and disciplines, resulting in a highly productive but unorganized knowledge domain. This paper is motivated to exploit bibliometric techniques to derive insights that exceed the boundaries of individual systems and identify the potentially transformative changes from 268 published articles. The explicit features (i.e., affiliation, author, citation, and keywords) and implicit information (i.e., topic distribution, potential structure, hidden insights, and evolutionary trend) of domain literature are discovered by network analysis, cluster analysis, and timeline analysis. We summarize the generic framework based on knowledge domain structure and highlight the position of knowledge source, especially textual information, in task recommendation models. Drawing on the Shneider four-stage model, the temporal evolution trend is graphically illustrated to emphasize avenues for future research. Our study conveys accumulated and synthesized specialty knowledge to researchers or newcomers to help them design, initiate, implement, manage, and evaluate recommender systems in crowdsourcing.;Xicheng Yin and Hongwei Wang and Wei Wang and Kevin Zhu;"Business and International Management (Q1); Education (Q1); Human Factors and Ergonomics (Q1); Sociology and Political Science (Q1)";222.0;475.0;United Kingdom;1979-2020;10.1016/j.techsoc.2020.101337;0.00214;51.0;;0160791X;0160791X;0160791X;0160791X;4.192;Task recommendation, Task assignment, Crowdsourcing, Bibliometrics, Recommender system;Elsevier Ltd.;;7198.0;Western Europe;819.0;Q1;18676.0;Technology in society;Task recommendation in crowdsourcing systems: a bibliometric analysis;2735.0;1056.0;201.0;224.0;14468.0;journal;article;2020
"Purpose
To report the incidence of acute postoperative endophthalmitis (POE) after cataract surgery from 2005 to 2014 in France.
Design
Cohort study.
Participants
Patients undergoing operation for cataract surgery by phacoemulsification and presenting acute POE.
Methods
We identified acute POE occurring within 6 weeks after phacoemulsification cataract surgery and the use of intracameral antibiotic injection during the surgical procedure by means of billing codes from a national database.
Main Outcome Measures
Incidence of acute POE.
Results
From January 2005 to December 2014, 6 371 242 eyes in 3 983 525 patients underwent phacoemulsification cataract surgery. The incidence of acute POE after phacoemulsification decreased from 0.145% to 0.053% during this 10-year period; the unadjusted incidence rate ratio (IRR) (95% confidence interval) was 0.37 (0.32–0.42; P < 0.001). In multivariate analysis, intracameral antibiotic injection was associated with a lower risk of acute POE 0.53 (0.50–0.57; P < 0.001), whereas intraoperative posterior capsule rupture, combined surgery, and gender (male) were associated with a higher risk of acute POE: 5.24 (4.11–6.68), 1.77 (1.53–2.05), and 1.48 (1.40–1.56) (P < 0.001), respectively.
Conclusions
Access to a national database allowed us to observe a decrease in acute POE after phacoemulsification cataract surgery from 2005 to 2014. Within the same period, the use of intracameral antibiotics during the surgical procedures increased.";Catherine Creuzot-Garcher and Eric Benzenine and Anne-Sophie Mariet and Aurélie {de Lazzer} and Christophe Chiquet and Alain M. Bron and Catherine Quantin;Ophthalmology (Q1);635.0;504.0;United States;1958, 1978-2020;10.1016/j.ophtha.2016.02.019;0.04197;244.0;;01616420;15494713;01616420;15494713;12.079;;Elsevier Inc.;;1748.0;Northern America;5028.0;Q1;14639.0;Ophthalmology;Incidence of acute postoperative endophthalmitis after cataract surgery: a nationwide study in france from 2005 to 2014;50321.0;6231.0;462.0;1261.0;8075.0;journal;article;2016
Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.;Naumann, Felix;"Information Systems (Q2); Software (Q2)";57.0;167.0;United States;1969, 1973-1978, 1981-2020;10.1145/2590989.2590995;0.00075;142.0;;01635808;01635808;01635808;01635808;0.775;;Association for Computing Machinery (ACM);Association for Computing Machinery;2072.0;Northern America;372.0;Q2;13622.0;Sigmod record;Data profiling revisited;1819.0;127.0;39.0;81.0;808.0;journal;article;2014
The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.;Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise;"Computer Networks and Communications (Q4); Hardware and Architecture (Q4); Information Systems (Q4)";33.0;223.0;United States;1980-1991, 1993, 1996-2010, 2013-2017, 2019;10.1145/3352020.3352033;;104.0;;01635980;01635980;01635980;01635980;;;Association for Computing Machinery (ACM);Association for Computing Machinery;5300.0;Northern America;180.0;Q4;19829.0;Operating systems review (acm);When the power of the crowd meets the intelligence of the middleware: the mobile phone sensing case;;58.0;7.0;36.0;371.0;journal;article;2019
"In this position paper we argue that the availability of ""big"" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.";Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei;"Computer Networks and Communications (Q3); Software (Q3); Hardware and Architecture (Q4)";301.0;104.0;United States;1980, 1982, 1984, 1986-1989, 1994, 1996-2020;10.1145/2627534.2627558;;80.0;;01635999;01635999;01635999;01635999;;;Association for Computing Machinery (ACM);Association for Computing Machinery;785.0;Northern America;223.0;Q3;26742.0;Performance evaluation review;Modeling and analytics for cyber-physical systems in the age of big data;;337.0;72.0;323.0;565.0;journal;article;2014
"The term ‘Digital Phenotyping’ has started to appear with increasing regularity in medical research, especially within psychiatry. This aims to bring together digital traces (e.g., from smartphones), medical data (e.g., electronic health records), and lived experiences (e.g., daily activity, location, social contact), to better monitor, intervene, and diagnose various psychiatric conditions. However, is this notion any different from digital traces or the quantified self? While digital phenotyping has the potential to transform and revolutionize medicine as we know it; there are a number of challenges that must be addressed if research is to blossom. At present, these issues include; (1) methodological issues, for example, the lack of clear theoretical links between digital markers (e.g., battery life, interactions with smartphones) and condition relapses, (2) the current tools being employed, where they typically have a number of security or privacy issues, and are invasive by nature, (3) analytical methods and approaches, where I question whether research should start in larger-scale epidemiological scale or in smaller (and potentially highly vulnerable) patient populations as is the current norm, (4) the current lack of security and privacy regulation adherence of apps used, and finally, (5) how do such technologies become integrated into various healthcare systems? This aims to provide deep insight into how the Digital Phenotyping could provide huge promise if we critically reflect now and gather clinical insights with a number of other disciplines such as epidemiology, computer- and the social sciences to move forward.";Brittany I. Davidson;Psychiatry and Mental Health (Q1);239.0;243.0;United States;1979-2020;10.1016/j.genhosppsych.2020.11.009;0.00605;103.0;;01638343;01638343;18737714;01638343;3.238;;Elsevier Inc.;;3952.0;Northern America;1211.0;Q1;15665.0;General hospital psychiatry;The crossroads of digital phenotyping;7175.0;818.0;144.0;297.0;5691.0;journal;article;2022
Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data.;Julie Moeyersoms and Enric {Junqué de Fortuny} and Karel Dejaeger and Bart Baesens and David Martens;"Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2)";590.0;494.0;United States;1979, 1981-2021;10.1016/j.jss.2014.10.032;0.00727;109.0;;01641212;01641212;01641212;01641212;2.829;Rule extraction, Software fault and effort prediction, Comprehensibility;Elsevier Inc.;;6473.0;Northern America;642.0;Q1;19309.0;Journal of systems and software;Comprehensible software fault and effort prediction: a data mining approach;6579.0;3058.0;183.0;619.0;11845.0;journal;article;2015
Possibility theory is applied to introduce and reason about the fundamental notion of a key for uncertain data. Uncertainty is modeled qualitatively by assigning to tuples of data a degree of possibility with which they occur in a relation, and assigning to keys a degree of certainty which says to which tuples the key applies. The associated implication problem is characterized axiomatically and algorithmically. Using extremal combinatorics, we then characterize the families of non-redundant possibilistic keys that attain maximum cardinality. In addition, we show how to compute for any given set of possibilistic keys a possibilistic Armstrong relation, that is, a possibilistic relation that satisfies every key in the given set and violates every possibilistic key not implied by the given set. We also establish an algorithm for the discovery of all possibilistic keys that are satisfied by a given possibilistic relation. It is shown that the computational complexity of computing possibilistic Armstrong relations is precisely exponential in the input, and the decision variant of the discovery problem is NP-complete as well as W[2]-complete in the size of the possibilistic key. Further applications of possibilistic keys in constraint maintenance, data cleaning, and query processing are illustrated by examples. The computation of possibilistic Armstrong relations and discovery of possibilistic keys from possibilistic relations have been implemented as prototypes. Extensive experiments with these prototypes provide insight into the size of possibilistic Armstrong relations and the time to compute them, as well as the time it takes to compute a cover of the possibilistic keys that hold on a possibilistic relation, and the time it takes to remove any redundant possibilistic keys from this cover.;Nishita Balamuralikrishna and Yingnan Jiang and Henning Koehler and Uwe Leck and Sebastian Link and Henri Prade;"Artificial Intelligence (Q1); Logic (Q1)";560.0;372.0;Netherlands;1978-2020;10.1016/j.fss.2019.01.008;0.007370000000000001;169.0;;01650114;01650114;01650114;01650114;3.343;Armstrong relation, Axiomatization, Constraint maintenance, Database, Extremal combinatorics, Discovery, Implication, Key, Possibility theory, Uncertain data;Elsevier;;3496.0;Western Europe;902.0;Q1;26529.0;Fuzzy sets and systems;Possibilistic keys;17883.0;2262.0;333.0;570.0;11642.0;journal;article;2019
"Background
Self-harm is preventable if the risk can be identified early. The co-occurrence of multiple diseases is related to self-harm risk. This study develops a comorbidity network-based deep learning framework to improve the prediction of individual self-harm.
Methods
Between 01/01/2007-12/31/2010, we obtained 2,323 patients with self-harm records and 46,460 randomly sampled controls from 1,764,094 inpatients across 44 public hospitals in Hong Kong. 80% of the samples were randomly selected for model training, and the remaining 20% were set aside for model testing. We propose a novel patient embedding method, namely Dx2Vec (Diagnoses to Vector), based on the comorbidity network constructed by all historical diagnoses. Dx2Vec represents the comorbidity patterns among diseases and temporal patterns of historical admissions for each patient.
Results
Experiments demonstrate that the Dx2Vec-based model outperforms the baseline deep learning model in identifying patients who would self-harm within 12 months (C-statistic: 0.89). The precision is 0.54 for positive cases and 0.98 for negative cases, whilst the recall is 0.72 for positive cases and 0.96 for negative cases. The model extracted the most predictive diagnoses, and pairwise comorbid diagnoses to help medical professionals identify patients with risk.
Limitations
The inpatient data does not contain lab test information.
Conclusions
Incorporation of a disease comorbidity network can significantly improve self-harm prediction performance, indicating that it is critical to consider comorbidity patterns in self-harm screening and prevention programs. The findings have the potential to be translated into effective self-harm screening systems.";Zhongzhi Xu and Qingpeng Zhang and Paul Siu Fai Yip;"Clinical Psychology (Q1); Psychiatry and Mental Health (Q1)";2503.0;444.0;Netherlands;1979-2021;10.1016/j.jad.2020.08.044;0.06272;188.0;;01650327;01650327;15732517;01650327;4.839;Self-harm prevention, Self-harm prediction, Deep learning, Network embedding, Comorbidity networks;Elsevier;;5024.0;Western Europe;1892.0;Q1;16245.0;Journal of affective disorders;Predicting post-discharge self-harm incidents using disease comorbidity networks: a retrospective machine learning study;46992.0;12430.0;1329.0;2580.0;66763.0;journal;article;2020
Multi-signal joint reconstruction is critical in distributed compressed sensing (DCS). We propose a joint reconstruction algorithm for subspace pursuit of residual correlation steps to balance reconstruction efficiency and data quality. The algorithm improves on two aspects. Firstly, the residuals are used as the step feedback condition to achieve dynamic step adjustment, reducing the iteration’s overall number. Secondly, based on the mixed-support set model (MSM), the residual non-decreasing surplus condition is set to reconstruct the common and the innovation components of the target signal in groups, which reduces the mixing error in joint reconstruction. This paper compares the performance of six algorithms of the same type under the conditions of Gaussian sparse signal and measured shock wave field sensor network data. The results show that the proposed algorithm can effectively reduce the number of measurements required for reconstruction and improve efficiency while maintaining accuracy.;Mingchi Ju and Man Zhao and Tailin Han and Hong Liu and Bo Xu and Xuan Liu;"Computer Vision and Pattern Recognition (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Signal Processing (Q1); Software (Q1)";1057.0;530.0;Netherlands;1979-2021;10.1016/j.sigpro.2022.108747;0.01737;136.0;;01651684;01651684;01651684;01651684;4.662;Distributed compressed sensing, Sparse reconstruction, Mixed-support set model;Elsevier;;4035.0;Western Europe;907.0;Q1;25548.0;Signal processing;A novel subspace pursuit of residual correlation step algorithm for distributed compressed sensing;15960.0;5800.0;415.0;1060.0;16747.0;journal;article;2023
"Depression is one of the most common mental health problems in middle-aged and elderly people. The establishment of risk factor-based depression risk assessment model is conducive to early detection and early treatment of high-risk groups of depression. Five machine learning models (logistic regression (LR); back propagation (BP); random forest (RF); support vector machines (SVM); category boosting (CatBoost) were used to evaluate the depression among 8374 middle-aged people and 4636 elderly people in the NHANES database from 2011 to 2018. In the 2011–2018 cycle, the estimated prevalence of depression was 8.97% in the middle-aged participants and 8.02% in the elderly participants. Among the middle-aged and elderly participants, CatBoost was the best model to identify depression, and its area under the working characteristic curve (AUC) reaches the highest. The second is LR model and SVM model, while the performance of BP and RF model was slightly worse. The primary influencing factor of depression in middle-aged male is alanine aminotransferase. All five machine learning models can identify the occurrence of depression in the NHANES data set through social demographics, lifestyle, laboratory data and other data of middle-aged and elderly people, and among five models, the CatBoost model performed best.";Chenyang Zhang and Xiaofei Chen and Song Wang and Junjun Hu and Chunpeng Wang and Xin Liu;"Psychiatry and Mental Health (Q1); Biological Psychiatry (Q2)";2425.0;296.0;Ireland;1979-2020;10.1016/j.psychres.2021.114261;0.0375;134.0;;01651781;01651781;18727123;01651781;3.222;Depression, Machine learning, Middle-aged and elderly, NHANES;Elsevier Ireland Ltd;;3918.0;Western Europe;1224.0;Q1;18729.0;Psychiatry research;Using catboost algorithm to identify middle-aged and elderly depression, national health and nutrition examination survey 2011–2018;32778.0;7924.0;799.0;2511.0;31304.0;journal;article;2021
As the push towards more sustainable ways to produce energy and chemicals intensifies, efforts are needed to refine and optimize the systems that can give an answer to these needs. In the present work, the use of neural networks as modelling tools for lignocellulosic biomass pyrolysis main products yields estimation was evaluated. In order to achieve this, the most relevant compositional and reaction parameters for lignocellulosic biomass pyrolysis were reviewed and their effect over the main products yields was assessed. Based on relevant literature data, a database was set up, containing parameters and experimental results from 32 published studies for a total of 482 samples, including both fast and slow pyrolysis experiments performed on a heterogeneous collection of lignocellulosic biomasses. The parameters that in the database configured as best predictors for the solid, liquid and gaseous products were determined through preliminary tests and were then used to build reduced models, one for each of the main products, which use five parameters instead of the full set for the estimation of yields. The procedures included hyperparameter optimizations steps. The performances of these reduced models were compared to those of the ones obtained using the full set of parameters as inputs by using the root mean squared error (RMSE) as metric. For both the char and gas products, the best results were consistently achieved by the reduced versions of the network (RMSE 5.1 wt% ar and 5.6 wt% ar respectively), while for the liquid product the best result was given by the full network (RMSE 6.9 wt% ar) indicating substantial value in proper selection of the input features. In general, the char models were the best performing ones. Additional models for the liquid and gas product featuring char as additional input to the system were also devised and obtained better performance (RMSE 5.5 wt% ar and 4.9 wt% ar respectively) compared to the original ones. Models based on single studies were also included in order to showcase both the capabilities of the tool and the challenges that arise when trying to build a generalizable model of this kind. Overall, artificial neural networks were shown to be an interesting tool for the construction of setup-unspecific biomass pyrolysis product yield models. The obstacles standing currently in the way of a more accurate modelling of the system were highlighted, along with certain literature discrepancies, which hinder reliable quantitative comparison of experimental conditions and results among separate studies.;C. Tsekos and S. Tandurella and W. {de Jong};"Analytical Chemistry (Q1); Fuel Technology (Q1)";818.0;561.0;Netherlands;1970, 1979-2020;10.1016/j.jaap.2021.105180;0.01196;133.0;;01652370;01652370;01652370;01652370;5.541;Pyrolysis, Artificial neural networks, Biomass modelling;Elsevier;;5224.0;Western Europe;1186.0;Q1;24154.0;Journal of analytical and applied pyrolysis;Estimation of lignocellulosic biomass pyrolysis product yields using artificial neural networks;17978.0;4452.0;191.0;820.0;9978.0;journal;article;2021
3D printing (3DP) is a progressive technology capable of transforming pharmaceutical development. However, despite its promising advantages, its transition into clinical settings remains slow. To make the vital leap to mainstream clinical practice and improve patient care, 3DP must harness modern technologies. Machine learning (ML), an influential branch of artificial intelligence, may be a key partner for 3DP. Together, 3DP and ML can utilise intelligence based on human learning to accelerate drug product development, ensure stringent quality control (QC), and inspire innovative dosage-form design. With ML’s capabilities, streamlined 3DP drug delivery could mark the next era of personalised medicine. This review details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly, how it can expedite 3DP’s integration into mainstream healthcare.;Moe Elbadawi and Laura E. McCoubrey and Francesca K.H. Gavins and Jun J. Ong and Alvaro Goyanes and Simon Gaisford and Abdul W. Basit;"Pharmacology (Q1); Toxicology (Q1)";276.0;993.0;United Kingdom;1979-2020;10.1016/j.tips.2021.06.002;0.01488;218.0;;01656147;18733735;01656147;18733735;14.819;additive manufacturing, 3D Printed drug products and formulations, Industry 4.0 and digital health, personalized oral drug delivery systems and medical devices, biomedical engineering and pharmaceutical sciences, translational pharmaceutics;Elsevier Ltd.;;6851.0;Western Europe;3878.0;Q1;22321.0;Trends in pharmacological sciences;Disrupting 3d printing of medicines with machine learning;15308.0;3178.0;101.0;288.0;6920.0;journal;article;2021
In the present study, a method for identifying the status of Acetes chinensis fishing vessels based on a 3D convolutional neural network is proposed, so as to protect marine biodiversity, monitor the working status of Acetes chinensis fishing vessels and assist in the realization of quota fishing. The Vessel Monitoring System (VMS) was installed on the quota fishing vessels to collect work data from June 16, 2021 to July 13, 2021. According to the characteristics of the fishing vessels, the work status of the fishing vessels was divided into five statuses such as stopping, sailing, putting net, waiting and pulling net. The 3D convolutional neural network Acetes3DNet was designed to extract the multi-dimensional and multi-level features of the data and trained in the training set. Finally, the effectiveness of the model was verified in the validation set. The training results were combined with the Beidou ship position data to restore the working process of the fishing vessel. The experimental results reveal that after 150 epochs of training, the precision, recall, and f1 score of Acetes3DNet on the training set reached 99.02%, 99.19%, and 99.09%, respectively, while the precision, recall, and f1 score on the validation set reached 97.09%, 96.82%, and 96.68%. Research shows that Acetes3DNet can circumvent the limitations of traditional 2D neural networks in dynamic target detection, complete recognition of the working status of Acetes chinensis quota fishing vessels, and show the historical work process of the ship in an intuitive manner. The experimental results are conducive to standardizing the management of fishing vessels and protecting marine life.;Shuxian Wang and Shengmao Zhang and Yang Liu and Jiaze Zhang and Yongwen Sun and Yuhao Yang and Huijuan Hu and Ying Xiong and Wei Fan and Fei Wang and Fenghua Tang;Aquatic Science (Q1);850.0;232.0;Netherlands;1981, 1983-2021;10.1016/j.fishres.2022.106226;0.00912;94.0;;01657836;01657836;01657836;01657836;2.422;Acetes chinensis, Quota fishing, Fishing vessels status recognition, 3D convolutional neural network;Elsevier;;5824.0;Western Europe;925.0;Q1;12781.0;Fisheries research;Recognition on the working status of acetes chinensis quota fishing vessels based on a 3d convolutional neural network;11049.0;2103.0;275.0;866.0;16015.0;journal;article;2022
The production and use of chemicals worldwide, and thus the number of those that can potentially leach into the environment, is constantly increasing. Recent advances in analytical techniques provide the opportunity to detect a wide range of contaminants in water that would not be detected by traditional targeted analysis (TA) methods. These advanced techniques include the use of high-resolution mass spectrometry (HRMS) or tandem HRMS in suspect screening analysis (SSA) or non-target analysis (NTA). This review presents the advances of the last five years for SSA and NTA of polar emerging contaminants (ECs) in various matrices, including drinking water, surface water, wastewater, and soil/sediment. We discuss all steps in the analytical procedure, including novel sampling and extraction approaches, GC or LC-HRMS analysis, (pre)data processing, evaluation, and reporting. We also identify challenges and future trends in SSA and NTA monitoring of polar ECs.;Monika Paszkiewicz and Klaudia Godlewska and Hanna Lis and Magda Caban and Anna Białk-Bielińska and Piotr Stepnowski;"Analytical Chemistry (Q1); Environmental Chemistry (Q1); Spectroscopy (Q1)";805.0;1172.0;Netherlands;1981-2020;10.1016/j.trac.2022.116671;;167.0;;01659936;01659936;01659936;01659936;;;Elsevier;;11515.0;Western Europe;2283.0;Q1;30867.0;Trac - trends in analytical chemistry;Advances in suspect screening and non-target analysis of polar emerging contaminants in the environmental monitoring;;9349.0;311.0;815.0;35811.0;journal;article;2022
Using a unique dataset containing gridded data on population densities, rents, housing sizes, and transportation in 192 cities worldwide, we investigate the empirical relevance of the monocentric standard urban model (SUM). Overall, the SUM seems surprisingly capable of capturing the inner structure of cities, both in developed and developing countries. As expected, cities spread out when they are richer, more populated, and when transportation or farmland is cheaper. Respectively 100% and 87% of the cities exhibit the expected negative density and rent gradients: on average, a 1% decrease in income net of transportation costs leads to a 21% decrease in densities and a 3% decrease in rents per m2. We also investigate the heterogeneity between cities of different characteristics in terms of monocentricity, informality, and amenities.;Charlotte Liotta and Vincent Viguié and Quentin Lepetit;"Economics and Econometrics (Q1); Urban Studies (Q1)";235.0;236.0;Netherlands;1973-2020;10.1016/j.regsciurbeco.2022.103832;0.00472;79.0;;01660462;18792308;01660462;18792308;2.613;Urbanization, Standard urban model, Urban spatial structure, Between-country comparisons;Elsevier;;4146.0;Western Europe;1343.0;Q1;15139.0;Regional science and urban economics;Testing the monocentric standard urban model in a global sample of cities;4552.0;606.0;84.0;236.0;3483.0;journal;article;2022
The human brain undergoes rapid growth in both structure and function from infancy through early childhood, and this significantly influences cognitive and behavioral development in later life. A newly emerging research framework, developmental connectomics, provides unprecedented opportunities for exploring the developing brain through non-invasive mapping of structural and functional connectivity patterns. Within this framework, we review recent neuroimaging and neurophysiological studies investigating connectome development from 20 postmenstrual weeks to 5 years of age. Specifically, we highlight five fundamental principles of brain network development during the critical first years of life, emphasizing strengthened segregation/integration balance, a remarkable hierarchical order from primary to higher-order regions, unparalleled structural and functional maturations, substantial individual variability, and high vulnerability to risk factors and developmental disorders.;Miao Cao and Hao Huang and Yong He;Neuroscience (miscellaneous) (Q1);274.0;780.0;United Kingdom;1978-2020;10.1016/j.tins.2017.06.003;0.019469999999999998;289.0;;01662236;03785912;01662236;1878108X;13.837;connectome, graph theory, segregation and integration, functional connectivity, structural connectivity, developmental disorder;Elsevier Ltd.;;8022.0;Western Europe;4715.0;Q1;19939.0;Trends in neurosciences;Developmental connectomics from infancy through early childhood;22858.0;2602.0;100.0;280.0;8022.0;journal;article;2017
Increasing supply chain complexity poses new challenges to managers. On the other hand, evolving information and communication technology offers ample opportunity for more reliable supply chain management practices. Event processing has established itself in many applications in logistics. Although the topic has enjoyed increasing popularity, there is no study taking stock of prior developments and guiding future research. Therefore, a systematic literature review on the topic of event processing in supply chain management from 2005 until the present is undertaken. Extant literature is synthesized and analyzed from technological and supply chain management perspectives to inform scholars and practitioners of existing field developments. Additionally, to guide future scholarly endeavors, a research agenda is derived from promising topics raised in papers and unfulfilled practical requirements. We find that current solutions primarily focus on a limited number of supply chain core processes and a restricted number of supply chain actors. The majority of publications focused on time-temperature sensitive products. Additionally, the domination of road transportation can be observed, while other modes of transport are often ignored in solution implementations. Decision support in terms of object traceability within the supply chain is found in most articles. RFID, typically accompanied by the Electronic Product Code Information Services standard, is the dominant enabling technology. Future research should focus on the topics of standardization, granularity, data sources, and cooperation. Moreover, holistic event processing supported by big data and machine learning techniques could create interfaces with other legacy business intelligence applications. Another promising area includes the exploration of new technologies, i.e. IoT, to enable new smart solutions.;Iurii Konovalenko and André Ludwig;"Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)";319.0;996.0;Netherlands;1979-2020;10.1016/j.compind.2018.12.009;0.00584;100.0;;01663615;01663615;01663615;01663615;7.635;Logistics, Supply chain event management, Event processing, Decision support, Literature review, Research agenda;Elsevier;;6023.0;Western Europe;1432.0;Q1;19080.0;Computers in industry;Event processing in supply chain management – the status quo and research outlook;6018.0;3165.0;115.0;323.0;6926.0;journal;article;2019
This paper examines how high-tech venture performance varies with AI-adoption intensity. We find that firm revenue increases only after sufficient investment in AI, and the benefits of AI adoption are greater at firms that also invest in complementary technologies and pursue internal R&D strategy. Specifically, AI adoption at low levels does not suggest significant revenue growth, but, as the intensity of AI adoption increases revenue growth occurs. We find that such performance gains from adoption is larger among firms that invest in complementary technologies such as cloud computing and database systems. Moreover, the positive relationship between AI adoption intensity and revenue growth is stronger among firms that pursue a more exclusive R&D strategy specific to the venture.;Yong Suk Lee and Taekyun Kim and Sukwoong Choi and Wonjoon Kim;"Engineering (miscellaneous) (Q1); Management of Technology and Innovation (Q1)";93.0;666.0;United Kingdom;1981-2020;10.1016/j.technovation.2022.102590;0.00351;130.0;;01664972;01664972;01664972;01664972;6.606;Artificial intelligence, High-tech ventures, Firm performance, Complementary investment, R&D strategy;Elsevier Ltd.;;9772.0;Western Europe;2300.0;Q1;14726.0;Technovation;When does ai pay off? ai-adoption intensity, complementary investments, and r&d strategy;8486.0;837.0;53.0;102.0;5179.0;journal;article;2022
Density log data is one of the key physical attributes used for reservoir characterization by quantifying and qualifying lithological attributes in a wellbore. The density log is most often acquired by geophysical wireline logging techniques after drilling. However, wireline logging can be difficult to execute in highly deviated wells and alternative data acquisition such as logging-while-drilling (LWD) can be very costly. This paper describes the process of using instantaneous drilling attributes together with a machine learning algorithm, extreme gradient boosting (XGBoost), to generate a pseudo density log. The mean absolute error (MAE) and root mean square error (RMSE) are used as the evaluation metrics. Case studies are performed using data from six coalbed methane (or coal seam gas) wells in the Surat Basin, Australia. The inputs include drilling data [weight on bit (WOB), rotations per minute (RPM), torque, true vertical depth (TVD) and rate of penetration (ROP)] and LWD data (i.e., natural gamma ray and hole diameter). The MAE of pseudo density log for most wells is between 0.08 and 0.11 g/cc (except Well 4 is 0.16 g/cc), which results in an average error rate less than 5%. It is found that TVD, gamma ray, ROP, and hole diameter are four most important features. Further experiment shows that the model with these four important features has almost the same performance as the model with all features. The proposed machine learning methodology can assist petroleum engineers and geologists in reservoir characterization by generating pseudo density logs from ongoing LWD and drilling data in real time. It can potentially mitigate the need to run wireline logging tools after drilling or costly LWD techniques whilst drilling.;Ruizhi Zhong and Raymond Johnson and Zhongwei Chen;"Economic Geology (Q1); Fuel Technology (Q1); Geology (Q1); Stratigraphy (Q1)";516.0;681.0;Netherlands;1980-2020;10.1016/j.coal.2020.103416;0.01168;136.0;;01665162;01665162;01665162;01665162;6.806;Density logging, Pseudo density log, Machine learning, Drilling, Coalbed methane (CBM), Coal seam gas (CSG), Extreme Gradient Boosting (XGBoost);Elsevier;;8187.0;Western Europe;2048.0;Q1;27524.0;International journal of coal geology;Generating pseudo density log from drilling and logging-while-drilling data using extreme gradient boosting (xgboost);17982.0;3772.0;178.0;520.0;14573.0;journal;article;2020
In forensic science, the emission of odours from objects or biological matrices is exploited for different purposes. For example, the monitoring of odours via biological or analytical detectors is used in thanatochemistry, the chemistry of death. The analysis of decomposition odour can be explored to support the localization of a missing body, a scenario encountered in urban search and rescue operations. A better understanding of the formation and evolution of decomposition odour is also of high interest to human remains detection canine handlers to improve training practices and chose appropriate training aids. Next to thanatochemistry, many other types of evidence evaluation benefit from the characterization of the volatile profile including the analysis of fire debris, chemical threat agents, explosives, and drugs. From a chemical point of view, an odour represents a complex mixture of gaseous molecules and its characterization demands for a powerful analytical technique. Especailly, in non-targeted analysis, the separation power provided by one-dimensional (1D) gas chromatography (GC) can be surpassed. Thus, a better insight is usually achieved using a multidimensional technique, such as comprehensive two-dimensional gas chromatography (GC×GC). This chapter focuses on scientific articles published between 2015 and 2020 reporting on the use of GC×GC for odour characterization in the context of forensic science. The main points are decomposition odour, volatolomic applications for profiling of human scent and illegal trade goods such as wildlife parts. Furthermore, the investigation of volatile traces of drugs and ignitable liquids in the context of arson investigations is addressed in detail. For each section, the length is proportional to the number of publications from the literature review.;Lena M. Dubois and Gwen O'Sullivan and Pierre-Hugues Stefanuto and Court D. Sandau and Jean-François Focant;"Analytical Chemistry (Q3); Spectroscopy (Q3)";63.0;98.0;Netherlands;1991-1992, 1996, 1999-2009, 2011-2020;10.1016/bs.coac.2021.11.007;;22.0;;0166526X;0166526X;0166526X;0166526X;;Forensic science, Odour analysis, Ignitable liquids, Fire debris investigation, VOCs, Volatolomics, Volatile organic compounds, GC×GC, Comprehensive two-dimensional gas chromatography;Elsevier;Elsevier;8889.0;Western Europe;439.0;Q3;17700155009.0;Comprehensive analytical chemistry;Chapter eleven - use of gc×gc for the characterization of odours in forensic applications;;239.0;57.0;153.0;5067.0;book series;incollection;2022
This study evaluates the impact of the anti-corruption campaign launched by President Xi since 2013 on coal mine mortality in China. Combining several unique provincial-level datasets on coal mines from 2004 to 2017, we find evidence that provinces with stronger exposure to the anti-corruption campaign have experienced a significantly larger decrease in coalmine death rates. This effect survives a vast array of robustness checks and also displays great heterogeneity. Further evidence suggests the campaign has led to fewer safety violations, more fixed investments and a decrease in profits accompanied by an increase in the costs of principal business in the coal mining industry. The above findings are most consistent with the interpretation that the campaign has made coal mining firms less likely to shirk on safety by curbing collusion between coal mines and local officials. We also rule out other channels such as intensified inspection and the change of employment composition in the industry.;Gang Xu and Xue Wang and Ruiting Wang and Go Yano and Rong Zou;"Economics and Econometrics (Q1); Organizational Behavior and Human Resource Management (Q1)";786.0;154.0;Netherlands;1980-2020;10.1016/j.jebo.2021.05.013;;115.0;;01672681;01672681;01672681;01672681;;Anti-corruption, Safety compliance, Coal mine deaths, China;Elsevier;;5150.0;Western Europe;1256.0;Q1;23865.0;Journal of economic behavior and organization;Anti-corruption, safety compliance and coal mine deaths: evidence from china;;1556.0;429.0;791.0;22092.0;journal;article;2021
Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.;Denglong Lv and Shibing Zhu;"Computer Science (miscellaneous) (Q1); Law (Q1)";550.0;675.0;United Kingdom;1982-2020;10.1016/j.cose.2020.101937;;92.0;;01674048;01674048;01674048;01674048;;Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network;Elsevier Ltd.;;5360.0;Western Europe;861.0;Q1;28898.0;Computers and security;Achieving secure big data collection based on trust evaluation and true data discovery;;3843.0;321.0;559.0;17204.0;journal;article;2020
"Background and objectives
Previous research has documented the role of different categories of psychosocial factors (i.e., sociodemographic factors, personality, subjective life circumstances, activity, physical health, and childhood circumstances) in predicting subjective well-being and quality of life among older adults. No previous study has simultaneously modeled a large number of these psychosocial factors using a well-powered sample and machine learning algorithms to predict quality of life, happiness, and life satisfaction among older adults. The aim of this paper was to investigate the correlates of quality of life, happiness, and life satisfaction among European adults older than 50 years using machine learning techniques.
Research design and methods
Data drawn from the Survey of Health, Ageing and Retirement in Europe (SHARE) Wave 7 were used. Participants were 62,500 persons aged 50 years and over living in 26 Continental EU Member States, Switzerland, and Israel. Multiple machine learning regression approaches were used.
Results
The algorithms captured 53%, 33%, and 18% of the variance of quality of life, life satisfaction, and happiness, respectively. The most important categories of correlates of quality of life and life satisfaction were physical health and subjective life circumstances. Sociodemographic factors (mostly country of residence) and psychological variables were the most important categories of correlates of happiness.
Discussion and implications
This study highlights subjective poverty, self-perceived health, country of residence, subjective survival probability, and personality factors (especially neuroticism) as important correlates of quality of life, happiness, and life satisfaction. These findings provide evidence-based recommendations for practice and/or policy implications.";Gabriele Prati;"Gerontology (Q1); Health (social science) (Q1); Aging (Q2); Geriatrics and Gerontology (Q2)";539.0;310.0;Ireland;1982-2020;10.1016/j.archger.2022.104791;0.00801;75.0;;01674943;01674943;01674943;01674943;3.250;Well-being, Machine learning, Random forest, Gradient boosting;Elsevier Ireland Ltd;;4267.0;Western Europe;985.0;Q1;28541.0;Archives of gerontology and geriatrics;Correlates of quality of life, happiness and life satisfaction among european adults older than 50 years: a machine‐learning approach;8210.0;1788.0;255.0;539.0;10881.0;journal;article;2022
Fasciolosis caused by the trematode Fasciola hepatica is an important parasitosis in both livestock and humans across the globe. Chronic infections in cattle are associated with considerable economic losses. As a prerequisite for an effective control and prevention of fasciolosis in cattle fine-scale predictive models on farm-level are needed. Since disease transmission will only occur where the mollusc intermediate host is present, the objective of our research was to develop a regression model that allows to predict the local presence or absence of Galba truncatula as principal intermediate host for Fasciola hepatica in Switzerland. By implementing generalized linear mixed models (GLMMs) a total amount of 70 variables were analysed for their potential influence on the likelihood πi of finding Galba truncatula at a certain site. Important site-specific features could be considered by selecting suitable modelling procedures. The statistical software R was used to conduct regression analysis, performing the grplasso and the glmmLasso method. The selection of parameters was based on 10-fold cross validation and the Bayesian Information Criterion (BIC). This yielded a total number of 19 potential predictor variables for the grplasso and 13 variables for the glmmLasso model, which also included random effects. Nine variables appeared to be relevant predictors for the occurrence of Galba truncatula in both models. These included reed/humid area, spring water, water bodies within a 100 m radius, and trees/bushes as powerful positive predictors. High soil depth, temperatures frequently exceeding 30 °C in the year preceding the search for snails and temperatures below 0 °C especially in the second year before were identified to exert an adverse effect on the occurrence of Galba truncatula. Temperatures measured near ground level proved to be more powerful predictors than macroclimatic parameters. Precipitation values seemed to be of minor impact in the given setting. Both regression models may be convenient for a fine-scale prediction of the occurrence of Galba truncatula, and thus provide useful approaches for the development of future spatial transmission models, mapping the risk of fasciolosis in Switzerland on farm-level.;Anne S. Roessler and Andreas W. Oehm and Gabriela Knubben-Schweizer and Andreas Groll;"Animal Science and Zoology (Q1); Food Animals (Q1)";609.0;250.0;Netherlands;1982-2020;10.1016/j.prevetmed.2022.105569;0.00645;95.0;;01675877;01675877;18731716;01675877;2.670;trematodes, liver fluke, spatial risk model, intermediate host, snail habitats, cattle diseases;Elsevier;;4196.0;Western Europe;816.0;Q1;18838.0;Preventive veterinary medicine;A machine learning approach for modelling the occurrence of galba truncatula as the major intermediate host for fasciola hepatica in switzerland;9469.0;1722.0;299.0;618.0;12546.0;journal;article;2022
The increasing cancer incidence and decreasing mortality rates in Taiwan worsened the loss ratio of cancer insurance products and created a financial crisis for insurers. In general, the loss ratio of long-term health products seems to increase with the policy year. In the present study, we used the data from Taiwan National Health Insurance Research Database to evaluate the challenge of designing cancer products. We found that the Lee–Carter and APC models have the smallest estimation errors, and the CBD and Gompertz models are good alternatives to explore the trend of cancer incidence and mortality rates, especially for the elderly people. The loss ratio of Taiwan’s cancer products is to grow and this can be deemed as a form of longevity risk. The longevity risk of health products is necessary to face in the future, similar to the annuity products.;Jack C. Yue and Hsin-Chung Wang and Yin-Yee Leong and Wei-Ping Su;"Economics and Econometrics (Q1); Statistics and Probability (Q1); Statistics, Probability and Uncertainty (Q2)";298.0;255.0;Netherlands;1982-2020;10.1016/j.insmatheco.2017.09.016;;75.0;;01676687;01676687;01676687;01676687;;Cancer insurance, Longevity risk, Big data, Stochastic models, National Health Insurance;Elsevier;;3876.0;Western Europe;1139.0;Q1;18111.0;Insurance: mathematics and economics;Using taiwan national health insurance database to model cancer incidence and mortality rates;;802.0;110.0;300.0;4264.0;journal;article;2018
"Highly Principled Data Science insists on methodologies that are: (1) scientifically justified; (2) statistically principled; and (3) computationally efficient. An astrostatistics collaboration, together with some reminiscences, illustrates the increased roles statisticians can and should play to ensure this trio, and to advance the science of data along the way.";Xiao-Li Meng;"Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)";860.0;117.0;Netherlands;1982-2021;10.1016/j.spl.2018.02.053;;66.0;;01677152;01677152;01677152;01677152;;Astrostatistics, Computational efficiency, Principled corner cutting, Scientific justification;Elsevier;;1687.0;Western Europe;576.0;Q2;14794.0;Statistics and probability letters;Conducting highly principled data science: a statistician’s job and joy;;1014.0;241.0;862.0;4066.0;journal;article;2018
With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.;Mouzhi Ge and Hind Bangui and Barbora Buhnova;"Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)";1868.0;911.0;Netherlands;1984-2021;10.1016/j.future.2018.04.053;;119.0;;0167739X;0167739X;0167739X;0167739X;;Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities;Elsevier;;4518.0;Western Europe;1262.0;Q1;12264.0;Future generation computer systems;Big data for internet of things: a survey;;16877.0;798.0;1935.0;36054.0;journal;article;2018
Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.;Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong;"Bioengineering (Q1); Biotechnology (Q1)";346.0;1186.0;United Kingdom;1983-2020;10.1016/j.tibtech.2022.02.005;0.01817;219.0;;01677799;01677799;18793096;01677799;19.536;artificial intelligence, batch effect, machine learning, RNA sequencing, single cell;Elsevier Ltd.;;6710.0;Western Europe;3192.0;Q1;16146.0;Trends in biotechnology;Are batch effects still relevant in the age of big data?;20693.0;4561.0;167.0;397.0;11206.0;journal;article;2022
"This paper presents a review of the Internet-of-Things (IoT) through four conceptualizations: IoT as liquification and density of information of resources; IoT as digital materiality; IoT as assemblage or service system; and IoT as modules, transactions, and service. From the conceptualizations, we provide a definition of IoT and present its implications and impact on future research in Marketing that interfaces with information systems, design and innovation, data science and cybersecurity, as well as organizational studies and economics. By integrating the implications of IoT with extant literature, we then propose a set of priorities for future research in this area.";Irene C.L. Ng and Susan Y.L. Wakenshaw;Marketing (Q1);129.0;515.0;Netherlands;1984-2020;10.1016/j.ijresmar.2016.11.003;0.00517;102.0;;01678116;01678116;01678116;01678116;4.513;Information technology, Consumer experience;Elsevier;;7493.0;Western Europe;3725.0;Q1;22833.0;International journal of research in marketing;The internet-of-things: review and research directions;6033.0;986.0;74.0;134.0;5545.0;journal;article;2017
Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within “Big Data”. Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.;Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas Brunner and Volker Budach and Daniel Büttner and Jürgen Debus and Andre Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin Lühr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Björn Zackrisson and Daniel Zips and Michael Baumann;"Hematology (Q1); Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";959.0;514.0;Ireland;1983-2020;10.1016/j.radonc.2014.10.001;0.02494;157.0;;01678140;18790887;01678140;18790887;6.280;Data pooling, Interoperability, Data exchange, Large scale studies, Public data, Radiotherapy;Elsevier Ireland Ltd;;3159.0;Western Europe;1892.0;Q1;17876.0;Radiotherapy and oncology;Creating a data exchange strategy for radiotherapy research: towards federated databases and anonymised public datasets;22462.0;5131.0;470.0;1026.0;14846.0;journal;article;2014
This paper describes a permutational-based Differential Evolution algorithm implemented in a wrapper scheme to find a feature subset to be applied in the construction of a near-optimal classifier. In this approach, the relevance of a feature chosen to build a better classifier is represented through its relative position in an integer-valued vector, and by using a permutational-based mutation operator, it is possible to create new feasible candidate solutions only. Furthermore, to provide a controlled diversity rate in the population, a straightforward repair-based recombination operator is utilized to evolve a population of candidate solutions. Unlike the other approaches in the existing literature using integer-valued vectors and requiring a predefined subset size, in this approach, this size is determined by an additional element included in the encoding scheme, allowing to find an adequate feature subset size to each specific dataset. Experimental results show that this approach is an effective way to create more accurate classifiers as they are compared with those obtained by other similar approaches.;Rafael Rivera-López and Efrén Mezura-Montes and Juana Canul-Reich and Marco Antonio Cruz-Chávez;"Computer Vision and Pattern Recognition (Q1); Software (Q1); Artificial Intelligence (Q2); Signal Processing (Q2)";902.0;493.0;Netherlands;1982-2020;10.1016/j.patrec.2020.02.021;0.011909999999999999;157.0;;01678655;01678655;01678655;01678655;3.756;Machine learning, Evolutionary algorithms, Wrapper scheme;Elsevier;;3489.0;Western Europe;669.0;Q1;24825.0;Pattern recognition letters;A permutational-based differential evolution algorithm for feature subset selection;16251.0;4748.0;512.0;929.0;17865.0;journal;article;2020
"Land use management of forests and croplands mainly drives the vegetation greening in China. Vegetation greening strongly modulates the trade-off between carbon sequestration via photosynthesis and water loss from evapotranspiration (ET) at the terrestrial ecosystem (representing by ecosystem water use efficiency, WUE). The function of vegetation greening in terrestrial carbon sequestration is well known, but the impacts of water loss from ET caused by vegetation greening on WUE are often neglected. Here, the GIS-based Priestley-Taylor Jet Propulsion Laboratory model was established to evaluate ET in China from 2001 to 2015, incorporating vegetation dynamics as a key component. To quantify the net effect of the ET caused by vegetation greening on WUE, we compared two different simulation scenarios: actual vegetation greening scenario and simulated without vegetation greening scenario. The results show that forests and croplands mainly contribute to the growth in GPP and NPP in China with annual rates of 2.53 gC·m−2 yr−2 and 1.59 gC·m−2 yr−2 from 2001 to 2015, respectively. With the increase of terrestrial carbon sequestration, the ET under actual vegetation greening scenario was generated 6.78 mm·yr−1 more than that under simulated without vegetation greening scenario. But as a result of the negative impacts of vegetation physiological effect (elevated CO2 concentration and the decreased VPD) on ET, values of ET under two different scenarios all exhibited a decline trend from 2001 to 2015 with rates of − 2.04% and − 3.63%, respectively. Consequently, although the WUE under two different scenarios exhibited increased trends (6.44%, actual vegetation dynamics scenario; 10.74%, simulated without vegetation greening scenario), the ET caused by vegetation greening led to an obvious divergence between the WUE under two different scenarios. For better understanding the impacts of human activities on carbon and water cycles at the terrestrial ecosystem, it is necessary to take the water loss from ET caused by vegetation greening into consideration, which is crucial for enhancing the sustainability of future vegetation-related projects.";Xin Lan and Zhiyong Liu and Xiaohong Chen and Kairong Lin and Linying Cheng;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Ecology (Q1)";1012.0;540.0;Netherlands;1983-2020;10.1016/j.agee.2021.107522;;174.0;;01678809;01678809;01678809;01678809;;Land use, Forest, Cropland, GPP, NPP, Evapotranspiration, PT-JPL model;Elsevier;;6568.0;Western Europe;1844.0;Q1;15110.0;Agriculture, ecosystems and environment;Trade-off between carbon sequestration and water loss for vegetation greening in china;;5842.0;316.0;1016.0;20756.0;journal;article;2021
Depression is a leading mental health problem affecting 300 million people globally. Recent studies show that social networks provide a tremendous potential for mental health professionals as a source of supplemental information about their patients. This study presents a methodological framework for clinical decision support systems (CDSSs) through analysis of social network data to distinguish the language usage of individuals with early signs of depression (i.e., contrast language analysis). By analyzing the contrast language patterns of different user groups, we are able to uncover constructive and actionable insights into the pain points and characteristics of users with signs of depression as decision support mechanisms for clinicians during intervention, (early) diagnosis and treatment plans. First, we discover terms that represent contrasting language by analyzing the percentage difference of terms in two user groups, labeled as”depressed” and”non-depressed” for ease of reference. Second, by building topic models based on social network contents, the topic-level contrast features are discovered. Finally, we consider the structure of the social network to discover the network-level contrast features. To illustrate the effectiveness of the proposed framework, we present a case study on early depression detection using a real-world dataset. The proposed framework has methodological contributions in enhancing the features and functionalities of CDSS for clinicians. It also contributes to evidence-based health research through cost-effective data and analytical insights that can supplement or improve the traditional survey and time-consuming interview methods.;Xingwei Yang and Alexandra Joukova and Anteneh Ayanso and Morteza Zihayat;"Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)";338.0;704.0;Netherlands;1985-2020;10.1016/j.dss.2022.113813;0.0081;151.0;;01679236;01679236;01679236;01679236;5.795;Contrast language analysis, Clinical Decision Support System (CDSS), Depression detection, Social network;Elsevier;;6219.0;Western Europe;1564.0;Q1;21933.0;Decision support systems;Social influence-based contrast language analysis framework for clinical decision support systems;13580.0;2672.0;115.0;342.0;7152.0;journal;article;2022
Large data sets that originate from administrative or operational activity are increasingly used for statistical analysis as they often contain very precise information and a large number of observations. But there is evidence that some variables can be subject to severe misclassification or contain missing values. Given the size of the data, a flexible semiparametric misclassification model would be good choice but their use in practise is scarce. To close this gap a semiparametric model for the probability of observing labour market transitions is estimated using a sample of 20 m observations from Germany. It is shown that estimated marginal effects of a number of covariates are sizeably affected by misclassification and missing values in the analysis data. The proposed generalized partially linear regression extends existing models by allowing a misclassified discrete covariate to be interacted with a nonparametric function of a continuous covariate.;Stephan Dlugosz and Enno Mammen and Ralf A. Wilke;"Applied Mathematics (Q1); Computational Mathematics (Q1); Computational Theory and Mathematics (Q1); Statistics and Probability (Q2)";495.0;214.0;Netherlands;1983-2021;10.1016/j.csda.2017.01.003;;115.0;;01679473;01679473;01679473;01679473;;Semiparametric regression, Measurement error, Side information;Elsevier;;3810.0;Western Europe;1093.0;Q1;28461.0;Computational statistics and data analysis;Generalized partially linear regression with misclassified data and an application to labour market transitions;;1122.0;168.0;502.0;6401.0;journal;article;2017
The observation of growing “difficulties” in IT-infrastructures in neuroscience research during the last years led to a search for reasons and an analysis on how this phenomenon is reflected in the scientific literature. With a retrospective analysis of nine examples of multicenter research projects in the neurosciences and a literature review the observation was systematically analyzed. Results show that the rise in complexity mainly stems from two reasons: (1) more and more need for information on quality and context of research data (metadata) and (2) long-term requirements to handle the consent and identity/pseudonyms of study participants and biomaterials in relation to legal requirements. The combination of these two aspects together with very long study times and data evaluation periods are components of the subjectively perceived “difficulties”. A direct consequence of this result is that big multicenter trials are becoming part of integrated research data environments and are not standing alone for themselves anymore. This drives up the resource needs regarding the IT-infrastructure in neuroscience research. In contrast to these findings, literature on this development is scarce and the problem probably underestimated.;Karoline Buckow and Matthias Quade and Otto Rienhoff and Sara Y. Nussbeck;"Medicine (miscellaneous) (Q1); Neuroscience (miscellaneous) (Q2)";281.0;290.0;Ireland;1984-2020;10.1016/j.neures.2014.08.005;0.004529999999999999;99.0;;01680102;18728111;01680102;18728111;3.304;IT-infrastructure, Metadata, Identity management, Data quality, Neuroscience, Infrastructure methodology;Elsevier Ireland Ltd;;5507.0;Western Europe;1234.0;Q1;18054.0;Neuroscience research;Changing requirements and resulting needs for it-infrastructure for longitudinal research in the neurosciences;5711.0;840.0;174.0;292.0;9582.0;journal;article;2016
As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.;Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)";1298.0;727.0;Netherlands;1985-2020;10.1016/j.compag.2018.08.045;0.01646;115.0;;01681699;01681699;01681699;01681699;5.565;Smart farm, Lossy compression, IoT, Signal processing, Data fidelity;Elsevier;;4433.0;Western Europe;1208.0;Q1;30441.0;Computers and electronics in agriculture;Evaluating fidelity of lossy compression on spatiotemporal data from an iot enabled smart farm;17657.0;9479.0;648.0;1300.0;28725.0;journal;article;2018
The peak photosynthesis timing (PPT) is a key factor that affects the seasonality of the terrestrial carbon uptake. Carbon phenology derived from gross primary production (GPP) has been used to validate the peak greenness timing (PGT) from satellite-based vegetation indices (VIs) in phenology research. However, PPT, derived from GPP, has not been comprehensively analyzed, especially taking different GPP estimates, fitting methods, and biomes into account. Moreover, whether or not the PPT trend is consistent with the reported PGT trend still unclear. We explored the above questions at widely used flux sites in Northern Hemisphere mid-latitudes and found that no significant differences in PPT derived from GPP using different carbon flux partitioning methods. Moreover, fitting methods performed well in grassland, cropland, wetland, and wood savannas compared with evergreen needleleaf forest, deciduous broadleaf forest, and mixed forest. Unexpectedly, we did not find an advancing trend in PPT derived from GPP compared with PGT from SPOT-VGT normalized difference vegetation index (NDVI). Our study suggests that the principle of the fitting method and physiological property of the biome should be taken into account when predicting PPT. More importantly, PGT is not a good proxy of the PPT. Therefore, PPT trends based on VIs should be viewed with caution. In general, this study is meaningful for better understanding photosynthesis and carbon cycling in the context of changing climate.;Zhongxi Ge and Jing Huang and Xufeng Wang and Xuguang Tang and Lei Fan and Yinjun Zhao and Mingguo Ma;"Agronomy and Crop Science (Q1); Atmospheric Science (Q1); Forestry (Q1); Global and Planetary Change (Q1)";1053.0;579.0;Netherlands;1984-2020;10.1016/j.agrformet.2022.109054;0.02089;165.0;;01681923;01681923;01681923;01681923;5.734;Phenology, Climate change, FLUXNET2015, Potential PPT, Actual PPT;Elsevier;;6838.0;Western Europe;1837.0;Q1;12067.0;Agricultural and forest meteorology;Contrasting trends between peak photosynthesis timing and peak greenness timing across seven typical biomes in northern hemisphere mid-latitudes;24457.0;6191.0;336.0;1055.0;22974.0;journal;article;2022
With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.;Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães;"Instrumentation (Q2); Nuclear and High Energy Physics (Q3)";1750.0;142.0;Netherlands;1983-2020;10.1016/j.nimb.2020.05.027;;119.0;;0168583X;0168583X;0168583X;0168583X;;Ion beam analysis, Big data, Data quality assurance, Artificial intelligence;Elsevier;;3041.0;Western Europe;429.0;Q2;29068.0;Nuclear instruments and methods in physics research, section b: beam interactions with materials and atoms;Ion beam analysis and big data: how data science can support next-generation instrumentation;;2403.0;514.0;1766.0;15630.0;journal;article;2020
"Summary
In this review article, we discuss the model for end-stage liver disease (MELD) score and its dual purpose in general and transplant hepatology. As the landscape of liver disease and transplantation has evolved considerably since the advent of the MELD score, we summarise emerging concepts, methodologies, and technologies that may improve mortality prognostication in the future. Finally, we explore how these novel concepts and technologies may be incorporated into clinical practice.";Jin Ge and W. Ray Kim and Jennifer C. Lai and Allison J. Kwong;Hepatology (Q1);651.0;966.0;Netherlands;1985-2020;10.1016/j.jhep.2022.03.003;0.07879;243.0;;01688278;16000641;01688278;16000641;25.083;MELD, Prognostication, Allocation, Frailty, Sarcopenia, EHR, OMOP, Clinical Decision Support;Elsevier;;3352.0;Western Europe;7112.0;Q1;25297.0;Journal of hepatology;“beyond meld” – emerging strategies and technologies for improving mortality prediction, organ allocation and outcomes in liver transplantation;52238.0;12466.0;406.0;1196.0;13611.0;journal;article;2022
"Background
The aim of this article is to evaluate the status, development, and perspectives of German claims data analyses in the international and health political context.
Methods
We conducted a comprehensive literature search in PubMed, Scopus, and DIMDI to identify empirical and methodological articles focusing on health insurance claims data studies published between 2000 and 2014. Inclusion criteria were (1) English/German full text articles or chapters in edited books that (2) focused on the claims data of statutory health insurance funds.
Findings
In total, 435 articles were included. Over time, the number of claims data studies has increased strongly and the frequency of policy-relevant research types increased. Along with the historical improvement path of claims data in Germany, we observed a rising percentage of international publications and an increase in the average quality of publications. In contrast to the US or Canada where comprehensive databases have been established, the most common data source in this search was data from a single SHI fund, while databases were rarely used.
Conclusions
Claims data are an important source of information for healthcare stakeholders, and their use for research purposes has further increased during recent years in Germany. Despite its potential in optimising the health system, we found a lack of German comprehensive all-payer claims databases compared to the US and Canada.";Kristine Kreis and Sarah Neubauer and Mike Klora and Ansgar Lange and Jan Zeidler;Health Policy (Q1);499.0;294.0;Ireland;1984-2020;10.1016/j.healthpol.2016.01.007;0.01001;92.0;;01688510;18726054;01688510;18726054;2.980;Claims data analysis, Administrative data, Health services research, Germany, Statutory health insurance, Data source;Elsevier Ireland Ltd;;4751.0;Western Europe;1214.0;Q1;21306.0;Health policy;Status and perspectives of claims data analyses in germany—a systematic review;9058.0;1526.0;175.0;514.0;8315.0;journal;article;2016
The vast majority of high energy physics experiments rely on data acquisition and hardware-based trigger systems performing a number of stringent selections before storing data for offline analysis. The online reconstruction and selection performed at the trigger level are bound to the synchronous nature of the data acquisition system, resulting in a trade-off between the amount of data collected and the complexity of the online reconstruction performed. Exotic physics processes, such as long-lived and slow-moving particles, are rarely targeted by online triggers as they require complex and nonstandard online reconstruction, usually incompatible with the time constraints of most data acquisition systems. The online trigger selection can thus impact as one of the main limiting factors to the experimental reach for exotic signatures. Alternative data acquisition solutions based on the continuous and asynchronous processing of the stream of data from the detectors are therefore foreseeable as a way to extend the experimental physics reach. Trigger-less data readout systems, paired with efficient streaming data processing solutions, can provide a viable alternative. In this document, an end-to-end implementation of a fully trigger-less data acquisition and online data processing system is discussed. An easily scalable and deployable implementation of such an architecture is proposed, based on open-source distributed computing frameworks capable of performing asynchronous online processing of streaming data. The proposed schema can be suitable for deployment as a fully integrated data acquisition system for small-scale experimental apparatus, or to complement the trigger-based data acquisition systems of larger experiments. A muon telescope setup consisting of a set of gaseous detectors is used as the experimental development testbed in this work, and a fully integrated online processing pipeline deployed on cloud computing resources is implemented and described.;Matteo Migliorini and Jacopo Pazzini and Andrea Triossi and Marco Zanetti and Alberto Zucchetta;"Instrumentation (Q1); Nuclear and High Energy Physics (Q2)";3037.0;182.0;Netherlands;1983-2020;10.1016/j.nima.2022.166869;;170.0;;01689002;01689002;01689002;01689002;;Data acquisition, Trigger, Online data processing;Elsevier;;2194.0;Western Europe;747.0;Q1;29067.0;Nuclear instruments and methods in physics research, section a: accelerators, spectrometers, detectors and associated equipment;A horizontally scalable online processing system for trigger-less data acquisition;;5597.0;1179.0;3046.0;25868.0;journal;article;2022
At the 4th International Plant Phenotyping Symposium meeting of the International Plant Phenotyping Network (IPPN) in 2016 at CIMMYT in Mexico, a workshop was convened to consider ways forward with sensors for phenotyping. The increasing number of field applications provides new challenges and requires specialised solutions. There are many traits vital to plant growth and development that demand phenotyping approaches that are still at early stages of development or elude current capabilities. Further, there is growing interest in low-cost sensor solutions, and mobile platforms that can be transported to the experiments, rather than the experiment coming to the platform. Various types of sensors are required to address diverse needs with respect to targets, precision and ease of operation and readout. Converting data into knowledge, and ensuring that those data (and the appropriate metadata) are stored in such a way that they will be sensible and available to others now and for future analysis is also vital. Here we are proposing mechanisms for “next generation phenomics” based on our learning in the past decade, current practice and discussions at the IPPN Symposium, to encourage further thinking and collaboration by plant scientists, physicists and engineering experts.;Thomas Roitsch and Llorenç Cabrera-Bosquet and Antoine Fournier and Kioumars Ghamkhar and José Jiménez-Berni and Francisco Pinto and Eric S. Ober;"Agronomy and Crop Science (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Plant Science (Q1)";727.0;454.0;Ireland;1983, 1985-2020;10.1016/j.plantsci.2019.01.011;0.013280000000000002;150.0;;01689452;01689452;18732259;01689452;4.729;Imaging, IPPN, Metadata, Next generation phenomics, Plant phenotyping, Sensor development, Trait value;Elsevier Ireland Ltd;;6950.0;Western Europe;1508.0;Q1;16620.0;Plant science;Review: new sensors and data-driven approaches—a path to next generation phenomics;20176.0;3411.0;317.0;735.0;22030.0;journal;article;2019
Data commons collate data with cloud computing infrastructure and commonly used software services, tools, and applications to create biomedical resources for the large-scale management, analysis, harmonization, and sharing of biomedical data. Over the past few years, data commons have been used to analyze, harmonize, and share large-scale genomics datasets. Data ecosystems can be built by interoperating multiple data commons. It can be quite labor intensive to curate, import, and analyze the data in a data commons. Data lakes provide an alternative to data commons and simply provide access to data, with the data curation and analysis deferred until later and delegated to those that access the data. We review software platforms for managing, analyzing, and sharing genomic data, with an emphasis on data commons, but also cover data ecosystems and data lakes.;Robert L. Grossman;Genetics (Q1);261.0;846.0;United Kingdom;1985-2020;10.1016/j.tig.2018.12.006;0.0188;223.0;;01689525;01689525;13624555;01689525;11.639;data commons, data clouds, data sharing, cancer genomics clouds;Elsevier Ltd.;;6562.0;Western Europe;5713.0;Q1;19049.0;Trends in genetics;Data lakes, clouds, and commons: a review of platforms for analyzing and sharing genomic data;14465.0;2607.0;126.0;281.0;8268.0;journal;article;2019
Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.;Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey;Information Systems and Management (Q2);152.0;255.0;Netherlands;1985, 1987-2020;10.1016/j.datak.2021.101911;;87.0;;0169023X;0169023X;0169023X;0169023X;;Conceptual modeling, Big Data, Machine learning, Artificial Intelligence;Elsevier;;4086.0;Western Europe;480.0;Q2;24437.0;Data and knowledge engineering;Conceptual modeling in the era of big data and artificial intelligence: research topics and introduction to the special issue;;440.0;37.0;155.0;1512.0;journal;article;2021
"The advent of modern data collection and storage technologies has brought about a huge increase in data volumes with both traditional and machine learning tools struggling to effectively handle, manage and analyse the very large data quantities that are now available. The mineral exploration industry is by no means immune to this big data issue. Exploration decision-making has become much more complex in the wake of big data, in particular with respect to questions about how to best manage and use the data to obtain information, generate knowledge and gain insight. One of the ways in which the mineral exploration industry works with big data is by using a geographic information system (GIS). For example, GIS platforms are often used for integration, interrogation and interpretation of diverse geoscience and mineral exploration data with the goal of refining and prioritising known and identifying new targets. Here we (i) briefly discuss the importance of carefully translating conceptual ore deposit models into effective exploration targeting maps, (ii) propose and describe what we term exploration information systems (EIS): a new idea for an information system designed to better integrate the conceptual mineral deposit model (i.e., the critical and constituent processes of the targeted mineral system) with data available to support exploration targeting, and (iii) discuss how best to categorise mineral systems in an EIS as scale-dependent subsystems to form mineral deposits. Our vision for the future use of EIS in exploration targeting is one whereby the mappable ingredients of a targeted mineral system are translated and combined into a set of weighted evidence (or proxy) maps automatically, resulting in an auto-generated mineral prospectivity map and a series of ranked exploration targets. We do not envisage the EIS replacing human input and ingenuity; rather we envisage the EIS as an additional tool in the exploration toolbox and as an intelligence amplifying system in which humans are making use of machines to achieve the best possible results.";Mahyar Yousefi and Oliver P. Kreuzer and Vesa Nykänen and Jon M.A. Hronsky;"Economic Geology (Q1); Geochemistry and Petrology (Q1); Geology (Q1)";1297.0;377.0;Netherlands;1986-2020;10.1016/j.oregeorev.2019.103005;0.01566;97.0;;01691368;01691368;01691368;01691368;3.809;Exploration information systems (EIS), Mineral exploration targeting, Geographic information systems (GIS), Mineral systems approach;Elsevier BV;;9404.0;Western Europe;1413.0;Q1;25702.0;Ore geology reviews;Exploration information systems – a proposal for the future use of gis in mineral exploration targeting;14843.0;5534.0;558.0;1326.0;52477.0;journal;article;2019
The last several decades have witnessed a rapid yet uneven urban expansion in developing countries. The existing studies rely heavily on official statistical yearbooks and remote sensing images. However, the former data sources have been criticized due to its non-objectivity and low quality, while the latter is labor and cost consuming in most cases. Recent efforts made by fractal analyses provide alternatives to scrutinize the corresponding “natural urban area”. In our proposed framework, the dynamics of internal urban contexts is reflected in a quasi-real-time manner using emerging new data and the expansion is a fractal concept instead of an absolute one based on the conventional Euclidean method. We then evaluate the magnitude and pattern of natural cities and their expansion in size and space. It turns out that the spatial expansion rate of official cities (OCs) in our study area China has been largely underestimated when compared with the results of natural cities (NCs). The perspective of NCs also provides a novel way to understanding the quality of uneven urban expansion. We detail our analysis for the 23 urban agglomerations in China, especially paying more attention to the three most dominating urban agglomerations of China: Beijing-Tianjin-Hebei (BTH), Yangtze River Delta (YRD) and Pearl River Delta (PRD). The findings from the OC method are not consistent with the NC method. The distinctions may arise from the definition of a city, and the bottom-up NC method contributes to our comprehensive understanding of uneven urban expansion.;Ying Long and Weixin Zhai and Yao Shen and Xinyue Ye;"Ecology (Q1); Management, Monitoring, Policy and Law (Q1); Nature and Landscape Conservation (Q1)";693.0;616.0;Netherlands;1986-2020;10.1016/j.landurbplan.2017.05.008;0.01602;161.0;;01692046;01692046;01692046;01692046;6.142;Urban expansion, Social media, Head/tail division, New data, Open data, China;Elsevier;;6701.0;Western Europe;1938.0;Q1;19041.0;Landscape and urban planning;Understanding uneven urban expansion with natural cities using open data;22743.0;5148.0;180.0;704.0;12061.0;journal;article;2018
;Tao Hong and Pierre Pinson;Business and International Management (Q1);261.0;450.0;Netherlands;1985-2020;10.1016/j.ijforecast.2019.05.004;0.0061200000000000004;96.0;;01692070;01692070;01692070;01692070;3.779;;Elsevier;;3928.0;Western Europe;1268.0;Q1;22706.0;International journal of forecasting;Energy forecasting in the big data world;7207.0;1254.0;142.0;277.0;5578.0;journal;article;2019
"Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.";Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon;"Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2)";753.0;627.0;Ireland;1985-2020;10.1016/j.cmpb.2018.10.016;0.01119;102.0;;01692607;01692607;18727565;01692607;5.428;Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic;Elsevier Ireland Ltd;;4409.0;Western Europe;924.0;Q1;23604.0;Computer methods and programs in biomedicine;Initializing a hospital-wide data quality program. the ap-hp experience.;12277.0;4743.0;538.0;789.0;23721.0;journal;article;2019
A paradigm shift from current population based medicine to personalized and participative medicine is underway. This transition is being supported by the development of clinical decision support systems based on prediction models of treatment outcome. In radiation oncology, these models ‘learn’ using advanced and innovative information technologies (ideally in a distributed fashion — please watch the animation: http://youtu.be/ZDJFOxpwqEA) from all available/appropriate medical data (clinical, treatment, imaging, biological/genetic, etc.) to achieve the highest possible accuracy with respect to prediction of tumor response and normal tissue toxicity. In this position paper, we deliver an overview of the factors that are associated with outcome in radiation oncology and discuss the methodology behind the development of accurate prediction models, which is a multi-faceted process. Subsequent to initial development/validation and clinical introduction, decision support systems should be constantly re-evaluated (through quality assurance procedures) in different patient datasets in order to refine and re-optimize the models, ensuring the continuous utility of the models. In the reasonably near future, decision support systems will be fully integrated within the clinic, with data and knowledge being shared in a standardized, dynamic, and potentially global manner enabling truly personalized and participative medicine.;Philippe Lambin and Jaap Zindler and Ben G.L. Vanneste and Lien Van {De Voorde} and Daniëlle Eekers and Inge Compter and Kranthi Marella Panth and Jurgen Peerlings and Ruben T.H.M. Larue and Timo M. Deist and Arthur Jochems and Tim Lustberg and Johan {van Soest} and Evelyn E.C. {de Jong} and Aniek J.G. Even and Bart Reymen and Nicolle Rekers and Marike {van Gisbergen} and Erik Roelofs and Sara Carvalho and Ralph T.H. Leijenaar and Catharina M.L. Zegers and Maria Jacobs and Janita {van Timmeren} and Patricia Brouwers and Jonathan A. Lal and Ludwig Dubois and Ala Yaromina and Evert Jan {Van Limbergen} and Maaike Berbee and Wouter {van Elmpt} and Cary Oberije and Bram Ramaekers and Andre Dekker and Liesbeth J. Boersma and Frank Hoebers and Kim M. Smits and Adriana J. Berlanga and Sean Walsh;Pharmaceutical Science (Q1);443.0;1323.0;Netherlands;1987-2020;10.1016/j.addr.2016.01.006;0.0286;313.0;;0169409X;0169409X;18728294;0169409X;15.470;Radiotherapy, Decision support systems, Prediction models, Shared decision making;Elsevier;;20675.0;Western Europe;3475.0;Q1;19409.0;Advanced drug delivery reviews;Decision support systems for personalized and participative radiation oncology;43769.0;6945.0;138.0;485.0;28532.0;journal;article;2017
"Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.";Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park;"Cancer Research (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine (Q1)";845.0;471.0;Ireland;1985-2020;10.1016/j.lungcan.2020.05.033;0.01966;129.0;;01695002;01695002;18728332;01695002;5.705;Real-time updated system, Big data, Real-world data, NSCLC, Treatment;Elsevier Ireland Ltd;;3011.0;Western Europe;1989.0;Q1;12391.0;Lung cancer;"Ten-year patient journey of stage iii non-small cell lung cancer patients: a single-center, observational, retrospective study in korea (realtime automatically updated data warehouse in health care; universe-root study)";15504.0;4671.0;349.0;917.0;10507.0;journal;article;2020
Technologies to identify individual animals, follow their movements, identify and locate animal and plant species, and assess the status of their habitats remotely have become better, faster, and cheaper as threats to the survival of species are increasing. New technologies alone do not save species, and new data create new problems. For example, improving technologies alone cannot prevent poaching: solutions require providing appropriate tools to the right people. Habitat loss is another driver: the challenge here is to connect existing sophisticated remote sensing with species occurrence data to predict where species remain. Other challenges include assembling a wider public to crowdsource data, managing the massive quantities of data generated, and developing solutions to rapidly emerging threats.;Stuart L. Pimm and Sky Alibhai and Richard Bergl and Alex Dehgan and Chandra Giri and Zoë Jewell and Lucas Joppa and Roland Kays and Scott Loarie;Ecology, Evolution, Behavior and Systematics (Q1);276.0;1003.0;United Kingdom;1986-2020;10.1016/j.tree.2015.08.008;;342.0;;01695347;01695347;18728383;01695347;;technology, conservation, crowdsourcing, remote-sensing, traditional knowledge, innovation;Elsevier Ltd.;;5642.0;Western Europe;6476.0;Q1;14365.0;Trends in ecology and evolution;Emerging technologies to conserve biodiversity;;3932.0;144.0;391.0;8124.0;journal;article;2015
In recent years, the wealth of technological development revolutionised our ability to collect data in geosciences. Due to the unprecedented level of detail of these datasets, geomorphologists are facing new challenges, giving more in-depth answers to a broad(er) range of fundamental questions across the full spectrum of the Earth's (and Planetary) processes. This contribution builds on the existing literature of geomorphometry (the science of quantitative land-surface analysis) and feature extraction (translate land surface parameters into extents of geomorphological elements). It provides evidence of critical themes as well as emerging fields of future research in the digital realm, supporting the likely effectiveness of geomorphometry and feature extractions as they are advancing the theoretical, empirical and applied dimension of geomorphology. The review further discusses the role of geomorphometric legacies, and scientific reproducibility, and how they can be implemented, in the hope that this will facilitate action towards improving the transparency, and efficiency of scientific research, and accelerate discoveries in geomorphology. In the current landscape, substantial changes in landforms, ecosystems, land use, hydrological routing, and direct anthropogenic modifications impact systems across the full spectrum of geomorphological processes. Although uncertainties in the precise nature and likelihood of changes exist, geomorphometry and feature extraction can aid exploring process regimes and landscape responses. Taken together, they can revolutionise geomorphology by opening the doors to improved investigations crossing space and time scales, blurring the boundaries between traditional approaches and computer modelling, and facilitating cross-disciplinary research. Ultimately, the exploitation of the available wealth of digital information can help to translate our understanding of geomorphic processes, which is often based on observations of past or current conditions, into the rapidly changing future.;G. Sofia;Earth-Surface Processes (Q1);1105.0;391.0;Netherlands;1984, 1987-2020;10.1016/j.geomorph.2020.107055;0.01992;159.0;;0169555X;0169555X;1872695X;0169555X;4.139;Geomorphometry, Feature extraction, Lidar, Digital elevation model, Global;Elsevier;;8297.0;Western Europe;1346.0;Q1;26979.0;Geomorphology;Combining geomorphometry, feature extraction techniques and earth-surface processes research: the way forward;28906.0;5052.0;398.0;1130.0;33022.0;journal;article;2020
While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.;Diana Moise and Denis Shestakov;"Applied Mathematics (Q4); Modeling and Simulation (Q4); Statistics and Probability (Q4)";1.0;112.0;Netherlands;1980, 1982-1985, 1988, 1991, 1993-1994, 1996-1998, 2000-2001, 2003, 2005-2007, 2009, 2012-2020;10.1016/B978-0-444-63492-4.00012-5;;42.0;;01697161;01697161;01697161;01697161;;Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves;Elsevier;Elsevier;5384.0;Western Europe;125.0;Q4;17700156445.0;Handbook of statistics;Chapter 12 - terabyte-scale image similarity search;;82.0;19.0;89.0;1023.0;book series;incollection;2015
"Conventional static soft sensor is incapable of handling the dynamic of processes. With abundance of data, the problem of variable correlations and a large number of samples are encountered; moreover, the quality of the data for the construction of the soft sensors can be crucial for performance. An active learning strategy based on a latent variable model (LVM) to select representative data for efficient development of the dynamic soft sensor model is proposed. The uncertainty information for data selection is provided by the Gaussian process (GP) model. The developed LVM with the auxiliary GP model can handle the process dynamic. An active forward-update scheme which can update the soft sensor model in advance is proposed to reflect the current status of the process and improve the prediction performance without waiting for the quality measurements. Two case studies are done to demonstrate the features and the applicability of the proposed method.";Lester Lik Teck Chan and Qing-Yang Wu and Junghui Chen;"Analytical Chemistry (Q2); Computer Science Applications (Q2); Process Chemistry and Technology (Q2); Software (Q2); Spectroscopy (Q2)";517.0;359.0;Netherlands;1986-2020;10.1016/j.chemolab.2018.01.015;0.006959999999999999;126.0;;01697439;01697439;01697439;01697439;3.491;Active learning, Forward-update, Large database, Latent variable model, Model uncertainty, Soft sensor;Elsevier;;4317.0;Western Europe;600.0;Q2;24595.0;Chemometrics and intelligent laboratory systems;Dynamic soft sensors with active forward-update learning for selection of useful data from historical big database;11735.0;1977.0;209.0;522.0;9022.0;journal;article;2018
In this paper, we comprehensively evaluated the utility of integrated long-term satellite-based precipitation and evapotranspiration products for drought monitoring over mainland China. The latest Integrated Multi-satelliteE Retrievals for Global Precipitation Measurement V06 three Runs precipitation products, i.e., the near real-time Early Run (IMERG-E) and Late Run (IMERG-L) and the post-real time Final Run (IMERG-F), and the Global Land Evaporation Amsterdam Model V3.3a (GLEAM) potential evapotranspiration (PET) products from 2001 to 2017 were considered. The accuracy of IMERG precipitation and GLEAM PET products was first evaluated against observed precipitation and Penman-Monteith method estimated PET, respectively, based on dense meteorological station network. The Standard Precipitation Evapotranspiration Index (SPEI) calculated based on IMERG precipitation and GLEAM PET products (SPEIs, including SPEIE, SPEIL and SPEIF corresponding to IMERG-E, IMERG-L and IMERG-F, respectively) were then validated by using SPEI calculated based on meteorological data (SPEIm) at multiple temporal-spatial scales. Finally, four typical drought events were selected to analyse the ability of SPEIs to characterize the temporal-spatial evolution of drought situations. The results showed that the IMERG-F presents much better performance than IMERG-E and IMERG-L in terms of higher CC and smaller BIAS and RMSE values over mainland China. The GLEAM PET well simulated the change trend of reference PET, but generally underestimated reference PET in Northwest China (NW), Xinjiang (XJ) and Qinghai–Tibet plateau (TP). In general, the performances of SPEIs over eastern China and Southwest China (SW) were significantly superior to their performances in the NW, XJ, and TP regions. Even though the SPEIF performed the best, the SPEIE and SPEIL also performed reasonably well in some specific regions. SPEIs can well capture the temporal process and reasonably reflect the spatial characteristics for four typical drought events. It is thus highlighted that the latest IMERG precipitation (especially for IMERG-F) and GLEAM PET products could be used as alternative data sources for comprehensive drought monitoring, on account of the water balance principle over mainland China, particularly in eastern China and SW China. The outcomes of this study will provide valuable references for drought monitoring by integration of multi-source remote-sensing datasets in the GPM era.;Shanhu Jiang and Linyong Wei and Liliang Ren and Chong-Yu Xu and Feng Zhong and Menghao Wang and Linqi Zhang and Fei Yuan and Yi Liu;Atmospheric Science (Q1);874.0;535.0;Netherlands;1986-2021;10.1016/j.atmosres.2020.105141;0.01714;106.0;;01698095;01698095;01698095;01698095;5.369;IMERG, GLEAM, Standardized Precipitation Evapotranspiration Index (SPEI), Drought monitoring, Mainland China;Elsevier BV;;6363.0;Western Europe;1488.0;Q1;12092.0;Atmospheric research;Utility of integrated imerg precipitation and gleam potential evapotranspiration products for drought monitoring over mainland china;16017.0;4864.0;365.0;875.0;23226.0;journal;article;2021
Cloud manufacturing adopts a cloud computing paradigm as the basis for delivering shared, on-demand manufacturing services. The result is customer-centric supply chains that can be configured for cost, quality, speed and customisation. While the technical capabilities required for cloud manufacturing are a current focus, there are many emerging questions relating to the impact, both positive and negative, on the people consuming or supporting cloud manufacturing services. Human factors can have a pivotal role in enabling the success and adoption of cloud manufacturing, while ensuring the safety, well-being and optimum user experience of those involved in a cloud manufacturing environment. This paper presents these issues, structured around groups of users (service providers, application providers and consumers). We also consider the issues of collaboration that are likely to arise from the manufacturing cloud. From this analysis we discuss the central role of human factors as an enabler of cloud manufacturing, and the opportunities that emerge.;David Golightly and Sarah Sharples and Harshada Patel and Svetan Ratchev;"Human Factors and Ergonomics (Q2); Public Health, Environmental and Occupational Health (Q2)";342.0;328.0;Netherlands;1986-2020;10.1016/j.ergon.2016.05.011;0.00249;79.0;;01698141;01698141;18728219;01698141;2.656;Cloud manufacturing, Assembly, Production, Collaboration;Elsevier;;5101.0;Western Europe;570.0;Q2;12385.0;International journal of industrial ergonomics;Manufacturing in the cloud: a human factors perspective;4782.0;1089.0;119.0;344.0;6070.0;journal;article;2016
"The quarterly Literature Listing is intended as a current awareness service for readers indicating newly published books, journal and conference articles on: patent search techniques, databases, analysis and classifications; patent searcher certification; patents relating to a) life sciences and pharmaceuticals and b) software; patent policy and strategic issues; trade marks; designs; domain names; and articles reviewing historical aspects of intellectual property or reviewing specific topics/persons. The current Literature Listing was compiled end-August 2019. Key resources used are Scopus, Digital Commons, publishers’ RSS feeds, and serendipity! Please feel free to send the author details of newly published reports/monographs/books for potential inclusion.";Susan Bates;"Library and Information Sciences (Q2); Bioengineering (Q3); Computer Science Applications (Q3); Energy Engineering and Power Technology (Q3); Fuel Technology (Q3); Process Chemistry and Technology (Q3); Renewable Energy, Sustainability and the Environment (Q3)";104.0;112.0;United Kingdom;1979-2020;10.1016/j.wpi.2019.101925;;30.0;;01722190;01722190;01722190;01722190;;Patents, Designs, Trade marks, Literature listing, Patent analysis, Current awareness;Elsevier Ltd.;;2697.0;Western Europe;320.0;Q2;15069.0;World patent information;Literature listing;;126.0;30.0;117.0;809.0;journal;article;2019
Soil respiration (Rs) is the second largest carbon flux next to GPP between the terrestrial ecosystem (the largest organic carbon pool) and the atmosphere at a global scale. Given their critical role in the global carbon cycle, Rs measurement and modeling issues have been well reviewed in previous studies. In this paper, we briefly review advances in soil organic carbon (SOC) decomposition processes and the factors affecting Rs. We examine the spatial and temporal distribution of Rs measurements available in the literature and found that most of the measurements were conducted in North America, Europe, and East Asia, with major gaps in Africa, East Europe, North Asia, Southeast Asia, and Australia, especially in dry ecosystems. We discuss the potential problems of measuring Rs on slope soils and propose using obliquely-cut soil collars to solve the existing problems. We synthesize previous estimates of global Rs flux and find that the estimates ranged from 50 PgC/yr to 98 PgC/yr and the error associated with each estimation was also high (4 PgC/yr to 33.2 PgC/yr). Using a newly integrated database of Rs measurements and the MODIS vegetation map, we estimate that the global annual Rs flux is 94.3 PgC/yr with an estimation error of 17.9 PgC/yr at a 95% confidence level. The uneven distribution of Rs measurements limits our ability to improve the accuracy of estimation. Based on the global estimation of Rs flux, we found that Rs is highly correlated with GPP and NPP at the biome level, highlighting the role of Rs in global carbon budgets.;Ming Xu and Hua Shang;"Agronomy and Crop Science (Q1); Plant Science (Q1); Physiology (Q2)";565.0;345.0;Germany;1979, 1984-2020;10.1016/j.jplph.2016.08.007;0.00823;130.0;;01761617;01761617;16181328;01761617;3.549;Soil microbial decomposition, Soil respiration measurement, Soil respiration model, Global annual soil respiration, Belowground carbon allocation;Urban und Fischer Verlag GmbH und Co. KG;;5647.0;Western Europe;1032.0;Q1;19257.0;Journal of plant physiology;Contribution of soil respiration to the global carbon equation;16412.0;2039.0;146.0;567.0;8244.0;journal;article;2016
"Summary
Purpose
To utilize data mining for analysis of corneal transplantations (CT) in Florida from 2005-2014, segmented by demographics, geography, and transplantation technique.
Methods
A retrospective, database study was performed utilizing data queried from the Healthcare and Cost Utilization Project using Current Procedural Terminology codes for lamellar keratoplasty (ALK), endothelial keratoplasty (EK), and penetrating keratoplasty (PKP). Payer status, ethnic group, age, gender, and geography (urban versus rural) was extracted from each surgical encounter and reconfigured to provide a “clean”, congruous dataset for statistical analysis. This Institutional Review Board-approved study did not utilize identifiable patient information; thus, individual informed consent was not required.
Results
From 2005–2014, CT (n=28,607) represented less than 1% of the total ambulatory surgeries (n=12,695,932) performed in Florida. EK volume increased while PKP and ALK volume decreased, year-over-year. Statistical significance was found between transplantation technique by sex (P<0.001) and ethnic group (P<0.001). The largest sex discrepancy was EK (59% female, 41% male). White patients underwent relatively fewer PKP than EK (71% vs. 83% of totals), while Black patients underwent relatively more PKP than EK (14% vs 6% of totals). Statistical significance was found between techniques by payer (P<0.001). Medicare was the most common payer for all techniques, but ALK and PKP had higher percentages of private insurance and self-pay. No statistical significance was found between techniques by geographic location. Corneal edema (22.4%), endothelial dystrophy (17.5%), and bullous keratopathy (10.9%) were erroneously coded as indications for ALK. Corneal scars (2.5%) and corneal opacity (1.7%) were erroneously coded as indications for EK.
Conclusions
CT rates in Florida appear to overrepresent the female sex and underrepresent ethnic minorities, with propensities between PKP and African Americans, EK and female patients, and EK and Medicare reimbursement. Our study further confirms the utility of data mining for providing efficient, detailed, and practical insights into ophthalmology procedures, while highlighting the intrinsic challenges of large datasets.
Résumé
Objectif
Utiliser l’exploration de données pour analyser les transplantations de cornée (CT) en Floride de 2005 à 2014, segmentées par démographie, géographie et technique de transplantation.
Méthodes
Une étude rétrospective de la base de données a été réalisée à partir de données extraites du Healthcare and Cost Utilization Project en utilisant les codes de la terminologie procédurale courante pour la kératoplastie lamellaire (ALK), la kératoplastie endothéliale (EK) et la kératoplastie pénétrante (PKP). Le statut du payeur, le groupe ethnique, l’âge, le sexe et la géographie (urbaine ou rurale) ont été extraits de chaque rencontre chirurgicale et reconfigurés pour fournir un ensemble de données « propre » et congruent pour l’analyse statistique. Cette étude approuvée par l’Institutional Review Board n’a pas utilisé d’informations identifiables sur les patients; le consentement éclairé individuel n’était donc pas nécessaire.
Résultats
De 2005 à 2014, les CT (n=28 607) ont représenté moins de 1 % du total des chirurgies ambulatoires (n=12 695 932) réalisées en Floride. Le volume des EK a augmenté tandis que celui des PKP et des ALK a diminué, d’une année sur l’autre. Une signification statistique a été trouvée entre la technique de transplantation par sexe (p<0,001) et par groupe ethnique (p<0,001). L’écart le plus important entre les sexes était l’EK (59 % de femmes, 41 % d’hommes). Les patients blancs ont subi relativement moins de PKP que d’EK (71 % vs. 83 % des totaux), tandis que les patients noirs ont subi relativement plus de PKP que d’EK (14 % vs. 6 % des totaux). Une significativité statistique a été trouvée entre les techniques par payeur (p<0,001). Medicare était le payeur le plus courant pour toutes les techniques, mais les techniques ALK et PKP présentaient des pourcentages plus élevés d’assurance privée et d’auto-paiement. Aucune significativité statistique n’a été trouvée entre les techniques selon la localisation géographique. L’œdème cornéen (22,4 %), la dystrophie endothéliale (17,5 %) et la kératopathie bulleuse (10,9 %) ont été codés par erreur comme des indications de l’ALK. Les cicatrices cornéennes (2,5 %) et l’opacité cornéenne (1,7 %) ont été codées par erreur comme des indications de l’EK.
Conclusions
Les taux de CT en Floride semblent surreprésenter le sexe féminin et sous-représenter les minorités ethniques, avec des propensions entre PKP et Afro-Américains, EK et patients féminins, et EK et remboursement Medicare. Notre étude confirme l’utilité de l’exploration de données pour fournir des informations efficaces, détaillées et pratiques sur les procédures ophtalmologiques, tout en soulignant les défis intrinsèques des grands ensembles de données.";J.A. Go and J. Tran and M. Khan and Z. Al-Mohtaseb;Ophthalmology (Q4);563.0;33.0;France;1978-2020;10.1016/j.jfo.2022.01.023;;30.0;;01815512;17730597;01815512;17730597;;Cornea, Transplant, Epidemiology, Demographic, Socioeconomic, Cornée, Transplantation, Épidémiologie, Démographique, Socio-économique;Elsevier Masson;;1343.0;Western Europe;252.0;Q4;14475.0;Journal francais d'ophtalmologie;Application of data mining algorithms to study data trends for corneal transplantation;;330.0;305.0;883.0;4095.0;journal;article;2022
This study uses a large administrative dataset, the Adoption and Foster Care Analysis and Reporting System (AFCARS), to explore how public child welfare agencies in the United States use parental disability in their data collection efforts through examining the use of parental disability as a removal reason. Using data from the 2012 AFCARS foster care file, this study explores how the parental disability removal reason is used and how this removal reason relates to parent and child demographics. The study found that 19% of foster children had parental disability as a removal reason. Children with disabilities and children of certain races had higher odds of having parental disability as a removal reason, as did both younger and older parents. The study also found great variation amongst states in the use of parental disability as a removal. Recommendations for more appropriate collection of parental disability related data are suggested, as basing child welfare decisions on diagnoses versus behavior contradicts guidance jointly put forth by the Departments of Justice and Health and Human Services.;Sharyn DeZelar and Elizabeth Lightfoot;"Education (Q1); Social Work (Q1); Sociology and Political Science (Q1); Developmental and Educational Psychology (Q2)";1315.0;229.0;United Kingdom;1979-2020;10.1016/j.childyouth.2018.01.027;0.013430000000000001;89.0;;01907409;01907409;01907409;01907409;2.393;Parents with disabilities, Child welfare, Foster care;Elsevier Ltd.;;5997.0;Western Europe;816.0;Q1;24482.0;Children and youth services review;Use of parental disability as a removal reason for children in foster care in the u.s.;13303.0;3628.0;978.0;1349.0;58646.0;journal;article;2018
"Background
Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards.
Objective
To seek the best artificial intelligence method for diagnosis of melanoma.
Methods
The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification.
Results
The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%.
Limitations
There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning.
Conclusion
The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous.";Xiaoyu Cui and Ran Wei and Lixin Gong and Ruiqun Qi and Zeyin Zhao and Hongduo Chen and Kaixin Song and Amer A.A. Abdulrahman and Yining Wang and John Z.S. Chen and Shuo Chen and Yue Zhao and Xinghua Gao;Dermatology (Q1);1607.0;349.0;United States;1979-2020;10.1016/j.jaad.2019.06.042;0.03692;208.0;;01909622;10976787;01909622;10976787;11.527;artificial intelligence, classification, deep learning, melanoma diagnosis, segmentation, traditional machine learning;Mosby Inc.;;1367.0;Northern America;1965.0;Q1;24245.0;Journal of the american academy of dermatology;Assessing the effectiveness of artificial intelligence methods for melanoma: a retrospective review;40257.0;7754.0;1185.0;2153.0;16196.0;journal;article;2019
;Aruna Sivakumar and Stephane Hess;"Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)";577.0;675.0;United Kingdom;1979-2020;10.1016/j.trb.2018.07.009;;140.0;;01912615;01912615;01912615;01912615;;;Elsevier Ltd.;;5247.0;Western Europe;3150.0;Q1;20892.0;Transportation research, series b: methodological;New and emerging methods for the improved modelling of travel behaviour: transportation research part b: special issue of papers from the iatbr 2015 conference;;4500.0;148.0;582.0;7766.0;journal;article;2019
"Summary
Electronic storage of healthcare data, including individual-level risk factors for both infectious and other diseases, is increasing. These data can be integrated at hospital, regional and national levels. Data sources that contain risk factor and outcome information for a wide range of conditions offer the potential for efficient epidemiological analysis of multiple diseases. Opportunities may also arise for monitoring healthcare processes. Integrating diverse data sources presents epidemiological, practical, and ethical challenges. For example, diagnostic criteria, outcome definitions, and ascertainment methods may differ across the data sources. Data volumes may be very large, requiring sophisticated computing technology. Given the large populations involved, perhaps the most challenging aspect is how informed consent can be obtained for the development of integrated databases, particularly when it is not easy to demonstrate their potential. In this article, we discuss some of the ups and downs of recent projects as well as the potential of data warehousing for antimicrobial resistance monitoring.";D. Wyllie and J. Davies;"Medicine (miscellaneous) (Q1); Infectious Diseases (Q2); Microbiology (medical) (Q2)";614.0;274.0;United Kingdom;1980-2020;10.1016/j.jhin.2015.01.005;0.01124;118.0;;01956701;15322939;01956701;15322939;3.926;Data management, Epidemiology, Healthcare data;W.B. Saunders Ltd;;2255.0;Western Europe;1142.0;Q1;22427.0;Journal of hospital infection;Role of data warehousing in healthcare epidemiology;12760.0;2298.0;391.0;848.0;8819.0;journal;article;2015
Here, we develop further the national chemical footprint assessment methods using Sweden as an example to enhance the precision of calculations. First, we integrate grid data on population density and distance-to-seacoast into the analytical framework to better match the European Pollutant Release and Transfer Register on the sub-compartment level with USEtox toxicity characterisation factors. Second, we use the latest USEtox 2.12 model version and its more punctual North European characterisation factors. Third, we conduct trend and geographic analysis and rank Swedish facilities in terms of toxicity potential. We show that total human toxicity potential in Sweden was smaller than previously estimated when using the North European USEtox landscape settings and sloped downwards over time. We confirm toxicity potential of major pollutants in previous research papers (Zn, Hg, Pb, Ni) and find that Hg’s relative human toxicity potential in a longer period can be larger than previously estimated on shorter periods. Human toxicity is estimated to be mostly non-cancer type in Sweden. Results are largely invariant to the choice of air sub-compartments. Companies in the metals manufacturing sector are estimated to have the largest human toxicity potential in Sweden in the period between 2001 and 2017 and companies in the paper manufacturing industry have the largest ecotoxicity potential.;Szilárd Erhart and Kornél Erhart;"Ecology (Q1); Geography, Planning and Development (Q1); Management, Monitoring, Policy and Law (Q1)";251.0;453.0;United States;1980-1983, 1985-2020;10.1016/j.eiar.2021.106686;0.0036299999999999995;92.0;;01959255;01959255;01959255;01959255;4.549;Chemical footprint, Hazardous chemicals, -PRTR, Human toxicity, Ecotoxicity, USEtox, ESG rating, Sustainability rating, European Green Deal, Sustainable Devoelopment Goals (SDGs);Elsevier Inc.;;6211.0;Northern America;1138.0;Q1;21004.0;Environmental impact assessment review;Application of north european characterisation factors, population density and distance-to-coast grid data for refreshing the swedish human toxicity and ecotoxicity footprint analysis;5305.0;1214.0;106.0;251.0;6584.0;journal;article;2022
Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.;Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy;Emergency Medicine (Q1);703.0;147.0;United States;1980-2020;10.1016/j.annemergmed.2015.06.024;0.01573;153.0;;01960644;01960644;10976760;01960644;5.721;;Mosby Inc.;;1603.0;Northern America;1241.0;Q1;15220.0;Annals of emergency medicine;Exploring the potential of predictive analytics and big data in emergency care;14808.0;2154.0;392.0;1312.0;6283.0;journal;article;2016
;Corrianne Billings and Heather Bernard and Lisa Caffery and Susan A. Dolan and John Donaldson and Ericka Kalp and Angel Mueller;"Health Policy (Q1); Public Health, Environmental and Occupational Health (Q1); Epidemiology (Q2); Infectious Diseases (Q2)";879.0;240.0;United States;1980-2020;10.1016/j.ajic.2019.04.003;0.01544;107.0;;01966553;15273296;01966553;15273296;2.918;APIC MegaSurvey, certification, career stage, leadership, professional stewardship, professional and practice standards;Mosby Inc.;;2225.0;Northern America;1004.0;Q1;21786.0;American journal of infection control;Advancing the profession: an updated future-oriented competency model for professional development in infection prevention and control;12101.0;2575.0;410.0;1046.0;9122.0;journal;article;2019
"In the face of green energy initiatives and progressively increasing shares of more energy-efficient buildings, there is a pressing need to transform district heating towards low-temperature district heating. The substantially lowered supply temperature of low-temperature district heating broadens the opportunities and challenges to integrate distributed renewable energy, which requires enhancement on intelligent heating load prediction. Meanwhile, to fulfill the temperature requirements for domestic hot water and space heating, separate energy conversion units on user-side, such as building-sized boosting heat pumps shall be implemented to upgrade the temperature level of the low-temperature district heating network. This study conducted hybrid heating load prediction methods with long-term and short-term prediction, and the main work consisted of four steps: (1) acquisition and processing of district heating data of 20 district heating supplied nursing homes in the Nordic climate (2016–2019); (2) long-term district heating load prediction through linear regression, energy signature curve in hourly resolution, providing an overall view and boundary conditions for the unit sizing; (3) short-term district heating load prediction through two Artificial Neural Network models, f72 and g120, with different prediction input parameters; (4) evaluation of the predicted load profiles based on the measured data. Although the three prediction models met the quality criteria, it was found that including the historical hourly heating loads as the input to the forecasting model enhanced the prediction quality, especially for the peak load and low-mild heating season. Furthermore, a possible application of the heating load profiles was proposed by integrating two building-sized heat pumps in low-temperature district heating, which may be a promising heat supply method in low-temperature district heating.";Yiyu Ding and Thomas Ohlson Timoudas and Qian Wang and Shuqin Chen and Helge Brattebø and Natasa Nord;"Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Nuclear Energy and Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1)";3458.0;1003.0;United Kingdom;1979-2020;10.1016/j.enconman.2022.116163;0.0823;192.0;;01968904;01968904;01968904;01968904;9.709;Nursing homes, District heating load prediction, Linear regression, Artificial neural network, Low-temperature district heating;Elsevier Ltd.;;5438.0;Western Europe;2743.0;Q1;29372.0;Energy conversion and management;A study on data-driven hybrid heating load prediction methods in low-temperature district heating: an example for nursing homes in nordic countries;80377.0;35051.0;1125.0;3479.0;61183.0;journal;article;2022
Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.;Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng;"Nature and Landscape Conservation (Q1); Urban Studies (Q1)";361.0;546.0;United Kingdom;1970, 1976-2020;10.1016/j.habitatint.2017.04.007;0.00827;78.0;;01973975;01973975;01973975;01973975;5.369;Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression;Elsevier Ltd.;;6843.0;Western Europe;1542.0;Q1;13760.0;Habitat international;A geo-big data approach to intra-urban food deserts: transit-varying accessibility, social inequalities, and implications for urban planning;8872.0;2114.0;117.0;365.0;8006.0;journal;article;2017
;;"Medicine (miscellaneous) (Q2); Immunology (Q3); Immunology and Allergy (Q3)";387.0;230.0;United States;1980-2020;10.1016/j.humimm.2022.08.008;0.0045899999999999995;95.0;;01988859;01988859;18791166;01988859;2.850;;Elsevier Inc.;;3072.0;Northern America;908.0;Q2;20795.0;Human immunology;Poster abstracts (p);6042.0;904.0;128.0;400.0;3932.0;journal;article;2022
The era of intelligence is the development of human science and technology at a higher level, bringing a new layout for the financial market. Then, how to realize the good layout of the financial market in the era of intelligence is an important problem facing all the countries in the world. We in the financial industry as the origin and change as the clue, analyzes the physical outlets as the representative of the financial institutions, banking as standard electronic banking and mobile phone to the bank as the representative of the mobile financial development of three major formats. On this basis, we analyze the current mobile phone terminal model of mobile banking three shortcomings, and propose the new mobile financial formats. This new format is tentatively known as “smart financial format”, it has the equipment personality, wearable, low carbon environmental protection, offline interaction, security, privacy and intelligence, efficiency, and other five major characteristics. In the future, the financial format is likely to be achieved by mobile financial formats to the upgrading of intelligent financial formats, to meet the needs of customers wider and deeper levels. The artificial intelligence method has its advantages in dealing with the problems of the economic system. Therefore, the introduction of artificial intelligence methods into the economic control will become a trend. The proposed model is validated through the public databases to verify the effectiveness.;Lei Ruan and Chunyan Li and Yan Zhang and Haoxiang Wang;"Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)";324.0;611.0;United Kingdom;1980-1986, 1988-2020;10.1016/j.compenvurbsys.2018.07.002;;92.0;;01989715;01989715;01989715;01989715;;Soft computing model, Financial, Spatiotemporal, Social network analysis, Data visualization, Smart cities, Computing paradigms;Elsevier Ltd.;;5696.0;Western Europe;1549.0;Q1;23269.0;Computers, environment and urban systems;Soft computing model based financial aware spatiotemporal social network analysis and visualization for smart cities;;2135.0;85.0;327.0;4842.0;journal;article;2019
The most common type of liver cancer is hepatocellular carcinoma (HCC), which begins in hepatocytes. The HCC, like most types of cancer, does not show symptoms in the early stages and hence it is difficult to detect at this stage. The symptoms begin to appear in the advanced stages of the disease due to the unlimited growth of cancer cells. So, early detection can help to get timely treatment and reduce the mortality rate. In this paper, we proposes a novel machine learning model using seven classifiers such as K-nearest neighbor (KNN), random forest, Naïve Bayes, and other four classifiers combined to form stacking learning (ensemble) method with genetic optimization helping to select the features for each classifier to obtain highest HCC detection accuracy. In addition to preparing the data and make it suitable for further processing, we performed the normalization techniques. We have used KNN algorithm to fill in the missing values. We trained and evaluated our developed algorithm using 165 HCC patients collected from Coimbra’s Hospital and University Centre (CHUC) using stratified cross-validation techniques. There are total of 49 clinically significant features in this dataset, which are divided into two groups such as quantitative and qualitative groups. Our proposed algorithm has achieved the highest accuracy and F1-score of 0.9030 and 0.8857, respectively. The developed model is ready to be tested with huge database and can be employed in cancer screening laboratories to aid the clinicians to make an accurate diagnosis.;Wojciech Książek and Mohamed Hammad and Paweł Pławiak and U. Rajendra Acharya and Ryszard Tadeusiewicz;Biomedical Engineering (Q2);224.0;544.0;Poland;2008-2020;10.1016/j.bbe.2020.08.007;0.00176;29.0;;02085216;02085216;02085216;02085216;4.314;HCC, Stacking learning, Ensemble method, Machine learning, Genetic algorithm;Elsevier Sp. z o.o.;;5363.0;Eastern Europe;665.0;Q2;19400158663.0;Biocybernetics and biomedical engineering;Development of novel ensemble model using stacking learning and evolutionary computation techniques for automated hepatocellular carcinoma detection;1512.0;1103.0;122.0;224.0;6543.0;journal;article;2020
"The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.";Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García};Critical Care and Intensive Care Medicine (Q3);346.0;98.0;Spain;1988-2020;10.1016/j.medin.2018.06.002;0.00147;28.0;;02105691;15786749;02105691;15786749;2.491;Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo;Ediciones Doyma, S.L.;;1816.0;Western Europe;336.0;Q3;18370.0;Medicina intensiva;Big data and machine learning in critical care: opportunities for collaborative research;1177.0;416.0;237.0;401.0;4303.0;journal;article;2019
The intensification of catalytic reactors is expected to play a crucial role to address the challenges that the chemical industry is facing in the transition to more sustainable productions. An advanced design paradigm is necessary to develop customized and process-tailored reactor solutions able to provide the optimal operating conditions, transport properties and geometry. This can be achieved by a detailed understanding of the catalyst functionality in the reactive environment. Multiscale Modeling provides such in-depth insights into the complex physical-chemical phenomena enabling to achieve a first-principles-based understanding and design of the most suitable reactor geometry and configuration. To overcome the intrinsic complexity of the approach, Machine Learning can be synergically employed to reduce the computational cost fostering the inclusion of detailed numerical simulations since the early stage of the design process. Moreover, hybrid machine learning models trained with the data and enforced by the physics are envisioned to assist the work of designers facilitating the development of disruptive intensified solutions. The manufacturing of these unconventional systems requires adequate techniques. Additive Manufacturing is showing enormous potential in this direction and their future developments are expected to make it possible to routinely fabricate intensified reactors.;Mauro Bracconi;"Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Industrial and Manufacturing Engineering (Q1); Process Chemistry and Technology (Q2)";783.0;431.0;Netherlands;1984-2020;10.1016/j.cep.2022.109148;;103.0;;02552701;02552701;02552701;02552701;;Catalytic reactors, Process intensification, Multiscale modeling, Additive manufacturing, Machine learning;Elsevier;;4644.0;Western Europe;828.0;Q1;16392.0;Chemical engineering and processing: process intensification;Intensification of catalytic reactors: a synergic effort of multiscale modeling, machine learning and additive manufacturing;;3314.0;367.0;787.0;17043.0;journal;article;2022
New sources of geotagged information derived from social media like Twitter show great promise for geographic research in tourism. This paper describes an approach to analyze geotagged social media data from Twitter to characterize spatial, temporal and demographic features of tourist flows in Cilento - a regional tourist attraction in southern Italy. It demonstrates how the analysis of geotagged social media data yields more detailed spatial, temporal and demographic information of tourist movements, in comparison to the current understanding of tourist flows in the region. The insights obtained from our case study illustrate the potential of the proposed methodology yet attention should be paid to biases in the data as well as methodological limitations when drawing conclusions from analytical results.;Alvin Chua and Loris Servillo and Ernesto Marcheggiani and Andrew Vande Moere;"Development (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1); Transportation (Q1)";687.0;1106.0;United Kingdom;1982-2021;10.1016/j.tourman.2016.06.013;0.02226;199.0;;02615177;02615177;02615177;02615177;10.967;Data mining, Visual analytics, Flow analysis, Geotagged social media data;Elsevier Ltd.;;7491.0;Western Europe;3328.0;Q1;16547.0;Tourism management;Mapping cilento: using geotagged social media data to characterize tourist flows in southern italy;37117.0;8354.0;159.0;690.0;11910.0;journal;article;2016
Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining.;Pui Yi Lee and Wei Ping Loh and Jeng Feng Chin;"Computer Vision and Pattern Recognition (Q2); Electrical and Electronic Engineering (Q2); Signal Processing (Q2)";276.0;448.0;United Kingdom;1983-2020;10.1016/j.imavis.2017.09.004;0.0036200000000000004;132.0;;02628856;02628856;02628856;02628856;2.818;Feature selection, Multimedia, Data mining, Search strategies;Elsevier Ltd.;;4671.0;Western Europe;570.0;Q2;25549.0;Image and vision computing;Feature selection in multimedia: the state-of-the-art review;5653.0;1464.0;123.0;282.0;5745.0;journal;article;2017
This paper proposes a modification to a well-known alignment method, correlation optimized warping (COW), to improve the efficiency of the method and reduce the positional errors in the measurements of linear assets. The modified method relaxes the restrictions of COW in aligning the start and end of datasets and decreases the computational time. Furthermore, the method takes advantage of the interdependencies between simultaneously measured channels to overcome the missing data problem. A case study on railway track geometry measurements was conducted to implement the proposed method and assess its performance in reducing the positioning inaccuracy of the measurements. The findings revealed that the modified method could decrease the positional errors of defects to below 25 cm in 94 % of the trials.;Mahdi Khosravi and Iman Soleimanmeigouni and Alireza Ahmadi and Arne Nissen and Xun Xiao;"Condensed Matter Physics (Q1); Education (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1); Applied Mathematics (Q2); Statistics and Probability (Q2)";2630.0;441.0;Netherlands;1983-2021;10.1016/j.measurement.2022.111707;;91.0;;02632241;02632241;02632241;02632241;;Position alignment, Correlation optimized warping, Data quality, Linear assets, Positional error, Condition measurements;Elsevier;;3750.0;Western Europe;772.0;Q1;15424.0;Measurement: journal of the international measurement confederation;Modification of correlation optimized warping method for position alignment of condition measurements of linear assets;;11589.0;1092.0;2645.0;40953.0;journal;article;2022
"Focus
The transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.
Objectives
To determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.
Methodology
Content analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.
Findings
The central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI – in assisting HR – and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.";Aleksandar Radonjić and Henrique Duarte and Nádia Pereira;Strategy and Management (Q1);208.0;572.0;United Kingdom;1982-2020;10.1016/j.emj.2022.07.001;0.00371;102.0;;02632373;02632373;02632373;02632373;5.075;Human resources, HR, HRM, e-HRM, Decision-making, Big data (BD), Big data maturity models (BDMM), Artificial intelligence (AI);Elsevier Ltd.;;9046.0;Western Europe;1365.0;Q1;22491.0;European management journal;Artificial intelligence and hrm: hr managers’ perspective on decisiveness and challenges;5790.0;1147.0;103.0;215.0;9317.0;journal;article;2022
This study presents a broad view of the current state of the art of ML applications in the manufacturing sectors that have a considerable impact on sustainability and the environment, namely renewable energies (solar, wind, hydropower, and biomass), smart grids, the industry of catalysis and power storage and distribution. Artificial neural networks are the most preferred techniques over other ML algorithms because of their generalization capabilities. Demands for ML techniques in the energy sectors will increase considerably in the coming years, since there is a growing demand of academic programmes related to artificial intelligence in science, math, and engineering. Data generation, management, and safety are expected to play a key role for the successful implementation of ML algorithms that can be shared by major stakeholders in the energy sector, thereby promoting the development of ambitious energy management projects. New algorithms for producing reliable data and the addition of other sources of information (e.g., novel sensors) will enhance flow of information between ML and systems. It is expected that unsupervised and reinforcement learning will take a central role in the energy sector, but this will depend on the expansion of other major fields in data science such as big data analytics. Massive implementations, specialized algorithms, and new technologies like 5G will promote the development of sustainable applications of ML in non-industrial applications for energy management.;Daniel Rangel-Martinez and K.D.P. Nigam and Luis A. Ricardez-Sandoval;"Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1)";1397.0;384.0;United Kingdom;1983-2020;10.1016/j.cherd.2021.08.013;;99.0;;02638762;17443563;02638762;17443563;;Machine learning, Artificial neural networks, Renewable energies, Catalysis, Power systems, Sustainability, Energy efficiency;Institution of Chemical Engineers;;4680.0;Western Europe;788.0;Q1;16411.0;Chemical engineering research and design;Machine learning on sustainable energy: a review and outlook on renewable energy systems, catalysis, smart grid and energy storage;;5429.0;418.0;1403.0;19563.0;journal;article;2021
This research applies a unique conceptual model and methodology incorporating popularity, commitment, and virality to measure the social media engagement with residents and visitors of smart cities and how they communicate ‘smart’ elements and their brands. Digital content analysis was applied to a sample of ten Spanish smart cities (including Barcelona, Bilbao, Madrid, Seville and Valencia, among others), with measurable and quantifiable elements of engagement (e.g., likes, shares and comments). The smart cities analysed achieved acceptable, but rudimentary, levels of engagement via social media using Facebook, Twitter and Instagram. However, they displayed weaknesses related to their image and branding as well as the effectiveness with which they communicated their smart characteristics. The main implication of this research is that these Spanish smart cities have considerable scope to improve their use of social media to enhance their communications and branding. Greater emphasis is required on delivering emotional (affective) messages and a higher priority needs to be given to business and business event travellers and those visiting friends and relatives.;Sebastian Molinillo and Rafael Anaya-Sánchez and Alastair M. Morrison and J. Andres Coca-Stefaniak;"Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)";723.0;619.0;United Kingdom;1983-2020;10.1016/j.cities.2019.06.003;0.01251;90.0;;02642751;02642751;02642751;02642751;5.835;Smart cities, Social media, Engagement, City branding, User-generated content (UGC);Elsevier Ltd.;;6780.0;Western Europe;1771.0;Q1;16956.0;Cities;Smart city communication via social media: analysing residents' and visitors' engagement;11076.0;4866.0;419.0;732.0;28409.0;journal;article;2019
Great progress has been made in the development of whole sporozoite vaccines including the manufacturing of cryopreserved Plasmodium falciparum sporozoites (PfSPZ) suitable for clinical application. Such whole sporozoites are being used for clinical studies of controlled human malaria infection (CHMI) as well as for evaluation of candidate vaccine approaches (both attenuated sporozoites and infectious sporozoites administered with chemoprophylaxis) and as reagents for immunology and cell biology assays. CHMI studies with whole sporozoites provide a great opportunity to better understand the intrinsic mechanisms of resistance to P. falciparum (e.g. due to sickle cell trait and other hemoglobinopathies) as well as host responses to an initial P. falciparum infection. High-level protective efficacy has been demonstrated in a small number of volunteers after intravenous (IV) inoculation of radiation-attenuated PfSPZ or in those who were exposed to live PfSPZ while on malaria chemoprophylaxis. These advances and data warrant further investigations of the immunological mechanism(s) whereby whole sporozoite inoculation elicits protective immunity in order to facilitate whole sporozoite vaccine development. The National Institute of Allergy and Infectious Diseases (NIAID) convened a workshop on Sept. 2–3, 2014 involving participation of international experts in the field of malaria vaccine development, and in basic and clinical immunology research. The workshop discussed the current understanding of host immune responses to whole malaria sporozoite inoculation, identified gaps in knowledge, resources to facilitate progress, and applicable new technologies and approaches to accelerate immunologic and vaccinologic studies and biomarker identification. This report summarizes the discussions and major conclusions from the workshop participants.;Annie X.Y. Mo and John Pesce and B. Fenton Hall;"Immunology and Microbiology (miscellaneous)  (Q1); Infectious Diseases (Q1); Molecular Medicine (Q1); Public Health, Environmental and Occupational Health (Q1); Veterinary (miscellaneous) (Q1)";2957.0;327.0;Netherlands;1983-2020;10.1016/j.vaccine.2015.04.056;0.0618;184.0;;0264410X;0264410X;18732518;0264410X;3.641;Malaria, , Sporozoite, Vaccine, Immunology;Elsevier BV;;3684.0;Western Europe;1585.0;Q1;21376.0;Vaccine;Exploring immunological mechanisms of the whole sporozoite vaccination against p. falciparum malaria;50113.0;10913.0;1148.0;3184.0;42289.0;journal;article;2015
"A novel lithofacies and stratigraphic, supervised machine-learning prediction methodology, coupling a standardized well-log representation with an optimized nearest-neighbour algorithm, is introduced and its broader data mining capabilities described. This approach can generate highly accurate predictions and, because of its transparency and standardized representation, can provide unprecedented insight to the predictions it makes. It also provides alternative predictions that help to rectify at least some of the few prediction errors it generates. A detailed case study applying the methodology to published well-log data (8 metrics), plus lithology and stratigraphic information, for the Triassic reservoir section of the giant Hassi R'Mel gas field (Algeria) demonstrates the insight this methodology can achieve. For its lithofacies index, optimized solutions for a 9-independent-variable model generate only about 30 prediction errors from a 1000 interval configured well-log network (RMSE~0.2; R2 ~0.98). A 6-independent-variable lithofacies model achieves only slightly inferior prediction accuracies. For its stratigraphic index, optimized solutions for a 9-independent-variable model generate only about 40 prediction errors from a 1000 interval configured well-log network (RMSE~0.23; R2 ~0.97). Further data mining of these models provides understanding of causes of the few data prediction errors it generates. The lithofacies prediction errors are more easily rectified than the stratigraphic prediction errors. The level of detail provided for each prediction in its two-stage generation process significantly exceeds that provided by regression-based and less-transparent neural-network-based lithofacies prediction algorithms.";David A. Wood;"Economic Geology (Q1); Geology (Q1); Geophysics (Q1); Oceanography (Q1); Stratigraphy (Q1)";1229.0;442.0;Netherlands;1984-2020;10.1016/j.marpetgeo.2019.07.026;0.02026;116.0;;02648172;02648172;18734073;02648172;4.348;Lithofacies/stratigraphy prediction from well logs, Data-matching machine learning, Standardized well-log interval representation, Transparent data mining, Prediction error analysis;Elsevier BV;;8005.0;Western Europe;1336.0;Q1;24548.0;Marine and petroleum geology;Lithofacies and stratigraphy prediction methodology exploiting an optimized nearest-neighbour algorithm to mine well-log data;19822.0;5823.0;624.0;1249.0;49950.0;journal;article;2019
"How to reduce neighborhood socioeconomic status- (SES-) related health inequalities has been prioritized in the recent political literature. Park green spaces (PGSs) are essential neighborhood land use assets, as they are beneficial for population health and thus should mitigate SES-related health inequalities. This paper aims to elaborate the knowledge on the complex interrelationships among PGSs, health outcomes and social inequalities through developing a set of mixed indicators in an integrated manner. The data are obtained at the community level (N=8117) for three health outcomes (cardiopathy, chronic pneumonia and hypertension) and five SES variables. The PGS characteristics are described from three dimensions (coverage, quality and accessibility) at two geographical levels (15-minute walking distance (15 WD) neighborhoods and 30-minute walking distance (30 WD) neighborhoods). Spatial regressions reveal the following: 1) socioeconomically disadvantaged communities enjoy fewer PGSs and limited access to parks of high quality; 2) socioeconomically disadvantaged communities present higher incidences of diseases; and 3) PGS coverage within 30 WD neighborhoods and PGS accessibility within 15 WD neighborhoods are related to health outcomes. Structural equation modeling further confirms that PGSs, especially those of higher quality, could mitigate SES-related health inequalities. The findings of this study highlight the necessity of improving the PGS quality within walking distance of socioeconomically disadvantaged communities. We thus argue that land use policy makers should collaborate with social researchers and health professionals; and through health impact assessment, they can translate the professional knowledge into land use planning and consider health promotion into land use policies.";Qian Wang and Zili Lan;"Forestry (Q1); Geography, Planning and Development (Q1); Management, Monitoring, Policy and Law (Q1); Nature and Landscape Conservation (Q1)";1703.0;557.0;United Kingdom;1984-2021;10.1016/j.landusepol.2019.01.026;0.02454;115.0;;02648377;02648377;02648377;02648377;5.398;park green spaces, health outcomes, inequalities, neighborhood socioeconomic status, indicators, greening policy;Elsevier Ltd.;;6580.0;Western Europe;1668.0;Q1;14500.0;Land use policy;Park green spaces, public health and social inequalities: understanding the interrelationships for policy implications;22020.0;9371.0;730.0;1706.0;48032.0;journal;article;2019
Despite colossal economic and human losses caused by conflict and violence, designing effective policies to avoid conflict remains challenging. While the literature has proposed a voluminous set of candidate predictors, their robustness is questionable and model uncertainty masks the true drivers of conflicts and wars. Considering a comprehensive set of 34 potential determinants in 175 post-Cold-War countries, we employ stochastic search variable selection (SSVS) to sort through all 234 possible models to address model uncertainty. We find past conflict constitutes the most powerful predictor of current conflict: Path dependency matters. Also, larger shares of Jewish, Muslim, or Christian citizens are associated with increased conflict, while economic and political factors remain less relevant than colonial origin and religion. Our results help future researchers and policymakers by inching towards causality and providing a standard set of covariates that need to be accounted for in designing any relevant policies.;Michael Jetter and Rafat Mahmood and Christopher F. Parmeter and Andrés Ramírez-Hassan;Economics and Econometrics (Q2);751.0;312.0;Netherlands;1984-2020;10.1016/j.econmod.2022.105907;0.010409999999999999;77.0;;02649993;02649993;02649993;02649993;3.127;Civil conflict, Civil war, Stochastic search variable selection (), Greed versus grievances, Religion and conflict;Elsevier;;5040.0;Western Europe;1049.0;Q2;28467.0;Economic modelling;Post-cold war civil conflict and the role of history and religion: a stochastic search variable selection approach;11536.0;2604.0;374.0;756.0;18849.0;journal;article;2022
Covering: 2010–2020 The digital revolution is driving significant changes in how people store, distribute, and use information. With the advent of new technologies around linked data, machine learning and large-scale network inference, the natural products research field is beginning to embrace real-time sharing and large-scale analysis of digitized experimental data. Databases play a key role in this, as they allow systematic annotation and storage of data for both basic and advanced applications. The quality of the content, structure, and accessibility of these databases all contribute to their usefulness for the scientific community in practice. This review covers the development of databases relevant for microbial natural product discovery during the past decade (2010–2020), including repositories of chemical structures/properties, metabolomics, and genomic data (biosynthetic gene clusters). It provides an overview of the most important databases and their functionalities, highlights some early meta-analyses using such databases, and discusses basic principles to enable widespread interoperability between databases. Furthermore, it points out conceptual and practical challenges in the curation and usage of natural products databases. Finally, the review closes with a discussion of key action points required for the field moving forward, not only for database developers but for any scientist active in the field.;Jeffrey A. {van Santen} and Satria A. Kautsar and Marnix H. Medema and Roger G. Linington;"Biochemistry (Q1); Drug Discovery (Q1); Organic Chemistry (Q1)";239.0;845.0;United Kingdom;1984-2020;10.1039/d0np00053a;0.01116;177.0;;02650568;02650568;14604752;02650568;13.423;;Royal Society of Chemistry;;13800.0;Western Europe;2703.0;Q1;26371.0;Natural product reports;Microbial natural product databases: moving forward in the multi-omics era;13293.0;2233.0;92.0;246.0;12696.0;journal;article;2021
"Objective: Variation in practice in relation to indications and timing for both induction of labour (IOL) and planned caesarean section (CS) clearly exists. However, the extent of this variation, and how this variation is explained by clinicians remains unclear. The aim of this study was to map the variation in IOL and planned CS at eight Australian hospitals, and understand why variation occurs from the perspective of clinicians at these hospitals. Our ultimate aim was to identify opportunities for improvement as evidenced by hospital data, clinician experiences, and feedback. Design: A two-phased mixed method study using sequential explanatory study design. The first phase consisted of an analysis of routinely collected patient data to map variation between hospitals. The second phase consisted of focus groups with clinicians to gain their perspectives on the reasons for variation. Setting and Participants: Patient data consisted of routine data from 19,073 women giving birth at eight Sydney hospitals between November 2017 and October 2018. Focus groups were attended by a total of 61 medical staff and 121 midwives. Results: Hospital data analysis found substantial variation, before and after adjustment for case-mix, in rates of both IOL (adjusted rates 27.6%–42%) and planned CS (adjusted rate 15.4%–22.6%). Planned CS by gestation also showed variation, although after restricting analysis to term (≥37 weeks gestation) births, variation was reduced. At focus groups, five main themes explaining variation emerged: local guidelines, policies and procedures (inconsistency and ambiguity); uncertainty of the evidence/what is best practice (contradictory research and different interpretations of evidence); clinician preferences, beliefs and values; the culture of the unit; and organisational influences (access to specialised clinics, theatre time). Key conclusions: Considerable variation in IOL and planned CS, even after case-mix adjustment, was found in this sample of Australian hospitals. Engagement with hospital clinicians identified likely sources of this variation and enabled clinicians at each hospital to consider appropriate local responses to address variation, such as more detailed review of their planned birth cases. Implications for practice: At a macro level, measures to reduce unwarranted variation should initially focus on consistent national guidelines, while supporting equitable access to operating theatres for optimal CS timing, and shared decision-making training to reduce influence of clinician preference.";Coates Dominiek and Henry Amanda and Chambers Georgina and Paul Repon and Makris Angela and Clerke Teena and Natasha Donnolley;"Maternity and Midwifery (Q1); Obstetrics and Gynecology (Q2)";584.0;238.0;United States;1985-2020;10.1016/j.midw.2021.102988;0.00705;66.0;;02666138;02666138;15323099;02666138;2.372;Unwarranted variation, Induction of labour, Planned caesarean section, Clinician perspectives;Churchill Livingstone;;4220.0;Northern America;899.0;Q1;64864.0;Midwifery;Exploring variation in the performance of planned birth: a mixed method study;5894.0;1516.0;176.0;591.0;7428.0;journal;article;2021
The deployment of artificial intelligence on automated platforms needs to go hand in hand with the development of a legal framework safeguarding socio-ethical values as well as fundamental rights, particularly the self-determination and the non-discrimination principle. A trust-based approach focused on human values can mitigate a potential clash between a solely market- and technology-oriented use of artificial intelligence and a more inclusive multistakeholder approach. The regulatory tools are to be designed in a manner that leads to a symbiotic relationship between ethics and law.;Rolf H. Weber;"Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)";223.0;339.0;United Kingdom;1985-2020;10.1016/j.clsr.2019.105380;;38.0;;02673649;02673649;02673649;02673649;;Compliance, Impact assessment, Rule-making, Standard terms, Trust;Elsevier Ltd.;;1886.0;Western Europe;815.0;Q1;28888.0;Computer law and security review;Socio-ethical values and legal rules on automated platforms: the quest for a symbiotic relationship;;707.0;66.0;242.0;1245.0;journal;article;2020
In 2019, the International Journal of Information Management (IJIM) celebrated its 40th year of publication. This study commemorates this event by presenting a retrospect of the journal. Using a range of bibliometric tools, we find that the journal has grown impressively in terms of publication and citation. The contributions come from all over the world, but the majority are from Europe and the United States. The journal has mostly published empirical articles, with its authors dominantly using quantitative methodology. Further, the culture of collaboration has increased among authors over the years. The journal publishes on a number of including managing information systems, information technologies and their application in business, technology acceptance among consumers, using information systems for decision making, social perspectives on knowledge management, and information research from the social science perspective. Regression analysis reveals that article attributes such as article order, methodology, presence of authors from Europe, number of references, number of keywords, and abstract length have a significant association with the citations. Finally, we find that conceptual and review articles have a positive association with citations.;Naveen Donthu and Satish Kumar and Nitesh Pandey and Prashant Gupta;"Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)";373.0;1616.0;United Kingdom;1970, 1986-2021;10.1016/j.ijinfomgt.2020.102307;0.01167;114.0;;02684012;02684012;02684012;02684012;14.098;Bibliometric analysis, International Journal of Information Management, Performance analysis, Science mapping, Negative binomial regression, Citation analysis;Elsevier Ltd.;;9071.0;Western Europe;2770.0;Q1;15631.0;International journal of information management;Forty years of the international journal of information management: a bibliometric analysis;12245.0;5853.0;224.0;382.0;20318.0;journal;article;2021
Most national health-care systems approve new drugs based on data of safety and efficacy from large randomized clinical trials (RCTs). Strict selection biases and study-entry criteria of subjects included in RCTs often do not reflect those of the population where a therapy is intended to be used. Compliance to treatment in RCTs also differs considerably from real world settings and the relatively small size of most RCTs make them unlikely to detect rare but important safety signals. These and other considerations may explain the gap between evidence generated in RCTs and translating conclusions to health-care policies in the real world. Real-world evidence (RWE) derived from real-world data (RWD) is receiving increasing attention from scientists, clinicians, and health-care policy decision-makers - especially when it is processed by artificial intelligence (AI). We describe the potential of using RWD and AI in Hematology to support research and health-care decisions.;Francesco Passamonti and Giovanni Corrao and Gastone Castellani and Barbara Mora and Giulia Maggioni and Robert Peter Gale and Matteo Giovanni {Della Porta};"Hematology (Q1); Oncology (Q1)";152.0;759.0;United States;1987-2020;10.1016/j.blre.2021.100914;0.00536;95.0;;0268960X;15321681;0268960X;15321681;8.250;Real-world evidence, Real-world data, Artificial intelligence, Haematological cancers, Laeukemia, Lymphoma, Myelofibrosis;Churchill Livingstone;;12529.0;Northern America;2670.0;Q1;25472.0;Blood reviews;The future of research in hematology: integration of conventional studies with real-world data and artificial intelligence;4171.0;1176.0;76.0;152.0;9522.0;journal;article;2022
An accurate estimation of PM2.5 (fine particulate matters with diameters ≤ 2.5 μm) concentration is critical for health risk assessment and generating air pollution control strategies. In this study, a hybrid remote sensing and machine learning approach, named RSRF model is proposed to estimate daily ground-level PM2.5 concentrations, which integrates Random Forest (RF), one of machine learning (ML) models, and aerosol optical depth (AOD), one of remote sensing (RS) products. The proposed RSRF model provides an opportunity for an adequate characterization of real-time spatiotemporal PM2.5 distributions at uninhabited places and complex surfaces. It also offers advantages in handling complicated non-linear relationships among a large number of meteorological, environmental and air pollutant factors, as well as ever-increasing environmental data sets. The applicability of the proposed RSRF model is tested in the Beijing-Tianjin-Hebei region (BTH region) during 2015–2017. Deep Blue (DB) AOD from Aqua-retrieved Collection 6.1 (C_61) aerosol products of Moderate Resolution Imaging Spectroradiometer (MODIS) is validated with Aerosol Robotic Network. The validation results indicate C_61 DB AOD has a high correlation with ground based AOD in the BTH region. The proposed RSRF model performed well in characterizing spatiotemporal variations of annual and seasonal PM2.5 concentrations. It not only is useful to quantify the relationships between PM2.5 and relevant factors such as DB AOD, meteorological and air pollutant variables, but also can provide decision support for air pollution control at a regional environment during haze periods.;Xintong Li and Xiaodong Zhang;"Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)";4169.0;804.0;United Kingdom;1970-1980, 1986-2020;10.1016/j.envpol.2019.03.068;0.07982;227.0;;02697491;02697491;18736424;02697491;8.071;Remote sensing, Aerosol optical depth, Machine learning, PM, Random forest;Elsevier Ltd.;;6149.0;Western Europe;2136.0;Q1;23916.0;Environmental pollution;Predicting ground-level pm2.5 concentrations in the beijing-tianjin-hebei region: a hybrid remote sensing and machine learning approach;84491.0;35383.0;2109.0;4204.0;129684.0;journal;article;2019
;Matthew S. Lebo and Limin Hao and Chiao-Feng Lin and Arti Singh;"Biochemistry (medical) (Q2); Clinical Biochemistry (Q3)";149.0;161.0;United Kingdom;1981-2020;10.1016/j.cll.2020.02.003;0.00186;55.0;;02722712;15579832;02722712;15579832;1.935;Genome sequencing, Exome sequencing, Alignment, Variant calling, Annotation, Filtration, Validation, Bioinformatic infrastructure;W.B. Saunders Ltd;;4063.0;Western Europe;690.0;Q2;26792.0;Clinics in laboratory medicine;Bioinformatics in clinical genomic sequencing;1772.0;398.0;52.0;173.0;2113.0;journal;article;2020
;;Nephrology (Q1);538.0;445.0;United Kingdom;1981-2020;10.1053/j.ajkd.2016.07.005;0.0271;214.0;;02726386;15236838;02726386;15236838;8.860;;W.B. Saunders Ltd;;2867.0;Western Europe;2677.0;Q1;19366.0;American journal of kidney diseases;Detailed contents;27640.0;3775.0;264.0;857.0;7570.0;journal;article;2017
"Carbon fiber-reinforced carbon matrix composites (C/C) will be easily oxidized in high temperatures, which will have a great negative effect on their performance. Preparing ultra-high-temperature ceramic (UHTC) coatings is a well-established method to improve the oxidation and ablation resistance of C/C. However, it is time-consuming and costly to obtain these coatings through the traditional experimental method. Motivated by the outstanding performance of machine learning (ML) algorithms in many fields, this study adopts ML algorithms based on historical experimental datasets to build a model. This model will predict the oxidation and ablation resistance, represented by mass ablation rate. For this purpose, variables that affect the mass ablation rate and are easily accessible were used as input features. That includes the chemical composition and essential physics/chemistry properties of coatings and experimental parameters. Seven different ML algorithms were used to establish the model; namely, ridge regression (Ridge), lasso regression (Lasso), kernel ridge regression (KRR), support vector regression (SVR), random forest regression (RFR), AdaBoost regression (ABR), and bagging regression (Bagging). The results show that RFR has the optimal generalization performance with a mean absolute error (MAE) of 0.55, mean-squared error (MSE) of 0.71 and coefficient of determination (R2) of 0.87 on the testing set. SHapley Additive exPlanations (SHAP) analysis of the RFR model explained how these input features affect the mass ablation rate and further provided the critical features for performance prediction. The model established in this study can predict coating performance accurately and accelerate the development of UHTC-coated C/C composites from a data-driven perspective.";Jie Hao and Lihong Gao and Zhuang Ma and Yanbo Liu and Ling Liu and Shizhen Zhu and Weizhi Tian and Xiaoyu Liu and Zhigang Zhou and Alexandr A. Rogachev and Hanyang Liu;"Ceramics and Composites (Q1); Electronic, Optical and Magnetic Materials (Q1); Materials Chemistry (Q1); Surfaces, Coatings and Films (Q1); Process Chemistry and Technology (Q2)";8867.0;447.0;United Kingdom;1981-2020;10.1016/j.ceramint.2022.06.156;0.07668;110.0;;02728842;02728842;02728842;02728842;4.527;Ultra-high-temperature ceramic coatings, Oxidation and ablation resistance, Machine learning, Property prediction;Elsevier Ltd.;;4323.0;Western Europe;936.0;Q1;21522.0;Ceramics international;Exploration of the oxidation and ablation resistance of ultra-high-temperature ceramic coatings using machine learning;81215.0;38493.0;3503.0;8879.0;151439.0;journal;article;2022
The ionosphere is one of the largest contributors to errors in GNSS positioning. Although in Precise Point Positioning (PPP) the ionospheric delay is corrected to a first order through the ‘iono-free combination’, significant errors may still be observed when large electron density gradients are present. To confirm this phenomenon, the temporal behavior of intense fluctuations of total electron content (TEC) and PPP altitude accuracy at equatorial latitudes are analyzed during four years of different solar activity. For this purpose, equatorial plasma irregularities are identified with periods of high rate of change of TEC (ROT). The largest ROT values are observed from 19:00 to 01:00LT, especially around magnetic equinoxes, although some differences exist between the stations depending on their location. Highest ROT values are observed in the American and African regions. In general, large ROT events are accompanied by frequent satellite signal losses and an increase in the PPP altitude error during years 2001, 2004 and 2011. A significant increase in the PPP altitude error RMS is observed in epochs of high ROT with respect to epochs of low ROT in years 2001, 2004 and 2011, reaching up to 0.26m in the 19:00–01:00LT period.;I. Rodríguez-Bilbao and B. {Moreno Monge} and G. Rodríguez-Caderot and M. Herraiz and S.M. Radicella;"Aerospace Engineering (Q1); Astronomy and Astrophysics (Q2); Earth and Planetary Sciences (miscellaneous) (Q2); Geophysics (Q2); Space and Planetary Science (Q2); Atmospheric Science (Q3)";1519.0;265.0;United Kingdom;1981-2020;10.1016/j.asr.2014.11.004;0.01377;96.0;;02731177;02731177;18791948;02731177;2.152;Ionosphere, GNSS, PPP, Equatorial plasma irregularities;Elsevier Ltd.;;4178.0;Western Europe;682.0;Q1;12375.0;Advances in space research;Evaluation of precise point positioning accuracy under large total electron content variations in equatorial latitudes;13721.0;4143.0;556.0;1539.0;23227.0;journal;article;2015
This paper presents a 10-step read-across (RAX) framework for use in cases where a threshold of toxicological concern (TTC) approach to cosmetics safety assessment is not possible. RAX builds on established approaches that have existed for more than two decades using chemical properties and in silico toxicology predictions, by further substantiating hypotheses on toxicological similarity of substances, and integrating new approach methodologies (NAM) in the biological and kinetic domains. NAM include new types of data on biological observations from, for example, in vitro assays, toxicogenomics, metabolomics, receptor binding screens and uses physiologically-based kinetic (PBK) modelling to inform about systemic exposure. NAM data can help to substantiate a mode/mechanism of action (MoA), and if similar chemicals can be shown to work by a similar MoA, a next generation risk assessment (NGRA) may be performed with acceptable confidence for a data-poor target substance with no or inadequate safety data, based on RAX approaches using data-rich analogue(s), and taking account of potency or kinetic/dynamic differences.;Camilla Alexander-White and Dagmar Bury and Mark Cronin and Matthew Dent and Eric Hack and Nicola J. Hewitt and Gerry Kenna and Jorge Naciff and Gladys Ouedraogo and Andreas Schepky and Catherine Mahony and Cosmetics Europe;"Medicine (miscellaneous) (Q2); Toxicology (Q2)";684.0;289.0;United States;1970, 1981-2020;10.1016/j.yrtph.2021.105094;0.0091;107.0;;02732300;02732300;10960295;02732300;3.271;Next generation read-across (RAX), New approach methodology (NAM), Next generation risk assessment (NGRA), Cosmetics safety assessment, Systemic toxicity, Physiologically-based biokinetic modelling (PBK), Caffeine, Parabens;Academic Press Inc.;;4417.0;Northern America;890.0;Q2;25176.0;Regulatory toxicology and pharmacology;A 10-step framework for use of read-across (rax) in next generation risk assessment (ngra) for cosmetics safety assessment;9991.0;2516.0;218.0;755.0;9628.0;journal;article;2022
The years-long high-precision photometric data observed by Kepler satellite combining with huge amount of spectra observed by LAMOST provide a great opportunity to study the relations between surface rotation and lithium abundance of li-rich giants. In this study, we cross match the Kepler data with li-rich giants catalog from LAMOST, and obtain 619 common sources. Then we measure 36 rotation periods from the full set of 295 li-rich giants with good data quality which consists of two sub-samples. The rotation periods of 14 stars was extracted from 205 stars with evolutionary stages determined using asteroseismology, including 11 core helium-burning stars (HeBs), 2 red giant branch stars (RGBs), and 1 unclassified star. All the super lithium-rich giant stars (A(Li)>3.3   dex) are HeBs in our sample. The remaining 90 giants do not have evolutionary stages confirmed, and in this sub-sample, 22 giants have their rotation period measured. The rotation detection rate of the former sub-sample is 6.8%, which is significantly higher than the detection rate of a large giant sample in previous studies (2.08%). With the surface rotation period measured, we confirm the relation between stellar rotation and lithium enrichment of giants. Meanwhile, we find that the less Li-enriched stars have a relatively dispersed distribution of rotation periods, and the giants with a high Li enrichment is concentrated on the rapidly rotating area, which is consistent with earlier studies. The present work also shows a jump at A(Li)≈3.3   dex in the relation between rotation period and Li abundance that coincidently is the boundary between Li-rich giants and super Li-rich giants which may indicate the different mechanisms. The rotation periods of super Li-rich giants become shorter as the lithium enrichment increases. This correlation provides an evidence for the rotational induced extra-mixing mechanism responsible for Li enrichment of giants.;DU Ming-hao and BI Shao-lan and SHI Jian-rong and YAN Hong-liang;"Astronomy and Astrophysics (Q4); Space and Planetary Science (Q4)";109.0;21.0;United Kingdom;1981-2020;10.1016/j.chinastron.2021.02.003;;12.0;;02751062;02751062;02751062;02751062;;Stars: rotation, Stars: abundance, stars: chemically peculiar, method: data analysis;Elsevier Ltd.;;2971.0;Western Europe;132.0;Q4;27149.0;Chinese astronomy and astrophysics;Surface rotation of lamost-kepler li-rich giant stars;;33.0;34.0;109.0;1010.0;journal;article;2021
By analyzing the dynamic behavior of institutional and retail investors in the Indonesia Stock Exchange using their completed transactions (comprising over 250 million observations), this study highlights that their trading strategies and behavior, in which institutions play a more important role than individuals in the market, are indeed different. Specifically, past trading activities by individual (institutional) investors have significantly affected the current trading behaviors and strategies of individual investors (both investor types). Furthermore, retail (institutional) investors are most likely to perform contrarian (momentum) strategies and trade frequently (infrequently) with small (large) amounts of money and short (long) holding periods.;Deddy P. Koesrindartoto and Aurelius Aaron and Inka Yusgiantoro and Wirata A. Dharma and Abdurrohman Arroisi;"Business, Management and Accounting (miscellaneous) (Q1); Finance (Q2)";529.0;439.0;Netherlands;2004-2021;10.1016/j.ribaf.2019.101061;0.00412;42.0;;02755319;02755319;02755319;02755319;4.091;Market microstructure, Emerging market, Institutional investors, Individual investors, Trading strategies;Elsevier BV;;6051.0;Western Europe;767.0;Q1;12056.0;Research in international business and finance;Who moves the stock market in an emerging country – institutional or retail investors?;3698.0;2127.0;202.0;532.0;12223.0;journal;article;2020
;Fabian T. Pfeffer;"Social Sciences (miscellaneous) (Q1); Sociology and Political Science (Q1)";108.0;140.0;United States;2004-2020;10.1016/j.rssm.2014.01.001;0.0022199999999999998;37.0;;02765624;02765624;02765624;02765624;1.587;;JAI Press;;5425.0;Northern America;769.0;Q1;4100151507.0;Research in social stratification and mobility;Multigenerational approaches to social mobility. a multifaceted research agenda;1256.0;186.0;65.0;111.0;3526.0;book series;article;2014
The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.;P. Galetsi and K. Katsaliaki and S. Kumar;"Health (social science) (Q1); History and Philosophy of Science (Q1); Medicine (miscellaneous) (Q1)";1526.0;439.0;United Kingdom;1967-2020;10.1016/j.socscimed.2019.112533;;243.0;;02779536;18735347;02779536;18735347;;Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses;Elsevier Ltd.;;5675.0;Western Europe;1913.0;Q1;18983.0;Social science and medicine;Values, challenges and future directions of big data analytics in healthcare: a systematic review;;7601.0;692.0;1607.0;39274.0;journal;article;2019
In this paper, we introduce a model that incorporates features of the fully transparent hotel booking systems and enables estimates of hotel choice probabilities in a group based on the room charges. Firstly, we extract necessary information for the estimation from big data of online booking for major four hotels near Kyoto station.11The data were provided by National Institute of Informatics. Then, we consider a nested logit model as well as a multinomial logit model for the choice behavior of the customers, where the number of rooms available for booking for each hotel are possibly limited. In addition, we apply the model to an optimal room charge problem for a hotel that aims to maximize its expected sales of a certain room type in the transparent online booking systems. We show numerical examples of the maximization problem using the data of the four hotels of November 2012 which is a high season in Kyoto city. This model is useful in that hotel managers as well as hotel investors, such as hotel REITs and hotel funds, are able to predict the potential sales increase of hotels from online booking data and make use of the result as a tool for investment decisions.;Taiga Saito and Akihiko Takahashi and Hiroshi Tsuda;"Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1)";490.0;898.0;United Kingdom;1982-2020;10.1016/j.ijhm.2016.06.006;0.011340000000000001;122.0;;02784319;02784319;02784319;02784319;9.237;Hotels in Kyoto, Revenue management, Online booking, Discrete choice model;Elsevier Ltd.;;7886.0;Western Europe;2321.0;Q1;28686.0;International journal of hospitality management;Optimal room charge and expected sales under discrete choice models with limited capacity;17219.0;4796.0;321.0;509.0;25314.0;journal;article;2016
"The definition of the threshold of sediment motion is critical for continental shelf sediment dynamics. The work by A. Shields laid the foundation for this research direction, leading to the well-known Shields curve. Here we review the most widely used threshold curves that have followed from the original Shields curve over the last 80 years, and propose that in terms of physical processes the threshold (critical Shields parameter) is a function of at least six variables, i.e. grain Reynolds number, grain size distribution, sphericity, roundness, particle cohesiveness and the scale effects of turbulence. Identifying these key factors, we paid a special attention to the role of the scale effects of turbulence. Turbulence was thought to be a random process, but the improvement of measurement techniques revealed that it has both temporal and spatial structures: the magnitude of instantaneous velocity fluctuations varies in time and in location, which can cause the deviation between in situ measurements and flume experiments. In coastal and shelf waters, in situ measurements of tidal currents and suspended sediment concentrations have revealed that resuspension takes place even though the bed shear stress is well below the Shields curve. Further process and mechanism studies are required to improve the theoretical framework regarding the turbulence structures and their interplay with sediment threshold. The scientific problems for future studies include the establishment of laboratory experiments, in situ measurements and process-based modelling under different water depths and hydrodynamic conditions to quantify the scale effects of turbulence; the development of new observation techniques for higher resolution and for extreme environments; development of new data processing methods, including big data methods to analyse turbulence structures; and the quantification of the effects of biological contributions and non-particle components on the family of Shields curves.";Yang Yang and Shu Gao and Ya Ping Wang and Jianjun Jia and Jilian Xiong and Liang Zhou;"Aquatic Science (Q1); Geology (Q1); Oceanography (Q1)";496.0;234.0;United Kingdom;1982-2020;10.1016/j.csr.2019.103960;0.006409999999999999;111.0;;02784343;02784343;02784343;02784343;2.391;Sediment threshold, Shields curves, Critical shear stress, Turbulence structures, Scale effects, Continental shelf environments;Elsevier Ltd.;;6453.0;Western Europe;893.0;Q1;26817.0;Continental shelf research;Revisiting the problem of sediment motion threshold;11676.0;1380.0;122.0;501.0;7873.0;journal;article;2019
;David Wasserstein and Ujash Sheth;"Physical Therapy, Sports Therapy and Rehabilitation (Q1); Orthopedics and Sports Medicine (Q2); Sports Science (Q2)";135.0;171.0;United Kingdom;1982-2020;10.1016/j.csm.2018.03.002;0.0018100000000000002;81.0;;02785919;1556228X;02785919;1556228X;2.182;Administrative database, Cohort study, Epidemiology, Incidence rate, Sports medicine;W.B. Saunders Ltd;;4817.0;Western Europe;780.0;Q1;19819.0;Clinics in sports medicine;Administrative databases in sports medicine research;2732.0;367.0;66.0;170.0;3179.0;journal;article;2018
In recent years, the introduction of Industry 4.0 technologies in the manufacturing landscape promoted the development of smart factories characterised by relevant socio-technical interactions between humans and machines. In this context, understanding and modelling the role of humans turns out to be crucial to develop efficient manufacturing systems of the future. Grounding on previous researches in the field of Human-in-the-Loop and Human Cyber-Physical Systems, the paper aims at contributing to a deep reflection about human-machine interaction in the wider perspective of Social Human-in-the-Loop Cyber-Physical Production Systems, in which more agents collaborate and are socially connected. After presenting an evolution of manufacturing control organisations, an architecture to depict social interactions in smart factories is proposed. The proposed architecture contributes to the representation of different human roles in the smart factory and the exploration of both hierarchical and heterarchical data-driven decision-making processes in manufacturing.;Chiara Cimini and Fabiana Pirola and Roberto Pinto and Sergio Cavalieri;"Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)";288.0;1088.0;Netherlands;1982-2020;10.1016/j.jmsy.2020.01.002;0.00561;70.0;;02786125;02786125;02786125;02786125;8.633;Industry 4.0, Human-in-the-loop, Cyber-physical production systems, Manufacturing control architecture;Elsevier;;5746.0;Western Europe;2310.0;Q1;14966.0;Journal of manufacturing systems;A human-in-the-loop manufacturing control architecture for the next generation of production systems;5413.0;2949.0;155.0;294.0;8906.0;journal;article;2020
;David Heath and Milena Horvat and Nives Ogrinc;"Food Science (Q1); Toxicology (Q1); Medicine (miscellaneous) (Q2)";2028.0;440.0;United Kingdom;1982-2020;10.1016/j.fct.2021.112372;0.01967;172.0;;02786915;18736351;02786915;18736351;6.023;;Elsevier Ltd.;;5619.0;Western Europe;951.0;Q1;25096.0;Food and chemical toxicology;Preface;37730.0;9232.0;770.0;2060.0;43269.0;journal;article;2021
These European Resuscitation Council Ethics guidelines provide evidence-based recommendations for the ethical, routine practice of resuscitation and end-of-life care of adults and children. The guideline primarily focus on major ethical practice interventions (i.e. advance directives, advance care planning, and shared decision making), decision making regarding resuscitation, education, and research. These areas are tightly related to the application of the principles of bioethics in the practice of resuscitation and end-of-life care.;Spyros D. Mentzelopoulos and Keith Couper and Patrick Van de Voorde and Patrick Druwé and Marieke Blom and Gavin D. Perkins and Ileana Lulic and Jana Djakow and Violetta Raffay and Gisela Lilja and Leo Bossaert;"Cardiology and Cardiovascular Medicine (Q1); Emergency Medicine (Q1); Emergency Nursing (Q1)";872.0;289.0;Ireland;1972-1976, 1978-2020;10.1016/j.resuscitation.2021.02.017;0.03323;134.0;;03009572;18731570;03009572;18731570;5.262;;Elsevier Ireland Ltd;;2627.0;Western Europe;2366.0;Q1;19064.0;Resuscitation;European resuscitation council guidelines 2021: ethics of resuscitation and end of life decisions;18976.0;3778.0;491.0;1290.0;12899.0;journal;article;2021
There are many challenges to developing treatments for complex diseases. This review explores the question of whether it is possible to imagine a data repository that would increase the pace of understanding complex diseases sufficiently well to facilitate the development of effective treatments. First, consideration is given to the amount of data that might be needed for such a data repository and whether the existing data storage infrastructure is enough. Several successful data repositories are then examined to see if they have common characteristics. An area of science where unsuccessful attempts to develop a data infrastructure is then described to see what lessons could be learned for a data repository devoted to complex disease. Then, a variety of issues related to sharing data are discussed. In some of these areas, it is reasonably clear how to move forward. In other areas, there are significant open questions that need to be addressed by all data repositories. Using that baseline information, the question of whether data archives can be effective in understanding a complex disease is explored. The major goal of such a data archive is likely to be identifying biomarkers that define sub-populations of the disease.;Gregory K. Farber;Neuroscience (miscellaneous) (Q1);201.0;1098.0;United Kingdom;1959, 1962, 1973-2020;10.1016/j.pneurobio.2016.03.008;0.0103;231.0;;03010082;18735118;03010082;18735118;11.685;Data sharing, Data repositories, Data infrastructure, Imaging data, Common data elements, Consents;Elsevier Ltd.;;11300.0;Western Europe;3911.0;Q1;24025.0;Progress in neurobiology;Can data repositories help find effective treatments for complex diseases?;15161.0;2447.0;86.0;208.0;9718.0;journal;article;2017
The forces of digital transformation have delivered significant benefits like sustainable development and economic growth in a range of early adopter industries such as retail and manufacturing but, despite these potential benefits, the resource and energy sectors have been relative latecomers to digitalization simply because they are frequently slower to absorb new technologies. Here we present the results of a systematic literature review identifying the ways in which digital technologies have been applied in the oil and gas, mining, and energy domains. We applied content and descriptive analysis to evaluate and discuss 151 academic articles selected from the Scopus database. Two particularly interesting trends emerge from the analysis. First, over 75% of the papers were about the energy sector excluding the oil & gas industry, and only a small minority were from the mining or oil & gas sectors. Second, the most frequently discussed objective of digital transformation was the reduction of operational expenses. By surveying the different ways in which these innovations have been used in these industries and identifying trends and patterns in how digital technologies have been applied, the findings of this review deepen our understanding of the current state of digital technologies within the resource and energy sectors and, in so doing, shine a useful amount of light on the contributions that digital transformation has made to businesses in these sectors. This paper also highlights for future scholars, practitioners, and policymakers the six research areas that they should focus on in the future to help the resource and energy sectors accelerate the digital transformation process and improve their ability to deliver value with these innovations.;Parisa Maroufkhani and Kevin C. Desouza and Robert K. Perrons and Mohammad Iranmanesh;"Economics and Econometrics (Q1); Law (Q1); Management, Monitoring, Policy and Law (Q1); Sociology and Political Science (Q1)";543.0;544.0;United Kingdom;1974-2020;10.1016/j.resourpol.2022.102622;0.00695;69.0;;03014207;03014207;03014207;03014207;5.634;Digital technologies, Resource sector, Energy, Oil and gas, Mining, Digitalization;Elsevier Ltd.;;6140.0;Western Europe;1276.0;Q1;110031.0;Resources policy;Digital transformation in the resource and energy sectors: a systematic review;7000.0;3147.0;309.0;556.0;18973.0;journal;article;2022
The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.;Robert K. Perrons and Jesse W. Jensen;"Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1)";2135.0;629.0;United Kingdom;1973-2020;10.1016/j.enpol.2015.02.020;0.04319;217.0;;03014215;03014215;03014215;03014215;6.142;Big data, Oil and gas, Information technologies, Data;Elsevier BV;;5977.0;Western Europe;2093.0;Q1;29403.0;Energy policy;Data as an asset: what the oil and gas sector can learn from other industries about “big data”;60369.0;14111.0;679.0;2165.0;40582.0;journal;article;2015
Material flow analysis (MFA) is one of the most widely accepted and utilized tools in the industrial-ecology discipline, that measures the input-output materials and examines the pathways and flux of each material flow within the whole system. The application of MFA in e-waste management has recently increased and quite a few academic articles have been published on this issue providing decision support at the policy level. However, there is a need to understand the dynamics of MFA methodology, the data requirements (as well as the data sources used in the previous studies) and the lessons learnt from the studies, so that countries where such an E-waste-MFA study has not yet been performed can apply the international experience of such an emerging research technique. This comprehensive review article presents the recent applications, trends, characteristics, research gaps and challenges of the MFA method that may help e-waste management with an overview of the need for a such tool to be applied. A country-wise analysis is presented and MFA models complemented by various associated methods are summarized with national-level, regional-level, product-level, and element-level assessment. The highlighted future research perspectives discussed in this study will help to analyze e-waste management systems more critically, including the hidden and known flows of waste products and associated materials, economic assessment of material recovery and the role of responsible authorities. This invaluable contribution will help future researchers, particularly from the data collection techniques and previously applied MFA models complemented by various associated methods.;Md Tasbirul Islam and Nazmul Huda;"Environmental Engineering (Q1); Management, Monitoring, Policy and Law (Q1); Medicine (miscellaneous) (Q1); Waste Management and Disposal (Q1)";3798.0;689.0;United States;1970, 1973, 1975, 1977-2021;10.1016/j.jenvman.2019.05.062;0.057120000000000004;179.0;;03014797;10958630;03014797;10958630;6.789;Electrical and electronic equipment (EEE), Waste electrical and electronic equipment (WEEE), Recycling, Substance flow analysis, Circular economy, Literature review;Academic Press Inc.;;6069.0;Northern America;1441.0;Q1;23371.0;Journal of environmental management;Material flow analysis (mfa) as a strategic tool in e-waste management: applications, trends and future directions;62838.0;27155.0;1438.0;3825.0;87268.0;journal;article;2019
"Context
The optimal treatment for men with high-risk localized or locally advanced prostate cancer (PCa) remains unknown.
Objective
To perform a systematic review of the existing literature on the effectiveness of the different primary treatment modalities for high-risk localized and locally advanced PCa. The primary oncological outcome is the development of distant metastases at ≥5 yr of follow-up. Secondary oncological outcomes are PCa-specific mortality, overall mortality, biochemical recurrence, and need for salvage treatment with ≥5 yr of follow-up. Nononcological outcomes are quality of life (QoL), functional outcomes, and treatment-related side effects reported.
Evidence acquisition
Medline, Medline In-Process, Embase, and the Cochrane Central Register of Randomized Controlled Trials were searched. All comparative (randomized and nonrandomized) studies published between January 2000 and May 2019 with at least 50 participants in each arm were included. Studies reporting on high-risk localized PCa (International Society of Urologic Pathologists [ISUP] grade 4–5 [Gleason score {GS} 8–10] or prostate-specific antigen [PSA] >20 ng/ml or ≥ cT2c) and/or locally advanced PCa (any PSA, cT3–4 or cN+, any ISUP grade/GS) or where subanalyses were performed on either group were included. The following primary local treatments were mandated: radical prostatectomy (RP), external beam radiotherapy (EBRT) (≥64 Gy), brachytherapy (BT), or multimodality treatment combining any of the local treatments above (±any systemic treatment). Risk of bias (RoB) and confounding factors were assessed for each study. A narrative synthesis was performed.
Evidence synthesis
Overall, 90 studies met the inclusion criteria. RoB and confounding factors revealed high RoB for selection, performance, and detection bias, and low RoB for correction of initial PSA and biopsy GS. When comparing RP with EBRT, retrospective series suggested an advantage for RP, although with a low level of evidence. Both RT and RP should be seen as part of a multimodal treatment plan with possible addition of (postoperative) RT and/or androgen deprivation therapy (ADT), respectively. High levels of evidence exist for EBRT treatment, with several randomized clinical trials showing superior outcome for adding long-term ADT or BT to EBRT. No clear cutoff can be proposed for RT dose, but higher RT doses by means of dose escalation schemes result in an improved biochemical control. Twenty studies reported data on QoL, with RP resulting mainly in genitourinary toxicity and sexual dysfunction, and EBRT in bowel problems.
Conclusions
Based on the results of this systematic review, both RP as part of multimodal treatment and EBRT + long-term ADT can be recommended as primary treatment in high-risk and locally advanced PCa. For high-risk PCa, EBRT + BT can also be offered despite more grade 3 toxicity. Interestingly, for selected patients, for example, those with higher comorbidity, a shorter duration of ADT might be an option. For locally advanced PCa, EBRT + BT shows promising result but still needs further validation. In this setting, it is important that patients are aware that the offered therapy will most likely be in the context a multimodality treatment plan. In particular, if radiation is used, the combination of local with systemic treatment provides the best outcome, provided the patient is fit enough to receive both. Until the results of the SPCG15 trial are known, the optimal local treatment remains a matter of debate. Patients should at all times be fully informed about all available options, and the likelihood of a multimodal approach including the potential side effects of both local and systemic treatment.
Patient summary
We reviewed the literature to see whether the evidence from clinical studies would tell us the best way of curing men with aggressive prostate cancer that had not spread to other parts of the body such as lymph glands or bones. Based on the results of this systematic review, there is good evidence that both surgery and radiation therapy are good treatment options, in terms of prolonging life and preserving quality of life, provided they are combined with other treatments. In the case of surgery this means including radiotherapy (RT), and in the case of RT this means either hormonal therapy or combined RT and brachytherapy.";Lisa Moris and Marcus G. Cumberbatch and Thomas {Van den Broeck} and Giorgio Gandaglia and Nicola Fossati and Brian Kelly and Raj Pal and Erik Briers and Philip Cornford and Maria {De Santis} and Stefano Fanti and Silke Gillessen and Jeremy P. Grummet and Ann M. Henry and Thomas B.L. Lam and Michael Lardas and Matthew Liew and Malcolm D. Mason and Muhammad Imran Omar and Olivier Rouvière and Ivo G. Schoots and Derya Tilki and Roderick C.N. {van den Bergh} and Theodorus H. {van Der Kwast} and Henk G. {van Der Poel} and Peter-Paul M. Willemse and Cathy Y. Yuan and Badrinath Konety and Tanya Dorff and Suneil Jain and Nicolas Mottet and Thomas Wiegel;Urology (Q1);538.0;636.0;Netherlands;1975-2020;10.1016/j.eururo.2020.01.033;0.06226;216.0;;03022838;03022838;1421993X;03022838;20.096;Prostate cancer, Localized, Locally advanced, Primary therapy, Radical prostatectomy, External beam radiotherapy, Brachytherapy, Modality treatment, Systemic treatment, Systematic review;Elsevier;;1525.0;Western Europe;9799.0;Q1;19897.0;European urology;Benefits and risks of primary treatments for high-risk localized and locally advanced prostate cancer: an international multidisciplinary systematic review;42109.0;10533.0;502.0;1501.0;7656.0;journal;article;2020
Polycystic Ovary Syndrome (PCOS) is the most common endocrine disorder amongst women of reproductive age, whose aetiology remains unclear. To improve our understanding of the molecular mechanisms underlying the disease, we conducted a genome-wide DNA methylation profiling in granulosa lutein cells collected from 16 women suffering from PCOS, in comparison to 16 healthy controls. Samples were collected by follicular aspiration during routine egg collection for IVF treatment. Study groups were matched for age and BMI, did not suffer from other disease and were not taking confounding medication. Comparing women with polycystic versus normal ovarian morphology, after correcting for multiple comparisons, we identified 106 differentially methylated CpG sites with p-values <5.8 × 10−8 that were associated with 88 genes, several of which are known to relate either to PCOS or to ovarian function. Replication and validation of the experiment was done using pyrosequencing to analyse six of the identified differentially methylated sites. Pathway analysis indicated potential disruption in canonical pathways and gene networks that are, amongst other, associated with cancer, cardiogenesis, Hedgehog signalling and immune response. In conclusion, these novel findings indicate that women with PCOS display epigenetic changes in ovarian granulosa cells that may be associated with the heterogeneity of the disorder.;E. Makrinou and A.W. Drong and G. Christopoulos and A. Lerner and I. Chapa-Chorda and T. Karaderi and S. Lavery and K. Hardy and C.M. Lindgren and S. Franks;"Biochemistry (Q1); Endocrinology (Q1); Molecular Biology (Q2)";858.0;376.0;Ireland;1974-2020;10.1016/j.mce.2019.110611;0.01564;144.0;;03037207;03037207;18728057;03037207;4.102;PCOS, EWAS, DNA methylation, Metabolic syndrome, Reproduction;Elsevier Ireland Ltd;;8065.0;Western Europe;1296.0;Q1;26206.0;Molecular and cellular endocrinology;Genome-wide methylation profiling in granulosa lutein cells of women with polycystic ovary syndrome (pcos);18715.0;3543.0;293.0;885.0;23629.0;journal;article;2020
"Central nervous system (CNS) diseases are associated with complexity and diversity; as a result, it is urgent to search for a simple approach for effectively improving the clinical decision-making ability and precise treatment currently. Radiomics can collect plenty of quantitative features based on the massive medical image data; meanwhile, related diagnosis and prediction can be performed through quantitative analysis. The main steps of radiomics analysis include image collection as well as reconstruction, segmentation of the region of interest (ROI), feature extraction as well as quantification, and establishment of the predictive as well as prognostic models. Compared with traditional imaging features, radiomics allows to transform the visual image data to the in-depth features, so as to carry out quantitative research. Our findings suggest that radiomics has broad application prospects in the early screening, accurate diagnosis, grading and staging, treatment and prognosis, and molecular characteristics of CNS diseases, which can improve the capacities to diagnose and predict CNS diseases prognosis through complementing and combining with traditional imaging.";Yanghua Fan and Ming Feng and Renzhi Wang;"Medicine (miscellaneous) (Q2); Surgery (Q2); Neurology (clinical) (Q3)";930.0;178.0;Netherlands;1974-2020;10.1016/j.clineuro.2019.105565;0.00829;73.0;;03038467;18726968;03038467;18726968;1.876;Central nervous system, Radiomics, Diagnose, Prognosis;Elsevier;;2841.0;Western Europe;587.0;Q2;14895.0;Clinical neurology and neurosurgery;Application of radiomics in central nervous system diseases: a systematic literature review;7916.0;1813.0;627.0;966.0;17815.0;journal;article;2019
;Judy Che-Castaldo and Owen R. Jones and Bruce E. Kendall and Jean H. Burns and Dylan Z. Childs and Thomas H.G. Ezard and Haydee Hernandez-Yanez and David J. Hodgson and Eelke Jongejans and Tiffany Knight and Cory Merow and Satu Ramula and Iain Stott and Yngvild Vindenes and Hiroyuki Yokomizo and Roberto Salguero-Gómez;Ecological Modeling (Q2);814.0;293.0;Netherlands;1975-2020;10.1016/j.ecolmodel.2019.108913;0.01125;156.0;;03043800;18727026;03043800;18727026;2.974;;Elsevier;;6467.0;Western Europe;876.0;Q2;23274.0;Ecological modelling;Comments to “persistent problems in the construction of matrix population models”;22498.0;2539.0;283.0;826.0;18301.0;journal;article;2020
Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.;Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun;"Cancer Research (Q1); Oncology (Q1)";1409.0;785.0;Ireland;1975-2020;10.1016/j.canlet.2016.05.033;0.04013;182.0;;03043835;03043835;18727980;03043835;8.679;Radiation oncology, Big Data, Predictive model, Machine learning;Elsevier Ireland Ltd;;6430.0;Western Europe;2470.0;Q1;29160.0;Cancer letters;Big data and machine learning in radiation oncology: state of the art and future prospects;42174.0;11608.0;430.0;1423.0;27650.0;journal;article;2016
"This paper investigates the differential impacts of a unique car restriction policy – the car purchase lottery in Beijing – on the housing markets across locations within the city. I use a difference-in-differences approach to compare heterogeneous neighborhoods before and after the implementation of the policy. Housing prices experience a relative increase at locations closer to common destinations (employment centers: 0.7% per kilometer; primary schools: 3.3% per kilometer) and with better access to public transit (subways: 1.2% per kilometer; buses: 0.08% per line). These changes reflect capitalization of the car restriction policy and imply a large wealth redistribution as large as 6 years of average disposable income across homeowners. The results are relevant to policy, both in the context of unintended consequences and for efforts to develop offsetting measures.";Xueying Lyu;"Development (Q1); Economics and Econometrics (Q1)";279.0;388.0;Netherlands;1974-2020;10.1016/j.jdeveco.2022.102850;0.016280000000000003;142.0;;03043878;03043878;03043878;03043878;3.875;Car restriction policies, Housing prices, Capitalization, Wealth redistribution;Elsevier;;5212.0;Western Europe;3588.0;Q1;14834.0;Journal of development economics;Car restriction policies and housing markets;12164.0;1172.0;129.0;281.0;6723.0;journal;article;2022
Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.;Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu;"Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Pollution (Q1); Waste Management and Disposal (Q1)";2975.0;1039.0;Netherlands;1975-2021;10.1016/j.jhazmat.2022.129487;0.07094;284.0;;03043894;03043894;03043894;03043894;10.588;Machine learning, Chemical, Toxicity, Environmental health, Big data;Elsevier;;5917.0;Western Europe;2034.0;Q1;25858.0;Journal of hazardous materials;Machine learning in the identification, prediction and exploration of environmental toxicology: challenges and perspectives;137983.0;31399.0;2143.0;2994.0;126809.0;journal;article;2022
"In its traditional definition, a repair of an inconsistent database is a consistent database that differs from the inconsistent one in a “minimal way.” Often, repairs are not equally legitimate, as it is desired to prefer one over another; for example, one fact is regarded more reliable than another, or a more recent fact should be preferred to an earlier one. Motivated by these considerations, researchers have introduced and investigated the framework of preferred repairs, in the context of denial constraints and subset repairs. There, a priority relation between facts is lifted towards a priority relation between consistent databases, and repairs are restricted to the ones that are optimal in the lifted sense. Three notions of lifting (and preferred repairs) have been proposed: Pareto, global, and completion. In this article, we investigate the complexity of three problems on preferred repairs. The first is the problem of deciding whether the priority relation contains enough information to clean the database unambiguously, or in other words, whether there is exactly one preferred repair. We show that the different lifting semantics entail highly different complexities for this problem. Then, we study the ability to quantify ambiguity, by investigating two classes of problems. The first is that of counting the preferred repairs. We establish a dichotomy in data complexity for the entire space of (sets of) functional dependencies for all three notions. The second class of problems is that of enumerating (i.e., generating) the preferred repairs. We devise enumeration algorithms with efficiency guarantees on the delay between generated repairs, even for constraints represented as general conflict graphs or hypergraphs.";Benny Kimelfeld and Ester Livshits and Liat Peterfreund;"Computer Science (miscellaneous) (Q2); Theoretical Computer Science (Q3)";1144.0;150.0;Netherlands;1975-2020;10.1016/j.tcs.2020.05.016;0.010459999999999999;122.0;;03043975;03043975;03043975;03043975;0.827;Inconsistent databases, Repair, Subset repair, Preferred repair, Categoricity, Repair counting, Repair enumeration, Functional dependencies, Conflict hypergraph;Elsevier;;2768.0;Western Europe;464.0;Q2;20571.0;Theoretical computer science;Counting and enumerating preferred database repairs;8571.0;1839.0;587.0;1196.0;16251.0;journal;article;2020
Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.;Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch;"Applied Mathematics (Q1); Economics and Econometrics (Q1); History and Philosophy of Science (Q1)";408.0;338.0;Netherlands;1973-2020;10.1016/j.jeconom.2019.04.019;0.02085;159.0;;03044076;03044076;03044076;03044076;2.388;Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model;Elsevier BV;;4583.0;Western Europe;3769.0;Q1;28973.0;Journal of econometrics;Term structure analysis with big data: one-step estimation using bond prices;25569.0;1497.0;241.0;418.0;11045.0;journal;article;2019
Current anticancer paradigms largely target driver mutations considered integral for cancer cell survival and tumor progression. Although initially successful, many of these strategies are unable to overcome the tremendous heterogeneity that characterizes advanced tumors, resulting in the emergence of resistant disease. Cancer is a rapidly evolving, multifactorial disease that accumulates numerous genetic and epigenetic alterations. This results in wide phenotypic and molecular heterogeneity within the tumor, the complexity of which is further amplified through specific interactions between cancer cells and the tumor microenvironment. In this context, cancer may be perceived as an “ecomolecular” disease that involves cooperation between several neoplastic clones and their interactions with immune cells, stromal fibroblasts, and other cell types present in the microenvironment. This collaboration is mediated by a variety of secreted factors. Cancer is therefore analogous to complex ecosystems such as microbial consortia. In the present article, we comment on the current paradigms and perspectives guiding the development of cancer diagnostics and therapeutics and the potential application of systems biology to untangle the complexity of neoplasia. In our opinion, conceptualization of neoplasia as an ecomolecular disease is warranted. Advances in knowledge pertinent to the complexity and dynamics of interactions within the cancer ecosystem are likely to improve understanding of tumor etiology, pathogenesis, and progression. This knowledge is anticipated to facilitate the design of new and more effective therapeutic approaches that target the tumor ecosystem in its entirety.;Santiago {Ramón y Cajal} and Claudia Capdevila and Javier Hernandez-Losa and Leticia {De Mattos-Arruda} and Abhishek Ghosh and Julie Lorent and Ola Larsson and Trond Aasen and Lynne-Marie Postovit and Ivan Topisirovic;"Cancer Research (Q1); Genetics (Q1); Oncology (Q1)";190.0;975.0;Netherlands;1974-2020;10.1016/j.bbcan.2017.09.004;;137.0;;0304419X;0304419X;0304419X;0304419X;;Cancer, Ecomolecular, Consortium, Paradigms, Systems biology, Heterogeneity;Elsevier;;15393.0;Western Europe;3435.0;Q1;80280.0;Biochimica et biophysica acta - reviews on cancer;Cancer as an ecomolecular disease and a neoplastic consortium;;1826.0;105.0;192.0;16163.0;journal;article;2017
"The questions that chemical oceanographers prioritize over the coming decades, and the methods we use to address these questions, will define our field's contribution to 21st century science. In recognition of this, the U.S. National Science Foundation and National Oceanic and Atmospheric Administration galvanized a community effort (the Chemical Oceanography MEeting: A BOttom-up Approach to Research Directions, or COME ABOARD) to synthesize bottom-up perspectives on selected areas of research in Chemical Oceanography. Representing only a small subset of the community, COME ABOARD participants did not attempt to identify targeted research directions for the field. Instead, we focused on how best to foster diverse research in Chemical Oceanography, placing emphasis on the following themes: strengthening our core chemical skillset; expanding our tools through collaboration with chemists, engineers, and computer scientists; considering new roles for large programs; enhancing interface research through interdisciplinary collaboration; and expanding ocean literacy by engaging with the public. For each theme, COME ABOARD participants reflected on the present state of Chemical Oceanography, where the community hopes to go and why, and actionable pathways to get there. A unifying concept among the discussions was that dissimilar funding structures and metrics of success may be required to accommodate the various levels of readiness and stages of knowledge development found throughout our community. In addition to the science, participants of the concurrent Dissertations Symposium in Chemical Oceanography (DISCO) XXV, a meeting of recent and forthcoming Ph.D. graduates in Chemical Oceanography, provided perspectives on how our field could show leadership in addressing long-standing diversity and early-career challenges that are pervasive throughout science. Here we summarize the COME ABOARD Meeting discussions, providing a synthesis of reflections and perspectives on the field.";Andrea J. Fassbender and Hilary I. Palevsky and Todd R. Martz and Anitra E. Ingalls and Martha Gledhill and Sarah E. Fawcett and Jay A. Brandes and Lihini I. Aluwihare;"Chemistry (miscellaneous) (Q1); Oceanography (Q1); Water Science and Technology (Q1); Environmental Chemistry (Q2)";271.0;366.0;Netherlands;1972-2020;10.1016/j.marchem.2017.09.002;0.00635;130.0;;03044203;03044203;03044203;03044203;3.807;;Elsevier;;7315.0;Western Europe;1269.0;Q1;23255.0;Marine chemistry;Perspectives on chemical oceanography in the 21st century: participants of the come aboard meeting examine aspects of the field in the context of 40years of disco;11297.0;988.0;88.0;279.0;6437.0;journal;article;2017
This paper proposes an integrative approach to feature (input and output) selection in Data Envelopment Analysis (DEA). The DEA model is enriched with zero-one decision variables modelling the selection of features, yielding a Mixed Integer Linear Programming formulation. This single-model approach can handle different objective functions as well as constraints to incorporate desirable properties from the real-world application. Our approach is illustrated on the benchmarking of electricity Distribution System Operators (DSOs). The numerical results highlight the advantages of our single-model approach provide to the user, in terms of making the choice of the number of features, as well as modeling their costs and their nature.;Sandra Benítez-Peña and Peter Bogetoft and Dolores {Romero Morales};"Information Systems and Management (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1)";361.0;755.0;United Kingdom;1973-2020;10.1016/j.omega.2019.05.004;;142.0;;03050483;03050483;03050483;03050483;;Benchmarking, Data Envelopment Analysis, Feature Selection, Mixed Integer Linear Programming;Elsevier BV;;5155.0;Western Europe;2500.0;Q1;21915.0;Omega;Feature selection in data envelopment analysis: a mathematical optimization approach;;2965.0;196.0;363.0;10104.0;journal;article;2020
;Thanos Papadopoulos and Angappa Gunasekaran;"Computer Science (miscellaneous) (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1)";746.0;485.0;United Kingdom;1974-2021;10.1016/j.cor.2018.05.015;;152.0;;03050548;03050548;1873765X;03050548;;;Elsevier Ltd.;;4468.0;Western Europe;1506.0;Q1;24355.0;Computers and operations research;Editorial;;3837.0;210.0;755.0;9383.0;journal;article;2018
"Background
Burns are not only major personal catastrophic events but also constitute a national health problem due to its associated morbidity, rehabilitation, mortality and high cost medical services. Advances in care and treatment have increased survival from major burn injury. However, information on the epidemiology and risk factors of burn mortality in Taiwan is limited. The study aim was to determine the nationwide epidemiological characteristics, trends, and mortality risk factors of burn inpatients in Taiwan.
Methods
This nationwide population-based study evaluated data retrieved from the Taiwan National Health Insurance database. Patients hospitalized for burns (ICD-9-CM codes 940-949) between 2003 and 2013 were identified from hospitalization records.
Results
A total of 73,774 patients were included. The data showed increases in age, revised Baux score, and Charlson Comorbidity Index during the study period, but it was also accompanied by a continuing decrease in burn incidence and a significant shortening of the length of hospital stay. The average in-hospital mortality was 17.5/1000 in 2003 and 12.2/1000 in 2013 but did not showed significant change. Male gender, older age, higher Charlson Comorbidity Index, presence of inhalation injury, large total burn surface area (TBSA), and higher revised Baux score were significant predictors of mortality.
Conclusion
Population-based burn epidemiology data demonstrated ongoing improvement in hospital care during the past decade. Male gender, older age, higher Charlson Comorbidity Index, presence of inhalation injury, large TBSA, and higher revised Baux score were significant predictors of mortality.";Cheng-I Yen and Meng-Jiun Chiou and Chang-Fu Kuo and Han-Tsung Liao;"Critical Care and Intensive Care Medicine (Q1); Emergency Medicine (Q1); Surgery (Q1); Medicine (miscellaneous) (Q2)";660.0;210.0;United Kingdom;1974-2020;10.1016/j.burns.2018.02.030;0.007659999999999999;101.0;;03054179;03054179;18791409;03054179;2.744;Burn, Epidemiology, Mortality, Risk factors;Elsevier Ltd.;;2879.0;Western Europe;901.0;Q1;29574.0;Burns;Determination of risk factors for burn mortality based on a regional population study in taiwan;10973.0;1893.0;367.0;836.0;10566.0;journal;article;2018
As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.;Mark D. McCoy;"Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1)";362.0;304.0;United States;1974-2020;10.1016/j.jas.2017.06.003;0.01111;126.0;;03054403;03054403;10959238;03054403;3.216;Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science;Academic Press Inc.;;7601.0;Northern America;1572.0;Q1;31405.0;Journal of archaeological science;Geospatial big data and archaeology: prospects and problems too great to ignore;17761.0;1239.0;137.0;365.0;10414.0;journal;article;2017
"Summary
This paper analyzes the impact of data gap in Millennium Development Goals’ (MDGs) performance indicators on actual performance success of MDGs. Performance success, within the MDG framework, is quantified using six different ways proposed in the existing literature, including both absolute and relative performance and deviation from historical transition paths of MDG indicators. The empirical analysis clearly shows that the data gap in performance measurement is a significant predictor of poor MDG performance in terms of any of the six progress measures. Larger the data gap or weaker the performance measurement system, lesser is the probability of MDG performance success. The empirical methodology used in the paper combines a Heckman correction and instrumental variable estimation strategies to simultaneously account for potential endogeneity of the key data gap variable and bias due to sample selection. This result holds true even after controlling for overall national statistical capacity and a variety of socioeconomic factors. The paper underlines the need to strengthen the performance measurement system attached to the 2030 agenda for sustainable development and the associated Sustainable Development Goals (SDGs). This paper is the first attempt at empirically evaluating the value of data in the context of international development goals and gives empirical evidence for the need to harness the “data revolution” for sustainable development.";Arun Jacob;"Development (Q1); Economics and Econometrics (Q1); Geography, Planning and Development (Q1); Sociology and Political Science (Q1)";911.0;538.0;United Kingdom;1973-2021;10.1016/j.worlddev.2016.12.016;0.02985;175.0;;0305750X;0305750X;18735991;0305750X;5.278;MDG, SDGs, value of data, performance measurement, monitoring and evaluation, goal setting;Elsevier BV;;6023.0;Western Europe;2386.0;Q1;30060.0;World development;Mind the gap: analyzing the impact of data gap in millennium development goals’ (mdgs) indicators on the progress toward mdgs;27106.0;5451.0;381.0;916.0;22949.0;journal;article;2017
Establishing an accurate mathematical model is fundamental to managing, monitoring, and protecting the battery pack in electric vehicles (EVs). The application of the deep learning algorithm-based state estimation method can significantly improve the accuracy and stability of the battery model but is hindered by the great demand for training data. This paper addresses the challenge of health-conscious battery modeling by utilizing multi-source data based on a novel deep transfer learning method. Firstly, a cloud-based battery management framework is designed, which is able to collect and process battery operation data from various EVs and provide a foundation for deploying the transfer learning method. Battery healthy state information in the collected dataset is labeled by a generic perception model, which can be commonly used to quantify the aging state of different battery packs and facilitate the knowledge transfer process. Additionally, a deep transfer learning method is developed to boost the training process of the battery model, where the operation data from different types of EVs can be used for establishing state estimators. The method is verified by the battery operation data collected from two types of electric buses. With the developed healthy state perception model and transfer learning method, battery model error can be limited to 2.43% and 1.27% in the whole life cycle.;Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng;"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)";5304.0;1059.0;United Kingdom;1975-2020;10.1016/j.apenergy.2022.119120;0.15329;212.0;;03062619;03062619;03062619;03062619;9.746;Transportation electrification, Electric vehicles, Battery energy storage, Deep transfer learning, Battery management system, Battery state estimation;Elsevier BV;;5792.0;Western Europe;3035.0;Q1;28801.0;Applied energy;Health-conscious vehicle battery state estimation based on deep transfer learning;122712.0;56804.0;1729.0;5329.0;100144.0;journal;article;2022
;;"Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)";255.0;339.0;United Kingdom;1975-2021;10.1016/S0306-4379(16)30216-2;0.00331;85.0;;03064379;03064379;03064379;03064379;2.309;;Elsevier Ltd.;;4739.0;Western Europe;547.0;Q2;12305.0;Information systems;Ifc edbd/aims and scope;2604.0;1027.0;100.0;271.0;4739.0;journal;article;2016
The perennial operation of nuclear power primary loop piping leads to deterioration of the piping and the potential risk of leakage. At present, there is no reliable technology to detect the leakage directly. This paper proposes a method to obtain the piping deterioration evolution trend based on the analysis of sets of data that can reflect real state of the piping. The specific method is to quantitatively construct a fusion health index model based on the improved Mahalanobis distance, which integrates the analytic hierarchy process and the entropy weight method. A prediction model combining convolutional neural networks and long short-term memory neural networks is established to make the trend analysis and prediction. The experimental results show that the method can better reflect the actual health state of the piping and effectively predict the deterioration evolution trend, which provides a specific reference value for ensuring the safe and stable operation of equipment.;Hong Qian and Bangzhi Xu and Jun Zhang;Nuclear Energy and Engineering (Q1);1777.0;197.0;United Kingdom;1975-2021;10.1016/j.anucene.2022.109394;0.01265;67.0;;03064549;03064549;18732100;03064549;1.776;Nuclear power plant, Piping deterioration, Health index, Improved Mahalanobis distance, Convolutional neural networks, Long short-term memory neural networks;Elsevier Ltd.;;2878.0;Western Europe;1159.0;Q1;29363.0;Annals of nuclear energy;Research on deterioration evolution trend of primary loop piping in nuclear power plant based on fusion health index;9907.0;3459.0;755.0;1786.0;21726.0;journal;article;2022
Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.;Mohamed Aboelmaged and Samar Mouakket;"Computer Science Applications (Q1); Information Systems (Q1); Library and Information Sciences (Q1); Management Science and Operations Research (Q1); Media Technology (Q1)";298.0;801.0;United Kingdom;1975-2020;10.1016/j.ipm.2020.102234;;101.0;;03064573;18735371;03064573;18735371;;Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks;Elsevier Ltd.;;5841.0;Western Europe;1061.0;Q1;12689.0;Information processing and management;Influencing models and determinants in big data analytics research: a bibliometric analysis;;2449.0;250.0;303.0;14602.0;journal;article;2020
Compulsivity is recognized as a transdiagnostic phenotype, underlying a variety of addictive and obsessive–compulsive behaviors. However, current understanding of how it should be operationalized and the processes contributing to its development and maintenance is limited. The present study investigated if there was a relationship between the affective process Experiential Avoidance (EA), an unwillingness to tolerate negative internal experiences, and the frequency and severity of transdiagnostic compulsive behaviors. A large sample of adults (N = 469) completed online questionnaires measuring EA, psychological distress and the severity of seven obsessive–compulsive and addiction-related behaviors. Using structural equation modelling, results indicated a one-factor model of compulsivity was superior to the two-factor model (addictive- vs OCD-related behaviors). The effect of EA on compulsivity was fully mediated by psychological distress, which in turn had a strong direct effect on compulsivity. This suggests distress is a key mechanism in explaining why people with high EA are more prone to compulsive behaviors. The final model explained 41% of the variance in compulsivity, underscoring the importance of these constructs as likely risk and maintenance factors for compulsive behavior. Implications for designing effective psychological interventions for compulsivity are discussed.;Lauren {Den Ouden} and Jeggan Tiego and Rico S.C. Lee and Lucy Albertella and Lisa-Marie Greenwood and Leonardo Fontenelle and Murat Yücel and Rebecca Segrave;"Clinical Psychology (Q1); Medicine (miscellaneous) (Q1); Psychiatry and Mental Health (Q1); Toxicology (Q1)";1148.0;356.0;United Kingdom;1975-2021;10.1016/j.addbeh.2020.106464;0.02223;127.0;;03064603;03064603;03064603;03064603;3.913;Addiction, Obsessive–compulsive disorder, Gambling, Compulsive buying, Binge-eating, Experiential Avoidance;Elsevier Ltd.;;4711.0;Western Europe;1520.0;Q1;24763.0;Addictive behaviors;The role of experiential avoidance in transdiagnostic compulsive behavior: a structural model analysis;17650.0;4787.0;345.0;1180.0;16254.0;journal;article;2020
Identifying food insecure households in an accurate and cost-effective way is important for targeted food policy interventions. Since predictive accuracy depends partly on which indicators are used to identify food insecure households, it is important to assess the performance of indicators that are relatively easy and inexpensive to collect yet can proxy for the “gold standard” food security indicator, calorie intake. We study the effectiveness of different variable combinations and methods in predicting calorie-based food security among poor households and communities in rural Bangladesh. We use basic household information as a benchmark set for predicting calorie-based food security. We then assess the gain in predictive power obtained by adding subjective food security indicators (e.g., self-reported days without sufficient food), the dietary diversity score (DDS), and the combination of both sets to our model of calorie-based food security. We apply machine learning as well as traditional econometric methods in estimation. We find that the overall predictive accuracy rises from 63% to 69% when we add the subjective and DDS sets to the benchmark set. Our study demonstrates that while alternative indicators and methods are not always accurate in predicting calorie intake, DDS related indicators do improve accuracy compared to a simple benchmark set.;Marup Hossain and Conner Mullally and M. Niaz Asadullah;"Development (Q1); Economics and Econometrics (Q1); Food Science (Q1); Management, Monitoring, Policy and Law (Q1); Sociology and Political Science (Q1)";333.0;454.0;United Kingdom;1975-2020;10.1016/j.foodpol.2019.03.001;0.00839;102.0;;03069192;03069192;03069192;03069192;4.552;Food security, Poverty, Dietary diversity, Program targeting, Machine learning;Elsevier BV;;5573.0;Western Europe;2092.0;Q1;35048.0;Food policy;Alternatives to calorie-based indicators of food security: an application of machine learning methods;8894.0;1852.0;118.0;337.0;6576.0;journal;article;2019
"For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.";Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer;Medicine (miscellaneous) (Q3);883.0;140.0;United States;1975-2020;10.1016/j.mehy.2013.11.030;0.005;87.0;;03069877;03069877;15322777;03069877;1.538;;Churchill Livingstone;;3513.0;Northern America;441.0;Q3;17833.0;Medical hypotheses;Psycho-informatics: big data shaping modern psychometrics;9727.0;1452.0;816.0;979.0;28666.0;journal;article;2014
One of the most relevant inputs for hydrological modeling is the soil map. The soil sources and scales for the soil properties are diverse, and the quality of soil mapping is increasing, but soil surveying is time-consuming and large area campaigns are expensive. The taxonomic unit approach for soil mapping is common and limited to one layer of data. This limitation causes errors in simulated water fluxes through the soil when taxonomic units approach is implemented during hydrological modeling analysis. Some strategies using geostatistics and machine learning algorithms such as Kriging and Self-Organizing maps (SOM) are improving the taxonomic units’ approach and could serve as an alternative for soil mapping for hydrological purposes. The aim of this work is to study the influence of different soil maps and resolutions on the main hydrological components of a sub-arid watershed in central Spain. For this, the Soil Water and Assessment Tool (SWAT) was parameterized with three different soil maps. A first one was based on Harmonized World Soil database from FAO, at scale 1:1,000,000 (HWSD). The other two were based on a Kriging interpolation at 100 × 100 m from soil samples. To obtain soil properties map from it, two strategies were applied: one was to average the soil properties following the official taxonomic soil units at 1:400,000 scale (Agricultural Technological Institute of Castilla and Leon - ITACyL) and the other was to applied Self-organizing map (SOM) to create the soil units (SOMM). The results suggest that scale and soil properties mapping influence HRU definition, which in turn affects water flow through the soils. Statistical metrics of model performance were improved from R2 =0.62 and NSE=0.46 with HWSD soil map to R2 =0.86 and NSE=0.84 with SOM and similar values were achieved during validation. Thus, the SOM is presented as an innovative algorithm applied for hydrological modeling with SWAT, significantly increasing the level of model accuracy to stream flow in sub-arid watersheds.;David Rivas-Tabares and Ángel {de Miguel} and Bárbara Willaarts and Ana M. Tarquis;"Applied Mathematics (Q1); Modeling and Simulation (Q1)";1602.0;540.0;United States;1976-2021;10.1016/j.apm.2020.06.044;0.026160000000000003;112.0;;0307904X;0307904X;0307904X;0307904X;5.129;Soil properties, Self-organizing Maps, Hydrological modeling, SWAT, Soils spatial patterns, SOM;Elsevier Inc.;;4323.0;Northern America;1011.0;Q1;28065.0;Applied mathematical modelling;Self-organizing map of soil properties in the context of hydrological modeling;24972.0;7948.0;578.0;1606.0;24985.0;journal;article;2020
Many studies have applied machine learning to crop yield prediction with a focus on specific case studies. The data and methods they used may not be transferable to other crops and locations. On the other hand, operational large-scale systems, such as the European Commission's MARS Crop Yield Forecasting System (MCYFS), do not use machine learning. Machine learning is a promising method especially when large amounts of data are being collected and published. We combined agronomic principles of crop modeling with machine learning to build a machine learning baseline for large-scale crop yield forecasting. The baseline is a workflow emphasizing correctness, modularity and reusability. For correctness, we focused on designing explainable predictors or features (in relation to crop growth and development) and applying machine learning without information leakage. We created features using crop simulation outputs and weather, remote sensing and soil data from the MCYFS database. We emphasized a modular and reusable workflow to support different crops and countries with small configuration changes. The workflow can be used to run repeatable experiments (e.g. early season or end of season predictions) using standard input data to obtain reproducible results. The results serve as a starting point for further optimizations. In our case studies, we predicted yield at regional level for five crops (soft wheat, spring barley, sunflower, sugar beet, potatoes) and three countries (the Netherlands (NL), Germany (DE), France (FR)). We compared the performance with a simple method with no prediction skill, which either predicted a linear yield trend or the average of the training set. We also aggregated the predictions to the national level and compared with past MCYFS forecasts. The normalized RMSE (NRMSE) for early season predictions (30 days after planting) were comparable for NL (all crops), DE (all except soft wheat) and FR (soft wheat, spring barley, sunflower). For example, NRMSE was 7.87 for soft wheat (NL) (6.32 for MCYFS) and 8.21 for sugar beet (DE) (8.79 for MCYFS). In contrast, NRMSEs for soft wheat (DE), sugar beet (FR) and potatoes (FR) were twice as much compared to MCYFS. NRMSEs for end of season were still comparable to MCYFS for NL, but worse for DE and FR. The baseline can be improved by adding new data sources, designing more predictive features and evaluating different machine learning algorithms. The baseline will motivate the use of machine learning in large-scale crop yield forecasting.;Dilli Paudel and Hendrik Boogaard and Allard {de Wit} and Sander Janssen and Sjoukje Osinga and Christos Pylianidis and Ioannis N. Athanasiadis;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1)";503.0;552.0;United Kingdom;1976-2020;10.1016/j.agsy.2020.103016;0.00937;107.0;;0308521X;0308521X;18732267;0308521X;5.370;Crop yield prediction, Machine learning, Modularity, Reusability, Large-scale crop yield forecasting;Elsevier BV;;6302.0;Western Europe;1694.0;Q1;15061.0;Agricultural systems;Machine learning for large-scale crop yield forecasting;9779.0;3229.0;197.0;511.0;12414.0;journal;article;2021
The increased economic importance of digital services has profoundly changed the power structure in telecommunications and media markets. Although these services sometimes directly compete with traditional telecommunications services, the regulatory obligations for both players differ significantly. This article discusses three important areas deemed relevant in order to define a coherent regulatory framework and to account for the specific peculiarities of digital markets: First, challenges associated with assessing market power in digital markets. Second, challenges in harmonizing different regulatory obligations for digital services, and third, the vital role of data and data protection in the context of data-driven business models.;Jan Krämer and Michael Wohlfarth;"Electrical and Electronic Engineering (Q1); Human Factors and Ergonomics (Q1); Information Systems (Q1)";212.0;351.0;United Kingdom;1976-2020;10.1016/j.telpol.2017.10.004;0.0026899999999999997;69.0;;03085961;03085961;03085961;03085961;3.036;Market power, Market definition, Level-playing field, Data-driven business models, Big data analysis, Data protection, GDPR, Over-the-top services, Regulatory framework, Policy;Elsevier Ltd.;;6152.0;Western Europe;840.0;Q1;20870.0;Telecommunications policy;Market power, regulatory convergence, and the role of data in digital markets;2745.0;831.0;87.0;224.0;5352.0;journal;article;2018
"Maritime industries routinely collect critical environmental data needed for sustainable management of marine ecosystems, supporting both the blue economy and future growth. Collating this information would provide a valuable resource for all stakeholders. For the North Sea, the oil and gas industry has been a dominant presence for over 50 years that has contributed to a wealth of knowledge about the environment. As the industry begins to decommission its offshore structures, this information will be critical for avoiding duplication of effort in data collection and ensuring best environmental management of offshore activities. This paper summarises the outcomes of a Blue Growth Data Challenge Workshop held in 2017 with participants from: the oil and gas industry; the key UK regulatory and management bodies for oil and gas decommissioning; open access data facilitators; and academic and research institutes. Here, environmental data collection and archiving by oil and gas operators in the North Sea are described, alongside how this compares to other offshore industries; what the barriers and opportunities surrounding environmental data sharing are; and how wider data sharing from offshore industries could be achieved. Five primary barriers to data sharing were identified: 1) Incentives, 2) Risk Perception, 3) Working Cultures, 4) Financial Models, and 5) Data Ownership. Active and transparent communication and collaboration between stakeholders including industry, regulatory bodies, data portals and academic institutions will be key to unlocking the data that will be critical to informing responsible decommissioning decisions for offshore oil and gas structures in the North Sea.";Fiona Murray and Katherine Needham and Kate Gormley and Sally Rouse and Joop W.P. Coolen and David Billett and Jennifer Dannheim and Silvana N.R. Birchenough and Kieran Hyder and Richard Heard and Joseph S. Ferris and Jan M. Holstein and Lea-Anne Henry and Oonagh McMeel and Jan-Bart Calewaert and J. Murray Roberts;"Aquatic Science (Q1); Economics and Econometrics (Q1); Environmental Science (miscellaneous) (Q1); Law (Q1); Management, Monitoring, Policy and Law (Q1)";1095.0;396.0;United Kingdom;1977-2020;10.1016/j.marpol.2018.05.021;0.01592;95.0;;0308597X;0308597X;0308597X;0308597X;4.173;Decommissioning, Offshore energy, Environmental assessment, Blue economy, Open access, ROV survey;Elsevier Ltd.;;5994.0;Western Europe;1355.0;Q1;27851.0;Marine policy;Data challenges and opportunities for environmental management of north sea oil and gas decommissioning in an era of blue growth;12969.0;4368.0;477.0;1106.0;28589.0;journal;article;2018
Long-term persistence (LTP) of annual river runoff is a topic of ongoing hydrological research, due to its implications to water resources management. Here, we estimate its strength, measured by the Hurst coefficient H, in 696 annual, globally distributed, streamflow records with at least 80 years of data. We use three estimation methods (maximum likelihood estimator, Whittle estimator and least squares variance) resulting in similar mean values of H close to 0.65. Subsequently, we explore potential factors influencing H by two linear (Spearman's rank correlation, multiple linear regression) and two non-linear (self-organizing maps, random forests) techniques. Catchment area is found to be crucial for medium to larger watersheds, while climatic controls, such as aridity index, have higher impact to smaller ones. Our findings indicate that long-term persistence is weaker than found in other studies, suggesting that enhanced LTP is encountered in large-catchment rivers, were the effect of spatial aggregation is more intense. However, we also show that the estimated values of H can be reproduced by a short-term persistence stochastic model such as an auto-regressive AR(1) process. A direct consequence is that some of the most common methods for the estimation of H coefficient, might not be suitable for discriminating short- and long-term persistence even in long observational records.;Y. Markonis and Y. Moustakis and C. Nasika and P. Sychova and P. Dimitriadis and M. Hanel and P. Máca and S.M. Papalexiou;Water Science and Technology (Q1);699.0;471.0;United Kingdom;1977-2020;10.1016/j.advwatres.2018.01.003;0.014730000000000002;138.0;;03091708;03091708;03091708;03091708;4.510;River runoff, Long-term persistence, Long-range dependence, Self-Organizing Maps, Random forests, Catchment classification;Elsevier Ltd.;;6246.0;Western Europe;1314.0;Q1;19627.0;Advances in water resources;Global estimation of long-term persistence in annual river runoff;15018.0;3413.0;233.0;705.0;14553.0;journal;article;2018
Carbon trading is a vital market mechanism to achieve carbon emission reduction. The accurate prediction of the carbon price is conducive to the effective management and decision-making of the carbon trading market. However, existing research on carbon price forecasting has ignored the impacts of multiple factors on the carbon price, especially climate change. This study proposes a text-based framework for carbon price prediction that considers the impact of climate change. Textual online news is innovatively employed to construct a climate-related variable. The information is combined with other variables affecting the carbon price to forecast the carbon price, using a long short-term memory network and random forest model. The results demonstrate that the prediction accuracy of the carbon price in the Hubei and Guangdong carbon markets is enhanced by adding the textual variable that measures climate change.;Qiwei Xie and Jingjing Hao and Jingyu Li and Xiaolong Zheng;"Economics, Econometrics and Finance (miscellaneous) (Q1); Economics and Econometrics (Q2)";203.0;281.0;Netherlands;1970-2020;10.1016/j.eap.2022.02.010;0.00141;29.0;;03135926;03135926;03135926;03135926;2.497;Carbon price prediction, Text mining, Climate change, Long short-term memory (LSTM), Random forest (RF);Elsevier BV;;4969.0;Western Europe;625.0;Q1;5800191435.0;Economic analysis and policy;Carbon price prediction considering climate change: a text-based framework;1024.0;561.0;95.0;203.0;4721.0;journal;article;2022
A large imbalance in soil nitrogen (N) and phosphorus (P) inputs induced by anthropogenic activities is anticipated to profoundly influence soil carbon (C) budgets in salt marshes. In this study, we hypothesized that imbalances in the nitrogen–phosphorus (N–P) input would result in the nonlinear response of soil organic carbon (SOC) content, fractions and mineralization to the N–P input ratio. We applied three N–P input ratios (low (5:1), medium (15:1), high (45:1)) in a salt marsh of the Yellow River Delta (YRD) for four years (in which N added increased from 8.67 to 26.01 g N m−2 y−1 and P added decreased from 1.73 to 0.58 g P m−2 y−1) and quantified their impacts on SOC fractions and SOC mineralisation. The control treatment did not receive fertilization. The results showed that the N and P input led to overall increases in the availability of soil nutrients (i.e., inorganic N (IN) and available P (AP)), stimulation of plant biomass and changes of microbial community structure (i.e., γ- and δ-Proteobacteria and Acidobacteria). N and P input increased soil dissolved organic carbon (DOC) and decreased aromatic DOC components through improving N availability and stimulating plant growth. Notably, though, there may be a threshold N–P input ratio between 15:1 and 45:1 that, once crossed, triggers the loss of SOC. Appropriate increase in N availability induced by low and medium N-P input ratios would stimulate the SOC mineralization. However, excessive N-P input ratio would reduce SOC mineralization. Path analysis indicated that N–P input ratios dominantly regulate SOC mineralisation by changing soil DOC and microbial biomass (MBC)contents and microbial community structure. Thus, we speculate that the continuous increase in N input causes a growing N–P imbalance that reduces SOC stocks, despite a reduction in SOC mineralisation.;Juanyong Li and Guangxuan Han and Guangmei Wang and Xiaoling Liu and Qiqi Zhang and Yawen Chen and Weimin Song and Wendi Qu and Xiaojing Chu and Peiguang Li;Earth-Surface Processes (Q1);1270.0;525.0;Netherlands;1973, 1975-2021;10.1016/j.catena.2021.105720;0.019530000000000002;128.0;;03418162;03418162;03418162;03418162;5.198;Imbalanced N and P input, SOC cycling, Microbial community structure, Salt marsh;Elsevier;;6758.0;Western Europe;1440.0;Q1;25155.0;Catena;Imbalanced nitrogen–phosphorus input alters soil organic carbon storage and mineralisation in a salt marsh;21519.0;7175.0;516.0;1299.0;34869.0;journal;article;2022
The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.;McDaniel, Melinda and Storey, Veda C.;"Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)";365.0;1916.0;United States;1969-2020;10.1145/3329124;0.01438;163.0;;03600300;03600300;15577341;03600300;10.282;domain ontology, evaluation, assessment, metrics, Ontology, ontology development, ontology application, applied ontology, task-ontology fit;Association for Computing Machinery (ACM);Association for Computing Machinery;14160.0;Northern America;2079.0;Q1;23038.0;Acm computing surveys;Evaluating domain ontologies: clarification, classification, and challenges;10790.0;6978.0;112.0;365.0;15859.0;journal;article;2019
A critical review of the existing Internal Combustion Engine (ICE) modeling, optimization, diagnosis, and control challenges and the promising state-of-the-art Machine Learning (ML) solutions for them is provided in this paper. Some of the major challenges include Real Driving Emission (RDE) modeling and control, combustion knock detection and control, combustion mode transition in multi-mode engines, combustion noise modeling and control, combustion instability and cyclic variability control, costly and time-consuming engine calibration, and fault diagnostics of some ICE components. In this paper, conventional ICE modeling approaches are discussed along with their limitations for realtime ICE optimization and control. Promising ML approaches to address ICE challenges are then classified into three main groups of unsupervised learning, supervised learning, and reinforcement learning. The working principles of each approach along with their advantages and disadvantages in addressing ICE challenges are discussed. ML-based grey-box approach is proposed as a solution that combines the benefits from physics-based and ML-based models to provide robust and high fidelity solutions for ICE modeling and control challenges. This review provides in-depth insight into the applications of ML for ICEs and provides recommendations for future directions to address ICE challenges.;Masoud Aliramezani and Charles Robert Koch and Mahdi Shahbakhti;"Chemical Engineering (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1)";97.0;3093.0;United Kingdom;1975-2021;10.1016/j.pecs.2021.100967;0.00952;183.0;;03601285;03601285;03601285;03601285;29.394;Internal combustion engines, Combustion control, Optimization, Artificial intelligence, Machine learning, Emissions, Energy;Elsevier BV;;27152.0;Western Europe;8089.0;Q1;27538.0;Progress in energy and combustion science;Modeling, diagnostics, optimization, and control of internal combustion engines via modern machine learning techniques: a review and future directions;14154.0;3523.0;31.0;98.0;8417.0;journal;article;2022
It is indisputable that young children are exposed to digital media since birth and start using them very early. This fuels debate that engages scholars and researchers, industry and brands, policymakers, and parents. Our study aimed to contrast these different perspectives, adding the view of children, who are frequently left out of this debate. Using an exploratory qualitative approach, we conducted interviews with children under 8 years old and their parents in 81 families, and with 17 expert stakeholders in different fields. We focused on their perceptions and practices regarding digital media, and specifically on how they assess and select apps, concluding that parents value safety and learning, children enjoy entertainment, and stakeholders highlight the importance of a good user experience.;Patrícia Dias and Rita Brito;"Computer Science (miscellaneous) (Q1); Education (Q1); E-learning (Q1)";557.0;1088.0;United Kingdom;1976-2020;10.1016/j.compedu.2021.104134;;179.0;;03601315;03601315;03601315;03601315;;Young children, Mobile media, Apps, Parents, Stakeholders;Elsevier Ltd.;;7224.0;Western Europe;3026.0;Q1;17645.0;Computers and education;Criteria for selecting apps: debating the perceptions of young children, parents and industry stakeholders;;6272.0;230.0;560.0;16615.0;journal;article;2021
There is a growing recognition of the importance of proper urban design in the improvement of air flow and pollution dispersion and in reducing human exposure to air pollution. However, a limited number of studies have been published so far focusing on the development of standard procedures which could be applied by urban planners to effectively evaluate urban conditions with respect to air quality. To fill this gap, a new approach for the determination of urban Air Quality Management Zones (AQMZs) was proposed and presented based on two case studies: Antwerp, Belgium and Gdańsk, Poland. The main objectives of the study were to 1) formulate a theoretical framework for the management of urban ventilation potential and human exposure to air pollution and to 2) develop methods for its implementation by means of a geographic information system (GIS). As a result of the analysis, the typologies that may be associated with decreased ventilation potential and the areas that require close monitoring due to potential human exposure to air pollution were identified for both cities. It is advocated that delimiting these typologies – combined with investigating local climate, wind and topography conditions and air pollution characteristics – could constitute a preliminary step in the urban planning process aimed at air quality improvement. These methods can be further applied to other urban areas in order to indicate where detailed studies are required and to facilitate the development of planning guidelines. Moreover, the directions for further research and urban planning strategies were discussed.;Joanna Badach and Dimitri Voordeckers and Lucyna Nyka and Maarten {Van Acker};"Building and Construction (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1)";1687.0;690.0;United Kingdom;1976-2020;10.1016/j.buildenv.2020.106743;0.02925;154.0;;03601323;03601323;03601323;03601323;6.456;Air quality management, Urban ventilation, Urban planning, Urban morphology, GIS-Based analysis;Elsevier BV;;5827.0;Western Europe;1736.0;Q1;26874.0;Building and environment;A framework for air quality management zones - useful gis-based tool for urban planning: case studies in antwerp and gdańsk;38699.0;12000.0;754.0;1692.0;43938.0;journal;article;2020
;Clifton D. Fuller and Lisanne V. {van Dijk} and Reid F. Thompson and Jacob G. Scott and Ethan B. Ludmir and Charles R. Thomas;"Cancer Research (Q1); Oncology (Q1); Radiation (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";1167.0;348.0;United States;1975-2020;10.1016/j.ijrobp.2020.06.066;0.03941;248.0;;03603016;03603016;1879355X;03603016;7.038;;Elsevier Inc.;;2412.0;Northern America;2117.0;Q1;17191.0;International journal of radiation oncology biology physics;Meeting the challenge of scientific dissemination in the era of covid-19: toward a modular approach to knowledge-sharing for radiation oncology;50525.0;6588.0;584.0;1807.0;14085.0;journal;article;2020
Saturation pressure is a vital parameter of oil reservoir which can reflect the oilfield characteristics and determine the oilfield development process, and it is determined by experiments in the laboratory in general. However, there was only one well with saturation pressure test in this target reservoir, and it is necessary to determine whether this parameter is right or not. In this work, we present a new method for quickly determining saturation pressure using machine learning algorithms, including random forest regressor (RF), support vector machine (SVM), decision trees (DT), and artificial neural network (ANN or NN). Using these approaches, saturation pressure was obtained by using the initial solution gas-oil ratio (GOR), temperature, API gravity and other reservoir-fluid data available in the oilfields. Compared with the empirical formula for saturation pressure calculation, the calculated result shows that the accuracy given from machine learning is higher than that from other formulas at home and abroad, and has a good match with the lab test. On the basis of the calculated saturation pressure, it can determine whether the reservoir enters into the stage of dissolved gas drive or not, which also provides the basis for maintaining the reservoir pressure by water injection in advance, rational development decision-making and work over measures. This approach above can provide technical guidance for predicting the saturation pressure in the development of different kinds of reservoirs, including the sandstone reservoirs and carbonate reservoirs.;Guoyi Yu and Feng Xu and Yingzhi Cui and Xiangling Li and Chujuan Kang and Cheng Lu and Siyu Li and Lin Bai and Shuheng Du;"Condensed Matter Physics (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Renewable Energy, Sustainability and the Environment (Q1)";8065.0;575.0;United Kingdom;1976-2020;10.1016/j.ijhydene.2020.08.042;0.08541;215.0;;03603199;03603199;03603199;03603199;5.816;Oil reservoir, Saturation pressure, Random forest, Decision tree, ANN, Empirical formula;Elsevier Ltd.;;5409.0;Western Europe;1212.0;Q1;26991.0;International journal of hydrogen energy;A new method of predicting the saturation pressure of oil reservoir and its application;120051.0;45807.0;3124.0;8134.0;168963.0;journal;article;2020
Accelerating urbanization has created tremendous pressure on the global environment and energy supply, making accurate estimates of energy use of great importance. Most current models for estimating electric power consumption (EPC) from nighttime light (NTL) imagery are oversimplified, ignoring influential social and economic factors. Here we propose first classifying cities by economic focus and then separately estimating each category’s EPC using NTL data. We tested this approach using statistical employment data for 198 Chinese cities, 2015 NTL data from the Visible Infrared Imaging Radiometer Suite (VIIRS), and annual electricity consumption statistics. We used cluster analysis of employment by sector to divide the cities into three types (industrial, service, and technology and education), then established a linear regression model for each city’s NTL and EPC. Compared with the estimation results before city classification (R2: 0.785), the R2 of the separately modeled service cities and technology and education cities increased to 0.866 and 0.830, respectively. However, the results for industrial cities were less consistent due to their more complex energy consumption structure. In general, using classification before modeling helps reflect factors affecting the relationship between EPC and NTL, making the estimation process more reasonable and improving the accuracy of the results.;Shuyi Li and Liang Cheng and Xiaoqiang Liu and Junya Mao and Jie Wu and Manchun Li;"Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Energy (miscellaneous) (Q1); Fuel Technology (Q1); Industrial and Manufacturing Engineering (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1); Modeling and Simulation (Q1); Pollution (Q1); Renewable Energy, Sustainability and the Environment (Q1)";6476.0;755.0;United Kingdom;1976-2020;10.1016/j.energy.2019.116040;0.11575;193.0;;03605442;18736785;03605442;18736785;7.147;NPP-VIIRS, Electric power consumption, City type, Cluster analysis, Regression model;Elsevier Ltd.;;4734.0;Western Europe;1961.0;Q1;29348.0;Energy;City type-oriented modeling electric power consumption in china using npp-viirs nighttime stable light data;103367.0;49084.0;2315.0;6498.0;109602.0;journal;article;2019
Most of traditional industries in emerging countries may not be ready to migrate for Industry 4.0 directly. There is a need of effective solutions to support digital transformation of traditional industries. Textile industry is facing global competition for mass customization to address dynamic customer demands. To enable the challenge from mass production to build-on-demand with small lot size and diversified product mixes, this study aims to develop a solution to support traditional industries to adopt smart manufacturing and empower digital transformation. Following a framework as systematic approach to collect, identify, and analyze related steps and decisions for an organization, a decision support system for dyeing machine scheduling is developed to empower smart manufacturing and break down information silos. In particular, setup time for textile dyeing operations is sequence-dependent, and products of different types and colors require setups for tank cleaning. The results have shown the practical viability of the proposed approach and Industry 3.5. Indeed, the developed solution has been implemented in a textile company in Taiwan.;Chien-Chun Ku and Chen-Fu Chien and Kang-Ting Ma;"Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)";1557.0;597.0;United Kingdom;1976-2020;10.1016/j.cie.2020.106297;;128.0;;03608352;03608352;03608352;03608352;;Smart production, Digital transformation, Textile dyeing machine scheduling, Industry 3.5, Decision support system, Traditional industry;Elsevier Ltd.;;4966.0;Western Europe;1315.0;Q1;18164.0;Computers and industrial engineering;Digital transformation to empower smart production for industry 3.5 and an empirical study for textile dyeing;;9597.0;641.0;1567.0;31833.0;journal;article;2020
This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.;Turgut Ozkan;"Sociology and Political Science (Q2); Social Psychology (Q3)";194.0;161.0;United States;1978, 1980, 1982-2020;10.1016/j.soscij.2018.10.010;0.0016699999999999998;39.0;;03623319;03623319;03623319;03623319;2.376;Social science, Big data, Crime, Social media, Data-driven social science;Elsevier Inc.;;5978.0;Northern America;349.0;Q2;26437.0;Social science journal;Criminology in the age of data explosion: new directions;1817.0;340.0;138.0;198.0;8250.0;journal;article;2019
We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.;Wei, Ziheng and Link, Sebastian;Information Systems (Q1);53.0;312.0;United States;1976-2020;10.1145/3450518;0.00131;84.0;;03625915;15574644;03625915;15574644;1.086;functional dependency, normal form, third normal form, database design, redundancy, key, updates, decomposition, missing value, synthesis, Boyce-Codd normal form;Association for Computing Machinery (ACM);Association for Computing Machinery;5285.0;Northern America;988.0;Q1;13044.0;Acm transactions on database systems;Embedded functional dependencies and data-completeness tailored database design;1306.0;428.0;20.0;55.0;1057.0;journal;article;2021
The coupling of computational thermodynamics and kinetics has been the central research theme in Integrated Computational Material Engineering (ICME). Two major bottlenecks in implementing this coupling and performing efficient ICME-guided high-throughput multi-component industrial alloys discovery or process parameters optimization, are slow responses in kinetic calculations to a given set of compositions and processing conditions and the quality of a large amount of calculated thermodynamic data. Here, we employ machine learning techniques to eliminate them, including (1) intelligent corrupt data detection and re-interpolation (i.e. data purge/cleaning) to a big tabulated thermodynamic dataset based on an unsupervised learning algorithm and (2) parameterization via artificial neural networks of the purged big thermodynamic dataset into a non-linear equation consisting of base functions and parameterization coefficients. The two techniques enable the efficient linkage of high-quality data with a previously developed microstructure model. This proposed approach not only improves the model performance by eliminating the interference of the corrupt data and stability due to the boundedness and continuity of the obtained non-linear equation but also dramatically reduces the running time and demand for computer physical memory simultaneously. The high computational robustness, efficiency, and accuracy, which are prerequisites for high-throughput computing, are verified by a series of case studies on multi-component aluminum, steel, and high-entropy alloys. The proposed data purge and parameterization methods are expected to apply to various microstructure simulation approaches or to bridging the multi-scale simulation where handling a large amount of input data is required. It is concluded that machine learning is a valuable tool in fueling the development of ICME and high throughput materials simulations.;Yue Li and Bjørn Holmedal and Boyu Liu and Hongxiang Li and Linzhong Zhuang and Jishan Zhang and Qiang Du and Jianxin Xie;"Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1); Computer Science Applications (Q2)";316.0;209.0;United Kingdom;1977-2020;10.1016/j.calphad.2020.102231;;62.0;;03645916;03645916;03645916;03645916;;Materials informatics, Machine learning, High-throughput computing, Microstructure simulation, Tabulation;Elsevier Ltd.;;5057.0;Western Europe;757.0;Q1;24593.0;Calphad: computer coupling of phase diagrams and thermochemistry;Towards high-throughput microstructure simulation in compositionally complex alloys via machine learning;;734.0;120.0;317.0;6068.0;journal;article;2021
Biological entities are involved in intricate and complex interactions, in which uncovering the biological information from the network concepts are of great significance. Benefiting from the advances of network science and high-throughput biomedical technologies, studying the biological systems from network biology has attracted much attention in recent years, and networks have long been central to our understanding of biological systems, in the form of linkage maps among genotypes, phenotypes, and the corresponding environmental factors. In this review, we summarize the recent developments of computational network biology, first introducing various types of biological networks and network structural properties. We then review the network-based approaches, ranging from some network metrics to the complicated machine-learning methods, and emphasize how to use these algorithms to gain new biological insights. Furthermore, we highlight the application in neuroscience, human disease, and drug developments from the perspectives of network science, and we discuss some major challenges and future directions. We hope that this review will draw increasing interdisciplinary attention from physicists, computer scientists, and biologists.;Chuang Liu and Yifang Ma and Jing Zhao and Ruth Nussinov and Yi-Cheng Zhang and Feixiong Cheng and Zi-Ke Zhang;Physics and Astronomy (miscellaneous) (Q1);147.0;3602.0;Netherlands;1971-2020;10.1016/j.physrep.2019.12.004;;297.0;;03701573;03701573;03701573;03701573;;Complex networks, Network biology, Disease module, Machine learning;Elsevier;;40140.0;Western Europe;6914.0;Q1;29229.0;Physics reports;Computational network biology: data, models, and applications;;5960.0;53.0;147.0;21274.0;journal;article;2020
There is a critical need to establish a global geochemical observation network to provide data for monitoring the chemical changes of the Earths near-surface environment. The International Centre on Global-scale Geochemistry, under auspices of UNESCO and Government of China, has initiated an International Scientific Cooperation Project called Mapping Chemical Earth. The project focuses on the establishment of Global Geochemical Observatory Network for documenting baselines and changes of nearly all natural chemical elements in the Earths surface and creating a digital Chemical Earth platform allowing anyone to access vast amounts of geochemical data through the Internet. A total area of about 37 million km2, nearly accounting for 27% of the global land, has been covered by global-/continental-scale sampling. Comparing the data of China, the US, Europe and Australia, the percentage of sites with toxic metals exceeding the risk limits of soil pollution according to “Environmental Quality Standard for Soil of China (GB 15618-1995)” to the total sample sites is 30.9%, 17.1%, 23.5% and 10.9% in Europe, China, USA, and Australia respectively. Comparing the China datasets of 15 years interval sampling between 1994, 1995 and in 2008–2012, toxic metals of As, Cd, Cr, Cu, Hg, Ni, Pb and Zn, particularly Cd at top soils significantly increase from 1990s to 2010s. The proportion of top soil samples exceeding the China Standard risk limit of 0.2 mg/kg Cd increases from 12.2% to 24.9%. The facts show that chemical changes of toxic metals induced by human activities can be well observed using catchment sediment sampling.;Xueqiu Wang and Bimin Zhang and Lanshi Nie and Wei Wang and Jian Zhou and Shanfa Xu and Qinhua Chi and Dongsheng Liu and Hanliang Liu and Zhixuan Han and Qingqing Liu and Mi Tian and Baoyun Zhang and Hui Wu and Ruihong Li and Qinghai Hu and Taotao Yan and Yanfang Gao;"Economic Geology (Q1); Geochemistry and Petrology (Q2)";586.0;366.0;Netherlands;1972-2020;10.1016/j.gexplo.2020.106578;0.00675;87.0;;03756742;03756742;03756742;03756742;3.746;Mapping Chemical Earth, Global Geochemical Observatory Networks, Geochemical baselines, Progress and challenge;Elsevier;;7016.0;Western Europe;994.0;Q1;23222.0;Journal of geochemical exploration;Mapping chemical earth program: progress and challenge;9705.0;2522.0;146.0;596.0;10243.0;journal;article;2020
Quantification and forecasting of cost uncertainty for aerospace innovations is challenged by conditions of small data which arises out of having few measurement points, little prior experience, unknown history, low data quality, and conditions of deep uncertainty. Literature research suggests that no frameworks exist which specifically address cost estimation under such conditions. In order to provide contemporary cost estimating techniques with an innovative perspective for addressing such challenges a framework based on the principles of spatial geometry is described. The framework consists of a method for visualising cost uncertainty and a dependency model for quantifying and forecasting cost uncertainty. Cost uncertainty is declared to represent manifested and unintended future cost variance with a probability of 100% and an unknown quantity and innovative starting conditions considered to exist when no verified and accurate cost model is available. The shape of data is used as an organising principle and the attribute of geometrical symmetry of cost variance point clouds used for the quantification of cost uncertainty. The results of the investigation suggest that the uncertainty of a cost estimate at any future point in time may be determined by the geometric symmetry of the cost variance data in its point cloud form at the time of estimation. Recommendations for future research include using the framework to determine the “most likely values” of estimates in Monte Carlo simulations and generalising the dependency model introduced. Future work is also recommended to reduce the framework limitations noted.;Oliver Schwabe and Essam Shehab and John Erkoyuncu;"Aerospace Engineering (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1)";109.0;1213.0;United Kingdom;1961-1962, 1964-1968, 1970, 1972-1979, 1981, 1983-1992, 1994-2020;10.1016/j.paerosci.2016.05.001;0.00471;113.0;;03760421;03760421;03760421;03760421;8.653;Cost estimate, Geometry, Symmetry, Topology, Uncertainty;Elsevier Ltd.;;17071.0;Western Europe;2328.0;Q1;12274.0;Progress in aerospace sciences;A framework for geometric quantification and forecasting of cost uncertainty for aerospace innovations;4816.0;1463.0;35.0;109.0;5975.0;journal;article;2016
Machine learning overfitting caused by data scarcity greatly limits the application of chemical artificial intelligence in membrane materials. As the original data for thin film polyamide nanofiltration membranes is limited, here we propose to extract the natural features of monomer molecular structures and rationally distort them to augment the data availability. This few-shot learning method allows a chemical engineering project to leverage the powerful fit of deep learning without big data at the outset, which is advantageous over traditional machine learning models. The rejection and flux predictions of polyamide nanofiltration membranes are practiced by the molecular augmentation in deep learning. Convergence of loss function indicates that the model is effectively optimized. Correlation coefficients over 0.80 and the mean relative error below 5% prove an accurate prediction of nanofiltration performance. The success of predicting nanofiltration membrane performances is widely instructive for molecule and material science.;Ziyang Zhang and Yingtao Luo and Huawen Peng and Yu Chen and Rong-Zhen Liao and Qiang Zhao;"Biochemistry (Q1); Filtration and Separation (Q1); Materials Science (miscellaneous) (Q1); Physical and Theoretical Chemistry (Q1)";2621.0;854.0;Netherlands;1976-2021;10.1016/j.memsci.2020.118910;0.048510000000000005;249.0;;03767388;03767388;03767388;03767388;8.742;Nanofiltration, Thin film composite membranes, Feature engineering, Machine learning, Data augmentation, Molecular vibration;Elsevier;;5452.0;Western Europe;1929.0;Q1;26953.0;Journal of membrane science;Deep spatial representation learning of polyamide nanofiltration membranes;87097.0;22599.0;984.0;2622.0;53650.0;journal;article;2021
Banking is a popular topic for empirical and methodological research that applies operational research (OR) and artificial intelligence (AI) methods. This article provides a comprehensive and structured bibliographic survey of OR- and AI-based research devoted to the banking industry over the last decade. The article reviews the main topics of this research, including bank efficiency, risk assessment, bank performance, mergers and acquisitions, banking regulation, customer-related studies, and fintech in the banking industry. The survey results provide comprehensive insights into the contributions of OR and AI methods to banking. Finally, we propose several research directions for future studies that include emerging topics and methods based on the survey results.;Michalis Doumpos and Constantin Zopounidis and Dimitrios Gounopoulos and Emmanouil Platanakis and Wenke Zhang;"Computer Science (miscellaneous) (Q1); Information Systems and Management (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1)";2068.0;602.0;Netherlands;1977-2021;10.1016/j.ejor.2022.04.027;0.047639999999999995;260.0;;03772217;03772217;03772217;03772217;5.334;Artificial Intelligence, Operational research, Banking;Elsevier;;4874.0;Western Europe;2161.0;Q1;22489.0;European journal of operational research;Operational research and artificial intelligence methods in banking;64756.0;13341.0;657.0;2080.0;32022.0;journal;article;2022
"Vegetation phenology is the study of recurring plant life cycle stages, seasonality which is linked to many ecosystem processes and is an important proxy of climate and environmental change. Remote sensing has been playing an important and increasing role in the monitoring and assessment of vegetation phenology. The aim of this review is to critically examine key studies related to remote sensing of vegetation phenology, with a special focus on temperate and boreal forests. Specifically, we focus on how the latest ground, near-surface and aerial data have been used to assess the satellite-derived Land Surface Phenology (LSP) metrics and the agreements that has been achieved in the last 15 years. Results demonstrated that the timing of satellite-derived LSP events can be detected, in the best-case scenarios, with a certainty of around half-week for spring metrics (e.g. Day of Year -DOY- of start of growing season) and around one week for autumn metrics (e.g. DOY of end of growing season). With expected shifts in plant phenology averaging <1 day per decade, such LSP uncertainties (in terms of absolute phenological dates) could greatly over- or under-estimate these species-level shifts; but the spatial variation in phenology can be consistently monitored. An increasing number of studies have investigated autumn phenology in the last decade, but autumn phenological dates continue to be more challenging to retrieve and interpret than spring dates. Emerging opportunities to further advance remote sensing of forest phenology is presented that includes synergetic use of multiple orbital sensors and its LSP evaluation with data from new sensors at a ground, near-surface and airborne level; yet traditional ground-based observations will continue to be highly useful to accurately record the timing of species-specific phenological events. This review might provide a guide for planning and managing remote sensing of forest phenology.";Elias F. Berra and Rachel Gaulton;"Forestry (Q1); Management, Monitoring, Policy and Law (Q1); Nature and Landscape Conservation (Q1)";1944.0;341.0;Netherlands;1909, 1976, 1978-2021;10.1016/j.foreco.2020.118663;0.026080000000000002;176.0;;03781127;03781127;03781127;03781127;3.558;Satellite data, Land surface phenology, Phenometrics, Ground observations, SOS, EOS, Validation;Elsevier;;7310.0;Western Europe;1288.0;Q1;25720.0;Forest ecology and management;Remote sensing of temperate and boreal forest phenology: a review of progress, challenges and opportunities in the intercomparison of in-situ and satellite phenological metrics;42557.0;7143.0;686.0;1964.0;50149.0;journal;article;2021
This study reviews soil water balance (SWB) model approaches to determine crop irrigation requirements and scheduling irrigation adopting the FAO56 method. The Kc-ETo approach is discussed with consideration of baseline concepts namely standard vs. actual Kc concepts, as well as single and dual Kc approaches. Requirements for accurate SWB and appropriate parameterization and calibration are introduced. The one-step vs. the two-step computational approaches is discussed before the review of the FAO56 method to compute and partition crop evapotranspiration and related soil water balance. A brief review on transient state models is also included. Baseline information is concluded with a discussion on yields prediction and performance indicators related to water productivity. The study is continued with an overview on models development and use after publication of FAO24, essentially single Kc models, followed by a review on models following FAO56, particularly adopting the dual Kc approach. Features of dual Kc modeling approaches are analyzed through a few applications of the SWB model SIMDualKc, mainly for derivation of basal and single Kc, extending the basal Kc approach to relay intercrop cultivation, assessing alternative planting dates, determining beneficial and non-beneficial uses of water by an irrigated crop, and assessing the groundwater contribution to crop ET in the presence of a shallow water table. The review finally discusses the challenges placed to SWB modeling for real time irrigation scheduling, particularly the new modeling approaches for large scale multi-users application, use of cloud computing and adopting the internet of things (IoT), as well as an improved wireless association of modeling with soil and plant sensors. Further challenges refer to the use of remote sensing energy balance and vegetation indices to map Kc, ET and crop water and irrigation requirements. Trends are expected to change research issues relative to SWB modeling, with traditional models mainly used for research while new, fast-responding and multi-users models based on cloud and IoT technologies will develop into applications to the farm practice. Likely, the Kc-ETo will continue to be used, with ETo from gridded networks, re-analysis and other sources, and Kc data available in real time from large databases and remote sensing.;L.S. Pereira and P. Paredes and N. Jovanovic;"Agronomy and Crop Science (Q1); Earth-Surface Processes (Q1); Soil Science (Q1); Water Science and Technology (Q1)";1201.0;472.0;Netherlands;1976-1977, 1979-2021;10.1016/j.agwat.2020.106357;0.01577;128.0;;03783774;03783774;03783774;03783774;4.516;Crop coefficients, Crop evapotranspiration, Dual K approach, Real time irrigation management, Water use assessment, SIMDualKc model;Elsevier;;5822.0;Western Europe;1493.0;Q1;35824.0;Agricultural water management;Soil water balance models for determining crop water and irrigation requirements and irrigation scheduling focusing on the fao56 method and the dual kc approach;22091.0;6257.0;429.0;1207.0;24976.0;journal;article;2020
;;"Medicine (miscellaneous) (Q1); Toxicology (Q1)";821.0;393.0;Netherlands;1977-2020;10.1016/S0378-4274(18)31785-5;0.01073;145.0;;03784274;03784274;18793169;03784274;4.372;;Elsevier BV;;4670.0;Western Europe;1007.0;Q1;25240.0;Toxicology letters;Keyword index;18729.0;3425.0;303.0;840.0;14151.0;journal;article;2018
Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.;João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma;"Agronomy and Crop Science (Q1); Soil Science (Q1)";795.0;539.0;Netherlands;1978-2020;10.1016/j.fcr.2020.107828;0.016069999999999997;150.0;;03784290;18726852;03784290;18726852;5.224;Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands;Elsevier;;5586.0;Western Europe;1951.0;Q1;78796.0;Field crops research;Can big data explain yield variability and water productivity in intensive cropping systems?;24118.0;4752.0;191.0;798.0;10669.0;journal;article;2020
"In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.";Jingjing Li and Ling Tang and Shouyang Wang;"Condensed Matter Physics (Q2); Statistics and Probability (Q2)";4288.0;360.0;Netherlands;1975-2021;10.1016/j.physa.2020.124178;;166.0;;03784371;03784371;03784371;03784371;;Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence;Elsevier;;4277.0;Western Europe;640.0;Q2;29115.0;Physica a: statistical mechanics and its applications;Forecasting crude oil price with multilingual search engine data;;15067.0;1210.0;4300.0;51750.0;journal;article;2020
The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.;Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Obstetrics and Gynecology (Q1)";475.0;397.0;Ireland;1978-2021;10.1016/j.maturitas.2015.03.015;0.01085;105.0;;03785122;03785122;03785122;03785122;4.342;Connected health, Integrated care, Personal health system, Electronic health;Elsevier Ireland Ltd;;3862.0;Western Europe;1346.0;Q1;27660.0;Maturitas;Connected health and integrated care: toward new models for chronic disease management;9715.0;2319.0;146.0;527.0;5638.0;journal;article;2015
This work proposes the application of artificial neural networks (ANN) to non-destructively predict the in vitro dissolution of pharmaceutical tablets from Process Analytical Technology (PAT) data. An extended release tablet formulation was studied, where the dissolution was influenced by the composition of the tablets and the tableting compression force. NIR and Raman spectra of the intact tablets were measured, and the dissolution of the tablets was modeled directly from the spectral data. Partial Least Square (PLS) regression and ANN models were developed for the different spectroscopic measurements individually as well as by combining them together. ANN provided up to 3% lower root mean square error for prediction (RMSEP) than the PLS models, due to its capability of modeling non-linearity between the process parameters and dissolution curves. The ANN model using reflection NIR spectra provided the most accurate predictions with 6.5 and 63 mean f1 and f2 values between the computed and measured dissolution curves, respectively. Furthermore, ANN served as a straightforward data fusion method without the need for additional preprocessing steps. The method could significantly advance data processing in the PAT environment, contribute to an enhanced real-time release testing procedure and hence the increased efficacy of dissolution testing.;Brigitta Nagy and Dulichár Petra and Dorián László Galata and Balázs Démuth and Enikő Borbás and György Marosi and Zsombor Kristóf Nagy and Attila Farkas;Pharmaceutical Science (Q1);2532.0;572.0;Netherlands;1978-2020;10.1016/j.ijpharm.2019.118464;0.03126;217.0;;03785173;18733476;03785173;18733476;5.875;Dissolution prediction, Real-time release testing, Artificial neural network, Process Analytical Technology, Raman spectroscopy, NIR spectroscopy;Elsevier;;5544.0;Western Europe;1153.0;Q1;22454.0;International journal of pharmaceutics;Application of artificial neural networks for process analytical technology-based dissolution testing;61118.0;14707.0;1056.0;2544.0;58542.0;journal;article;2019
"Background
The heterogeneity of tinnitus is substantial. Its numerous pathophysiological mechanisms and clinical manifestations have hampered fundamental and treatment research significantly. A decade ago, the Tinnitus Research Initiative introduced the Tinnitus Sample Case History Questionnaire, a case history instrument for standardised collection of information about the characteristics of the tinnitus patient. Since then, a number of studies have been published which characterise individuals and groups using data collected with this questionnaire. However, its use has been restricted to a clinical setting and to the evaluation of people with tinnitus only. In addition, it is limited in the ability to capture relevant comorbidities and evaluate their temporal relationship with tinnitus.
Method
Here we present a new case history instrument which is comprehensive in scope and can be answered by people with and without tinnitus alike. This ‘European School for Interdisciplinary Tinnitus Research Screening Questionnaire’ (ESIT-SQ) was developed with specific attention to questions about potential risk factors for tinnitus (including demographics, lifestyle, general medical and otological histories), and tinnitus characteristics (including perceptual characteristics, modulating factors, and associations with co-existing conditions). It was first developed in English, then translated into Dutch, German, Italian, Polish, Spanish, and Swedish, thus having broad applicability and supporting international collaboration.
Conclusions
With respect to better understanding tinnitus profiles, we anticipate the ESIT-SQ to be a starting point for comprehensive multi-variate analyses of tinnitus. Data collected with the ESIT-SQ can allow establishment of patterns that distinguish tinnitus from non-tinnitus, and definition of common sets of tinnitus characteristics which might be indicated by the presence of otological or comorbid systemic diseases for which tinnitus is a known symptom.";Eleni Genitsaridi and Marta Partyka and Silvano Gallus and Jose A. Lopez-Escamez and Martin Schecklmann and Marzena Mielczarek and Natalia Trpchevska and Jose L. Santacruz and Stefan Schoisswohl and Constanze Riha and Matheus Lourenco and Roshni Biswas and Nuwan Liyanage and Christopher R. Cederroth and Patricia Perez-Carpena and Jana Devos and Thomas Fuller and Niklas K. Edvall and Matilda Prada Hellberg and Alessia D'Antonio and Stefania Gerevini and Magdalena Sereda and Andreas Rein and Theodore Kypraios and Derek J. Hoare and Alain Londero and Rüdiger Pryss and Winfried Schlee and Deborah A. Hall;Sensory Systems (Q2);559.0;290.0;Netherlands;1978-2020;10.1016/j.heares.2019.02.017;0.010190000000000001;116.0;;03785955;18785891;03785955;18785891;3.208;Heterogeneity, Classification, Data collection, Self report, Translations;Elsevier;;7287.0;Western Europe;1389.0;Q2;13707.0;Hearing research;Standardised profiling for tinnitus research: the european school for interdisciplinary tinnitus research screening questionnaire (esit-sq);11475.0;1914.0;142.0;575.0;10348.0;journal;article;2019
"ABSTRACT
This study investigates customer satisfaction through aspect-level sentiment analysis and visual analytics. We collected and examined the flight reviews on TripAdvisor from January 2016 to August 2020 to gauge the impact of COVID-19 on passenger travel sentiment in several aspects. Till now, information systems, management, and tourism research have paid little attention to the use of deep learning and word embedding techniques, such as bidirectional encoder representations from transformers, especially for aspect-level sentiment analysis. This paper aims to identify perceived aspect-based sentiments and predict unrated sentiments for various categories to address this research gap. Ultimately, this study complements existing sentiment analysis methods and extends the use of data-driven and visual analytics approaches to better understand customer satisfaction in the airline industry and within the context of the COVID-19. Our proposed method outperforms baseline comparisons and therefore contributes to the theoretical and managerial literature.";Yung-Chun Chang and Chih-Hao Ku and Duy-Duc Le Nguyen;"Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)";246.0;894.0;Netherlands;1977-2020;10.1016/j.im.2021.103587;;162.0;;03787206;03787206;03787206;03787206;;Aspect-based Sentiment Analysis, Social Media Analysis, Natural Language Processing, Deep Learning, Information Visualization, Bidirectional Encoder Representations from Transformers;Elsevier;;8758.0;Western Europe;2147.0;Q1;12303.0;Information and management;Predicting aspect-based sentiment using deep learning and information visualization: the impact of covid-19 on the airline industry;;2482.0;130.0;248.0;11385.0;journal;article;2022
Accurately predicting batteries’ ageing trajectory and remaining useful life is not only required to ensure safe and reliable operation of electric vehicles (EVs) but is also the fundamental step towards health-conscious use and residual value assessment of the battery. The non-linearity, wide range of operating conditions, and cell to cell variations make battery health prediction challenging. This paper proposes a prediction framework that is based on a combination of global models offline developed by different machine learning methods and cell individualised models that are online adapted. For any format of raw data collected under diverse operating conditions, statistic properties of histograms can be still extracted and used as features to learn battery ageing. Our framework is trained and tested on three large datasets, one being retrieved from 7296 plug-in hybrid EVs. While the best global models achieve 0.93% mean absolute percentage error (MAPE) on laboratory data and 1.41% MAPE on the real-world fleet data, the adaptation algorithm further reduced the errors by up to 13.7%, all requiring low computational power and memory. Overall, this work proves the feasibility and benefits of using histogram data and also highlights how online adaptation can be used to improve predictions.;Yizhou Zhang and Torsten Wik and John Bergström and Michael Pecht and Changfu Zou;"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Physical and Theoretical Chemistry (Q1); Renewable Energy, Sustainability and the Environment (Q1)";3757.0;887.0;Netherlands;1976-2021;10.1016/j.jpowsour.2022.231110;0.1054;302.0;;03787753;03787753;03787753;03787753;9.127;Lithium-ion batteries, State of health prediction, Remaining useful life, Machine learning, Online adaptive learning, Real-world fleet data;Elsevier;;5207.0;Western Europe;2139.0;Q1;18063.0;Journal of power sources;A machine learning-based framework for online prediction of battery ageing trajectory and lifetime using histogram data;140819.0;33068.0;1356.0;3763.0;70605.0;journal;article;2022
As our need for energy information of buildings evolves, and the tools and methods at our disposal increase in scale and complexity, it is perhaps reasonable to expect a similar level of change in the way energy in buildings is assessed within national energy compliance frameworks. By comparing the available opportunities for building energy modelling with the current methodologies underlying Energy Performance Certificates, this study proposes future directions for standardised energy assessment of residential buildings and the impact this could have on different facets of energy policy. In carrying out this exercise, a number of criteria are proposed that could be used to appraise methodologies that align with future requirements of energy assessment, with two potential candidates for future energy assessment considered as part of this appraisal. An argument is thus proposed for better aligning future forms of standardised energy assessment with directions and requirements of future low-carbon energy policy.;D.P. Jenkins and  S.Semple and S. Patidar and P. McCallum;"Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)";2394.0;633.0;Netherlands;1970, 1977-2020;10.1016/j.enbuild.2021.111239;0.04387;184.0;;03787788;03787788;03787788;03787788;5.879;EPC, Dynamic simulation, Urban energy modelling;Elsevier BV;;5580.0;Western Europe;1737.0;Q1;29359.0;Energy and buildings;Changing the approach to energy compliance in residential buildings – re-imagining epcs;51254.0;15832.0;705.0;2402.0;39338.0;journal;article;2021
This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.;Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa;"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1)";1158.0;425.0;Netherlands;1977-2021;10.1016/j.epsr.2020.106788;0.01287;122.0;;03787796;03787796;03787796;03787796;3.414;Electricity grids, Analytics, Big data, Decision-making;Elsevier BV;;3258.0;Western Europe;845.0;Q1;16044.0;Electric power systems research;Big data analytics for future electricity grids;13115.0;5369.0;643.0;1165.0;20949.0;journal;article;2020
This study tested two recall aids for the name generator procedure via a randomized web experiment with 447 college students, eliciting their personal networks. Compared to participants solely presented with the name generator, participants being prompted and probed to consult records saved in their communication devices provided more comprehensive network data and more weak ties. Furthermore, these data were garnered without either a substantial increase in item nonresponse or a decrease in completion time for subsequent name interpreters. Thus, ICT recall aids are deemed cost-effective and context-neutral techniques to improve the recall accuracy of data collected by the name generator.;Yuli Patrick Hsieh;"Anthropology (Q1); Psychology (miscellaneous) (Q1); Social Sciences (miscellaneous) (Q1); Sociology and Political Science (Q1)";193.0;372.0;Netherlands;1978-2021;10.1016/j.socnet.2014.11.006;0.00562;98.0;;03788733;03788733;03788733;03788733;3.406;Name generator, Egocentric networks, Memory aids, Survey design, ICT use;Elsevier BV;;6561.0;Western Europe;1599.0;Q1;25987.0;Social networks;Check the phone book: testing information and communication technology (ict) recall aids for personal network surveys;7604.0;825.0;69.0;195.0;4527.0;journal;article;2015
We review the current and potential uses of Geographic Information Software (GIS) and “spatial thinking” for understanding body disposal behaviour in times of mass fatalities, particularly armed conflict contexts. The review includes observations made by the authors during the course of their academic research and professional consulting on the use of spatial analysis and GIS to support Humanitarian Forensic Action (HFA) to search for the dead, theoretical and statistical considerations in modelling grave site locations, and suggestions on how this work may be advanced further.;Derek Congram and Michael Kenyhercz and Arthur Gill Green;Pathology and Forensic Medicine (Q1);1319.0;244.0;Ireland;1978-2020;10.1016/j.forsciint.2017.07.021;0.01174;120.0;;03790738;03790738;18726283;03790738;2.395;Forensic anthropology, Forensic archaeology, Spatial analysis, Geographic Information Science, Forensic Humanitarian Action;Elsevier Ireland Ltd;;3744.0;Western Europe;912.0;Q1;27743.0;Forensic science international;Grave mapping in support of the search for missing persons in conflict contexts;16719.0;3468.0;381.0;1361.0;14263.0;journal;article;2017
Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.;Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime};"Chemistry (miscellaneous) (Q1); Materials Science (miscellaneous) (Q1); Physics and Astronomy (miscellaneous) (Q1); Safety, Risk, Reliability and Quality (Q1)";437.0;313.0;United Kingdom;1977-2020;10.1016/j.firesaf.2019.01.006;0.00398;78.0;;03797112;03797112;03797112;03797112;2.764;Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence;Elsevier Ltd.;;3225.0;Western Europe;958.0;Q1;28425.0;Fire safety journal;Predictive modeling of wildfires: a new dataset and machine learning approach;5861.0;1566.0;260.0;442.0;8386.0;journal;article;2019
The Great Lakes are a vital resource for drinking water and recreation and provide a major fishery for millions of people. As part of the Great Lakes Water Quality Agreement, the US and Canadian governments have been charged with the protection of this system. Persistent, bioaccumulative, and toxic (PBTs) contaminants were found to be affecting the lake water quality as early as the late 1960s, and various programs sponsored by the US and Canada have been created to monitor PBTs such as polychlorinated biphenyls (PCBs) and organochlorine pesticides (OCPs). These programs have refined measurement techniques to quantify trace level contaminants using a targeted analytical approach. However, new PBTs are being detected in the environment, and the traditional targeted methodology is inadequate for understanding the complex chemical mixture affecting Great Lakes wildlife. Fortunately, new analytical technologies are emerging that allow for comprehensive screening of PBTs beyond targeted methods. The current commentary presents an outline of a new framework for contemporary monitoring programs. The goal is to facilitate the compilation of legacy, emerging PBT, and archive PBT signatures by utilizing the basic practices of traditional targeted analysis. This example focuses on fish monitoring programs, and how they are ideally suited for legacy monitoring as well as data-driven discovery of new chemicals of concern.;Bernard S. Crimmins and Harry B. McCarty and Sujan Fernando and Michael S. Milligan and James J. Pagano and Thomas M. Holsen and Philip K. Hopke;"Aquatic Science (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)";375.0;255.0;United States;1970, 1975-2020;10.1016/j.jglr.2018.07.011;0.004079999999999999;78.0;;03801330;03801330;03801330;03801330;2.480;Great Lakes, Monitoring, Fish, Non-targeted, Persistent bioacccumulative toxics (PBTs), Emerging chemicals of concern;International Association of Great Lakes Research;;6581.0;Northern America;720.0;Q2;17510.0;Journal of great lakes research;Commentary: integrating non-targeted and targeted chemical screening in great lakes fish monitoring programs;5567.0;924.0;204.0;390.0;13425.0;journal;article;2018
In the past decades various categories of chemometrics for laser-induced breakdown spectroscopy (LIBS) analysis have been developed, among which an important category is that based on artificial neural network (ANN). The most common ANN scheme employed in LIBS researches so far is back-propagation neural network (BPNN), while there are also several other kinds of neural networks appreciated by the LIBS community, including radial basis function neural network (RBFNN), convolutional neural network (CNN), self-organizing map (SOM), etc. In this paper, we introduce the principles of some representative ANN methods, and offer criticism on their features along with comparison between them. Then we afford an overview of ANN-based chemometrics applied in LIBS analysis, involving material identification/classification, component concentration quantification, and some unconventional applications as well. Furthermore, a comprehensive discussion on ANN-LIBS methodologies is provided from four aspects. First, a few general progressing trends are displayed. Next we expound some specific implementation techniques, including variable selection, network construction, data set utilization, network training, model evaluation, and chemometrics selection. In addition, the limitations of ANN approaches are remarked, mainly concerning overfitting and interpretability. Finally a prospect of future development of ANN-LIBS chemometrics is presented. Throughout the discussion quite a few good practices have been highlighted. This review is expected to shed light on the further upgrade of ANN-based LIBS chemometrics in the future.;Lu-Ning Li and Xiang-Feng Liu and Fan Yang and Wei-Ming Xu and Jian-Yu Wang and Rong Shu;"Instrumentation (Q1); Analytical Chemistry (Q2); Atomic and Molecular Physics, and Optics (Q2); Spectroscopy (Q2)";498.0;363.0;Netherlands;1967-2020;10.1016/j.sab.2021.106183;;114.0;;05848547;05848547;05848547;05848547;;Laser-induced breakdown spectroscopy, Artificial neural network, Machine learning, Chemometrics, Classification;Elsevier;;3965.0;Western Europe;793.0;Q1;24532.0;Spectrochimica acta, part b: atomic spectroscopy;A review of artificial neural network based chemometrics applied in laser-induced breakdown spectroscopy analysis;;1815.0;181.0;507.0;7176.0;journal;article;2021
Artificial intelligence (AI) will continue to cause substantial changes within the field of radiology, and it will become increasingly important for clinicians to be familiar with several concepts behind AI algorithms in order to effectively guide their clinical implementation. This review aims to give medical professionals the basic information needed to understand AI development and research. The general concepts behind several AI algorithms, including their data requirements, training, and evaluation methods are explained. The potential legal implications of using AI algorithms in clinical practice are also discussed.;Marly {van Assen} and Scott J. Lee and Carlo N. {De Cecco};"Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";1082.0;325.0;Ireland;1981-2020;10.1016/j.ejrad.2020.109083;0.01605;115.0;;0720048X;18727727;0720048X;18727727;3.528;Artificial intelligence, Cardiac, Chest;Elsevier Ireland Ltd;;3029.0;Western Europe;1025.0;Q1;16677.0;European journal of radiology;Artificial intelligence from a to z: from neural network to legal framework;16452.0;3680.0;518.0;1115.0;15691.0;journal;article;2020
Microbiology builds upon biological material deposited in biological resource centers (BRCs) as a reference framework for collaborative research. BRCs assign so-called strain numbers to label the deposited material and are responsible for long-term preservation and worldwide distribution of the material. Cultured microorganisms can be deposited into multiple BRCs and BRCs also mutually exchange their holdings. As a result, many different strain numbers can be attached to biological material that stems from the same isolate. In practice, this material is considered equivalent and used interchangeably. This implies that finding information on given biological material requires all equivalent strain numbers to be used when searching. StrainInfo introduces strain passports for microorganisms: a uniform overview of information known about a given microbial strain. It contains all known equivalent strain numbers and information on the exchange history, sequences and related literature of the strain. Each passport has an associated strain browser that gives direct access to the underlying BRC catalog entries on which the passport was based. Taxon, sequence and literature passports are implemented in a similar manner. In addition to web pages that serve human users, integrated information is also offered in machine readable formats useful for automated, large-scale analysis. StrainInfo is envisioned to be an open platform integrating microbial information. This platform can form the basis for new methods of microbiological research, leveraging the vast amount of electronic information available online. StrainInfo is available from http://www.StrainInfo.net.;Bert Verslyppe and Wim {De Smet} and Bernard {De Baets} and Paul {De Vos} and Peter Dawyndt;"Applied Microbiology and Biotechnology (Q1); Ecology, Evolution, Behavior and Systematics (Q1); Microbiology (Q2)";191.0;372.0;Germany;1983-2020;10.1016/j.syapm.2013.11.002;0.00346;99.0;;07232020;16180984;07232020;16180984;4.022;StrainInfo, Strain passport, Strain browser, Biological resource center, Semantic integration;Urban und Fischer Verlag Jena;;5443.0;Western Europe;933.0;Q1;20861.0;Systematic and applied microbiology;Straininfo introduces electronic passports for microorganisms;6244.0;686.0;76.0;195.0;4137.0;journal;article;2014
"Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of ""big data"" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.";Wang, May D.;"Computer Science Applications; Software";834.0;123.0;United States;1979-2019;10.1109/COMPSAC.2015.343;;47.0;;07303157;07303157;07306512;07303157;;"Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making";Institute of Electrical and Electronics Engineers Inc.;;0.0;Northern America;216.0;-;18705.0;Proceedings - ieee computer society's international computer software and applications conference;Biomedical big data analytics for patient-centric and outcome-driven precision health;;1110.0;0.0;908.0;0.0;conference and proceedings;inproceedings;2015
The ability to evaluate empirical diffusion MRI acquisitions for quality and to correct the resulting imaging metrics allows for improved inference and increased replicability. Previous work has shown promise for estimation of bias and variance of generalized fractional anisotropy (GFA) but comes at the price of computational complexity. This paper aims to provide methods for estimating GFA, bias of GFA and standard deviation of GFA quickly and accurately. In order to provide a method for bias and variance estimation that can return results faster than the previously studied statistical techniques, three deep, fully-connected neural networks are developed for GFA, bias of GFA, and standard deviation of GFA. The results of these networks are compared to the observed values of the metrics as well as those fit from the statistical techniques (i.e. Simulation Extrapolation (SIMEX) for bias estimation and wild bootstrap for variance estimation). Our GFA network provides predictions that are closer to the true GFA values than a Q-ball fit of the observed data (root-mean-square error (RMSE) 0.0077 vs 0.0082, p < .001). The bias network also shows statistically significant improvement in comparison to the SIMEX-estimated error of GFA (RMSE 0.0071 vs. 0.01, p < .001).;Allison E. Hainline and Vishwesh Nath and Prasanna Parvathaneni and Kurt G. Schilling and Justin A. Blaber and Adam W. Anderson and Hakmook Kang and Bennett A. Landman;"Biomedical Engineering (Q2); Biophysics (Q2); Radiology, Nuclear Medicine and Imaging (Q2)";682.0;250.0;United States;1982, 1984-2020;10.1016/j.mri.2019.03.021;0.00787;111.0;;0730725X;18735894;0730725X;18735894;2.546;HARDI, Q-ball, Bias correction, GFA, Measurement error, Neural network;Elsevier Inc.;;3885.0;Northern America;723.0;Q2;17264.0;Magnetic resonance imaging;A deep learning approach to estimation of subject-level bias and variance in high angular resolution diffusion imaging;8690.0;1720.0;211.0;692.0;8197.0;journal;article;2019
;Susanna K.P. Lau and Patrick C.Y. Woo;"Medicine (miscellaneous) (Q1); Infectious Diseases (Q2); Microbiology (medical) (Q2)";677.0;263.0;United States;1983-2020;10.1016/j.diagmicrobio.2018.12.006;0.00954;95.0;;07328893;18790070;07328893;18790070;2.803;;Elsevier Inc.;;2557.0;Northern America;1027.0;Q1;19678.0;Diagnostic microbiology and infectious disease;Pitfalls in big data analysis: next-generation technologies, last-generation data;8091.0;1788.0;192.0;693.0;4910.0;journal;article;2019
Glycans are complex, yet ubiquitous across biological systems. They are involved in diverse essential organismal functions. Aberrant glycosylation may lead to disease development, such as cancer, autoimmune diseases, and inflammatory diseases. Glycans, both normal and aberrant, are synthesized using extensive glycosylation machinery, and understanding this machinery can provide invaluable insights for diagnosis, prognosis, and treatment of various diseases. Increasing amounts of glycomics data are being generated thanks to advances in glycoanalytics technologies, but to maximize the value of such data, innovations are needed for analyzing and interpreting large-scale glycomics data. Artificial intelligence (AI) provides a powerful analysis toolbox in many scientific fields, and here we review state-of-the-art AI approaches on glycosylation analysis. We further discuss how models can be analyzed to gain mechanistic insights into glycosylation machinery and how the machinery shapes glycans under different scenarios. Finally, we propose how to leverage the gained knowledge for developing predictive AI-based models of glycosylation. Thus, guiding future research of AI-based glycosylation model development will provide valuable insights into glycosylation and glycan machinery.;Haining Li and Austin W.T. Chiang and Nathan E. Lewis;"Applied Microbiology and Biotechnology (Q1); Bioengineering (Q1); Biotechnology (Q1)";321.0;1368.0;United States;1983-2020;10.1016/j.biotechadv.2022.108008;0.017;191.0;;07349750;18731899;07349750;18731899;14.227;Glycosylation machinery, Artificial intelligence, Multi-omics integration, Interpretable models;Elsevier Inc.;;19805.0;Northern America;2772.0;Q1;15461.0;Biotechnology advances;Artificial intelligence in the analysis of glycosylation data;23792.0;4598.0;130.0;326.0;25746.0;journal;article;2022
;Deepak L. Bhatt and Joseph P. Drozda and David M. Shahian and Paul S. Chan and Gregg C. Fonarow and Paul A. Heidenreich and Jeffrey P. Jacobs and Frederick A. Masoudi and Eric D. Peterson and Karl F. Welke;Cardiology and Cardiovascular Medicine (Q1);1191.0;744.0;United States;1983-2020;10.1016/j.jacc.2015.07.010;0.177;431.0;;07351097;15583597;07351097;15583597;24.094;ACC/AHA Performance Measures, health policy and outcome research, quality indicators, registries;Elsevier USA;;2392.0;Northern America;10315.0;Q1;22401.0;Journal of the american college of cardiology;Acc/aha/sts statement on the future of registries and the performance measurement enterprise: a report of the american college of cardiology/american heart association task force on performance measures and the society of thoracic surgeons;125873.0;23475.0;935.0;2960.0;22363.0;journal;article;2015
"Objectives
The increasing use of sepsis screening in the Emergency Department (ED) and the Sepsis-3 recommendation to use the quick Sepsis-related Organ Failure Assessment (qSOFA) necessitates validation. We compared Systemic Inflammatory Response Syndrome (SIRS), qSOFA, and the National Early Warning Score (NEWS) for the identification of severe sepsis and septic shock (SS/SS) during ED triage.
Methods
This was a retrospective analysis from an urban, tertiary-care academic center that included 130,595 adult visits to the ED, excluding dispositions lacking adequate clinical evaluation (n = 14,861, 11.4%). The SS/SS group (n = 930) was selected using discharge diagnoses and chart review. We measured sensitivity, specificity, and area under the receiver-operating characteristic (AUROC) for the detection of sepsis endpoints.
Results
NEWS was most accurate for triage detection of SS/SS (AUROC = 0.91, 0.88, 0.81), septic shock (AUROC = 0.93, 0.88, 0.84), and sepsis-related mortality (AUROC = 0.95, 0.89, 0.87) for NEWS, SIRS, and qSOFA, respectively (p < 0.01 for NEWS versus SIRS and qSOFA). For the detection of SS/SS (95% CI), sensitivities were 84.2% (81.5–86.5%), 86.1% (83.6–88.2%), and 28.5% (25.6–31.7%) and specificities were 85.0% (84.8–85.3%), 79.1% (78.9–79.3%), and 98.9% (98.8–99.0%) for NEWS ≥ 4, SIRS ≥ 2, and qSOFA ≥ 2, respectively.
Conclusions
NEWS was the most accurate scoring system for the detection of all sepsis endpoints. Furthermore, NEWS was more specific with similar sensitivity relative to SIRS, improves with disease severity, and is immediately available as it does not require laboratories. However, scoring NEWS is more involved and may be better suited for automated computation. QSOFA had the lowest sensitivity and is a poor tool for ED sepsis screening.";Omar A. Usman and Asad A. Usman and Michael A. Ward;"Emergency Medicine (Q1); Medicine (miscellaneous) (Q2)";1489.0;152.0;United Kingdom;1983-2020;10.1016/j.ajem.2018.10.058;0.01803;86.0;;07356757;15328171;07356757;15328171;2.469;Sepsis, Triage, Critical care, qSOFA, SIRS, NEWS;W.B. Saunders Ltd;;2082.0;Western Europe;725.0;Q1;15207.0;American journal of emergency medicine;Comparison of sirs, qsofa, and news for the early identification of sepsis in the emergency department;12567.0;3229.0;1085.0;2177.0;22588.0;journal;article;2019
"New emerging manufacturing paradigms such as cloud manufacturing, IoT enabled manufacturing and service-oriented manufacturing, have brought many advantages to the manufacturing industry and metamorphosis the industrial IT infrastructure. However, all existing paradigms still suffer from the main problem related to centralized industrial network and third part trust operation. In a nutshell, centralized networking has had issues with flexibility, efficiency, availability, and security. Therefore, the main aim of this paper is to present a distributed peer to peer network architecture that improves the security and scalability of the CMfg. The proposed architecture was developed based on blockchain technology, this facilitated the development of a distributed peer to peer network with high security, scalability and a well-structured cloud system. The proposed architecture which was named as the “BCmfg” is made up of five layers namely; resource layer, perception layer, manufacturing layer, infrastructure layer and application layer. In this paper, the concept of its architecture, secure data sharing, and typical characteristic are discussed and investigated as well as the key technologies required for the implementation of this proposed architecture is explained based on demonstrative case study. The proposed architecture is explained based on a case study which contains five service providers and 15 end users with considering 32 OnCloud services. For evaluation purpose, the qualitative and quantitative methods are utilized and the results show that the proposed methodology can bring more advantages to CMfg than the security and scalability.";Zhi Li and Ali Vatankhah Barenji and George Q. Huang;"Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1)";400.0;735.0;United Kingdom;1984-1994, 1996-2021;10.1016/j.rcim.2018.05.011;0.0057;93.0;;07365845;07365845;07365845;07365845;5.666;Blockchain, Cloud manufacturing, Peer to peer network, Security and scalability;Elsevier Ltd.;;4639.0;Western Europe;1561.0;Q1;18080.0;Robotics and computer-integrated manufacturing;Toward a blockchain cloud manufacturing system as a peer to peer distributed network platform;6215.0;2949.0;139.0;404.0;6448.0;journal;article;2018
Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.;Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii;"Communication (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1); Law (Q1)";438.0;745.0;United Kingdom;1984-2020;10.1016/j.tele.2020.101517;0.00899;66.0;;07365853;07365853;07365853;07365853;6.182;Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review;Elsevier Ltd.;;7228.0;Western Europe;1567.0;Q1;20896.0;Telematics and informatics;Big data analytics meets social media: a systematic review of techniques, open issues, and future directions;5351.0;3832.0;96.0;483.0;6939.0;journal;article;2021
;Jessica Spence and C. David Mazer;Anesthesiology and Pain Medicine (Q3);34.0;116.0;United States;2003-2020;10.1016/j.aan.2020.09.003;;7.0;;07376146;07376146;07376146;07376146;;Cardiothoracic anesthesia, Research, Trial design, Future;Academic Press Inc.;;6433.0;Northern America;371.0;Q3;21401.0;Advances in anesthesia;The future directions of research in cardiac anesthesiology;;49.0;15.0;37.0;965.0;book series;article;2020
A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.;Gabriel E. Sánchez-Martínez and Marcela Munizaga;"Economics, Econometrics and Finance (miscellaneous) (Q1); Transportation (Q1)";205.0;279.0;United States;1994, 1996, 1999, 2001, 2004-2020;10.1016/j.retrec.2016.10.008;0.00218;46.0;;07398859;07398859;07398859;07398859;2.627;Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices;JAI Press;;4648.0;Northern America;1019.0;Q1;4100151536.0;Research in transportation economics;Workshop 5 report: harnessing big data;2095.0;694.0;160.0;220.0;7437.0;journal;article;2016
Despite the increasing interest and exploration of the use of blockchain technology in public service organisations (PSOs), academic understanding of its transformative impact on the operational excellence of PSOs remains limited. This study adopts an action design science research methodology to develop a proof of concept (POC) blockchain based application for Companies House, a government agency that is registering companies across UK. The application addresses the operational challenges of Companies House as well as issues citizens face when accessing its services. We draw from the public value framework proposed by Twizeyimana and Andersson (2019) and demonstrate the significance of the emerging blockchain technology in relation to their democratic practices based on six dimensions. We further discuss the related challenges and barriers for its implementation and evaluate the POC with the stakeholders of Companies House. We also present an illustrative case study, where we explored the appropriateness of the POC in relation to the draft legislation, “Registration of Overseas Entities and Beneficial Owners” (ROEBO) bill which proposes the introduction of a register of the beneficial owners of overseas legal entities that own real estate in the UK. Our research is one of the few studies that will provide in-depth empirical insights about the relationship between blockchain and operational excellence of PSOs.;Ali Shahaab and Imtiaz A. Khan and Ross Maude and Chaminda Hewage and Yingli Wang;"E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)";193.0;839.0;United Kingdom;1984-2020;10.1016/j.giq.2022.101759;0.00502;103.0;;0740624X;0740624X;0740624X;0740624X;7.279;Blockchain, Distributed ledger technology, Public service operation, Design science, Case study;Elsevier Ltd.;;7755.0;Western Europe;2121.0;Q1;14735.0;Government information quarterly;Public service operational efficiency and blockchain – a case study of companies house, uk;5379.0;1946.0;76.0;205.0;5894.0;journal;article;2022
Research designs are key to the research process and the production of knowledge that supports performance and development. The appropriateness of the methodologies used in research has implications for the trustworthiness and validity of the outcomes of research and practice. The research designs used in library information science (LIS) research in Nigeria and South Africa between 2009 and 2015 were investigated. The objective was to map out the contours of the research designs that are utilised in LIS, particularly to keep the profession abreast of the trends in the field and the patterns in research designs used. Qualitative content analysis was used to examine 104 PhD dissertations, using six taxonomies to categorise research designs used in the two countries. Positivist epistemologies and quantitative methodologies predominated research in LIS. A handful of studies used basic mixed method research designs. Questionnaires and interviews were commonly used for data collection, but the triangulation of methods was not prevalent. The value of this study lies in that it will lead to the accumulation of knowledge of research designs and provide a baseline for studies on methodological practices.;Patrick Ngulube and Scholastica C. Ukwoma;"Information Systems (Q1); Library and Information Sciences (Q1)";98.0;282.0;United Kingdom;1987-2020;10.1016/j.lisr.2019.100966;;57.0;;07408188;07408188;07408188;07408188;;;Elsevier BV;;5792.0;Western Europe;1225.0;Q1;13971.0;Library and information science research;Cartographies of research designs in library information science research in nigeria and south africa, 2009–2015;;305.0;37.0;104.0;2143.0;journal;article;2019
The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.;Oane Visser and Sarah Ruth Sippel and Louis Thiemann;"Development (Q1); Forestry (Q1); Geography, Planning and Development (Q1); Sociology and Political Science (Q1)";557.0;443.0;United Kingdom;1985-2020;10.1016/j.jrurstud.2021.07.024;0.01005;104.0;;07430167;07430167;07430167;07430167;4.849;Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data;Elsevier Ltd.;;7409.0;Western Europe;1497.0;Q1;15673.0;Journal of rural studies;Imprecision farming? examining the (in)accuracy and risks of digital agriculture;9142.0;2862.0;280.0;573.0;20744.0;journal;article;2021
Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.;Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi;"Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)";568.0;472.0;United States;1984-2021;10.1016/j.jpdc.2014.08.007;0.00477;87.0;;07437315;07437315;10960848;07437315;3.734;Adaptivity, Monitoring, Cloud computing, Big data, Scalability;Academic Press Inc.;;4240.0;Northern America;638.0;Q1;25621.0;Journal of parallel and distributed computing;Adaptive, scalable and reliable monitoring of big data on clouds;4371.0;2473.0;169.0;592.0;7166.0;journal;article;2015
"The growth of social media usage questions the old-style idea of customer relationship management (CRM). Social CRM strategy is a novel version of CRM empowered by social media technology that offers a new way of managing relationships with customers effectively. This study aims to forecast the predictors of social CRM strategy adoption by small and medium enterprises (SMEs). The proposed model used in this study derived its theoretical support from IT/IS, marketing, and CRM literature. In the proposed Technology-Organization-Environment-Process (TOEP) adoption model, several hypotheses are developed which examine the role of Technological factors, such as Cost of Adoption, Relative Advantages, Complexity, and Compatibility; Organizational factors, such as IT/IS knowledge of employee, and Top management support; Environmental factors such as Competitive Pressure, and Customer Pressure; and Process factors such as Information Capture, Information Use, and Information Sharing; all having a positive relationship with social CRM adoption. This research applied a following two staged SEM-neural network method combining both structural equation modelling (SEM) and neural network analyses. The proposed hypothetical model is examined by using SEM on the collected data of SMEs in Kuala Lumpur, the central city of Malaysia. The SEM approach with a neural network method can be used to investigate the complicated relations involved in the adoption of social CRM. The study finds that compatibility, information capture, IT/IS knowledge of employee, top management support, information sharing, competitive pressure, cost, relative advantage, and customer pressure are the most important factors influencing social CRM adoption. Remarkably, the results of neural network analysis show that compatibility and information capture of social CRM are the most significant factors which affect SMEs' adoption of this form of customer relationship management. The outcomes of this research benefit executives' decision-making by identifying and ranking factors that enable them to discover how they can advance the usage of social CRM in their firms. Furthermore, the findings of this study can help the managers/owners of SMEs assign their resources, according to the ranking of social CRM adoption factors, when they are making plans to adopt social CRM. This study differs from previous studies as it proposes an innovative new approach to determine what influences the adoption of social CRM. By proposing the TOEP adoption model, additional information process factors advance the traditional TOE adoption model.";Ali Ahani and Nor Zairah Ab. Rahim and Mehrbakhsh Nilashi;"Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1)";1573.0;783.0;United Kingdom;1985-2021;10.1016/j.chb.2017.05.032;0.05973;178.0;;07475632;07475632;07475632;07475632;6.829;CRM adoption, Social CRM, Technology-organization-environment-process, SMEs, Social media, SEM-Neural network;Elsevier Ltd.;;6978.0;Western Europe;2108.0;Q1;19419.0;Computers in human behavior;Forecasting social crm adoption in smes: a combined sem-neural network method;45035.0;14501.0;385.0;1597.0;26867.0;journal;article;2017
Proficiency with data analytics is an increasingly important skill within in the accounting profession. However, successful data analysis requires clean source data (i.e., source data without errors) in order to draw reliable conclusions. Although users often assume clean source data, this assumption is frequently incorrect. Therefore, identifying and remediating “dirty data” is a prerequisite to effective data analysis. You, an accountant working at a firm that specializes in data analytics, have been hired by Rigorous House Insurance to analyze the company’s claim insurance data. In addition to investigating specific issues mentioned by the company’s controller, you are tasked with identifying any other data integrity issues that you encounter and providing preventative information system internal control suggestions to the client to mitigate these issues in the future.;James G. Lawson and Daniel A. Street;"Accounting (Q1); Education (Q1)";69.0;276.0;United Kingdom;1983-2020;10.1016/j.jaccedu.2021.100714;;35.0;;07485751;07485751;07485751;07485751;;Data analytics, Accounting education, Dirty data, Structured query language (“SQL”), Data integrity;Elsevier BV;;3463.0;Western Europe;931.0;Q1;29852.0;Journal of accounting education;Detecting dirty data using sql: rigorous house insurance case;;227.0;24.0;73.0;831.0;journal;article;2021
;Lee Squitieri and Kevin C. Chung;"Orthopedics and Sports Medicine (Q2); Surgery (Q2)";181.0;152.0;United Kingdom;1985-2020;10.1016/j.hcl.2020.01.011;0.00176;55.0;;07490712;15581969;07490712;15581969;1.907;Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis;W.B. Saunders Ltd;;3727.0;Western Europe;742.0;Q2;29770.0;Hand clinics;Deriving evidence from secondary data in hand surgery: strengths, limitations, and future directions;2329.0;332.0;67.0;207.0;2497.0;journal;article;2020
;Miles E. Theurer and David G. Renter and Brad J. White;"Food Animals (Q1); Medicine (miscellaneous) (Q1)";104.0;278.0;United Kingdom;1971-1978, 1985-2020;10.1016/j.cvfa.2015.05.004;;70.0;;07490720;07490720;15584240;07490720;;Epidemiology, Feedlot, Data, Distribution;W.B. Saunders Ltd;;5865.0;Western Europe;981.0;Q1;19514.0;Veterinary clinics of north america - food animal practice;Using feedlot operational data to make valid conclusions for improving health management;;314.0;55.0;122.0;3226.0;journal;article;2015
"ABSTRACT
Objectives
Predictive risk models are advocated in psychosocial oncology practice to provide timely and appropriate support to those likely to experience the emotional and psychological consequences of cancer and its treatments. New digital technologies mean that large scale and routine data collection are becoming part of everyday clinical practice. Using these data to try to identify those at greatest risk for late psychosocial effects of cancer is an attractive proposition in a climate of unmet need and limited resource. In this paper, we present a framework to support the development of high-quality predictive risk models in psychosocial and supportive oncology. The aim is to provide awareness and increase accessibility of best practice literature to support researchers in psychosocial and supportive care to undertake a structured evidence-based approach.
Data Sources
Statistical prediction risk model publications.
Conclusion
In statistical modeling and data science different approaches are needed if the goal is to predict rather than explain. The deployment of a poorly developed and tested predictive risk model has the potential to do great harm. Recommendations for best practice to develop predictive risk models have been developed but there appears to be little application within psychosocial and supportive oncology care.
Implications for Nursing Practice
Use of best practice evidence will ensure the development and validation of predictive models that are robust as these are currently lacking. These models have the potential to enhance supportive oncology care through harnessing routine digital collection of patient-reported outcomes and the targeting of interventions according to risk characteristics.";Jenny Harris and Edward Purssell and Emma Ream and Anne Jones and Jo Armes and Victoria Cornelius;Oncology (nursing) (Q2);178.0;186.0;United Kingdom;1985-2020;10.1016/j.soncn.2020.151089;0.0016899999999999999;45.0;;07492081;18783449;07492081;18783449;2.315;Predictive risk models, Regression models, Cancer, Psychosocial, Supportive care, Psychological, Distress;W.B. Saunders Ltd;;4950.0;Western Europe;596.0;Q2;29925.0;Seminars in oncology nursing;How to develop statistical predictive risk models in oncology nursing to enhance psychosocial and supportive care;1412.0;396.0;66.0;195.0;3267.0;journal;article;2020
;David M. Klurfeld and Eric B. Hekler and Camille Nebeker and Kevin Patrick and Chor San H. Khoo;"Epidemiology (Q1); Public Health, Environmental and Occupational Health (Q1)";875.0;400.0;United States;1985-2020;10.1016/j.amepre.2018.06.013;0.03731;216.0;;07493797;07493797;18732607;07493797;5.043;;Elsevier Inc.;;3477.0;Northern America;2287.0;Q1;27063.0;American journal of preventive medicine;Technology innovations in dietary intake and physical activity assessment: challenges and recommendations for future directions;28400.0;4351.0;287.0;970.0;9980.0;journal;article;2018
"Machine learning has seen slow but steady uptake in diagnostic pathology over the past decade to assess digital whole-slide images. Machine learning tools have incredible potential to standardise, and likely even improve, histopathologic diagnoses, but they are not yet widely used in clinical practice. We describe the principles of these tools and technologies and some successful preclinical and pretranslational efforts in cardiovascular pathology, as well as a roadmap for moving forward. In nonhuman animal models, one proof-of-principle application is in rodent progressive cardiomyopathy, which is of particular significance to drug toxicity studies. Basic science successes include screening the quality of differentiated stem cells and characterising cardiomyocyte developmental stages, with potential applications for research and toxicology/drug safety screening using derived or native human pluripotent stem cells differentiated into cardiomyocytes. Translational studies of particular note include those with success in diagnosing the various forms of heart allograft rejection. For fully realising the value of these tools in clinical cardiovascular pathology, we identify 3 essential challenges. First is image quality standardisation to ensure that algorithms can be developed and implemented on robust, consistent data. The second is consensus diagnosis; experts don’t always agree, and thus “truth” may be difficult to establish, but the algorithms themselves may provide a solution. The third is the need for large-enough data sets to facilitate robust algorithm development, necessitating large cross-institutional shared image databases. The power of histopathology-based machine learning technologies is tremendous, and we outline the next steps needed to capitalise on this power.
Résumé
Au cours de la dernière décennie, l’apprentissage automatique a connu une adoption lente, mais constante, dans l’analyse des images numérisées de lamelle entière pour le diagnostic de maladies. Les outils d’apprentissage automatique ont un fabuleux potentiel de standardisation, voire d’amélioration, des diagnostics histopathologiques; ils ne sont toutefois pas encore largement utilisés en pratique clinique. Cet article vise à expliquer les principes de ces outils et technologies et à présenter certaines expériences précliniques et prétranslationnelles fructueuses en pathologie cardiovasculaire, de même qu’une feuille de route pour aller de l’avant. Dans les modèles animaux non humains, la cardiomyopathie évolutive chez le rongeur constitue une démonstration de principe qui revêt une importance particulière pour le succès des études toxicologiques. Les réussites en sciences fondamentales comprennent la sélection de cellules souches différenciées de qualité et la caractérisation des étapes du développement des cardiomyocytes, de même que leurs applications potentielles en recherche et en analyse toxicologique/de l’innocuité des médicaments à l’aide de cellules souches pluripotentes humaines ou dérivées se différenciant en cardiomyocytes. Les études translationnelles notables comprennent celles ayant permis de diagnostiquer les diverses formes de rejet d’allogreffe cardiaque. Pour pleinement comprendre la valeur de ces outils en pathologie clinique cardiovasculaire, nous avons repéré trois défis essentiels. Le premier est la standardisation de la qualité des images pour garantir que les algorithmes soient élaborés et mis en œuvre à partir de données solides et cohérentes. Le second est le diagnostic consensuel; les experts ne sont pas toujours d’accord, et par conséquent, il peut être difficile d’établir la « vérité », mais les algorithmes pourraient eux-mêmes fournir une solution. Le troisième est la nécessité de constituer des ensembles de données suffisantes pour favoriser l’élaboration d’algorithmes solides, ce qui nécessite de vastes bases de données d’images partagées entre les établissements. La puissance des technologies d’apprentissage automatique fondées sur l’histopathologie est immense; nous décrirons les prochaines étapes qui nous permettront de tirer profit de leur puissance.";Carolyn Glass and Kyle J. Lafata and William Jeck and Roarke Horstmeyer and Colin Cooke and Jeffrey Everitt and Matthew Glass and David Dov and Michael A. Seidman;Cardiology and Cardiovascular Medicine (Q1);691.0;249.0;Canada;1985-2020;10.1016/j.cjca.2021.11.008;0.015090000000000001;90.0;;0828282X;19167075;0828282X;19167075;5.223;;Elsevier Inc.;;2647.0;Northern America;1395.0;Q1;22504.0;Canadian journal of cardiology;The role of machine learning in cardiovascular pathology;8782.0;2482.0;342.0;921.0;9052.0;journal;article;2022
"Artificial intelligence (AI) software that analyzes medical images is becoming increasingly prevalent. Unlike earlier generations of AI software, which relied on expert knowledge to identify imaging features, machine learning approaches automatically learn to recognize these features. However, the promise of accurate personalized medicine can only be fulfilled with access to large quantities of medical data from patients. This data could be used for purposes such as predicting disease, diagnosis, treatment optimization, and prognostication. Radiology is positioned to lead development and implementation of AI algorithms and to manage the associated ethical and legal challenges. This white paper from the Canadian Association of Radiologists provides a framework for study of the legal and ethical issues related to AI in medical imaging, related to patient data (privacy, confidentiality, ownership, and sharing); algorithms (levels of autonomy, liability, and jurisprudence); practice (best practices and current legal framework); and finally, opportunities in AI from the perspective of a universal health care system.
Résumé
Les logiciels d’intelligence artificielle (IA) analysant les images médicales sont de plus en plus prévalents. Contrairement aux générations précédentes de logiciels d’IA qui reposaient sur un savoir d’expert pour identifier les caractéristiques d’une image, les approches d’apprentissage machine permettent aux systèmes d’apprendre à reconnaître ces caractéristiques. Toutefois, la promesse d’une médecine personnalisée précise ne peut être remplie qu’avec un accès à de très grandes quantités de données médicales de patients. Ces données pourraient servir à prédire les maladies, à les diagnostiquer, à optimiser les traitements et à établir un pronostic. La radiologie se trouve en position de chef de file pour le développement et la mise en œuvre des algorithmes d’IA, ainsi que pour la gestion des défis éthiques et légaux qui leur sont associés. Ce livre blanc de l’Association canadienne des radiologistes (CAR) propose un cadre d’étude pour les problèmes légaux et éthiques en rapport avec l’utilisation de l’IA dans l’imagerie médicale pour les données des patients (vie privée, confidentialité, propriété et partage), les algorithmes (degrés d’autonomie, responsabilité et jurisprudence), la pratique médicale (meilleures pratiques et cadre réglementaire actuel) et les opportunités offertes par l’IA du point de vue d’un système de soins de santé universel.";Jacob L. Jaremko and Marleine Azar and Rebecca Bromwich and Andrea Lum and Li Hsia {Alicia Cheong} and Martin Gibert and François Laviolette and Bruce Gray and Caroline Reinhold and Mark Cicero and Jaron Chong and James Shaw and Frank J. Rybicki and Casey Hurrell and Emil Lee and An Tang;"Medicine (miscellaneous) (Q2); Radiology, Nuclear Medicine and Imaging (Q2)";190.0;194.0;Canada;1973-2020;10.1016/j.carj.2019.03.001;;34.0;;08465371;08465371;14882361;08465371;;Artificial intelligence, Machine learning, Ethics, Legal, Radiology, Imaging;Elsevier Inc.;;2852.0;Northern America;580.0;Q2;16593.0;Canadian association of radiologists journal;Canadian association of radiologists white paper on ethical and legal issues related to artificial intelligence in radiology;;389.0;150.0;209.0;4278.0;journal;article;2019
"The Bushveld Complex, the largest layered mafic-ultramafic intrusion worldwide, is host of numerous, laterally continuous and chemically similar chromitite layers. Based on their stratigraphic position the layers are subdivided into a lower, middle and upper group (LG, MG and UG). Within these groups the layers are numbered successively – from the base to the top of each group. Attempts of discriminating between single layers based on their composition have failed – mainly due to the significant overlap of compositional fields, e.g. of chromitite mineral assemblages and chromite mineral chemistry between (neighboured) layers. In this contribution a tailored and easy to use multivariate classification scheme for the chromitite layers is proposed, based on a comprehensive classification routine for the LG and MG chromitites. This routine allows a clear attribution with known uncertainty of eight distinct chromitite layers. The study was carried out at the Thaba Mine, a chromite mine located on the western limb of the Bushveld Complex. The classification is based on a large geochemical database (N = 1205) from Thaba Mine. It comprises of a hierarchical discrimination approach relying on linear discriminant analysis and involves five distinct steps. Using default homogeneous prior probabilities, classification results are excellent for the first discrimination steps (LGs vs. MGs, 97 %; LG-6 vs. LG-6A, 94 %) and very good for the following steps (MG-1/2 vs. MG-3/4, 86 %; MG-1 vs. MG-2, 92 %; MG-3 vs. MG-4, 93 %; MG-4 vs. MG-4Z, 97 %; MG-4 vs. MG-4A, 88 %). The classification scheme was tested using the same sample set as a training set with unknown composition. Overall classification results for unknown samples belonging to one of the layers are 81 %. Hence, the classification scheme is at least valid for the Thaba mine. The approach may, however, be extended across the entire Bushveld, provided that an appropriate geochemical data set is available.";Kai Bachmann and Peter Menzel and Raimon Tolosana-Delgado and Christopher Schmidt and Moritz Hill and Jens Gutzmer;"Pollution (Q1); Environmental Chemistry (Q2); Geochemistry and Petrology (Q2)";696.0;348.0;United Kingdom;1986-2020;10.1016/j.apgeochem.2019.02.009;0.00895;130.0;;08832927;08832927;08832927;08832927;3.524;Linear discriminant analysis, PGE, Thaba Mine, Lower group chromitites, Middle group chromitites, Compositional data analysis;Elsevier Ltd.;;6502.0;Western Europe;1015.0;Q1;24615.0;Applied geochemistry;Multivariate geochemical classification of chromitite layers in the bushveld complex, south africa;15355.0;2526.0;262.0;706.0;17035.0;journal;article;2019
"Background
Periprosthetic joint infection (PJI) data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection. The goal of this study is to develop a natural language processing (NLP) algorithm to replicate manual chart review for PJI data elements.
Methods
PJI was identified among all total joint arthroplasty (TJA) procedures performed at a single academic institution between 2000 and 2017. Data elements that comprise the Musculoskeletal Infection Society (MSIS) criteria were manually extracted and used as the gold standard for validation. A training sample of 1208 TJA surgeries (170 PJI cases) was randomly selected to develop the prototype NLP algorithms and an additional 1179 surgeries (150 PJI cases) were randomly selected as the test sample. The algorithms were applied to all consultation notes, operative notes, pathology reports, and microbiology reports to predict the correct status of PJI based on MSIS criteria.
Results
The algorithm, which identified patients with PJI based on MSIS criteria, achieved an f1-score (harmonic mean of precision and recall) of 0.911. Algorithm performance in extracting the presence of sinus tract, purulence, pathologic documentation of inflammation, and growth of cultured organisms from the involved TJA achieved f1-scores that ranged from 0.771 to 0.982, sensitivity that ranged from 0.730 to 1.000, and specificity that ranged from 0.947 to 1.000.
Conclusion
NLP-enabled algorithms have the potential to automate data collection for PJI diagnostic elements, which could directly improve patient care and augment cohort surveillance and research efforts. Further validation is needed in other hospital settings.
Level of Evidence
Level III, Diagnostic.";Sunyang Fu and Cody C. Wyles and Douglas R. Osmon and Martha L. Carvour and Elham Sagheb and Taghi Ramazanian and Walter K. Kremers and David G. Lewallen and Daniel J. Berry and Sunghwan Sohn and Hilal Maradit Kremers;Orthopedics and Sports Medicine (Q1);1949.0;410.0;United States;1986-2020;10.1016/j.arth.2020.07.076;0.03779;135.0;;08835403;08835403;15328406;08835403;4.757;total joint arthroplasty, periprosthetic joint infection, natural language processing, electronic health records, artificial intelligence;Churchill Livingstone;;2954.0;Northern America;2766.0;Q1;12191.0;Journal of arthroplasty;Automated detection of periprosthetic joint infections and data elements using natural language processing;27716.0;9335.0;810.0;2159.0;23925.0;journal;article;2021
;Walter Verbrugghe and Kirsten Colpaert;Critical Care and Intensive Care Medicine (Q1);847.0;253.0;Netherlands;1986-2020;10.1016/j.jcrc.2019.09.005;0.01662;83.0;;08839441;15578615;08839441;15578615;3.425;;Elsevier BV;;3014.0;Western Europe;1149.0;Q1;12212.0;Journal of critical care;The electronic medical record: big data, little information?;9746.0;2889.0;300.0;1095.0;9042.0;journal;article;2019
Regularized empirical risk minimization including support vector machines plays an important role in machine learning theory. In this paper regularized pairwise learning (RPL) methods based on kernels will be investigated. One example is regularized minimization of the error entropy loss which has recently attracted quite some interest from the viewpoint of consistency and learning rates. This paper shows that such RPL methods and also their empirical bootstrap have additionally good statistical robustness properties, if the loss function and the kernel are chosen appropriately. We treat two cases of particular interest: (i) a bounded and non-convex loss function and (ii) an unbounded convex loss function satisfying a certain Lipschitz type condition.;Andreas Christmann and Ding-Xuan Zhou;"Algebra and Number Theory (Q2); Applied Mathematics (Q2); Control and Optimization (Q2); Mathematics (miscellaneous) (Q2); Numerical Analysis (Q2); Statistics and Probability (Q2)";110.0;203.0;United States;1985-2020;10.1016/j.jco.2016.07.001;0.00158;54.0;;0885064X;0885064X;10902708;0885064X;1.397;Machine learning, Pairwise loss function, Regularized risk, Robustness;Academic Press Inc.;;2559.0;Northern America;656.0;Q2;28500.0;Journal of complexity;On the robustness of regularized pairwise learning methods based on kernels;1034.0;216.0;37.0;111.0;947.0;journal;article;2016
Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.;Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani;"Building and Construction (Q1); Geotechnical Engineering and Engineering Geology (Q1)";942.0;653.0;United Kingdom;1986-2020;10.1016/j.tust.2020.103691;0.0149;98.0;;08867798;08867798;08867798;08867798;5.915;Big data, Empirical model, Deep learning, Critical event, Smart firefighting;Elsevier Ltd.;;4400.0;Western Europe;2172.0;Q1;14642.0;Tunnelling and underground space technology;Perspectives of big experimental database and artificial intelligence in tunnel fire research;16336.0;6440.0;375.0;946.0;16500.0;journal;article;2021
In response to the need to minimize the use of experimental animals, new approach methodologies (NAMs) using advanced technology have emerged in the 21st century. ToxCast/Tox21 aims to evaluate the adverse effects of chemicals quickly and efficiently using a high-throughput screening and to transform the paradigm of toxicity assessment into mechanism-based toxicity prediction. The ToxCast/Tox21 database, which contains extensive data from over 1400 assays with numerous biological targets and activity data for over 9000 chemicals, can be used for various purposes in the field of chemical prioritization and toxicity prediction. In this study, an overview of the database was explored to aid mechanism-based chemical prioritization and toxicity prediction. Implications for the utilization of the ToxCast/Tox21 database in chemical prioritization and toxicity prediction were derived. The research trends in ToxCast/Tox21 assay data were reviewed in the context of toxicity mechanism identification, chemical priority, environmental monitoring, assay development, and toxicity prediction. Finally, the potential applications and limitations of using ToxCast/Tox21 assay data in chemical risk assessment were discussed. The analysis of the toxicity mechanism-based assays of ToxCast/Tox21 will help in chemical prioritization and regulatory applications without the use of laboratory animals.;Jaeseong Jeong and Donghyeon Kim and Jinhee Choi;"Medicine (miscellaneous) (Q2); Toxicology (Q2)";780.0;318.0;United Kingdom;1987-2020;10.1016/j.tiv.2022.105451;0.00805;107.0;;08872333;18793177;08872333;18793177;3.500;ToxCast/Tox21, Bioassay, Toxicity mechanism, Prioritization, Toxicity prediction, Risk assessment;Elsevier Ltd.;;4821.0;Western Europe;834.0;Q2;25224.0;Toxicology in vitro;Application of toxcast/tox21 data for toxicity mechanism-based evaluation and prioritization of environmental chemicals: perspective and limitations;11567.0;2710.0;270.0;792.0;13018.0;journal;article;2022
The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.;Matthew Russell and Peng Wang;"Aerospace Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Mechanical Engineering (Q1); Signal Processing (Q1)";2017.0;795.0;United States;1987-2021;10.1016/j.ymssp.2021.108709;0.03775;167.0;;08883270;08883270;10961216;08883270;6.823;Physics-informed deep learning, Prognostics and health management, Data compression, Big data;Elsevier;;4336.0;Northern America;2275.0;Q1;21080.0;Mechanical systems and signal processing;Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring;30686.0;16068.0;538.0;2031.0;23327.0;journal;article;2022
During the last decade, cloud computing became a natural choice to host and provide various computing resources as on-demand services. The correct characterization and management of cloud environment objects (clouds, data centers, providers, services, data, users, etc.) is the first step towards effective provisioning and integration of cloud services. However, cloud computing environment is often subject to uncertainty. This could be attributed to the incompleteness and imprecision of cloud available information, as well as the highly changing conditions. The purpose of this survey is to study, criticize and classify the already existing works that deal with uncertainty in the cloud. We present a taxonomy on the uncertainty in the cloud and we study how such concept was tackled by researchers in cloud environments. Finally, we identify the challenges and the requirements to deal with uncertain data in the cloud, as well as the future directions.;Haithem Mezni and Sabeur Aridhi and Allel Hadjali;"Applied Mathematics (Q1); Artificial Intelligence (Q1); Software (Q1); Theoretical Computer Science (Q1)";475.0;455.0;United States;1974, 1987-2020;10.1016/j.ijar.2018.09.009;0.00487;97.0;;0888613X;0888613X;18734731;0888613X;3.816;Cloud computing, Uncertainty, Uncertain cloud services, Uncertainty models;Elsevier Inc.;;4671.0;Northern America;1039.0;Q1;24286.0;International journal of approximate reasoning;The uncertain cloud: state of the art and research challenges;4819.0;2071.0;129.0;496.0;6026.0;journal;article;2018
Acute prediction of SNPs (Single Nucleotide Polymorphisms) from high throughput sequencing data is a challenging problem, having potential to explore possible variation within plants species. For the extraction of profitable information from bulk of data, machine learning (ML) could lead to development of accurate model based on the learning of prior information. We performed state of art, in-depth learning on six different plant species. Comparative evaluation of five different algorithms showed that Random Forest substantially outperformed in selection of potential SNPs, with markedly improved prediction accuracy via 10-fold cross validation technique and integrated in system known as PLANET-SNP. We present the accurate method to extract the potential SNPs with user specific customizable parameters. It will facilitate the identification of efficient and functional SNPs in most easy and intuitive way. PLANET-SNP pipeline is very flexible in terms of data input and output formats. PLANET-SNP Pipeline is available at http://www.ncgd.nbri.res.in/PLANET-SNP-Pipeline.aspx;Archana Bhardwaj and Sumit K. Bag;Genetics (Q3);338.0;376.0;United States;1987-2020;10.1016/j.ygeno.2018.07.001;0.0068;141.0;;08887543;08887543;10898646;08887543;5.736;Annotation, Diploid, Polyploidy, SNP, Machine learning;Academic Press Inc.;;5798.0;Northern America;703.0;Q3;22215.0;Genomics;Planet-snp pipeline: plants based annotation and establishment of true snp pipeline;11771.0;1316.0;559.0;341.0;32412.0;journal;article;2019
;James L. Vaden and Christopher S. Riolo and Michael L. Riolo;Orthodontics (Q1);547.0;141.0;United States;1986-2020;10.1016/j.ajodo.2017.06.009;0.00672;121.0;;08895406;10976752;08895406;10976752;2.650;;Mosby Inc.;;2161.0;Northern America;1183.0;Q1;23807.0;American journal of orthodontics and dentofacial orthopedics;An orthodontic registry: producing evidence from existing resources;15935.0;1368.0;319.0;907.0;6895.0;journal;article;2017
;Sindhu R. Johnson;Rheumatology (Q2);134.0;196.0;United Kingdom;1987-2020;10.1016/j.rdc.2018.02.001;0.00305;87.0;;0889857X;0889857X;15583163;0889857X;2.670;;W.B. Saunders Ltd;;5089.0;Western Europe;928.0;Q2;19768.0;Rheumatic disease clinics of north america;Advanced epidemiologic methods for the study of rheumatic and musculoskeletal diseases;2944.0;477.0;62.0;170.0;3155.0;journal;article;2018
;Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort;"Cardiology and Cardiovascular Medicine (Q2); Medicine (miscellaneous) (Q2); Surgery (Q2)";1527.0;137.0;United States;1986-2020;10.1016/j.avsg.2020.04.022;0.00916;74.0;;08905096;16155947;08905096;16155947;1.466;;Elsevier Inc.;;2003.0;Northern America;635.0;Q2;20492.0;Annals of vascular surgery;Artificial intelligence in vascular surgery: moving from big data to smart data;7121.0;2242.0;844.0;1561.0;16904.0;journal;article;2020
Tool condition monitoring (TCM) in machining operations is crucial to maximise the useful tool life while reducing the risks associated with tool breakage. Unlike progressive tool wear, tool breakage occurs randomly, with more severe implications for workpiece quality, machining system stiffness, and even operator safety. Existing literature reviews on TCM focus on tool wear monitoring, including wear state recognition and remaining useful life prediction. However, a comprehensive review of tool breakage monitoring (TBM) techniques is lacking. Generic signal processing and intelligent decision-making methods cannot fully satisfy the practical requirements of the TBM. In addition, developing and evaluating TBM models using imbalanced data is more challenging. Herein, we present the first systematic review on TBM to bridge these limitations, and provide adequate guidance for avoiding catastrophic tool failures during cutting processes. Signal acquisition, feature extraction, and decision-making methodologies for the TBM are outlined and compared with related techniques for tool wear monitoring. The effects of data imbalance on TBM models are considered, and feasible solutions are provided at the data and algorithm levels. Finally, the challenges faced by the TBM are discussed, and potential research directions are suggested. The research and application of TBM techniques will certainly better empower various machining operations in response to intelligent manufacturing demands.;Xuebing Li and Xianli Liu and Caixu Yue and Steven Y. Liang and Lihui Wang;"Industrial and Manufacturing Engineering (Q1); Mechanical Engineering (Q1)";241.0;824.0;United Kingdom;1987-2020;10.1016/j.ijmachtools.2022.103882;;156.0;;08906955;08906955;08906955;08906955;;Cutting tools, Tool breakage monitoring, Signal acquisition, Feature extraction, Intelligent decision-making, Imbalanced data;Elsevier Ltd.;;4738.0;Western Europe;3418.0;Q1;19164.0;International journal of machine tools and manufacture;Systematic review on tool breakage monitoring techniques in machining operations;;2146.0;65.0;250.0;3080.0;journal;article;2022
This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.;Jodie Moll and Ogan Yigitbasioglu;Accounting (Q1);112.0;516.0;United States;1988-2020;10.1016/j.bar.2019.04.002;0.0018899999999999998;67.0;;08908389;10958347;08908389;10958347;5.577;Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence;Academic Press Inc.;;8102.0;Northern America;1223.0;Q1;28634.0;British accounting review;The role of internet-related technologies in shaping the work of accountants: new directions for accounting research;2836.0;648.0;44.0;118.0;3565.0;journal;article;2019
;Aadia I. Rana and Michael J. Mugavero;"Infectious Diseases (Q1); Microbiology (medical) (Q1)";168.0;520.0;United Kingdom;1987-2020;10.1016/j.idc.2019.05.009;0.00687;96.0;;08915520;15579824;08915520;15579824;5.982;HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance;W.B. Saunders Ltd;;7935.0;Western Europe;1854.0;Q1;22350.0;Infectious disease clinics of north america;How big data science can improve linkage and retention in care;4090.0;1000.0;57.0;190.0;4523.0;journal;article;2019
Advancements in modern mineral processing has been driven by technology and fuelled by market economics of supply and demand. Over the last three decades, the demand for various minerals has steadily increased, while the mineral processing industry has seen an unavoidable increase in the treatment of complex ores, continuous decline in plant feed grade and poor plant performance partly due to blending of ores with dissimilar properties. Despite these challenges, production plant data that are routinely generated are usually underutilised. In this contribution and aligned with the direction of the 4th industrial revolution, we highlight the value of legacy metallurgical plant data and the concept of a dry laboratory approach. This study is presented in two parts. In the current paper (Part I), a comprehensive review of the potential for the combination of modern analytical technology with data analytics to generate a new competence for process optimisation are provided. To demonstrate the value of data within the extractive metallurgy discipline, we employ data analytics and simulation to examine gold plant performance and the flotation process in two separate case studies in the second paper (Part II). This was done with the aim of showcasing relevant plant data insights, and extract parameters that should be targeted for plant design and performance optimisation. We identify several promising technologies that integrate well with existing mineral processing plants and testing laboratories to exploit the concept of a dry laboratory, in order to enhance pre-existing mineral processing chains. It also sets the passage in terms of the value of innovative analysis of existing and simulation data as part of the new world of data analytics. Using data- and technology-driven initiatives, we propose the establishment of dry laboratories and data banks to ultimately leverage integrated data, analytics and process simulation for effective plant design and improved performance.;Yousef Ghorbani and Glen T. Nwaila and Steven E. Zhang and Martyn P. Hay and Lunga C. Bam and Pratama Istiadi Guntoro;"Chemistry (miscellaneous) (Q1); Control and Systems Engineering (Q1); Geotechnical Engineering and Engineering Geology (Q1); Mechanical Engineering (Q1)";954.0;481.0;United Kingdom;1988-2020;10.1016/j.mineng.2020.106646;0.01028;108.0;;08926875;08926875;08926875;08926875;4.765;Data analytics, Dry laboratories, Data bank, Legacy metallurgical data;Elsevier Ltd.;;4011.0;Western Europe;1092.0;Q1;16936.0;Minerals engineering;Repurposing legacy metallurgical data part i: a move toward dry laboratories and data bank;18516.0;4768.0;409.0;962.0;16406.0;journal;article;2020
Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing. However, despite the widespread use of NER models, they still require a large-scale labeled data set, which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.;Zhifeng Hao and Di Lv and Zijian Li and Ruichu Cai and Wen Wen and Boyan Xu;"Artificial Intelligence (Q1); Cognitive Neuroscience (Q1)";575.0;1102.0;United Kingdom;1988-2020;10.1016/j.neunet.2020.11.017;0.01942;148.0;;08936080;08936080;18792782;08936080;8.050;Named entity recognition, Semi-supervised learning, Transfer learning, Disentanglement;Elsevier Ltd.;;5301.0;Western Europe;1396.0;Q1;24804.0;Neural networks;Semi-supervised disentangled framework for transferable named entity recognition;18837.0;6106.0;373.0;580.0;19774.0;journal;article;2021
"Objective
To describe PCORnet, a clinical research network developed for patient-centered outcomes research on a national scale.
Study Design and Setting
Descriptive study of the current state and future directions for PCORnet. We conducted cross-sectional analyses of the health systems and patient populations of the 9 Clinical Research Networks and 2 Health Plan Research Networks that are part of PCORnet.
Results
Within the Clinical Research Networks, electronic health data are currently collected from 337 hospitals, 169,695 physicians, 3,564 primary care practices, 338 emergency departments, and 1,024 community clinics. Patients can be recruited for prospective studies from any of these clinical sites. The Clinical Research Networks have accumulated data from 80 million patients with at least one visit from 2009 to 2018. The PCORnet Health Plan Research Network population of individuals with a valid enrollment segment from 2009 to 2019 exceeds 60 million individuals, who on average have 2.63 years of follow-up.
Conclusion
PCORnet’s infrastructure comprises clinical data from a diverse cohort of patients and has the capacity to rapidly access these patient populations for pragmatic clinical trials, epidemiological research, and patient-centered research on rare diseases.";Christopher B. Forrest and Kathleen M. McTigue and Adrian F. Hernandez and Lauren W. Cohen and Henry Cruz and Kevin Haynes and Rainu Kaushal and Abel N. Kho and Keith A. Marsolo and Vinit P. Nair and Richard Platt and Jon E. Puro and Russell L. Rothman and Elizabeth A. Shenkman and Lemuel Russell Waitman and Neely A. Williams and Thomas W. Carton;Epidemiology (Q1);528.0;425.0;United States;1988-2020;10.1016/j.jclinepi.2020.09.036;0.028360000000000003;212.0;;08954356;08954356;18785921;08954356;6.437;Distributed data network, Clinical research network, Health plan research network, PCORnet, Pragmatic clinical trials, Electronic health records, Big data, Learning health system;Elsevier USA;;2825.0;Northern America;2993.0;Q1;15857.0;Journal of clinical epidemiology;Pcornet® 2020: current state, accomplishments, and future directions;36224.0;3242.0;314.0;726.0;8870.0;journal;article;2021
"Despite the progress made during the last two decades in the surgery and chemotherapy of ovarian cancer, more than 70 % of advanced patients are with recurrent cancer and decease. Surgical debulking of tumors following chemotherapy is the conventional treatment for advanced carcinoma, but patients with such treatment remain at great risk for recurrence and developing drug resistance, and only about 30 % of the women affected will be cured. Bevacizumab is a humanized monoclonal antibody, which blocks VEGF signaling in cancer, inhibits angiogenesis and causes tumor shrinkage, and has been recently approved by FDA as a monotherapy for advanced ovarian cancer in combination with chemotherapy. Considering the cost, potential toxicity, and finding that only a portion of patients will benefit from these drugs, the identification of new predictive method for the treatment of ovarian cancer remains an urgent unmet medical need. In this study, we develop weakly supervised deep learning approaches to accurately predict therapeutic effect for bevacizumab of ovarian cancer patients from histopathological hematoxylin and eosin stained whole slide images, without any pathologist-provided locally annotated regions. To the authors’ best knowledge, this is the first model demonstrated to be effective for prediction of the therapeutic effect of patients with epithelial ovarian cancer to bevacizumab. Quantitative evaluation of a whole section dataset shows that the proposed method achieves high accuracy, 0.882 ± 0.06; precision, 0.921 ± 0.04, recall, 0.912 ± 0.03; F-measure, 0.917 ± 0.07 using 5-fold cross validation and outperforms two state-of-the art deep learning approaches Coudray et al. (2018), Campanella et al. (2019). For an independent TMA testing set, the three proposed methods obtain promising results with high recall (sensitivity) 0.946, 0.893 and 0.964, respectively. The results suggest that the proposed method could be useful for guiding treatment by assisting in filtering out patients without positive therapeutic response to suffer from further treatments while keeping patients with positive response in the treatment process. Furthermore, according to the statistical analysis of the Cox Proportional Hazards Model, patients who were predicted to be invalid by the proposed model had a very high risk of cancer recurrence (hazard ratio = 13.727) than patients predicted to be effective with statistical signifcance (p < 0.05).";Ching-Wei Wang and Cheng-Chang Chang and Yu-Ching Lee and Yi-Jia Lin and Shih-Chang Lo and Po-Chao Hsu and Yi-An Liou and Chih-Hung Wang and Tai-Kuang Chao;"Computer Graphics and Computer-Aided Design (Q1); Computer Vision and Pattern Recognition (Q1); Health Informatics (Q1); Radiological and Ultrasound Technology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";188.0;654.0;United Kingdom;1988-2020;10.1016/j.compmedimag.2022.102093;0.0034700000000000004;76.0;;08956111;18790771;08956111;18790771;4.790;Epithelial ovarian cancer, Precision Oncology, Weakly supervised deep learningframework, Whole-slide image analysis, Histopathological Images;Elsevier Ltd.;;4217.0;Western Europe;1033.0;Q1;23607.0;Computerized medical imaging and graphics;Weakly supervised deep learning for prediction of treatment effectiveness on ovarian cancer from histopathology images;3232.0;1316.0;72.0;194.0;3036.0;journal;article;2022
"ABSTRACT
Basic foundations of artificial intelligence (AI) include analyzing large amounts of data, recognizing patterns, and predicting outcomes. At the core of AI are well-defined areas, such as machine learning, natural language processing, artificial neural networks, and computer vision. Although research and development of AI in health care is being conducted in many medical subspecialties, only a few applications have been implemented in clinical practice. This is true in vascular surgery, where applications are mostly in the translational research stage. These AI applications are being evaluated in the realms of vascular diagnostics, perioperative medicine, risk stratification, and outcome prediction, among others. Apart from the technical challenges of AI and research outcomes on safe and beneficial use in patient care, ethical issues and policy surrounding AI will present future challenges for its successful implementation. This review will give a brief overview and a basic understanding of AI and summarize the currently available and used clinical AI applications in vascular surgery.";Uwe M. Fischer and Paula K. Shireman and Judith C. Lin;"Cardiology and Cardiovascular Medicine (Q2); Surgery (Q2)";38.0;75.0;United Kingdom;1990-2020;10.1053/j.semvascsurg.2021.10.008;0.00065;48.0;;08957967;15584518;08957967;15584518;1.000;;W.B. Saunders Ltd;;2341.0;Western Europe;644.0;Q2;22285.0;Seminars in vascular surgery;Current applications of artificial intelligence in vascular surgery;734.0;66.0;17.0;46.0;398.0;journal;article;2021
"Summary
The large diversity of neuron types provides the means by which cortical circuits perform complex operations. Neuron can be described by biophysical and molecular characteristics, afferent inputs, and neuron targets. To quantify, visualize, and standardize those features, we developed the open-source, MATLAB-based framework CellExplorer. It consists of three components: a processing module, a flexible data structure, and a powerful graphical interface. The processing module calculates standardized physiological metrics, performs neuron-type classification, finds putative monosynaptic connections, and saves them to a standardized, yet flexible, machine-readable format. The graphical interface makes it possible to explore the computed features at the speed of a mouse click. The framework allows users to process, curate, and relate their data to a growing public collection of neurons. CellExplorer can link genetically identified cell types to physiological properties of neurons collected across laboratories and potentially lead to interlaboratory standards of single-cell metrics.";Peter C. Petersen and Joshua H. Siegle and Nicholas A. Steinmetz and Sara Mahallati and György Buzsáki;Neuroscience (miscellaneous) (Q1);1435.0;1137.0;United States;1988-2020;10.1016/j.neuron.2021.09.002;0.17522000000000001;473.0;;08966273;10974199;08966273;10974199;17.173;electrophysiology, extracellular electrodes, framework, graphical interface, standardized processing and data structure, single cell analysis;Cell Press;;6036.0;Northern America;9612.0;Q1;17978.0;Neuron;Cellexplorer: a framework for visualizing and characterizing single neurons;111115.0;18448.0;415.0;1456.0;25049.0;journal;article;2021
;Lynn E. Bayne;Critical Care Nursing (Q3);140.0;104.0;United Kingdom;1989-2020;10.1016/j.cnc.2018.07.005;0.00061;29.0;;08995885;08995885;15583481;08995885;1.326;Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics;W.B. Saunders Ltd;;3851.0;Western Europe;320.0;Q3;27632.0;Critical care nursing clinics of north america;Big data in neonatal health care: big reach, big reward?;592.0;188.0;51.0;164.0;1964.0;journal;article;2018
Preclinical imaging studies of posttraumatic epileptogenesis (PTE) have largely been proof-of-concept studies with limited animal numbers, and thus lack the statistical power for biomarker discovery. Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx) is a pioneering multicenter trial investigating preclinical imaging biomarkers of PTE. EpiBios4Rx faced the issue of harmonizing the magnetic resonance imaging (MRI) procedures and imaging data metrics prior to its execution. We present here the harmonization process between three preclinical MRI facilities at the University of Eastern Finland (UEF), the University of Melbourne (Melbourne), and the University of California, Los Angeles (UCLA), and evaluate the uniformity of the obtained MRI data. Adult, male rats underwent a lateral fluid percussion injury (FPI) and were followed by MRI 2 days, 9 days, 1 month, and 5 months post-injury. Ex vivo scans of fixed brains were conducted 7 months post-injury as an end point follow-up. Four MRI modalities were used: T2-weighted imaging, multi-gradient-echo imaging, diffusion-weighted imaging, and magnetization transfer imaging, and acquisition parameters for each modality were tailored to account for the different field strengths (4.7 T and 7 T) and different MR hardwares used at the three participating centers. Pilot data collection resulted in comparable image quality across sites. In interim analysis (of data obtained by April 30, 2018), the within-site variation of the quantified signal properties was low, while some differences between sites remained. In T2-weighted images the signal-to-noise ratios were high at each site, being 35 at UEF, 48 at Melbourne, and 32 at UCLA (p < 0.05). The contrast-to-noise ratios were similar between the sites (9, 10, and 8, respectively). Magnetization transfer ratio maps had identical white matter/ gray matter contrast between the sites, with white matter showing 15% higher MTR than gray matter despite different absolute MTR values (MTR both in white and gray matter was 3% lower in Melbourne than at UEF, p < 0.05). Diffusion-weighting yielded different degrees of signal attenuation across sites, being 83% at UEF, 76% in Melbourne, and 80% at UCLA (p < 0.05). Fractional anisotropy values differed as well, being 0.81 at UEF, 0.73 in Melbourne, and 0.84 at UCLA (p < 0.05). The obtained values in sham animals showed low variation within each site and no change over time, suggesting high repeatability of the measurements. Quality control scans with phantoms demonstrated stable hardware performance over time. Timing of post-TBI scans was designed to target specific phases of the dynamic pathology, and the execution at different centers was highly accurate. Besides a few outliers, the 2-day scans were done within an hour from the target time point. At day 9, most animals were scanned within an hour from the target time point, and all but 2 outliers within 24 h from the target. The 1-month post-TBI scans were done within 31 ± 3 days. MRI procedures and animal physiology during scans were similar between the sites. Taken together, the 10% inter-site difference in FA and 3% difference in MTR values should be included into analysis as a covariate or balanced out in post-processing in order to detect disease-related effects on brain structure at the same scale. However, for a MRI biomarker for post-traumatic epileptogenesis to have realistic chance of being successfully translated to validation in clinical trials, it would need to be a robust TBI-induced structural change which tolerates the inter-site methodological variability described here.;Riikka Immonen and Gregory Smith and Rhys D. Brady and David Wright and Leigh Johnston and Neil G. Harris and Eppu Manninen and Raimo Salo and Craig Branch and Dominique Duncan and Ryan Cabeen and Xavier Ekolle Ndode-Ekane and Cesar Santana Gomez and Pablo M. Casillas-Espinosa and Idrish Ali and Sandy R. Shultz and Pedro Andrade and Noora Puhakka and Richard J. Staba and Terence J. O’Brien and Arthur W. Toga and Asla Pitkänen and Olli Gröhn;"Neurology (clinical) (Q2); Neurology (Q3)";542.0;274.0;Netherlands;1987-2020;10.1016/j.eplepsyres.2019.01.001;0.00773;113.0;;09201211;09201211;18726844;09201211;3.045;Common data element, Diffusion tensor imaging, Magnetization transfer imaging, Multi-site harmonization, Post-traumatic epilepsy, Traumatic brain injury;Elsevier;;4132.0;Western Europe;863.0;Q2;15514.0;Epilepsy research;Harmonization of pipeline for preclinical multicenter mri biomarker discovery in a rat model of post-traumatic epileptogenesis;8587.0;1514.0;180.0;565.0;7437.0;journal;article;2019
This paper describes the core architectures and algorithms of an autonomous small-scale drilling agent. The agent operates in a laboratory rig, demonstrating drilling scenarios with limited or even no human intervention. The work illustrates its performance through self-coordinating state transition, Rate of Penetration (ROP, drilling speed) optimization capability, formation classification, and drilling incidents management. The agent is an original rule-based system, and its control architecture utilizes finite states automation. The novel ROP optimization strategy employs a gradient search in Weight on Bit (WOB)-rotational speed (RPM, Revolutions per Minute) control parameter space. It generates an increasing ROP trend with time and requires re-iteration at abrupt formation changes. Several drilling incidents are managed using ‘if-then’ logic-based activity decomposition. A key learning outcome from the study is the comprehension of the requirement of standard software architecture and Applications Programming Interfaces (API) for continuous research and development of the agent. Such interfaces enhance interoperability between systems and stimulate innovative thinking among independent developers to produce a better-faster set of algorithms. Laboratory testing and evaluation is an essential part of promoting the adaptation of digital technologies for drilling automation. Such studies are a useful, safe, and cost-effective solution for testing, integrating and improving hardware, software, and data management before expensive full-scale testing and integration.;Suranga Chaminda {Hemba Geekiyanage} and Erik A. Loeken and Dan Sui;"Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1)";2752.0;478.0;Netherlands;1987-2021;10.1016/j.petrol.2019.106834;0.026510000000000002;111.0;;09204105;09204105;09204105;09204105;4.346;Drilling systems automation, Autonomous agent, Applied artificial intelligence, ROP optimization, Laboratory rig;Elsevier;;5038.0;Western Europe;975.0;Q1;17013.0;Journal of petroleum science and engineering;Architectures and algorithms of an autonomous small-scale drilling agent;25616.0;13310.0;1174.0;2764.0;59146.0;journal;article;2020
Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.;Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral;"Law (Q1); Hardware and Architecture (Q2); Software (Q2)";243.0;371.0;Netherlands;1985-2020;10.1016/j.csi.2019.103378;;63.0;;09205489;09205489;09205489;09205489;;Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data;Elsevier;;6394.0;Western Europe;556.0;Q1;24303.0;Computer standards and interfaces;Adding value to linked open data using a multidimensional model approach based on the rdf data cube vocabulary;;1013.0;36.0;246.0;2302.0;journal;article;2020
Despite extensive research and prodigious advances in neuroscience, our comprehension of the nature of schizophrenia remains rudimentary. Our failure to make progress is attributed to the extreme heterogeneity of this condition, enormous complexity of the human brain, limitations of extant research paradigms, and inadequacy of traditional statistical methods to integrate or interpret increasingly large amounts of multidimensional information relevant to unravelling brain function. Fortunately, the rapidly developing science of machine learning appears to provide tools capable of addressing each of these impediments. Enthusiasm about the potential of machine learning methods to break the current impasse is reflected in the steep increase in the number of scientific publication about the application of machine learning to the study of schizophrenia. Machine learning approaches are, however, poorly understood by schizophrenia researchers and clinicians alike. In this paper, we provide a simple description of the nature and techniques of machine learning and their application to the study of schizophrenia. We then summarize its potential and constraints with illustrations from six studies of machine learning in schizophrenia and address some common misconceptions about machine learning. We suggest some guidelines for researchers, readers, science editors and reviewers of the burgeoning machine learning literature in schizophrenia. In order to realize its enormous promise, we suggest the need for the disciplined application of machine learning methods to the study of schizophrenia with a clear recognition of its capability and challenges accompanied by a concurrent effort to improve machine learning literacy among neuroscientists and mental health professionals.;Neeraj Tandon and Rajiv Tandon;"Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)";1164.0;348.0;Netherlands;1988-2020;10.1016/j.schres.2019.08.032;0.02779;176.0;;09209964;09209964;15732509;09209964;4.939;Machine learning, Schizophrenia, Methods, Research, Neuroscience, Computational psychiatry, -omics, Big data, Hype, Promise;Elsevier BV;;4827.0;Western Europe;1923.0;Q1;19355.0;Schizophrenia research;Using machine learning to explain the heterogeneity of schizophrenia. realizing the promise and avoiding the hype;26508.0;5380.0;576.0;1463.0;27806.0;journal;article;2019
The assessment of the criticality of raw materials allows the identification of the likelihood of a supply disruption of a material and the vulnerability of a system (e.g. a national economy, technology, or company) to this disruption. Inconclusive outcomes of various studies suggest that criticality assessments would benefit from the identification of best practices. To prepare the field for such guidance, this paper aims to clarify the mechanisms that affect methodological choices which influence the results of a study. This is achieved via literature review and round table discussions among international experts. The paper demonstrates that criticality studies are divergent in the system under study, the anticipated risk, the purpose of the study, and material selection. These differences in goal and scope naturally result in different choices regarding indicator selection, the required level of aggregation as well as the subsequent choice of aggregation method, and the need for a threshold value. However, this link is often weak, which suggests a lack of understanding of cause-and-effect mechanisms of indicators and outcomes. Data availability is a key factor that limits the evaluation of criticality. Furthermore, data quality, including both data uncertainty and data representativeness, is rarely addressed in the interpretation and communication of results. Clear guidance in the formulation of goals and scopes of criticality studies, the selection of adequate indicators and aggregation methods, and the interpretation of the outcomes, are important initial steps in improving the quality of criticality assessments.;Dieuwertje Schrijvers and Alessandra Hool and Gian Andrea Blengini and Wei-Qiang Chen and Jo Dewulf and Roderick Eggert and Layla {van Ellen} and Roland Gauss and James Goddin and Komal Habib and Christian Hagelüken and Atsufumi Hirohata and Margarethe Hofmann-Amtenbrink and Jan Kosmol and Maïté {Le Gleuher} and Milan Grohol and Anthony Ku and Min-Ha Lee and Gang Liu and Keisuke Nansai and Philip Nuss and David Peck and Armin Reller and Guido Sonnemann and Luis Tercero and Andrea Thorenz and Patrick A. Wäger;"Economics and Econometrics (Q1); Waste Management and Disposal (Q1)";1167.0;993.0;Netherlands;1988-2021;10.1016/j.resconrec.2019.104617;;130.0;;09213449;18790658;09213449;18790658;;Critical raw materials, Material criticality, Critical resources, Strategic raw materials, Criticality assessment;Elsevier;;6273.0;Western Europe;2468.0;Q1;26424.0;Resources, conservation and recycling;A review of methods and data to determine raw material criticality;;12067.0;484.0;1190.0;30363.0;journal;article;2020
This conceptual, interdisciplinary paper will start with an introduction to the new-networked knowledge-based global economy and the importance of intellectual and, specifically, human, capital. Next, an advanced definition of human and other forms of capital using information, energy and entropy will be introduced. This will be followed by a discussion of the premises framing the study of economics and will focus on the role of law in the economy. Afterwards, the paper will suggest the addition of a new model of humans that should serve as the base for the concept of law, the homo sustainabiliticus. Ensuing this discussion and consistent with the newly proposed definition of capital, a proposal for a new currency (“new gold”) will be offered. This proposal suggests viewing usable, renewable energy, knowledge and data as the most important assets for the 21st century and is seen as the building block for the new sustainabilistic economy.;Meir Russ;"Economics and Econometrics (Q1); Environmental Science (miscellaneous) (Q1)";1001.0;518.0;Netherlands;1989-2021;10.1016/j.ecolecon.2016.07.013;0.0219;202.0;;09218009;09218009;09218009;09218009;5.389;New networked knowledge-based global economy, Intellectual capital, Human capital, Entropy, , “New gold”, Sustainabilism;Elsevier;;7066.0;Western Europe;1917.0;Q1;20290.0;Ecological economics;The probable foundations of sustainabilism: information, energy and entropy based definition of capital, homo sustainabiliticus and the need for a “new gold”;32196.0;5742.0;268.0;1028.0;18938.0;journal;article;2016
Global warming may increase the frequency of climate extremes, but systematic examinations at different temperature thresholds are unknown over the Tibetan Plateau (TP). Changes in surface temperature and precipitation extreme indices derived from a multi-model ensemble mean (MMEM) of the Coupled Model Inter-comparison Project Phase 5 (CMIP5) models are examined under global warming of 1.5 °C (RCP2.6), 2 °C (RCP4.5) and 3 °C (RCP8.5) above pre-industrial levels. The TP amplification of future temperature and precipitation changes is evident for all three scenarios, with greater trend magnitudes in extreme indices than those for the whole China, regions between 25°N and 40°N, Northern Hemisphere (land only), Northern Hemisphere and the global mean. The TP amplification is also projected to intensify in each scenario, resulting in faster changes in intensity, duration and frequency of climate extremes. There appears to be greater difference for precipitation-based indices between 2 °C and 3 °C than for temperature, and the differences between 1.5 °C and 2 °C are less dramatic. Overall changes in climate extremes at 2 °C are greater than at 1.5 °C, but differences are less discernible between 3 °C and 2 °C. The Kolmogorov-Smirnov test between simulated and scaled temperature distributions shows that accelerated warming over the TP from 1.5 °C to 2 °C follows a broadly linear response, but the nonlinearity occurs between 2 °C and 3 °C. This suggests that the rate of warming might make a large difference to the future TP amplification at different thresholds.;Qinglong You and Fangying Wu and Liucheng Shen and Nick Pepin and Zhihong Jiang and Shichang Kang;"Global and Planetary Change (Q1); Oceanography (Q1)";506.0;490.0;Netherlands;1989-2020;10.1016/j.gloplacha.2020.103261;0.013930000000000001;133.0;;09218181;09218181;09218181;09218181;5.114;Tibetan Plateau, CMIP5, 1.5 °C, 2 °C and 3 °C, Linearity analysis;Elsevier;;9203.0;Western Europe;1706.0;Q1;27049.0;Global and planetary change;Tibetan plateau amplification of climate extremes under global warming of 1.5 °c, 2 °c and 3 °c;13351.0;2726.0;204.0;518.0;18775.0;journal;article;2020
"Disagreement and confusion about artificial intelligence (AI) terminology impede researchers, innovators, and practitioners when developing and implementing enterprise applications. The prevailing ambiguities and use of buzzwords are exacerbated by media and vendor marketing hype. This study identifies several ambiguities within and across AI fields and subfields. Combining a systematic review with a sequential mixed-models design, a total of 26,143 publications were reviewed and mapped, making this the largest conceptual study in the AI field. A unified framework is proposed as an Euler diagram to bring about clarity through a ""common language"" for AI researchers, innovators, and practitioners.";Heinz Herrmann;"Engineering (miscellaneous) (Q1); Industrial Relations (Q1); Information Systems and Management (Q1); Strategy and Management (Q1); Management Science and Operations Research (Q2)";69.0;385.0;Netherlands;1989-2020;10.1016/j.jengtecman.2022.101716;;65.0;;09234748;16084799;09234748;16084799;;Artificial intelligence, Unified framework, Systematic review, Science mapping, Systematic science mapping;Elsevier;;8716.0;Western Europe;833.0;Q1;20575.0;Journal of engineering and technology management - jet-m;The arcanum of artificial intelligence in enterprise applications: toward a unified framework;;281.0;25.0;79.0;2179.0;journal;article;2022
Today's manufacturing processes are pushed to their limits to generate products with ever-increasing quality at low costs. A prominent hurdle on this path arises from the multiscale, multiphysics, dynamic, and stochastic nature of many manufacturing systems, which motivated many innovations at the intersection of artificial intelligence (AI), data analytics, and manufacturing sciences. This study reviews recent advances in Mechanistic-AI, defined as a methodology that combines the raw mathematical power of AI methods with mechanism-driven principles and engineering insights. Mechanistic-AI solutions are systematically analyzed for three aspects of manufacturing processes, i.e., modeling, design, and control, with a focus on approaches that can improve data requirements, generalizability, explainability, and capability to handle challenging and heterogeneous manufacturing data. Additionally, we introduce a corpus of cutting-edge Mechanistic-AI methods that have shown to be very promising in other scientific fields but yet to be applied in manufacturing. Finally, gaps in the knowledge and under-explored research directions are identified, such as lack of incorporating manufacturing constraints into AI methods, lack of uncertainty analysis, and limited reproducibility and established benchmarks. This paper shows the immense potential of the Mechanistic-AI to address new problems in manufacturing systems and is expected to drive further advancements in manufacturing and related fields.;Mojtaba Mozaffar and Shuheng Liao and Xiaoyu Xie and Sourav Saha and Chanwook Park and Jian Cao and Wing Kam Liu and Zhengtao Gan;"Ceramics and Composites (Q1); Computer Science Applications (Q1); Industrial and Manufacturing Engineering (Q1); Metals and Alloys (Q1); Modeling and Simulation (Q1)";1367.0;589.0;Netherlands;1990-2021;10.1016/j.jmatprotec.2021.117485;0.02301;190.0;;09240136;09240136;18734774;09240136;5.551;Scientific data science, Deep learning, Additive manufacturing, Physics-informed machine learning, Data-driven discovery, Data-driven design;Elsevier BV;;2824.0;Western Europe;1736.0;Q1;20972.0;Journal of materials processing technology;Mechanistic artificial intelligence (mechanistic-ai) for modeling, design, and control of advanced manufacturing processes: current state and perspectives;41944.0;8479.0;408.0;1369.0;11521.0;journal;article;2022
"Background
Meat packaging and intelligent evaluation and monitoring of key parameters not only are important technologies to ensure meat quality and safety but also form the key foundation for optimizing packaging materials and improving the efficiency of cold chain operations. In recent years, numerous studies have focused on comprehensive (or multi-functional) packaging materials, multiple parameter evaluation methods, quality intelligent monitoring technology, and optimization of the control of various links in cold chain logistics (CCL). Such research has significant practical application value for extending meat shelf-life and reducing the risk of foodborne diseases.
Scope and approach
This paper reviews the current research status, existing problems, and future evolution of CCL by focusing on meat packaging, meat quality evaluation and monitoring, and meat quality prediction and control. We also elaborate in detail the challenges faced in researching these topics and discuss the focal points of future research aiming to improve the quality and efficiency of CCL.
Key findings and conclusions
Packaging material optimization and dynamic quality perception are vital for achieving meat quality and safety over the entire CCL and demand the digital and intelligent development of the meat cold chain. A key finding of this review is that the comprehensive (or composite) packaging and intelligent quality assessment and monitoring are important forces promoting the transformation of traditional meat CCL to smart, green, and efficient CLL involving the intelligent management and control of all links therein.";Qing-Shan Ren and Kui Fang and Xin-Ting Yang and Jia-Wei Han;"Biotechnology (Q1); Food Science (Q1)";670.0;1190.0;United Kingdom;1990-2020;10.1016/j.tifs.2021.12.006;;188.0;;09242244;09242244;09242244;09242244;;Meat, Cold chain logistics, Packaging, Quality perception, Intelligent development;Elsevier Ltd.;;10657.0;Western Europe;2676.0;Q1;22475.0;Trends in food science and technology;Ensuring the quality of meat in cold chain logistics: a comprehensive review;;9617.0;363.0;730.0;38684.0;journal;article;2022
"The Near Surface Concentrations (NSC) of O3, CO, and NO2 are crucial worldwide indicators of air quality. However, current frameworks devised for the estimation of the NSC of O3, CO, and NO2 have defects, such as coarse spatial resolution and large missing coverage. To address this issue, this study aims to estimate the daily (~13:30 local time) full-coverage NSC of O3, CO, and NO2 at a high spatial resolution (0.05° for O3 and NO2; 0.07° for CO) over China by using datasets from S5P-TROPOMI and GEOS-FP. In specific, the light gradient boosting machine is employed to train the estimation models. Validation results show that the NSC of O3, CO, and NO2 are well estimated, with the R2s of 0.91, 0.71, and 0.83 for the sample-based cross validation, respectively. Meanwhile, the proposed framework achieves a satisfactory performance in comparison to the latest related works, as reflected by the estimation accuracy and spatial resolution. As for the mapping, the estimated results show coherent spatial distribution and can accurately grasp the seasonal characteristics of each air pollutant. Finally, the estimated results are utilized to analyze the temporal variations of O3, CO, and NO2 during the COrona VIrus Disease 2019 (COVID-19) lockdown in China, which is an extend application for adopting the proposed framework in air quality monitoring. Results show that the estimated NSC of O3, CO, and NO2 in 2020 present significant variations during different periods of the COVID-19 lockdown in China compared to last year. In addition, the variations in the NSC of O3, CO, and NO2 during the COVID-19 lockdown in China possibly result from restrictions in the anthropogenic activities.";Yuan Wang and Qiangqiang Yuan and Tongwen Li and Liye Zhu and Liangpei Zhang;"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)";668.0;1056.0;Netherlands;1989-2020;10.1016/j.isprsjprs.2021.03.018;0.02145;138.0;;09242716;09242716;09242716;09242716;8.979;Full-coverage, Near surface concentrations, Air quality, S5P-TROPOMI, GEOS-FP, COVID-19;Elsevier;;6104.0;Western Europe;2960.0;Q1;29161.0;Isprs journal of photogrammetry and remote sensing;Estimating daily full-coverage near surface o3, co, and no2 concentrations at a high spatial resolution over china based on s5p-tropomi and geos-fp;18026.0;7306.0;264.0;677.0;16114.0;journal;article;2021
"Background
Public health monitoring is commonly undertaken in social media but has never been combined with data analysis from electronic health records. This study aimed to investigate the relationship between the emergence of novel psychoactive substances (NPS) in social media and their appearance in a large mental health database.
Methods
Insufficient numbers of mentions of other NPS in case records meant that the study focused on mephedrone. Data were extracted on the number of mephedrone (i) references in the clinical record at the South London and Maudsley NHS Trust, London, UK, (ii) mentions in Twitter, (iii) related searches in Google and (iv) visits in Wikipedia. The characteristics of current mephedrone users in the clinical record were also established.
Results
Increased activity related to mephedrone searches in Google and visits in Wikipedia preceded a peak in mephedrone-related references in the clinical record followed by a spike in the other 3 data sources in early 2010, when mephedrone was assigned a ‘class B’ status. Features of current mephedrone users widely matched those from community studies.
Conclusions
Combined analysis of information from social media and data from mental health records may assist public health and clinical surveillance for certain substance-related events of interest. There exists potential for early warning systems for health-care practitioners.";A. Kolliakou and M. Ball and L. Derczynski and D. Chandran and G. Gkotsis and P. Deluca and R. Jackson and H. Shetty and R. Stewart;Psychiatry and Mental Health (Q1);356.0;444.0;United Kingdom;1991-2020;10.1016/j.eurpsy.2016.05.006;0.010159999999999999;96.0;;09249338;09249338;17783585;09249338;5.361;Novel psychoactive substances, Mephedrone, Electronic health records, Social media, Public health monitoring;Cambridge University Press;;1208.0;Western Europe;1840.0;Q1;15578.0;European psychiatry;Novel psychoactive substances: an investigation of temporal trends in social media and electronic health records;7865.0;1798.0;106.0;397.0;1280.0;journal;article;2016
Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.;Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov;"Neurology (Q1); Neurology (clinical) (Q1); Pharmacology (Q1); Pharmacology (medical) (Q1); Psychiatry and Mental Health (Q1); Biological Psychiatry (Q2)";371.0;385.0;Netherlands;1990-2020;10.1016/j.euroneuro.2015.05.011;0.01119;112.0;;0924977X;0924977X;18737862;0924977X;4.600;Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical;Elsevier;;6310.0;Western Europe;1603.0;Q1;15576.0;European neuropsychopharmacology;The preclinical data forum network: a new ecnp initiative to improve data quality and robustness for (preclinical) neuroscience;8999.0;1610.0;154.0;383.0;9717.0;journal;article;2015
In this paper, an explainable artificial intelligence (AI) technique is employed to analyze the match style and gameplay of the national basketball association (NBA). A descriptive analysis on the evolution of the NBA gameplay is conducted by using clustering and principal component analysis. Supervised-learning based AI models (including the random forest and the feed-forward neural network) are applied to produce accurate predictions on NBA outcomes at a season-by-season and a month-by-month basis. To evaluate the interpretability of the established AI models, an explainable AI algorithm is utilized to deduce and assess the precise reasoning behind the model prediction based on the local interpretable model-agnostic explanation method. To illustrate its application potential, the method is applied to the open-source NBA data from 1980 to 2019. Experimental results demonstrate the effectiveness of the introduced explainable AI algorithm on predicting NBA outcomes with interpretation.;Yuanchen Wang and Weibo Liu and Xiaohui Liu;"Artificial Intelligence (Q1); Computer Science Applications (Q1); Cognitive Neuroscience (Q2)";3535.0;708.0;Netherlands;1989-2020;10.1016/j.neucom.2022.01.098;0.06669;143.0;;09252312;09252312;09252312;09252312;5.719;Data science, Explainable artificial intelligence, NBA, Clustering, Regression;Elsevier;;4768.0;Western Europe;1085.0;Q1;24807.0;Neurocomputing;Explainable ai techniques with application to nba gameplay prediction;46751.0;25554.0;1653.0;3586.0;78823.0;journal;article;2022
"A meta-learning algorithm, conventionally used for visual recognition, was applied to the recognition and classification of aroma oils. A printable chemiresistive sensor array was fabricated, based on composites of carbon black with various active materials. Standard aromatherapy kits with 30 types of essential oils were used as targets in an odor sensing experiment. Benefiting from the pattern recognition ability of the fabricated sensor array, a high-quality dataset was obtained with 30 aroma oil classes, in which each class had nine replicate samples. A deep metric learning model, based on a Siamese neural network and a multilayer perceptron, was used to perform the N-way k-shot meta-learning. A test accuracy of over 98.7% was obtained for 31-way 9-shot learning, on discriminating whether the input pair samples were taken from similar or dissimilar classes. The model was effective in extracting meta-features of the aroma oils; this was proved by the improved clustering effect of samples in the spaces of principal components analysis and t-distributed stochastic neighbor embedding. The 30 aroma oils were divided into two datasets according to 6-fold cross-validation: 25 aroma oil classes (plus one blank class) as seen classes for constructing 26-way 9-shot learning models and the remaining five aroma oils as unseen classes for prediction. Average accuracies of 93.5% and 93.9% were achieved for recognition of the unseen aroma oils from the seen classes and classification of the unseen aroma oils themselves, respectively, demonstrating the effectiveness of the developed sensor and model for odor recognition and classification.";Chuanjun Liu and Hitoshi Miyauchi and Kenshi Hayashi;"Condensed Matter Physics (Q1); Electrical and Electronic Engineering (Q1); Electronic, Optical and Magnetic Materials (Q1); Instrumentation (Q1); Materials Chemistry (Q1); Metals and Alloys (Q1); Surfaces, Coatings and Films (Q1)";6556.0;734.0;Netherlands;1970, 1990-2021;10.1016/j.snb.2021.130960;;197.0;;09254005;09254005;09254005;09254005;;Chemiresistive sensor array, Meta learning, Deep metric learning, Siamese network, Aroma oil, Recognition and classification;Elsevier;;4627.0;Western Europe;1601.0;Q1;25236.0;Sensors and actuators, b: chemical;Deepsniffer: a meta-learning-based chemiresistive odor sensor for recognition and classification of aroma oils;;46369.0;1444.0;6558.0;66816.0;journal;article;2022
"Glucocorticoids reduce phobic fear in anxiety disorders and enhance psychotherapy, possibly by reducing the retrieval of fear memories and enhancing the consolidation of new corrective memories. Glucocorticoid signaling in the basolateral amygdala can influence connected fear and memory-related cortical regions, but this is not fully understood. Previous studies investigated specific pathways moderated by glucocorticoids, for example, visual-temporal pathways; however, these analyses were limited to a-priori selected regions. Here, we performed whole-brain pattern analysis to localize phobic stimulus decoding related to the fear-reducing effect of glucocorticoids. We reanalyzed functional magnetic resonance imaging (fMRI) data from a previously published study with spider-phobic patients and healthy controls. The patients received glucocorticoids or a placebo before the exposure to spider images. There was moderate evidence that patients with phobia had higher decoding of phobic content in the anterior cingulate cortex (ACC) and the left and right anterior insula compared to controls. Decoding in the ACC and the right insula showed strong evidence for correlation with experienced fear. Patients with cortisol reported a reduction of fear by 10–13%; however, there was only weak evidence for changes in neural decoding compared to placebo which was found in the precuneus, the opercular cortex, and the left cerebellum.";Simon Schwab and Andrea Federspiel and Yosuke Morishima and Masahito Nakataki and Werner Strik and Roland Wiest and Markus Heinrichs and Dominique {de Quervain} and Leila M. Soravia;"Radiology, Nuclear Medicine and Imaging (Q1); Neuroscience (miscellaneous) (Q2); Psychiatry and Mental Health (Q2)";386.0;240.0;Ireland;1990-2020;10.1016/j.pscychresns.2020.111066;;108.0;;09254927;18727506;09254927;18727506;;Phobia, Anxiety disorder, Cortisol, fMRI, Pattern analysis;Elsevier Ireland Ltd;;5717.0;Western Europe;1030.0;Q1;18730.0;Psychiatry research - neuroimaging;Glucocorticoids and cortical decoding in the phobic brain;;1046.0;105.0;389.0;6003.0;journal;article;2020
The diary food factories in Japan are facing serious challenges of severe labor shortage and the increased diversity of demand. Food manufacturing companies are forced to improve factories to be more productive and flexible to deal with the expanding market scale in the future and also the product diversity. To improve the productivity and the flexibility, automation technologies have been implemented in manufacturing system with the popularization of Industrial 4.0 and Smart Factory. Based on the actual system construction practice of a dairy factory which is as a case study, this paper proposes a five-level horizontal model with automation technologies, aiming to realize high efficiency, rapid integration and relocation of the manufacturing system. This paper introduces the composition, the specifications and the functions of the horizontal model, and evaluates the function of each level. Finally, through the case study and numerical comparison on cost and labor hours, we verify the superiority of the proposed horizontal hierarchical system model for food factories.;Takao Matsumoto and Yijun Chen and Akihiro Nakatsuka and Qunzhi Wang;"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)";881.0;831.0;Netherlands;1991-2021;10.1016/j.ijpe.2020.107616;0.0228;185.0;;09255273;09255273;09255273;09255273;7.885;Dairy product, Industry 4.0, Smart factory, Operation technology, Plant control system, Manufacturing execution system;Elsevier;;6640.0;Western Europe;2406.0;Q1;19165.0;International journal of production economics;Research on horizontal system model for food factories: a case study of process cheese manufacturer;32606.0;8124.0;327.0;891.0;21712.0;journal;article;2020
Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.;Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz;"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)";1020.0;549.0;Netherlands;1991-2020;10.1016/j.ssci.2021.105569;0.01488;111.0;;09257535;09257535;09257535;09257535;4.877;Safety analytics, Data analytics, Readiness assessment, Occupational health;Elsevier;;5826.0;Western Europe;1178.0;Q1;12332.0;Safety science;Advancing safety analytics: a diagnostic framework for assessing system readiness within occupational safety and health;17184.0;5909.0;451.0;1047.0;26276.0;journal;article;2022
The needs to ground construction safety-related decisions under uncertainty on knowledge extracted from objective, empirical data are pressing. Although construction research has considered machine learning (ML) for more than two decades, it had yet to be applied to safety concerns. We applied two state-of-the-art ML models, Random Forest (RF) and Stochastic Gradient Tree Boosting (SGTB), to a data set of carefully featured attributes and categorical safety outcomes, extracted from a large pool of textual construction injury reports via a highly accurate Natural Language Processing (NLP) tool developed by past research. The models can predict injury type, energy type, and body part with high skill (0.236<RPSS<0.436), outperforming the parametric models found in the literature. The high predictive skill reached suggests that injuries do not occur at random, and that therefore construction safety should be studied empirically and quantitatively rather than strictly being approached through the analysis of subjective data, expert opinion, and with a regulatory and managerial perspective. This opens the gate to a new research field, where construction safety is considered an empirically grounded quantitative science. Finally, the absence of predictive skill for the output variable injury severity suggests that unlike other safety outcomes, injury severity is mainly random, or that extra layers of predictive information should be used in making predictions, like the energy level in the environment. In the context of construction safety analysis, this study makes important strides in that the results provide reliable probabilistic forecasts of likely outcomes should an accident occur, and show great potential for integration with building information modeling and work packaging due to the binary and physical nature of the input variables. Such data-driven predictions had been absent from the field since its inception.;Antoine J.-P. Tixier and Matthew R. Hallowell and Balaji Rajagopalan and Dean Bowman;"Building and Construction (Q1); Civil and Structural Engineering (Q1); Control and Systems Engineering (Q1)";778.0;916.0;Netherlands;1992-2020;10.1016/j.autcon.2016.05.016;0.013090000000000001;121.0;;09265805;09265805;09265805;09265805;7.700;Machine learning, Construction safety, Predictive modeling, Injury prevention, Random Forest, Boosting, Attribute;Elsevier;;5624.0;Western Europe;1837.0;Q1;24931.0;Automation in construction;Application of machine learning to construction injury prediction;16738.0;7433.0;364.0;779.0;20472.0;journal;article;2016
The integration of Bacillus thuringiensis (Bt) genes is often accompanied by unintended effects along with the improved resistance to targeted pests. The insertion information of transfer DNA (T-DNA) and the expression information of upstream and downstream genes are of great significance for related research on unexpected effects and molecular-level mechanisms. In this study, six dual Bt transgenic Populus × euramericana cv. Neva (poplar 107) lines were used as research objects. We determined growth and physiological indices, and characterized the T-DNA integration using next-generation sequencing technology. The transgenic and non-transgenic lines showed no significant difference in growth index. However, while insect resistance was enhanced, effects related to the integration sites of the Bt gene occurred. A total of 15 insertion sites were detected among the six transgenic lines, and T-DNA preferentially inserted into AT-rich regions of the poplar genome. RNA sequencing and weighted gene co-expression network analysis were used to explore the effects of T-DNA insertion on the gene expression of poplar 107. As a result, the metabolic pathways most affected by the Bt gene were starch and sucrose metabolism and pentose and gluconate conversion. The expression of exogenous Bt had a greater impact than the insertion position or copy number on poplar 107 gene expression, and the Cry1Ac toxin protein also played a major role. This study provides theoretical support for the cultivation and management of poplar 107 new varieties, and provides reference for Poplar insect resistance breeding and its molecular response to Bt gene.;Xinglu Zhou and Yachao Ren and Shijie Wang and Xinghao Chen and Chao Zhang and Minsheng Yang and Yan Dong;Agronomy and Crop Science (Q1);2660.0;562.0;Netherlands;1992-2020;10.1016/j.indcrop.2022.114636;0.03431;129.0;;09266690;09266690;09266690;09266690;5.645; gene, Poplar 107, T-DNA preferentially inserts, RNA-seq, Unintended effect;Elsevier;;5336.0;Western Europe;1066.0;Q1;32791.0;Industrial crops and products;T-dna integration and its effect on gene expression in dual bt gene transgenic populus ×euramericana cv. neva;41582.0;15080.0;988.0;2663.0;52718.0;journal;article;2022
The ambient noise in controlled-source electromagnetic (CSEM) data seriously affects the accuracy and reliability of the exploration result. Traditional correlation-based data selection method requires manually setting the threshold. To overcome the deficiency, we analyze the typical noises in CSEM data and find that normalized cross-correlation (NCC), absolute maximum value of the amplitude (Max), and detrend fluctuation analysis (DFA) can be used to accurately identify high-quality time series. Based on this discovery, we replace traditional manually intervention with unsupervised machine learning and propose a novel CSEM data processing method. We applied the newly proposed method to synthetic and measured CSEM data to verify the feasibility and effectiveness. Experimental results demonstrate that the newly proposed method is superior to the conventional data selection method because it accurately selects the best data fragments from noisy data automatically. The newly proposed method requires no human intervention which makes the results obtained free of subjective distortion caused by the operator.;Guang Li and Zhushi He and Juzhi Deng and Jingtian Tang and Youyao Fu and Xiaoqiong Liu and Changming Shen;Geophysics (Q2);750.0;209.0;Netherlands;1992-2020;10.1016/j.jappgeo.2021.104262;0.00758;82.0;;09269851;09269851;09269851;09269851;2.121;CSEM data processing, Periodic signal de-noising, Signal-noise identification, Fuzzy -means clustering (FCM), Machine Learning, Correlation Analysis;Elsevier;;3955.0;Western Europe;627.0;Q2;28435.0;Journal of applied geophysics;Robust csem data processing by unsupervised machine learning;6990.0;1705.0;243.0;751.0;9610.0;journal;article;2021
Matching-type estimators using the propensity score are the major workhorse in active labour market policy evaluation. This work investigates if machine learning algorithms for estimating the propensity score lead to more credible estimation of average treatment effects on the treated using a radius matching framework. Considering two popular methods, the results are ambiguous: We find that using LASSO based logit models to estimate the propensity score delivers more credible results than conventional methods in small and medium sized high dimensional datasets. However, the usage of Random Forests to estimate the propensity score may lead to a deterioration of the performance in situations with a low treatment share. The application reveals a positive effect of the training programme on days in employment for long-term unemployed. While the choice of the “first stage” is highly relevant for settings with low number of observations and few treated, machine learning and conventional estimation becomes more similar in larger samples and higher treatment shares.;Daniel Goller and Michael Lechner and Andreas Moczall and Joachim Wolff;"Economics and Econometrics (Q1); Organizational Behavior and Human Resource Management (Q1)";267.0;175.0;Netherlands;1993-2020;10.1016/j.labeco.2020.101855;0.00733;75.0;;09275371;09275371;09275371;09275371;1.772;Programme evaluation, active labour market policy, causal machine learning, treatment effects, radius matching, propensity score;Elsevier;;4949.0;Western Europe;1899.0;Q1;16922.0;Labour economics;Does the estimation of the propensity score by machine learning improve matching estimation? the case of germany's programmes for long term unemployed;4051.0;592.0;116.0;270.0;5741.0;journal;article;2020
This study explores perceptions of the impact of Ball and Brown (1968) (BB68) upon accounting research, education, standards-setting, and practice. These perceptions are gathered from an extensive literature review and from 27 interviewees drawn from academe, practice and standards-setting from around the globe. The study reports upon the genesis of BB68, it's disruptive effect on accounting research methodology and methods, and its broader impact on accounting education, practice and standards-setting. Reasons are provided for the popularity of BB68 and an exploration is undertaken of both the positive and negative impacts of the research methodology inspired by BB68. The characteristics of the Ball and Brown research team are also discussed and the paper finishes with perceptions regarding the future relevance of BB68. It is concluded that BB68 stands as an exemplar for innovation and for quality in the execution of accounting and finance research but that much of that innovative spirit has been lost as the BB68 methodology became mainstream.;Bryan A. Howieson;"Economics and Econometrics (Q2); Finance (Q2)";334.0;271.0;Netherlands;1993-2020;10.1016/j.pacfin.2019.05.003;;58.0;;0927538X;0927538X;0927538X;0927538X;;Accounting practices, Accounting education, Ball and Brown 1968, Finance practices, Normative research, Positive research, Research impact, Research methodology, Research paradigms, Standards-setting;Elsevier;;5248.0;Western Europe;697.0;Q2;15465.0;Pacific basin finance journal;Frankenstein's monster or the birth of venus? perceptions of the impact and contributions of ball and brown 1968;;1055.0;182.0;336.0;9551.0;journal;article;2019
;;Pharmaceutical Science (Q1);1326.0;423.0;Netherlands;1993-2020;10.1016/S0928-0987(17)30095-7;0.01375;136.0;;09280987;18790720;09280987;18790720;4.384;;Elsevier;;5458.0;Western Europe;840.0;Q1;21331.0;European journal of pharmaceutical sciences;News on pswc 2017;18169.0;5912.0;371.0;1337.0;20251.0;journal;article;2017
This paper reviews recent research on the causes and consequence of different forms of financial market misconduct and potential agency conflicts and the impact of regulating financial market misconduct. We examine regulatory responses to financial market misconduct and highlight the presence of complementarities in financial market misconduct regulation and enforcement. We feature papers that make use of natural experiments, rule changes, and market design changes. Further, the interdisciplinary nature of financial market misconduct research is highlighted, and potential avenues for future research are discussed.;Douglas Cumming and Robert Dannhauser and Sofia Johan;"Business and International Management (Q1); Economics and Econometrics (Q1); Finance (Q1); Strategy and Management (Q1)";391.0;431.0;Netherlands;1994-2020;10.1016/j.jcorpfin.2015.07.016;0.01112;101.0;;09291199;09291199;09291199;09291199;4.249;Financial market misconduct, Fraud, Insider trading, Regulation, Enforcement, Law and finance;Elsevier;;6798.0;Western Europe;1894.0;Q1;17496.0;Journal of corporate finance;Financial market misconduct and agency conflicts: a synthesis and future directions;9473.0;1943.0;246.0;395.0;16724.0;journal;article;2015
;Chun-Ju Chiang and Ying-Wei Wang and Wen-Chung Lee;Medicine (miscellaneous) (Q2);512.0;225.0;China;1961-1962, 1972-2020;10.1016/j.jfma.2019.01.012;0.00513;54.0;;09296646;09296646;18760821;09296646;3.282;;Excerpta Medica Asia Ltd.;;2803.0;Asiatic Region;708.0;Q2;27952.0;Journal of the formosan medical association;Taiwan's nationwide cancer registry system of 40 years: past, present, and future;5341.0;1377.0;340.0;615.0;9529.0;journal;article;2019
Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.;Jean-Baptiste Lamy;"Artificial Intelligence (Q1); Medicine (miscellaneous) (Q1)";237.0;669.0;Netherlands;1989-2020;10.1016/j.artmed.2021.102074;0.004220000000000001;87.0;;09333657;09333657;18732860;09333657;5.326;Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug events, Pain treatments, Painkillers;Elsevier;;5494.0;Western Europe;980.0;Q1;24140.0;Artificial intelligence in medicine;A data science approach to drug safety: semantic and visual mining of adverse drug events from clinical trials of pain treatments;4245.0;1746.0;165.0;245.0;9065.0;journal;article;2021
Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.;J.W. Wang and M. Williams;"Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)";304.0;247.0;United Kingdom;1975, 1984-1985, 1989-2020;10.1016/j.clon.2021.11.040;0.005379999999999999;76.0;;09366555;14332981;09366555;14332981;4.126;Artificial intelligence, Big Data, database, deep learning, registries, repository;W.B. Saunders Ltd;;3226.0;Western Europe;1037.0;Q1;29271.0;Clinical oncology;Registries, databases and repositories for developing artificial intelligence in cancer care;4553.0;1127.0;180.0;455.0;5806.0;journal;article;2022
OrBiTo was a precompetitive collaboration focused on the development of the next generation of Oral Biopharmaceutics Tools. The consortium included world leading scientists from nine universities, one regulatory agency, one non-profit research organisation, three small/medium sized specialist technology companies together with thirteen pharmaceutical companies. The goal of the OrBiTo project was to deliver a framework for rational application of predictive biopharmaceutics tools for oral drug delivery. This goal was achieved through novel prospective investigations to define new methodologies or refinement of existing tools. Extensive validation has been performed of novel and existing biopharmaceutics tools using historical datasets supplied by industry partners as well as laboratory ring studies. A combination of high quality in vitro and in vivo characterizations of active drugs and formulations have been integrated into physiologically based in silico biopharmaceutics models capturing the full complexity of gastrointestinal drug absorption and some of the best practices has been highlighted. This approach has given an unparalleled opportunity to deliver transformational change in European industrial research and development towards model based pharmaceutical product development in accordance with the vision of model-informed drug development.;B. Abrahamsson and M. McAllister and P. Augustijns and P. Zane and J. Butler and R. Holm and P. Langguth and A. Lindahl and A. Müllertz and X. Pepin and A. Rostami-Hodjegan and E. Sjögren and M. Berntsson and H. Lennernäs;"Biotechnology (Q1); Medicine (miscellaneous) (Q1); Pharmaceutical Science (Q1)";844.0;535.0;Netherlands;1991-2020;10.1016/j.ejpb.2020.05.008;0.01324;158.0;;09396411;18733441;09396411;18733441;5.571;Biopharmaceutics, PBPK, IVIVC, Dissolution, Drug absorption, Permeability;Elsevier;;5607.0;Western Europe;1103.0;Q1;21332.0;European journal of pharmaceutics and biopharmaceutics;Six years of progress in the oral biopharmaceutics area – a summary from the imi orbito project;20326.0;4755.0;259.0;849.0;14522.0;journal;article;2020
Structural health monitoring (SHM) is a method used to evaluate the performance of new structural systems and critical infrastructure. With mass-timber building construction on the rise, SHM programs have emerged to document hygrothermal, static, and dynamic behavior of these structures. To most efficiently document behavior and provide recommendations to industry, it is key that the research community work collaboratively to create consistent data by using standardized approaches. This paper presents a methodological approach for monitoring mass-timber buildings during construction to address this need. The approach was validated over ten months with a mass-timber building under construction at Oregon State University.;Esther J. Baas and Mariapaola Riggio and André R. Barbosa;"Building and Construction (Q1); Civil and Structural Engineering (Q1); Materials Science (miscellaneous) (Q1)";7705.0;650.0;United Kingdom;1987-2020;10.1016/j.conbuildmat.2020.121153;0.09849;170.0;;09500618;09500618;09500618;09500618;6.141;Big data, Cross-laminated timber, Construction monitoring, Mass plywood panel, Mass-timber, Self-centering rocking wall, Structural health monitoring;Elsevier Ltd.;;4899.0;Western Europe;1662.0;Q1;24443.0;Construction and building materials;A methodological approach for structural health monitoring of mass-timber buildings under construction;123941.0;52599.0;3583.0;7707.0;175521.0;journal;article;2021
As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.;Leah M. Hamilton and Jacob Lahne;"Food Science (Q1); Nutrition and Dietetics (Q1)";548.0;543.0;United Kingdom;1988-1991, 1993-2021;10.1016/j.foodqual.2020.103926;0.00829;120.0;;09503293;09503293;09503293;09503293;5.565;Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning;Elsevier Ltd.;;5414.0;Western Europe;1135.0;Q1;23161.0;Food quality and preference;Fast and automated sensory analysis: using natural language processing for descriptive lexicon development;13058.0;3000.0;227.0;558.0;12290.0;journal;article;2020
Emerging sensors, computers, network technologies, and connected platforms result potentially in an immeasurable collection of data within plant operations. This creates the possibility of solving problems innovatively. Because most of the data appear to be unstructured or semi-structured, organizations shall design and adopt new strategies. Further, workflow architectures with data analytics are needed including machine learning tools and artificial intelligence techniques before proto-type solutions can be developed. We shall discuss several prospects of using (big) data analytics integrated with cloud services to produce solutions for improving plant operations. The paper outlines the vision and a systematic framework highlighting the data analytics lifecycle in the area of plant operation, process safety, and environmental protection. Four rather diverse example case studies are demonstrated including (1) deep learning-based predictive maintenance monitoring modeling, (2) Natural Language Processing (NLP) for mining text, (3) barrier assessment for dynamic risk mapping (DRA), and (4) correlation development for sustainability indicators. It further discusses the challenges in both research and implementation of proposed solutions in the industry. It is concluded that a well-balanced integrated approach including machine supporting decisions integrated with expert knowledge and available information from various key resources is required to enable more informed policy, strategic, and operational risk decision-making leading to safer, reliable and more efficient operations.;Pankaj Goel and Prerna Jain and Hans J. Pasman and E.N. Pistikopoulos and Aniruddha Datta;"Chemical Engineering (miscellaneous) (Q1); Control and Systems Engineering (Q1); Energy Engineering and Power Technology (Q1); Food Science (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1); Safety, Risk, Reliability and Quality (Q1)";628.0;402.0;Netherlands;1988-2020;10.1016/j.jlp.2020.104316;0.00692;79.0;;09504230;09504230;09504230;09504230;3.660;Data analytics, Maintenance, Deep learning, Sustainability indicators, Natural language processing, Process safety;Elsevier BV;;4040.0;Western Europe;881.0;Q1;13559.0;Journal of loss prevention in the process industries;Integration of data analytics with cloud services for safer process systems, application examples and implementation challenges;8597.0;2568.0;278.0;631.0;11232.0;journal;article;2020
"Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.";Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero;"Computer Science Applications (Q2); Information Systems (Q2); Software (Q2)";403.0;512.0;Netherlands;1970, 1987-2020;10.1016/j.infsof.2020.106448;0.00554;103.0;;09505849;09505849;09505849;09505849;2.730;Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review;Elsevier;;5664.0;Western Europe;606.0;Q2;18732.0;Information and software technology;Big data analytics in agile software development: a systematic mapping study;5172.0;2316.0;121.0;420.0;6853.0;journal;article;2021
Feature selection has aroused extensive attention and aims at selecting features that are highly relevant to classification from raw datasets to improve the performance of a learning model. Fuzzy rough set theory is a powerful mathematical method for feature selection. The classical fuzzy rough set model is very sensitive to the noise while the noise samples in classification data often appear. In addition, fuzzy rough set theory does not fit well when the density distribution of the samples in the dataset varies greatly. Thus, it is of great significance to improve the robustness of fuzzy rough set models and its adaptability to data for feature selection. Inspired by these issues, we focus on the robust fuzzy rough set approach for feature selection. We first propose a robust fuzzy rough set model based on data distribution to achieve the purpose of anti-noise i.e., Noise-aware Fuzzy Rough Sets (NFRS) model. This model proposes a novel search mechanism, which weakens the sensitivity of the approximation operator to noise by considering the distribution of samples in the decision classes to weight the samples, further obtains three kinds of samples, i.e., intra-class samples, boundary samples, and outlier samples. Then, the degrees of relevance of the feature for class is defined by the dependency function based on the NFRS model to evaluate the significance of the feature subset. On this basis, an evaluation function about feature significance is constructed, which simultaneously considers the relevance and redundancy of a candidate feature provided for the selected subset and the remaining feature subset. A novel forward greedy search algorithm is presented to select a feature sequence. The selected features are subsequently evaluated with downstream classification tasks. Experimental using real-world datasets demonstrate the effectiveness of the proposed model and its superiority against comparison baseline methods.;Xiaoling Yang and Hongmei Chen and Tianrui Li and Chuan Luo;"Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1)";1181.0;942.0;Netherlands;1987-2020;10.1016/j.knosys.2022.109092;0.027939999999999996;121.0;;09507051;09507051;09507051;09507051;8.038;Fuzzy rough set, Robust feature selection, Noisy data, Density distribution, Dependency function;Elsevier;;5136.0;Western Europe;1587.0;Q1;24772.0;Knowledge-based systems;A noise-aware fuzzy rough set approach for feature selection;22261.0;11094.0;716.0;1187.0;36777.0;journal;article;2022
Transportation agencies are challenged with managing large-scale networks of roads, spanning geographic areas diverse in terms of jurisdictions, topography, demographics, economy, organizations, and others. Risk management, asset management, and resource allocation must be approached from a holistic perspective without compromising specific regional needs. Agencies collect a large amount of data, the utilization of which should be transparent and consistent with their stated aims. The innovation of this paper is a corridor trace analysis, a method for identifying anomalies of hazard intensity, exposure, and vulnerability along many thousands of kilometers of a transportation network by integrating key road characteristic and performance metrics to straight-line diagrams of corridor sections. Road segments under stress are identified by searching for one or more characteristics that are outliers with respect to the contextual data. The paper includes demonstrations of this method for big-data integration on a real-world system, focusing on how the method is useful to shift among geographic scales. The demonstrations suggest the efficacy of the approach to sustain the efficient, reliable, and safe movement of passengers and freight.;Heimir Thorisson and James H. Lambert;"Applied Mathematics (Q1); Industrial and Manufacturing Engineering (Q1); Safety, Risk, Reliability and Quality (Q1)";926.0;703.0;United Kingdom;1983, 1988-2021;10.1016/j.ress.2017.06.005;;146.0;;09518320;18790836;09518320;18790836;;Risk analysis, Transportation planning, Data visualization, Corridor trace analysis, Systems engineering;Elsevier Ltd.;;4714.0;Western Europe;1761.0;Q1;13853.0;Reliability engineering and system safety;Multiscale identification of emergent and future conditions along corridors of transportation networks;;6698.0;505.0;940.0;23805.0;journal;article;2017
In the railway industry, a significant amount of data is stored in the textual format. The advanced development of natural language processing and text mining techniques enable automatic knowledge extraction and discovery from such documents. This paper presents a systematic review with quantitative and qualitative analyses to understand the current state of text-based research in the context of railway transport. The paper collects 107 relevant publications in the past decade and identifies different channels for researchers to obtain text data in railways and the corresponding text analysis application use-cases. Moreover, a comprehensive analysis is performed on the state-of-the-art machine learning and natural language processing methods. Four key research directions, namely multilingual NLP, digital maintenance, external data integration, and railway-centred solution pipeline, are identified from Siemens Mobility’s perspective to highlight the most prominent challenges faced in the railway industry.;Kaitai Dong and Igor Romanov and Colin McLellan and Ahmet F. Esen;"Artificial Intelligence (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1)";634.0;732.0;United Kingdom;1988-2020;10.1016/j.engappai.2022.105435;0.01024;104.0;;09521976;09521976;09521976;09521976;6.212;Natural language processing, Machine learning, Railway, Text-based analysis, Critical review;Elsevier Ltd.;;5556.0;Western Europe;1106.0;Q1;24182.0;Engineering applications of artificial intelligence;Recent text-based research and applications in railways: a critical review and future trends;10577.0;4555.0;358.0;638.0;19889.0;journal;article;2022
The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.;Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni;Internal Medicine (Q2);557.0;214.0;Netherlands;1989-2020;10.1016/j.ejim.2018.02.012;0.00933;71.0;;09536205;09536205;18790828;09536205;4.487;Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management;Elsevier;;2330.0;Western Europe;894.0;Q2;26617.0;European journal of internal medicine;A new standardized data collection system for interdisciplinary thyroid cancer management: thyroid cobra;7083.0;1904.0;384.0;907.0;8947.0;journal;article;2018
The input-output (IO) model is a powerful economic tool with many extended applications. However, one of the widely criticized drawbacks is its rather lengthy time lag in data preparation, making it impossible to apply IO in high-resolution time-series analysis. The conventional IO model is thus unfortunately unsuited for time-series analysis. In this study, we present an innovative algorithm that integrates linear regression techniques into a derivative of the IO method, the Sequential Interindustry Model (SIM), to overcome the inherent shortcomings of statistical lags in conventional IO studies. The regressed relationship can thus be used to predict, in the short term, the accumulated chronological impacts induced by fluctuations in sectorial economic demands under disequilibrium conditions. A simulated calculation is presented to serve as an illustration and verification of the new method. In the future, this application can be extended beyond economic studies to broader problems of system analysis.;Kehan He and Zhifu Mi and D'Maris Coffman and Dabo Guan;Economics and Econometrics (Q2);197.0;338.0;Netherlands;1990-2020;10.1016/j.strueco.2022.03.017;0.0016;48.0;;0954349X;0954349X;0954349X;0954349X;3.579;Input-Output analysis, Sequential Interindustry model, Economic system modelling, Time series, Impact analysis;Elsevier;;6018.0;Western Europe;891.0;Q2;29487.0;Structural change and economic dynamics;Using a linear regression approach to sequential interindustry model for time-lagged economic impact analysis;2142.0;661.0;119.0;205.0;7161.0;journal;article;2022
"Background
The EU promotes ‘Open Science’ as a public good. Complementary to its implementation is Citizen Science, which redefines the relationship between the scientific community, civic society and the individual. Open Science and Citizen Science poses challenges for the substance use and addictions research community but may provide positive opportunities for future European addiction research. This paper explores both current barriers and potential facilitators for the implementation of Open Science and Citizen Science in substance use and addictions research.
Methodology
A scoping review was used to examine barriers and facilitators identified in the substance use and addiction research literature for the adoption of Open Science and Citizen Science.
Results
‘Technical’ facilitators included the pre-registration of study protocols; publication of open-source datasets; open peer review and online tools. ‘Motivational’ facilitators included enhanced reputation; embracing co-creation; engaged citizenship and gamification. ‘Economic’ facilitators included the use of free tools and balanced remuneration of crowdworkers. ‘Political’ facilitators included better informed debates through the ‘triple helix’ approach and trust-generating transparency. ‘Legal’ facilitators included epidemiologically informed law enforcement; better policy surveillance and the validation of other datasets. ‘Ethical’ facilitators included the ‘democratisation of science’ and opportunities to explore new concepts of ethics in addiction research.
Conclusion
Open Science and Citizen Science in substance use and addictions research may provide a range of benefits in relation to the democratisation of science; transparency; efficiency and the reliability/validity of data. However, its implementation raises a range of research integrity and ethical issues that need be considered. These include issues related to participant recruitment; privacy; confidentiality; security; cost and industry involvement. Progressive journal policies to support Open Science practices; a shift in researcher norms; the use of free tools and the greater availability of methodological and ethical standards are likely to increase adoption in the field.";Florian Scheibein and William Donnelly and John SG Wells;"Health Policy (Q1); Medicine (miscellaneous) (Q1)";634.0;434.0;Netherlands;1998-2020;10.1016/j.drugpo.2021.103505;0.01545;79.0;;09553959;18734758;09553959;18734758;5.009;Open science, Open data, Open source, Open peer review, Citizen science;Elsevier;;5202.0;Western Europe;1649.0;Q1;25290.0;International journal of drug policy;Assessing open science and citizen science in addictions and substance use research: a scoping review;8075.0;3372.0;322.0;715.0;16752.0;journal;article;2022
Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.;Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang;"Biomedical Engineering (Q1); Biophysics (Q1); Biotechnology (Q1); Electrochemistry (Q1); Medicine (miscellaneous) (Q1); Nanoscience and Nanotechnology (Q1)";2718.0;1020.0;United Kingdom;1990-2020;10.1016/j.bios.2020.112412;;192.0;;09565663;18734235;09565663;18734235;;Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare;Elsevier Ltd.;;5468.0;Western Europe;2546.0;Q1;15437.0;Biosensors and bioelectronics;Artificial intelligence biosensors: challenges and prospects;;27420.0;701.0;2720.0;38330.0;journal;article;2020
To discriminate among three poultry meat types (hybrid broiler, hybrid broiler affected by breast myopathies, and slow-growing native breed), and to predict the proximate and the amino acid (AA) composition of breast meat, two NIRs (Near-Infrared) instruments operating between 850 and 2500 nm coupled with chemometric algorithms and Machine Learning (ML) approaches, were tested. The Partial Least Square Discriminant Analysis was performed for genotype identification, resulting in a Mathew Correlation Coefficient (MCC) ranging from 0.61 to 1.00, according to the spectra pretreatments and instrument adopted. The Partial Least Square Regression allowed reaching a high cross-validation determination coefficient (R2cv) for crude protein (0.98) and ether extract (0.99), while only three AA (aspartic acid, alanine and methionine) reached R2cv > 0.55. The latter predictions were successfully used to discriminate between genotypes using Factorial Discriminant Analysis, with an MCC ranging from 0.67 to 0.95. Overall, both tested NIRs instruments allowed to determine the chemical composition of fresh and freeze-dried chicken meat. In this sense, a significant improvement of NIRs data interpretability was achieved thanks to the use of ML algorithms, as it was possible to discriminate the chemical composition of meat depending on the genetic group and the presence of breast myopathies.;Lorenzo Serva and Giorgio Marchesini and Marco Cullere and Rebecca Ricci and Antonella {Dalle Zotte};"Biotechnology (Q1); Food Science (Q1)";1681.0;548.0;Netherlands;1990-2021;10.1016/j.foodcont.2022.109391;0.024880000000000003;125.0;;09567135;09567135;09567135;09567135;5.548;Chicken, Genotype, Myopathies, Meat quality, Near infra-red spectroscopy, Machine learning;Elsevier BV;;4509.0;Western Europe;1371.0;Q1;22577.0;Food control;Testing two nirs instruments to predict chicken breast meat quality and exploiting machine learning approaches to discriminate among genotypes and presence of myopathies;30556.0;9417.0;553.0;1694.0;24935.0;journal;article;2023
The time overhead is huge and the clustering quality is unstable when running the K-means algorithm on massive raw data. To solve these problems, the concept of the influence space is introduced, and on this basis, a new clustering algorithm named ISBFK-means based on the influence space is proposed in this paper. First, the influence space divides the given data set into multiple small regions. Then, the representative data objects in each region are obtained to form a new data set, in which the class labels of representative data objects are those of all the data objects in the correlation influence space. Next, the K-means clustering is performed on the new data set, thereby obtaining the final clustering result. Theoretical analysis and experimental results show that this approach effectively reduces the amount of data in the clustering process and improves the stability of clustering quality. As a major feature of this work, the celestial spectral data observed by the LAMOST survey are especially employed to verify the algorithm ISBFK-means. The experimental results indicate that this algorithm has higher performance than other similar algorithms on the correctness, efficiency and sensitivity to the quality of spectral data.;Yuqing Yang and Jianghui Cai and Haifeng Yang and Yating Li and Xujun Zhao;"Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)";1943.0;867.0;United Kingdom;1990-2021;10.1016/j.eswa.2022.117018;0.040530000000000004;207.0;;09574174;09574174;09574174;09574174;6.954;Clustering, Influence space, Region partition, Representative data objects;Elsevier Ltd.;;5495.0;Western Europe;1368.0;Q1;24201.0;Expert systems with applications;Isbfk-means: a new clustering algorithm based on influence space;55444.0;17345.0;770.0;1945.0;42314.0;journal;article;2022
"In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.";Bing Wang;"Chemical Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Safety, Risk, Reliability and Quality (Q1); Environmental Chemistry (Q2)";1071.0;641.0;United Kingdom;1990-2021;10.1016/j.psep.2020.10.008;0.01335;76.0;;09575820;17443598;09575820;17443598;6.158;Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making;Institution of Chemical Engineers;;4979.0;Western Europe;1173.0;Q1;13754.0;Process safety and environmental protection;Safety intelligence as an essential perspective for safety management in the era of safety 4.0: from a theoretical to a practical framework;12452.0;6888.0;417.0;1079.0;20761.0;journal;article;2021
Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.;Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles;"Bioengineering (Q1); Biomedical Engineering (Q1); Biotechnology (Q1)";444.0;914.0;United Kingdom;1990-2020;10.1016/j.copbio.2020.09.003;0.0197;202.0;;09581669;18790429;09581669;18790429;9.740;;Elsevier Ltd.;;6160.0;Western Europe;2843.0;Q1;15579.0;Current opinion in biotechnology;The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems;18835.0;4236.0;193.0;475.0;11888.0;journal;article;2021
Data integration is one of the biggest challenges the dairy industry faces nowadays due to increased number of technologies and data overflow at the farm. Here we review the current situation of precision dairy farming technologies that use integrated data and its application on the management decision process at the dairy farm. The most common data connections were those from activity monitors, dairy herd improvement records, herd management, and milking recordings. Algorithms used can be defined in general as artificial intelligence and machine learning approaches. Most of the 22 revised papers, research or review, demonstrated that applying different algorithms to integrated data provides additional and complementary insights to improve decision-making tools and therefore enhance economic, management and animal welfare, and hence the sustainability of dairy farms. All revised studies acknowledge the importance of live data integration to develop relevant decision support tools to improve decision making.;Victor E. Cabrera and Liliana Fadul-Pacheco;"Food Science (Q1); Applied Microbiology and Biotechnology (Q2)";484.0;298.0;Netherlands;1991-2020;10.1016/j.idairyj.2021.105069;0.00451;140.0;;09586946;09586946;09586946;09586946;3.032;;Elsevier BV;;4311.0;Western Europe;903.0;Q1;51565.0;International dairy journal;Future of dairy farming from the dairy brain perspective: data integration, analytics, and applications;10764.0;1559.0;199.0;485.0;8579.0;journal;article;2021
The Expectation Maximization (EM) algorithm has been widely used for parameter estimation in data-driven process identification. EM is an algorithm for maximum likelihood estimation of parameters and ensures convergence of the likelihood function. In presence of missing variables and in ill conditioned problems, EM algorithm greatly assists the design of more robust identification algorithms. Such situations frequently occur in industrial environments. Missing observations due to sensor malfunctions, multiple process operating conditions and unknown time delay information are some of the examples that can resort to the EM algorithm. In this article, a review on applications of the EM algorithm to address such issues is provided. Future applications of EM algorithm as well as some open problems are also provided.;Nima Sammaknejad and Yujia Zhao and Biao Huang;"Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1)";408.0;439.0;United Kingdom;1991-2020;10.1016/j.jprocont.2018.12.010;0.00525;114.0;;09591524;09591524;09591524;09591524;3.666;Expectation Maximization algorithm, Data-driven process identification, Multiple models, Switching, State space, Time delay, Hidden Markov Models, Latent variable models, Outlier treatment, Missing data;Elsevier Ltd.;;4146.0;Western Europe;1102.0;Q1;14414.0;Journal of process control;A review of the expectation maximization algorithm in data-driven process identification;7446.0;1820.0;136.0;413.0;5639.0;journal;article;2019
"Environmental governance has the potential to be significantly transformed by Smart Earth technologies, which deploy enhanced environmental monitoring via combinations of information and communication technologies (ICT), conventional monitoring technologies (e.g. remote sensing), and Internet of Things (IoT) applications (e.g. Environmental Sensor Networks (ESNs)). This paper presents a systematic meta-review of Smart Earth scholarship, focusing our analysis on the potential implications and pitfalls of Smart Earth technologies for environmental governance. We present a meta-review of academic research on Smart Earth, covering 3187 across the full range of academic disciplines from 1997 to 2017, ranging from ecological informatics to the digital humanities. We then offer a critical perspective on potential pathways for evolution in environmental governance frameworks, exploring five key Smart Earth issues relevant to environmental governance: data; real-time regulation; predictive management; open source; and citizen sensing. We conclude by offering suggestions for future research directions and trans-disciplinary conversations about environmental governance in a Smart Earth world.";Karen Bakker and Max Ritts;"Ecology (Q1); Geography, Planning and Development (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)";351.0;935.0;United Kingdom;1990-2020;10.1016/j.gloenvcha.2018.07.011;;177.0;;09593780;09593780;18729495;09593780;;Eco-informatics, Environmental governance, Smart earth, Ecology, ICT, IoT, Information and communications technology, Internet of things, Sensors, Digital;Elsevier Ltd.;;8182.0;Western Europe;4659.0;Q1;36483.0;Global environmental change;Smart earth: a meta-review and implications for environmental governance;;4649.0;131.0;354.0;10718.0;journal;article;2018
Electrophysiology has long been the workhorse of neuroscience, allowing scientists to record with millisecond precision the action potentials generated by neurons in vivo. Recently, calcium imaging of fluorescent indicators has emerged as a powerful alternative. This technique has its own strengths and weaknesses and unique data processing problems and interpretation confounds. Here we review the computational methods that convert raw calcium movies to estimates of single neuron spike times with minimal human supervision. By computationally addressing the weaknesses of calcium imaging, these methods hold the promise of significantly improving data quality. We also introduce a new metric to evaluate the output of these processing pipelines, which is based on the cluster isolation distance routinely used in electrophysiology.;Carsen Stringer and Marius Pachitariu;Neuroscience (miscellaneous) (Q1);469.0;646.0;United Kingdom;1991-2021;10.1016/j.conb.2018.11.005;0.02518;229.0;;09594388;09594388;18736882;09594388;6.627;;Elsevier Ltd.;;6518.0;Western Europe;4229.0;Q1;14992.0;Current opinion in neurobiology;Computational processing of neural recordings from calcium imaging data;17009.0;3279.0;128.0;487.0;8343.0;journal;article;2019
At least 46 interactome studies, broad at proteome scale or biologically more focused, have together mapped about 75,000 human protein–protein interactions (PPIs). Many of the studies addressed local interactome data paucity analyzing specific homeostatic and regulatory systems, with recent focus demonstrating the involvement of post-translational protein modification (PTM) enzyme families in a wide range of cellular functions. These datasets provided insight into binding mechanisms, the dynamic modularity of complexes or delineated combinatorial enzymatic cascades. Furthermore, the combined study of PPI and PTM dynamics has begun to reveal conditional rewiring of molecular networks through PTM-mediated recognition events. Taken together these studies highlight the utility of local and global interaction networks to functionally prioritize the many changing PTMs mapped in human cells.;Jonathan Woodsmith and Ulrich Stelzl;"Molecular Biology (Q1); Structural Biology (Q1)";404.0;614.0;United Kingdom;1991-2021;10.1016/j.sbi.2013.11.009;0.018969999999999997;200.0;;0959440X;1879033X;0959440X;1879033X;6.809;;Elsevier Ltd.;;5828.0;Western Europe;3935.0;Q1;17400.0;Current opinion in structural biology;Studying post-translational modifications with protein interaction networks;12448.0;2860.0;144.0;442.0;8393.0;journal;article;2014
The Guangdong–Hong Kong–Macao Greater Bay Area (GBA) was recently proposed by the Chinese government for socioeconomic–political–environmental development. As one of the most important parts for the development of the GBA, the transportation sector has become a nonnegligible consumer of energy and source of emission. Therefore, vehicle energy consumption and related emissions in the GBA must be evaluated. However, an effective framework for accurately estimating the real-time transportation performance, vehicle energy consumption and emission in large urban areas is still lacking. This study aims to develop a novel method for examining the regional integration and the spatial connection that affect vehicle emission via crowdsourced traffic data and an emission model. The novelty of this study is that it explores the acquisition of large-scale and real-time data as well as the calculation of corresponding energy consumption and environmental impact. Based on the analysis of transportation in GBA, it is found that the vehicle energy consumption and emission on the expressway are nearly half as that on the other urban roads. Moreover, a significant difference exists in vehicle energy consumption and emission between passenger cars and medium-duty trucks. Regarding transportation performance, vehicle energy consumption and emissions, they are closely related to departure time. It is also found that most of the adjacent cities tend to have high vehicle energy consumption and emission. The contribution of the Hong Kong–Zhuhai–Macao Bridge to transportation from Hong Kong, Macao and Zhuhai is different. Hong Kong benefits most from the bridge in terms of traffic energy efficiency. This study would be valuable to both researchers and practitioners. It helps researchers apply a large dataset in replicating real-world travel, energy consumption and emission pattern at a large scale. Policymakers and practitioners would benefit from developing effective strategies for the sustainable development of the GBA.;Wenke Huang and Yuanyuan Guo and Xiaoxiao Xu;"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)";10577.0;956.0;United Kingdom;1993-2021;10.1016/j.jclepro.2020.121583;0.18299;200.0;;09596526;09596526;18791786;09596526;9.297;Guangdong–Hong Kong–Macao Greater Bay Area, Vehicle emission, Road traffic, Spatiotemporal, Directions API;Elsevier Ltd.;;5940.0;Western Europe;1937.0;Q1;19167.0;Journal of cleaner production;Evaluation of real-time vehicle energy consumption and related emissions in china: a case study of the guangdong–hong kong–macao greater bay area;170352.0;103295.0;5126.0;10603.0;304498.0;journal;article;2020
"Background
Significant advances in the management of patients with lymphoid and myeloid malignancies entered clinical practice in the early 2000’s. The EUROCARE-5 study database provides an opportunity to assess the impact of these changes at the population level by country in Europe. We provide survival estimates for clinically relevant haematological malignancies (HM), using the International Classification of Diseases for Oncology 3, by country, gender and age in Europe.
Methods
We estimated age-standardised relative survival using the complete cohort approach for 625,000 adult patients diagnosed in 2000–2007 and followed up to 2008. Survival information was provided by 89 participating cancer registries from 29 European countries. Mean survival in Europe was calculated as the population weighted average of country-specific estimates.
Results
On average in Europe, 5-year relative survival was highest for Hodgkin lymphoma (81%; 40,625 cases), poorest for acute myeloid leukaemia (17%; 57,026 cases), and intermediate for non-Hodgkin lymphoma (59%; 329,204 cases), chronic myeloid leukaemia (53%; 17,713 cases) and plasma cell neoplasms (39%; 94,024 cases). Survival was generally lower in Eastern Europe and highest in Central and Northern Europe. Wider between country differences (>10%) were observed for malignancies that benefited from therapeutic advances, such as chronic myeloid leukaemia, chronic lymphocytic leukaemia, follicular lymphoma, diffuse large B-cell lymphoma and multiple myeloma. Lower differences (<10%) were observed for Hodgkin lymphoma.
Conclusions
Delayed or reduced access to innovative and appropriate therapies could plausibly have contributed to the observed geographical disparities between European regions and countries. Population based survival by morphological sub-type is important for measuring outcomes of HM management. To better inform quality of care research, the collection of detailed clinical information at the population level should be prioritised.";Roberta {De Angelis} and Pamela Minicozzi and Milena Sant and Luigino {Dal Maso} and David H. Brewster and Gemma Osca-Gelis and Otto Visser and Marc Maynadié and Rafael Marcos-Gragera and Xavier Troussard and Dominic Agius and Paolo Roazzi and Elisabetta Meneghini and Alain Monnereau and M. Hackl and N. Zielonke and W. Oberaigner and E. {Van Eycken} and K. Henau and Z. Valerianova and N. Dimitrova and M. Sekerija and M. Zvolský and L. Dušek and H. Storm and G. Engholm and M. Mägi and T. Aareleid and N. Malila and K. Seppä and M. Velten and X. Troussard and V. Bouvier and G. Launoy and A.V. Guizard and J. Faivre and A.M. Bouvier and P. Arveux and M. Maynadié and A.S. Woronoff and M. Robaszkiewicz and I. Baldi and A. Monnereau and B. Tretarre and N. Bossard and A. Belot and M. Colonna and F. Molinié and S. Bara and C. Schvartz and B. Lapôtre-Ledoux and P. Grosclaude and M. Meyer and R. Stabenow and S. Luttmann and A. Eberle and H. Brenner and A. Nennecke and J. Engel and G. Schubert-Fritschle and J. Kieschke and J. Heidrich and B. Holleczek and A. Katalinic and J.G. Jónasson and L. Tryggvadóttir and H. Comber and G. Mazzoleni and A. Bulatko and C. Buzzoni and A. Giacomin and A. {Sutera Sardo} and P. Mancuso and S. Ferretti and E. Crocetti and A. Caldarella and G. Gatta and M. Sant and H. Amash and C. Amati and P. Baili and F. Berrino and S. Bonfarnuzzo and L. Botta and F. {Di Salvo} and R. Foschi and C. Margutti and E. Meneghini and P. Minicozzi and A. Trama and D. Serraino and L. {Dal Maso} and R. {De Angelis} and M. Caldora and R. Capocaccia and E. Carrani and S. Francisci and S. Mallone and D. Pierannunzio and P. Roazzi and S. Rossi and M. Santaquilani and A. Tavilla and F. Pannozzo and S. Busco and L. Bonelli and M. Vercelli and V. Gennaro and P. Ricci and M. Autelitano and G. Randi and M. {Ponz De Leon} and C. Marchesi and C. Cirilli and M. Fusco and M.F. Vitale and M. Usala and A. Traina and R. Staiti and F. Vitale and B. Ravazzolo and M. Michiara and R. Tumino and P. {Giorgi Rossi} and E. {Di Felice} and F. Falcini and A. Iannelli and O. Sechi and R. Cesaraccio and S. Piffer and A. Madeddu and F. Tisano and S. Maspero and A.C. Fanetti and R. Zanetti and S. Rosso and P. Candela and T. Scuderi and F. Stracci and F. Bianconi and G. Tagliabue and P. Contiero and A.P. {Dei Tos} and S. Guzzinati and S. Pildava and G. Smailyte and N. Calleja and D. Agius and T.B. Johannesen and J. Rachtan and S. Gózdz and R. Mezyk and J. Blaszczyk and M. Bebenek and M. Bielska-Lasota and G. {Forjaz de Lacerda} and M.J. Bento and C. Castro and A. Miranda and A. Mayer-da-Silva and F. Nicula and D. Coza and C. {Safaei Diba} and M. Primic-Zakelj and E. Almar and C. Ramírez and M. Errezola and J. Bidaurrazaga and A. Torrella-Ramos and J.M. {Díaz García} and R. Jimenez-Chillaron and R. Marcos-Gragera and A. {Izquierdo Font} and M.J. Sanchez and D.Y.L. Chang and C. Navarro and M.D. Chirlaque and C. Moreno-Iribas and E. Ardanaz and J. Galceran and M. Carulla and M. Lambe and S. Khan and M. Mousavi and C. Bouchardy and M. Usel and S.M. Ess and H. Frick and M. Lorez and S.M. Ess and C. Herrmann and A. Bordoni and A. Spitale and I. Konzelmann and O. Visser and V. Lemmens and M. Coleman and C. Allemani and B. Rachet and J. Verne and N. Easey and G. Lawrence and T. Moran and J. Rashbass and M. Roche and J. Wilkinson and A. Gavin and C. Donnelly and D.H. Brewster and D.W. Huws and C. White and R. Otter;"Cancer Research (Q1); Oncology (Q1)";961.0;651.0;United Kingdom;1990-2020;10.1016/j.ejca.2015.08.003;0.04649;214.0;;09598049;09598049;18790852;09598049;9.162;Relative survival, Europe, Cancer registry, Lymphoma, Leukaemia, Hodgkin lymphoma, Non-Hodgkin lymphoma, Multiple myeloma;Elsevier Ltd.;;1342.0;Western Europe;3354.0;Q1;29761.0;European journal of cancer;Survival variations by country and age for lymphoid and myeloid malignancies in europe 2000–2007: results of eurocare-5 population-based study;40294.0;7985.0;935.0;1157.0;12549.0;journal;article;2015
This paper explores a real-world fundamental theme under a data science perspective. It specifically discusses whether fraud or manipulation can be observed in and from municipality income tax size distributions, through their aggregation from citizen fiscal reports. The study case pertains to official data obtained from the Italian Ministry of Economics and Finance over the period 2007–2011. All Italian (20) regions are considered. The considered data science approach concretizes in the adoption of the Benford first digit law as quantitative tool. Marked disparities are found, - for several regions, leading to unexpected “conclusions”. The most eye browsing regions are not the expected ones according to classical imagination about Italy financial shadow matters.;Marcel Ausloos and Roy Cerqueti and Tariq A. Mir;"Applied Mathematics (Q1); Mathematics (miscellaneous) (Q1); Physics and Astronomy (miscellaneous) (Q1); Statistical and Nonlinear Physics (Q1)";1208.0;618.0;United Kingdom;1991-2020;10.1016/j.chaos.2017.08.012;;139.0;;09600779;09600779;09600779;09600779;;Data science, Benford law, Aggregated income tax, Data manipulation, Italy;Elsevier Ltd.;;4035.0;Western Europe;1043.0;Q1;25347.0;Chaos, solitons and fractals;Data science for assessing possible tax income manipulation: the case of italy;;6805.0;886.0;1216.0;35752.0;journal;article;2017
Increasing air pollutants significantly affect the proportion of diffuse (Rd) to global (Rs) solar radiation. This study proposed three new hybrid support vector machines (SVM) with particle swarm optimization algorithm (SVM-PSO), bat algorithm (SVM-BAT) and whale optimization algorithm (SVM-WOA) for predicting daily Rd in air-polluted regions. These models were further compared to standalone SVM, multivariate adaptive regression spline (MARS) and extreme gradient boosting (XGBoost) models. The results showed that models with suspended particulate matter with aerodynamic diameter smaller than 2.5 μm and 10 μm (PM2.5 and PM10) and ozone (O3) produced more accurate daily Rd estimates than those without air pollution parameters, with average relative decreases in root mean square deviation (RMSD) of 11.1%, 10.0% and 10.4% for sunshine duration-based, Rs-based and combined models, respectively. SVM showed better accuracy than XGBoost and MARS. However, compared to SVM, SVM-BAT further enhanced the prediction accuracy and convergence rate in daily Rd modeling, followed by SVM-WOA and SVM-PSO, with relative decreases in RMSD of 2.9%–5.6%, 1.9%–4.9% and 1.1%–3.3%, respectively. The results highlighted the significance of incorporating air pollutants for more accurate estimation of daily Rd in air-polluted regions. Heuristic algorithms, especially BAT, are highly recommended for improving performance of standalone machine learning models.;Junliang Fan and Lifeng Wu and Xin Ma and Hanmi Zhou and Fucang Zhang;Renewable Energy, Sustainability and the Environment (Q1);3587.0;839.0;United Kingdom;1991-2021;10.1016/j.renene.2019.07.104;0.06925;191.0;;09601481;09601481;09601481;09601481;8.001;Air pollution, Support vector machines, Extreme gradient boosting, Particle swarm optimization algorithm, Bat algorithm, Whale optimization algorithm;Elsevier BV;;4707.0;Western Europe;1825.0;Q1;27569.0;Renewable energy;Hybrid support vector machines with heuristic algorithms for prediction of daily diffuse solar radiation in air-polluted regions;72390.0;29828.0;2467.0;3605.0;116113.0;journal;article;2020
The development and application of bioenergy and biofuels conversion technology can play a significant role for the production of renewable and sustainable energy sources in the future. However, the complexity of bioenergy systems and the limitations of human understanding make it difficult to build models based on experience or theory for accurate predictions. Recent developments in data science and machine learning (ML), can provide new opportunities. Accordingly, this critical review provides a deep insight into the application of ML in the bioenergy context. The latest advances in ML assisted bioenergy technology, including energy utilization of lignocellulosic biomass, microalgae cultivation, biofuels conversion and application, are reviewed in detail. The strengths and limitations of ML in bioenergy systems are comprehensively analysed. Moreover, we highlight the capabilities and potential of advanced ML methods when encountering multifarious tasks in the future prospects to advance a new generation of bioenergy and biofuels conversion technologies.;Zhengxin Wang and Xinggan Peng and Ao Xia and Akeel A. Shah and Yun Huang and Xianqing Zhu and Xun Zhu and Qiang Liao;"Bioengineering (Q1); Environmental Engineering (Q1); Medicine (miscellaneous) (Q1); Renewable Energy, Sustainability and the Environment (Q1); Waste Management and Disposal (Q1)";4907.0;966.0;United Kingdom;1990-2021;10.1016/j.biortech.2021.126099;0.10296;294.0;;09608524;09608524;18732976;09608524;9.642;Bioenergy, Biofuels, Machine learning, Lignocellulosic biomass, Algae;Elsevier Ltd.;;4885.0;Western Europe;2489.0;Q1;15423.0;Bioresource technology;The role of machine learning to boost the bioenergy and biofuels conversion;166941.0;47449.0;1597.0;4915.0;78013.0;journal;article;2022
"Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.";Lotus A. Lofgren and Jason E. Stajich;"Agricultural and Biological Sciences (miscellaneous) (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Neuroscience (miscellaneous) (Q1)";2265.0;599.0;United States;1991-2020;10.1016/j.cub.2021.06.083;0.1161;316.0;;09609822;18790445;09609822;18790445;10.834;;Cell Press;;4143.0;Northern America;3822.0;Q1;15537.0;Current biology;Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes;78289.0;15523.0;929.0;2432.0;38487.0;journal;article;2021
As a novel means of researching China's Belt and Road Initiative (BRI), this article advances a critical remote sensing agenda that connects the view from above provided by satellite imagery with the grounded, qualitative methodologies more typical of political geography such as ethnographic fieldwork. Satellite imagery is widely used to produce empirics relating to the BRI, and the Chinese state is showing increasing interest in applying Earth observation data to governance. A more critical approach attentive to the politics of remote sensing, especially in light of China's emergence as a space and satellite power and its embrace of big data, is needed to more precisely reveal what changing pixels represent on the ground and expose the potential issues with data captured from high above the planet. This paper offers three theoretical and methodological objectives for critical remote sensing. First, I reflect on the geopolitics involved in the production and analysis of satellite imagery. Second, through analysis of night light imagery, which captures illuminated anthropogenic activities, I interrogate metanarratives of development. Third, I engage with qualitative methods by “ground-truthing” remote sensing with ethnographic observations along China's borders. I also seek to avoid the methodological nationalism often present in remote sensing research by situating these mixed-methods case studies at scales above and below the nation-state. As one of the largest development interventions in history materializes, pursuing critical remote sensing can create opportunities for social scientists to leverage quantitative and geospatial methods in support of more equitable and sustainable futures.;Mia M. Bennett;"Geography, Planning and Development (Q1); History (Q1); Sociology and Political Science (Q1)";294.0;305.0;United Kingdom;1983, 1992-2020;10.1016/j.polgeo.2019.102127;0.006940000000000001;97.0;;09626298;09626298;09626298;09626298;3.660;Critical remote sensing, Development, Night lights, Infrastructure, China, Belt and Road Initiative;Elsevier BV;;7548.0;Western Europe;1527.0;Q1;22857.0;Political geography;Is a pixel worth 1000 words? critical remote sensing and china's belt and road initiative;5342.0;1175.0;136.0;342.0;10265.0;journal;article;2020
Traditionally, macromolecular structure determination is performed ex situ, that is, with purified materials. But, there are strong incentives to develop approaches to study them in situ in their native functional context. In recent years, cryo-electron tomography (cryo-ET) has emerged as a powerful method for visualizing the molecular organization of unperturbed cellular landscapes with the potential to attain near-atomic resolution. Here, we review recent work on several macromolecular assemblies, demonstrating the power of in situ studies. We also highlight technical challenges and discuss ways to meet them.;Martin Beck and Wolfgang Baumeister;Cell Biology (Q1);266.0;1578.0;United Kingdom;1991-2020;10.1016/j.tcb.2016.08.006;0.030119999999999997;237.0;;09628924;09628924;18793088;09628924;20.808;;Elsevier Ltd.;;8288.0;Western Europe;8705.0;Q1;19066.0;Trends in cell biology;Cryo-electron tomography: can it reveal the molecular sociology of cells in atomic detail?;19007.0;4318.0;90.0;267.0;7459.0;journal;article;2016
This study develops and validates the concept of Data Analytics Competency as a five multidimensional formative index (i.e., data quality, bigness of data, analytical skills, domain knowledge, and tools sophistication) and empirically examines its impact on firm decision making performance (i.e., decision quality and decision efficiency). The findings based on an empirical analysis of survey data from 151 Information Technology managers and data analysts demonstrate a large, significant, positive relationship between data analytics competency and firm decision making performance. The results reveal that all dimensions of data analytics competency significantly improve decision quality. Furthermore, interestingly, all dimensions, except bigness of data, significantly increase decision efficiency. This is the first known empirical study to conceptualize, operationalize and validate the concept of data analytics competency and to study its impact on decision making performance. The validity of the data analytics competency construct as conceived and operationalized, suggests the potential for future research evaluating its relationships with possible antecedents and consequences. For practitioners, the results provide important guidelines for increasing firm decision making performance through the use of data analytics.;Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein;"Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)";61.0;1245.0;Netherlands;1991-2020;10.1016/j.jsis.2017.10.001;0.00233;88.0;;09638687;09638687;09638687;09638687;11.022;Data analytics competency, Data quality, Bigness of data, Analytical skills, Domain knowledge, Tools sophistication, Decision making performance;Elsevier;;8796.0;Western Europe;3133.0;Q1;12396.0;Journal of strategic information systems;Data analytics competency for improving firm decision making performance;2945.0;965.0;24.0;75.0;2111.0;journal;article;2018
Three series of flake graphite cast iron samples having different chemical compositions and different heat treatments within each series were investigated by the method of magnetic adaptive testing. The flat samples were magnetized by an attached yoke, and sensitive descriptors were obtained from the proper evaluation, based on the measurements of series of magnetic minor hysteresis loops, without magnetic saturation of the samples. Results of the non-destructive magnetic tests were compared with the destructive mechanical measurements of Brinell hardness and linear correlation was found between them in all cases, where the influence of chemical composition and influence of heat treatment were considered.;Gábor Vértesy and Tetsuya Uchimoto and Toshiyuki Takagi and Ivan Tomáš and Hidehiko Kage;"Condensed Matter Physics (Q1); Materials Science (miscellaneous) (Q1); Mechanical Engineering (Q1)";367.0;424.0;United Kingdom;1990-2020;10.1016/j.ndteint.2015.04.004;;91.0;;09638695;09638695;09638695;09638695;;Cast iron, Magnetic hysteresis, Magnetic adaptive testing, Nondestructive testing;Elsevier Ltd.;;3407.0;Western Europe;1230.0;Q1;17852.0;Ndt and e international;Nondestructive characterization of flake graphite cast iron by magnetic adaptive testing;;1595.0;115.0;369.0;3918.0;journal;article;2015
Embracing the concept of resilience within coastal management marks a step change in thinking, building on the inputs of more traditional risk assessments, and further accounting for capacities to respond, recover and implement contingency measures. Nevertheless, many past resilience assessments have been theoretical and have failed to address the requirements of practitioners. Assessment methods can also be subjective, relying on opinion-based judgements, and can lack empirical validation. Scope exists to address these challenges through drawing on rapidly emerging sources of data and smart analytics. This, alongside the careful selection of the metrics used in assessment of resilience, can facilitate more robust assessment methods. This work sets out to establish a set of core metrics, and data sources suitable for inclusion within a data-driven coastal resilience assessment. A case study region of East Anglia, UK, is focused on, and data types and sources associated with a set of proven assessment metrics were identified. Virtually all risk-specific metrics could be satisfied using available or derived data sources. However, a high percentage of the resilience-specific metrics would still require human input. This indicates that assessment of resilience is inherently more subjective than assessment of risk. Yet resilience assessments incorporate both risk and resilience specific variables. As such it was possible to link 75% of our selected metrics to empirical sources. Through taking a case study approach and discussing a set of requirements outlined by a coastal authority, this paper reveals scope for the incorporation of rapidly progressing data collection, dissemination, and analytical methods, within dynamic coastal resilience assessments. This could facilitate more sustainable evidence-based management of coastal regions.;Alexander G. Rumson and Andres Payo Garcia and Stephen H. Hallett;"Aquatic Science (Q1); Management, Monitoring, Policy and Law (Q1); Oceanography (Q1)";878.0;334.0;United Kingdom;1992-2020;10.1016/j.ocecoaman.2019.105004;;84.0;;09645691;09645691;09645691;09645691;;Coastal management, Resilience metrics, Geospatial data, Open source data, Big data;Elsevier BV;;6915.0;Western Europe;916.0;Q1;28333.0;Ocean and coastal management;The role of data within coastal resilience assessments: an east anglia, uk, case study;;3103.0;281.0;907.0;19431.0;journal;article;2020
Maritime administration and coastal states have become more aware of the need to enhance risk mitigation strategies primarily due to increased worldwide shipping activities, changing safety qualities of the world fleet and limited resources to deploy mitigation strategies. This paper introduces an innovative multi-layered framework to assess, predict and mitigate potential harm. The proposed approach addresses known restrictions of risk assessments in shipping. These restrictions are the lack of scalability to apply risk assessments over large areas using an automated routine, the absence of recognizing that the world fleet is heterogeneous, the lack of integrating location specific environmental conditions such as wind, currents or waves and most importantly, the lack of recognizing the uncertainties associated with each factor especially for predictions. The proposed framework is based on the idea of integrating various layers representing the most important factors that can influence risk in order to estimate and predict risk exposure for a given area. As proof of concept of the underlying ideas, the outcome of a pilot project with the Australian Maritime Safety Authority is presented which demonstrates the integration of the first two layers and is based on a unique and comprehensive combination of data. The results of selected endpoints of risk exposure compare well with observations. The article also discusses the integration of the remaining layers including the recognition and addition of uncertainties in the future.;Stephen {Vander Hoorn} and Sabine Knapp;"Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)";817.0;610.0;United Kingdom;1982, 1992-2020;10.1016/j.tra.2015.04.032;;133.0;;09658564;09658564;09658564;09658564;;Total risk exposure, Binary logistic regression, Spatial statistics, Incident models, Uncertainties, Monetary value at risk;Elsevier Ltd.;;5848.0;Western Europe;2178.0;Q1;20891.0;Transportation research, part a: policy and practice;A multi-layered risk exposure assessment approach for the shipping industry;;5663.0;349.0;834.0;20410.0;journal;article;2015
Trip purpose is closely related to travel patterns and plays an important role in urban planning and transportation management. Recently, there has been a growing interest in investigating the spatio-temporal patterns of dockless shared-bike usage and its influencing mechanisms. Few, however, have focused on revealing the travel patterns by inferring the purpose of dockless shared-bike trips at the individual level. We present a framework for inferring the purpose of dockless shared-bike users, based on gravity model and Bayesian rules, and conduct it in Shenzhen, China. We consider the comprehensive factors including distance, time, environment, activity type proportion, and service capacity of points of interest (POIs), the last two factors of which were usually neglected in previous transport studies. Especially, we integrated areas of interest (AOIs) and Tencent User density (TUD) social media data characterize the service capacity of POIs, which reflect the area and scale differences of different POI categories. Through the comparison between two improved models and the basic model, it is demonstrated that the introduction of activity type proportion and service capacity of POIs can improve the effectiveness of model for inferring the purposes of dockless shared-bike trips. Based on the obtained trip purposes, we further explore the spatio-temporal patterns of different activities and gain some insights into bike travel demand, which can inform scientific decisions for bicycle infrastructure planning and dockless shared- bike management.;Shaoying Li and Caigang Zhuang and Zhangzhi Tan and Feng Gao and Zhipeng Lai and Zhifeng Wu;"Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)";514.0;521.0;United Kingdom;1993-2020;10.1016/j.jtrangeo.2021.102974;0.01053;108.0;;09666923;09666923;09666923;09666923;4.986;Activity inference, Gravity model, Bayesian rules, Travel patterns, Dockless shared bikes;Elsevier BV;;5850.0;Western Europe;1809.0;Q1;29295.0;Journal of transport geography;Inferring the trip purposes and uncovering spatio-temporal activity patterns from dockless shared bike dataset in shenzhen, china;11719.0;2931.0;264.0;515.0;15444.0;journal;article;2021
Launched in March 2005, the Integrated Microbial Genomes (IMG) system is a comprehensive data management system that supports multidimensional comparative analysis of genomic data. At the core of the IMG system is a data warehouse that contains genome and metagenome datasets sequenced at the Joint Genome Institute or provided by scientific users, as well as public genome datasets available at the National Center for Biotechnology Information Genbank sequence data archive. Genomes and metagenome datasets are processed using IMG's microbial genome and metagenome sequence data processing pipelines and are integrated into the data warehouse using IMG's data integration toolkits. Microbial genome and metagenome application specific data marts and user interfaces provide access to different subsets of IMG's data and analysis toolkits. This review article revisits IMG's original aims, highlights key milestones reached by the system during the past 10 years, and discusses the main challenges faced by a rapidly expanding system, in particular the complexity of maintaining such a system in an academic setting with limited budgets and computing and data management infrastructure.;Victor M. Markowitz and I-Min A. Chen and Ken Chu and Amrita Pati and Natalia N. Ivanova and Nikos C. Kyrpides;"Infectious Diseases (Q1); Microbiology (Q1); Microbiology (medical) (Q1); Virology (Q1)";342.0;1067.0;United Kingdom;1993-2020;10.1016/j.tim.2015.07.012;0.02282;194.0;;0966842X;18784380;0966842X;18784380;17.079;microbial genomics, metagenomics, comparative genome analysis;Elsevier Ltd.;;6473.0;Western Europe;4491.0;Q1;20862.0;Trends in microbiology;Ten years of maintaining and expanding a microbial genome and metagenome analysis system;17553.0;3925.0;135.0;356.0;8738.0;journal;article;2015
In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.;Guangjie Chen and Zhiqiang Ge;"Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1)";611.0;442.0;United Kingdom;1993-2020;10.1016/j.conengprac.2020.104344;0.00717;119.0;;09670661;09670661;09670661;09670661;3.475;Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis;Elsevier Ltd.;;4087.0;Western Europe;1175.0;Q1;18174.0;Control engineering practice;Robust bayesian networks for low-quality data modeling and process monitoring applications;8368.0;2779.0;196.0;615.0;8010.0;journal;article;2020
High-speed rail (HSR) has become a competitive mode with aviation for medium-distance intercity travel, given the massive deployment of the HSR infrastructure network in China. While the travel experience with both HSR and air has become more convenient, the systems’ operational reliability in terms of punctuality remains a key concern, especially during disruptive events, such as under severe weather conditions. Although previous studies have attempted to investigate the impact of severe weather events on the operational performance of transportation systems, there is still a lack of ability to forecast to what extent the performance of different transportation systems may vary under various conditions. This study develops an integrated modeling framework that allows us to predict the performance of weather-induced delays of different transportation systems, including HSR and aviation. By applying machine-learning methods to real-world transportation performance data, the study examines the robustness of the method, variations of data characteristics and the different applications of the predictive modeling system. Overall, the concept and modeling framework provide important implications for the improvement of transportation system resilience to various severe weather-related disruptions through the understanding of the impact and its predictability of the system performance.;Zhenhua Chen and Yuxuan Wang and Lei Zhou;"Geography, Planning and Development (Q1); Law (Q1); Transportation (Q1)";486.0;512.0;United Kingdom;1993-2020;10.1016/j.tranpol.2020.11.008;0.0091;96.0;;0967070X;1879310X;0967070X;1879310X;4.674;High-speed rail, Aviation, Extreme weather event, Machine learning, Big data;Elsevier Ltd.;;5591.0;Western Europe;1687.0;Q1;20838.0;Transport policy;Predicting weather-induced delays of high-speed rail and aviation in china;9048.0;2674.0;201.0;494.0;11238.0;journal;article;2021
High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.;Piotr Grabowski and Juri Rappsilber;"Biochemistry (Q1); Molecular Biology (Q1)";286.0;884.0;United Kingdom;1976-2020;10.1016/j.tibs.2018.10.010;0.02576;272.0;;09680004;13624326;03765067;09680004;13.807;data integration, data science, functional genomics, machine learning, systems biology;Elsevier Ltd.;;7079.0;Western Europe;6290.0;Q1;14816.0;Trends in biochemical sciences;A primer on data analytics in functional genomics: how to move from data to insight?;22003.0;3346.0;115.0;301.0;8141.0;journal;article;2019
Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.;Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou;"Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Management Science and Operations Research (Q1); Transportation (Q1)";824.0;1015.0;United Kingdom;1993-2020;10.1016/j.trc.2015.06.007;;133.0;;0968090X;0968090X;0968090X;0968090X;;Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns;Elsevier Ltd.;;5442.0;Western Europe;3185.0;Q1;20893.0;Transportation research part c: emerging technologies;Traffic zone division based on big data from mobile phone base stations;;8956.0;327.0;834.0;17795.0;journal;article;2015
"Summary
More than 70% of the experimentally determined macromolecular structures in the Protein Data Bank (PDB) contain small-molecule ligands. Quality indicators of ∼643,000 ligands present in ∼106,000 PDB X-ray crystal structures have been analyzed. Ligand quality varies greatly with regard to goodness of fit between ligand structure and experimental data, deviations in bond lengths and angles from known chemical structures, and inappropriate interatomic clashes between the ligand and its surroundings. Based on principal component analysis, correlated quality indicators of ligand structure have been aggregated into two largely orthogonal composite indicators measuring goodness of fit to experimental data and deviation from ideal chemical structure. Ranking of the composite quality indicators across the PDB archive enabled construction of uniformly distributed composite ranking score. This score is implemented at RCSB.org to compare chemically identical ligands in distinct PDB structures with easy-to-interpret two-dimensional ligand quality plots, allowing PDB users to quickly assess ligand structure quality and select the best exemplars.";Chenghua Shao and John D. Westbrook and Changpeng Lu and Charmi Bhikadiya and Ezra Peisach and Jasmine Y. Young and Jose M. Duarte and Robert Lowe and Sijian Wang and Yana Rose and Zukang Feng and Stephen K. Burley;"Molecular Biology (Q1); Structural Biology (Q1)";574.0;408.0;United States;1993-2020;10.1016/j.str.2021.10.003;0.022940000000000002;182.0;;09692126;09692126;18784186;09692126;5.006;ligand structure quality, composite ranking score, ligand structure, small-molecule ligand, ligand quality indicator, multivariate analysis, principal component analysis, Protein Data Bank, PDB, RCSB PDB;Cell Press;;4983.0;Northern America;2907.0;Q1;14274.0;Structure;Simplified quality assessment for small-molecule ligands in the protein data bank;17041.0;2494.0;150.0;575.0;7474.0;journal;article;2022
Country risk analysis has been a topic of investigation for decades, often focused on forecasting the risks to business profitability and assets when investing in a country. While there have been gradual improvements in the analytic techniques and overall breadth of the research, many scholars and practitioners continue to focus on limited conceptualizations of risk, measures with a relatively small number of variables, and/or expert analysis. Others point to the need to expand the inquiry, produce better tools and models, take advantage of the greater availability of data and enhanced computing techniques, and tackle puzzles differently. Advancing this discussion, we make the case for a new conceptualization and measurement of country-level risk and introduce the Robinson Country Risk Index (RCRI), a tool which incorporates four broad dimensions—Governance, Economics, Operations, and Society (GEOS). Within this holistic macrostructure, the RCRI encompasses 70 sub-dimensions, 126 countries, and, at present, 8 years of data. Its ecological conceptualization, multifaceted goals, and embedded functionalities complement and offer advantages over other risk indexes. The RCRI addresses concerns surrounding the conceptualization and measurement of country risk and provides a dynamic new instrument for educators, researchers, and practitioners.;Christopher L. Brown and S. Tamer Cavusgil and A. Wayne Lord;"Business and International Management (Q1); Finance (Q1); Marketing (Q1)";274.0;598.0;United Kingdom;1993-2020;10.1016/j.ibusrev.2014.07.012;0.00604;95.0;;09695931;09695931;09695931;09695931;5.915;Big data, Country risk, Critical thinking, Education, Index, Organizational context, State development, Strategy;Elsevier Ltd.;;11609.0;Western Europe;1773.0;Q1;23665.0;International business review;Country-risk measurement and analysis: a new conceptualization and managerial tool;7978.0;1923.0;96.0;279.0;11145.0;journal;article;2015
Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.;Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag;Marketing (Q1);550.0;777.0;United Kingdom;1994-2021;10.1016/j.jretconser.2021.102827;0.008759999999999999;89.0;;09696989;09696989;09696989;09696989;7.135;Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry;Elsevier Ltd.;;8030.0;Western Europe;1568.0;Q1;22992.0;Journal of retailing and consumer services;Cross-national differences in big data analytics adoption in the retail industry;10506.0;4555.0;346.0;555.0;27784.0;journal;article;2022
The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.;Iris Hausladen and Maximilian Schosser;"Law (Q1); Management, Monitoring, Policy and Law (Q1); Strategy and Management (Q1); Transportation (Q1)";349.0;468.0;United Kingdom;1994-1995, 1997-2020;10.1016/j.jairtraman.2019.101721;0.00394;75.0;;09696997;09696997;09696997;09696997;4.134;Maturity model, Network planning, Big data analytics, Airlines, Case study;Elsevier Ltd.;;5195.0;Western Europe;1220.0;Q1;20532.0;Journal of air transport management;Towards a maturity model for big data analytics in airline network planning;4929.0;1612.0;142.0;356.0;7377.0;journal;article;2020
We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.;Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga;Neurology (Q1);710.0;544.0;United States;1994-2020;10.1016/j.nbd.2018.05.026;0.02068;166.0;;09699961;1095953X;09699961;1095953X;5.996;Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI;Academic Press Inc.;;8894.0;Northern America;2205.0;Q1;17466.0;Neurobiology of disease;Big data sharing and analysis to advance research in post-traumatic epilepsy;21360.0;4074.0;330.0;722.0;29350.0;journal;article;2019
The Basel III framework, whose main thrust has been enhancing the banking sector's safety and stability, emphasises the need to improve the quality and quantity of capital components, leverage ratio, liquidity standards, and enhanced disclosures. This article first lays the context of Basel III and then incorporates the views of senior executives of Indian banks and risk management experts on addressing the challenges of implementing the Basel III framework, especially in areas such as augmentation of capital resources, growth versus financial stability, challenges for enhanced profitability, deposit pricing, cost of credit, maintenance of liquidity standards, and strengthening of risk architecture.;M. Jayadev;"Business, Management and Accounting (miscellaneous) (Q2); Economics and Econometrics (Q3)";82.0;241.0;United Kingdom;2010-2020;10.1016/j.iimb.2013.03.010;;20.0;;09703896;09703896;09703896;09703896;;Basel III, Capital regulation, Capital management, Indian banking;Elsevier Ltd.;;4644.0;Western Europe;425.0;Q2;19700175280.0;Iimb management review;Basel iii implementation: issues and challenges for indian banks;;220.0;36.0;95.0;1672.0;journal;article;2013
Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.;Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau;"Environmental Chemistry (Q1); Environmental Engineering (Q1); Environmental Science (miscellaneous) (Q1); Medicine (miscellaneous) (Q1)";1001.0;583.0;China;1970, 1972-1973, 1978-1985, 1993, 1995-2021;10.1016/j.jes.2022.02.041;0.013340000000000001;99.0;;10010742;10010742;18787320;10010742;5.565;Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area;Chinese Academy of Sciences;;5341.0;Asiatic Region;1316.0;Q1;23393.0;Journal of environmental sciences;Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data;17274.0;5975.0;329.0;1018.0;17571.0;journal;article;2023
Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.;Daowen Liu and Liqi Lei and Tong Ruan and Ping He;Medicine (miscellaneous) (Q4);125.0;73.0;United Kingdom;1991-2020;10.24920/003579;;21.0;;10019294;10019294;10019294;10019294;;electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study;Elsevier;;2920.0;Western Europe;215.0;Q4;29227.0;Chinese medical sciences journal;Constructing large scale cohort for clinical study on heart failure with electronic health record in regional healthcare platform: challenges and strategies in data reuse;;97.0;51.0;128.0;1489.0;journal;article;2019
Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.;Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang;"Chemical Engineering (miscellaneous) (Q2); Chemistry (miscellaneous) (Q2); Environmental Engineering (Q2); Biochemistry (Q3)";871.0;316.0;China;1993-2020;10.1016/j.cjche.2016.05.039;0.00619;54.0;;10049541;10049541;10049541;10049541;3.171;Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data;Chemical Industry Press;;4399.0;Asiatic Region;595.0;Q2;12888.0;Chinese journal of chemical engineering;A review of control loop monitoring and diagnosis: prospects of controller maintenance in big data era;6469.0;2755.0;355.0;874.0;15618.0;journal;article;2016
Physical metallurgical (PM) and data-driven approaches can be independently applied to alloy design. Steel technology is a field of physical metallurgy around which some of the most comprehensive understanding has been developed, with vast models on the relationship between composition, processing, microstructure and properties. They have been applied to the design of new steel alloys in the pursuit of grades of improved properties. With the advent of rapid computing and low-cost data storage, a wealth of data has become available to a suite of modelling techniques referred to as machine learning (ML). ML is being emergingly applied in materials discovery while it requires data mining with its adoption being limited by insufficient high-quality datasets, often leading to unrealistic materials design predictions outside the boundaries of the intended properties. It is therefore required to appraise the strength and weaknesses of PM and ML approach, to assess the real design power of each towards designing novel steel grades. This work incorporates models and datasets from well-established literature on marageing steels. Combining genetic algorithm (GA) with PM models to optimise the parameters adopted for each dataset to maximise the prediction accuracy of PM models, and the results were compared with ML models. The results indicate that PM approaches provide a clearer picture of the overall composition-microstructure-properties relationship but are highly sensitive to the alloy system and hence lack on exploration ability of new domains. ML conversely provides little explicit physical insight whilst yielding a stronger prediction accuracy for large-scale data. Hybrid PM/ML approaches provide solutions maximising accuracy, while leading to a clearer physical picture and the desired properties.;Chunguang Shen and Chenchong Wang and Pedro E.J. Rivera-Díaz-del-Castillo and Dake Xu and Qian Zhang and Chi Zhang and Wei Xu;"Ceramics and Composites (Q1); Materials Chemistry (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1); Metals and Alloys (Q1); Polymers and Plastics (Q1)";883.0;814.0;Netherlands;1993-2021;10.1016/j.jmst.2021.02.017;;68.0;;10050302;10050302;10050302;10050302;;Machine learning, Physical metallurgy, Small sample problem, Marageing steel;Elsevier BV;;5264.0;Western Europe;1743.0;Q1;12330.0;Journal of materials science and technology;Discovery of marageing steels: machine learning vs. physical metallurgical modelling;;6659.0;572.0;890.0;30108.0;journal;article;2021
Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.;Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong;Multidisciplinary (Q1);199.0;279.0;China;2003-2021;10.23919/TST.2017.7914196;0.00116;43.0;;10070214;10070214;10070214;10070214;2.016;"Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining";Tsing Hua University;;3143.0;Asiatic Region;428.0;Q1;25522.0;Tsinghua science and technology;Efficient currency determination algorithms for dynamic data;1474.0;564.0;69.0;200.0;2169.0;journal;article;2017
High-quality data are the foundation to monitor the progress and evaluate the effects of road traffic injury prevention measures. Unfortunately, official road traffic injury statistics delivered by governments worldwide, are often believed somewhat unreliable and invalid. We summarized the reported problems concerning the road traffic injury statistics through systematically searching and reviewing the literature. The problems include absence of regular data, under-reporting, low specificity, distorted cause spectrum of road traffic injury, inconsistency, inaccessibility, and delay of data release. We also explored the mechanisms behind the problematic data and proposed the solutions to the addressed challenges for road traffic statistics.;Fang-Rong Chang and He-Lai Huang and David C. Schwebel and Alan H.S. Chan and Guo-Qing Hu;"Orthopedics and Sports Medicine (Q3); Surgery (Q3)";222.0;156.0;Netherlands;2001-2020;10.1016/j.cjtee.2020.06.001;;26.0;;10081275;10081275;10081275;10081275;;Traffic injury data, Reported problems, Mechanisms behind the data;Elsevier;;2900.0;Western Europe;442.0;Q3;29582.0;Chinese journal of traumatology - english edition;Global road traffic injury statistics: challenges, mechanisms and solutions;;423.0;78.0;227.0;2262.0;journal;article;2020
"Objective
Databases and softwares are important to manage modern high-throughput laboratories and store clinical and genomic information for quality assurance. Commercial softwares are expensive with proprietary code issue while academic versions have adaptation issue. Our aim was to develop an adaptable in-house software that can stores specimen and disease-associated genetic information in biobank to facilitate translational research.
Methods
Prototype was designed as per the research requirements and computational tools were used to develop software under three tiers; Visual Basic and ASP.net for presentation tier, SQL server for data tier, and Ajax and JavaScript for business tier. We retrieved specimens from biobank using this software and performed microarray based transcriptomic analysis to detect differentialy expressed genes (DEGs) with FC ±2 and P-value <0.05 in triple negative breast cancer cases. Ingenuity pathway analysis tool was used to predict canonical molecular pathways associated with disease. Overall performance and utility of software was evaluated by JMeter software, CRUD function test and set of feedback questioners.
Results
We developed “Biosearch System”, a web-based software enabling management of biobank samples (tissue, blood, FTTP slides) and their extracts (DNA, RNA and proteins) with clinical and experimental details. The client satisfaction feedback was excellent with score 4.7/5. We identified a total of 1181 DEGs including both upregulated (IFI6, LEF1, FANCI, CASC5, PLXNA3 etc.) and down-regulated (ADH1B, LYVE1, ADH1C, ADH1B, ADIPOQ, PLIN1, LYVE1 etc.) genes in triple negative breast cancer. Pathway analysis of DEGs revealed significant activation of interferon signaling (z-score 2.646) and kinetochore metaphase signaling pathway (z-score 2.138) in cancer.
Conclusion
Biosearch System is a user friendly LIMS for collection, storage and retrieval of specimen and clinical information. It is secure, efficient, and very convenient in sample tracking and data analysis. We illustrated its utility in transcriptomic study of breast cancer. Additionally, it can facilitate and speed up any genomic study and translational research publications.";Sajjad Karim and Mona Al-Kharraz and Zeenat Mirza and Hend Noureldin and Heba Abusamara and Nofe Alganmi and Adnan Merdad and Saddig Jastaniah and Sudhir Kumar and Mahmood Rasool and Adel Abuzenadah and Mohammed Al-Qahtani;Multidisciplinary (Q1);342.0;374.0;Saudi Arabia;1994, 2009-2020;10.1016/j.jksus.2021.101760;;38.0;;10183647;10183647;10183647;10183647;;Biosearch system, LIMS database, Biobank, Genomics, Microarray, Bioinformatics;King Saud University;;3350.0;Middle East;574.0;Q1;86891.0;Journal of king saud university - science;Development of “biosearch system” for biobank management and storage of disease associated genetic information;;1344.0;498.0;343.0;16685.0;journal;article;2022
;Pierre-Yves Cordier and Eliott Gaudray and Edouard Martin and Raphaël Paris and Salah Boussen and Philippe Goutorbe;"Critical Care Nursing (Q1); Emergency Nursing (Q1)";151.0;215.0;Ireland;1992-2020;10.1016/j.aucc.2019.08.001;0.00179;36.0;;10367314;10367314;10367314;10367314;2.737;;Elsevier Ireland Ltd;;3664.0;Western Europe;732.0;Q1;27139.0;Australian critical care;Comprehensive care documentation: a first step not to be missed;1194.0;437.0;117.0;186.0;4287.0;journal;article;2020
The Tibetan Plateau (TP) is a vast elevated plateau in central Asia, and profoundly impacts regional weather and climate, and even global atmospheric circulation. Here, two frequently used ERA-Interim reanalyses with a spatial resolution of 1.5° × 1.5° (EIN15) and 0.75° × 0.75° (EIN75) are evaluated using a gridded observation dataset at 0.25° spatial resolution from the National Climate Center in China across the TP that covers the period 1979–2012. Climatological characteristics, mean monthly changes, and spatial–temporal trends are examined, with a focus on air temperature and precipitation. Topographic corrections for temperature in ERA-Interim are first conducted based on a vertical temperature lapse rate. The results show that EIN15 and EIN75 with topographic correction closely reproduce the spatial distribution and mean monthly change of temperature on the TP, notwithstanding some cold biases not seen in the observations. The two reanalysis datasets exhibit significant temperature increases over most of the TP, which is similar to the observations. However, the trends exhibit different spatial patterns for all seasons aside from summer, and have lower magnitudes than the observations. EIN15 and EIN75 also reproduce the broad spatial distribution of precipitation, but overestimate precipitation amounts, especially on the southern TP. They also capture some of the observed spatial patterns in the precipitation trend for the period 1979–2012, particularly in winter. Overall, the mean monthly change, mean annual, winter, and summer climatology, and their temporal trends of temperature reproduced by the ERA-Interim data are much better than those of precipitation. As a result of its higher resolution and more accurate topography, EIN75 generates a closer fit to the observed temperatures on the TP than EIN15, but there are no significant differences in precipitation between the two reanalysis datasets. Evaluation of these datasets would be very informative for further climate research and simulations on the TP.;Xuejia Wang and Guojin Pang and Meixue Yang and Guohui Zhao;Earth-Surface Processes (Q1);1841.0;207.0;United Kingdom;1989-2020;10.1016/j.quaint.2016.12.041;0.02478;106.0;;10406182;10406182;10406182;10406182;2.130;Tibetan Plateau, ERA-Interim, Gridded observations, Climate, Spatial–temporal trends;Elsevier Ltd.;;8079.0;Western Europe;927.0;Q1;25776.0;Quaternary international;Evaluation of climate on the tibetan plateau using era-interim reanalysis and gridded observations during the period 1979–2012;18778.0;4556.0;656.0;1961.0;53000.0;journal;article;2017
Reliable data is needed to understand financial relationships in the power sector. However, relevant data acquisition and visualization can be a challenge due to the fragmented nature of the power sector. The US DOE and ORNL leveraged a Sankey prototype to elucidate the ‘big picture’ of financial flows to understand the complex relationships between specific actors within the power sector. The continued incorporation of high quality data can improve the fidelity of such an approach and lead to an increasingly detailed understanding of financial relationships in the power sector and their implications for policymakers.;Claire Zeng and Stephen Hendrickson and Sangkeun Matt Lee and Supriya Chinthavali and Jessica Lin and Eric Hsieh and Mallikarjun Shankar;"Business and International Management (Q1); Law (Q1); Energy (miscellaneous) (Q2); Management of Technology and Innovation (Q2)";276.0;205.0;United States;1988-2020;10.1016/j.tej.2017.03.001;;47.0;;10406190;10406190;10406190;10406190;;Energy, Financial data, Power sector revenue, Sankey, Visualization;Elsevier Inc.;;2527.0;Northern America;750.0;Q1;28802.0;Electricity journal;Energy finance data warehouse: tracking revenues through the power sector;;632.0;113.0;311.0;2855.0;journal;article;2017
This paper is a review of the literature on fintech and its interaction with banking. Included in fintech are innovations in payment systems (including cryptocurrencies), credit markets (including P2P lending), and insurance, with Blockchain-assisted smart contracts playing a role. The paper provides a definition of fintech, examines some statistics and stylized facts, and then reviews the theoretical and empirical literature. The review is organized around four main research questions. The paper summarizes our knowledge on these questions and concludes with questions for future research.;Anjan V. Thakor;"Economics and Econometrics (Q1); Finance (Q1)";83.0;482.0;United States;1990-2020;10.1016/j.jfi.2019.100833;0.0046700000000000005;77.0;;10429573;10960473;10429573;10960473;5.179;Fintech, Cryptocurrencies, P2P lending, Banking, Systemic risk;Academic Press Inc.;;4358.0;Northern America;5445.0;Q1;28996.0;Journal of financial intermediation;Fintech and banking: what do we know?;3216.0;436.0;48.0;87.0;2092.0;journal;article;2020
Ovarian function is central to female fertility, and several genome-wide association studies (GWAS) have been carried out to elucidate the genetic background of traits and disorders that reflect and affect ovarian physiology. While GWAS have been successful in reporting numerous genetic associations and highlighting involved pathways relevant to reproductive aging, for ovarian disorders, such as premature ovarian insufficiency and polycystic ovary syndrome, research has lagged behind due to insufficient study sample size. Novel approaches to study design and analysis methods that help to fit GWAS findings into biological context will improve our knowledge about genetics governing ovarian function in fertility and disease, and provide input for clinical tools and better patient management.;Triin Laisk-Podar and Cecilia M. Lindgren and Maire Peters and Juha S. Tapanainen and Cornelis B. Lambalk and Andres Salumets and Reedik Mägi;"Endocrinology (Q1); Endocrinology, Diabetes and Metabolism (Q1)";266.0;771.0;United States;1989-2020;10.1016/j.tem.2016.04.011;0.01355;168.0;;10432760;10432760;18793061;10432760;12.015;ovarian reserve, menopause, polycystic ovary syndrome, premature ovarian insufficiency;Elsevier Inc.;;7722.0;Northern America;3520.0;Q1;26746.0;Trends in endocrinology and metabolism;Ovarian physiology and gwas: biobanks, biology, and beyond;11973.0;2620.0;93.0;270.0;7181.0;journal;article;2016
Based on microdata from China's listed companies and macrodata for broadband internet access in prefecture-level cities, this paper explores the relationship between broadband internet and enterprise innovation. Using the change in market concentration caused by the North–South separation reform of China Telecom in 2002 as an instrumental variable, the results show that in general, a 1% increase in broadband internet access results in a 1.395% increase in the number of corporate patents. Specifically, the number of valid patents, patent citations and valid patent citations, reflecting patent quality, increases by 1.499%, 0.920% and 0.763%, respectively. The mechanistic analysis shows that broadband internet access contributes to increasing the number of R&D personnel and personal innovation efficiency, enhancing enterprises' willingness to innovate, and easing financing constraints. Further analysis suggests that broadband internet access mainly promotes invention patents rather than design patents. The innovation effect is more evident among high-tech, inventor-intensive, state-owned enterprises and enterprises located in the non-southeastern coastal region of China.;Mengjun Yang and Shilin Zheng and Lin Zhou;"Economics and Econometrics (Q1); Finance (Q1)";311.0;396.0;Netherlands;1989-2020;10.1016/j.chieco.2022.101802;0.00605;76.0;;1043951X;1043951X;1043951X;1043951X;4.227;Broadband internet, Innovation, Instrumental variable, Knowledge spillover, Financing constraints;Elsevier;;4520.0;Western Europe;1361.0;Q1;20566.0;China economic review;Broadband internet and enterprise innovation;5722.0;1318.0;184.0;319.0;8316.0;journal;article;2022
Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.;Guangqi Li and Xin Wu and Xuelei Ma;Cancer Research (Q1);286.0;1139.0;United States;1990-2020;10.1016/j.semcancer.2022.08.005;0.01211;148.0;;1044579X;10963650;1044579X;10963650;15.707;Artificial intelligence, Radiotherapy, Auto-segmentation, Auto-planning, Quality assurance;Academic Press Inc.;;16697.0;Northern America;3908.0;Q1;24046.0;Seminars in cancer biology;Artificial intelligence in radiotherapy;11552.0;3555.0;287.0;310.0;47920.0;journal;article;2022
"The COVID-19 pandemic has brought into sharp focus the importance of strategies supporting vaccine development. During the pandemic, TRANSVAC, the European vaccine–research-infrastructure initiative, undertook an in-depth consultation of stakeholders to identify how best to position and sustain a European vaccine R&D infrastructure. The consultation included an online survey incorporating a gaps-and-needs analysis, follow-up interviews and focus-group meetings. Between October 2020 and June 2021, 53 organisations completed the online survey, including 24 research institutes and universities, and 9 pharmaceutical companies; 24 organisations participated in interviews, and 14 in focus-group meetings. The arising recommendations covered all aspects of the vaccine-development value chain: from preclinical development to financing and business development; and covered prophylactic and therapeutic vaccines, for both human and veterinary indications. Overall, the recommendations supported the expansion and elaboration of services including training programmes, and improved or more extensive access to expertise, technologies, partnerships, curated databases, and-data analysis tools. Funding and financing featured as critical elements requiring support throughout the vaccine-development programmes, notably for academics and small companies, and for vaccine programmes that address medical and veterinary needs without a great potential for commercial gain. Centralizing the access to these research infrastructures via a single on-line portal was considered advantageous.";Stefan Jungbluth and Hilde Depraetere and Monika Slezak and Dennis Christensen and Norbert Stockhofe and Laurent Beloeil;"Applied Microbiology and Biotechnology (Q3); Bioengineering (Q3); Biotechnology (Q3); Immunology and Microbiology (miscellaneous)  (Q3); Medicine (miscellaneous) (Q3); Pharmacology (Q3)";233.0;180.0;United States;1990-2020;10.1016/j.biologicals.2022.02.003;0.00175;56.0;;10451056;10958320;10451056;10958320;1.856;Vaccines, Research and development, Research infrastructure, Science policy, Platform technologies, Manufacturing;Academic Press Inc.;;2942.0;Northern America;503.0;Q3;13671.0;Biologicals;A gaps-and-needs analysis of vaccine r&d in europe: recommendations to improve the research infrastructure;1973.0;442.0;76.0;238.0;2236.0;journal;article;2022
Graph databases are taking place in many different applications: smart city, smart cloud, smart education, etc. In most cases, the applications imply the creation of ontologies and the integration of a large set of knowledge to build a knowledge base as an RDF KB store, with ontologies, static data, historical data and real time data. Most of the RDF stores are endowed with inferential engines that materialize some knowledge as triples during indexing or querying. In these cases, deleting concepts may imply the removal and change of many triples, especially if the triples are those modeling the ontological part of the knowledge base, or are referred by many other concepts. For these solutions, the graph database versioning feature is not provided at level of the RDF stores tool, and it is quite complex and time consuming to be addressed as black box approach. In most cases the indexing is a time consuming process, and the rebuilding of the KB may imply manually edited long scripts that are error prone. Therefore, in order to solve these kinds of problems, this paper proposes a lifecycle methodology and a tool supporting versioning of indexes for RDF KB store. The solution proposed has been developed on the basis of a number of knowledge oriented projects as Sii-Mobility (smart city), RESOLUTE (smart city risk assessment), ICARO (smart cloud). Results are reported in terms of time saving and reliability.;Pierfrancesco Bellini and Ivan Bruno and Paolo Nesi and Nadia Rauch;"Language and Linguistics (Q2); Computer Science Applications (Q3); Human-Computer Interaction (Q3)";110.0;188.0;United Kingdom;1990-2019;10.1016/j.jvlc.2015.10.018;;49.0;;1045926X;10958533;1045926X;10958533;;RDF knowledge base versioning, Graph stores versioning, RDF store management, Knowledge base life cycle;Elsevier Ltd.;;0.0;Western Europe;227.0;Q2;27187.0;Journal of visual languages and computing;Graph databases methodology and tool supporting index/store versioning;;216.0;0.0;115.0;0.0;journal;article;2015
DNA methylation provides a pivotal layer of epigenetic regulation in eukaryotes that has significant involvement for numerous biological processes in health and disease. The function of methylation of cytosine bases in DNA was originally proposed as a “silencing” epigenetic marker and focused on promoter regions of genes for decades. Improved technologies and accumulating studies have been extending our understanding of the roles of DNA methylation to various genomic contexts including gene bodies, repeat sequences and transcriptional start sites. The demand for comprehensively describing DNA methylation patterns spawns a diversity of DNA methylation profiling technologies that target its genomic distribution. These approaches have enabled the measurement of cytosine methylation from specific loci at restricted regions to single-base-pair resolution on a genome-scale level. In this review, we discuss the different DNA methylation analysis technologies primarily based on the initial treatments of DNA samples: bisulfite conversion, endonuclease digestion and affinity enrichment, involving methodology evolution, principles, applications, and their relative merits. This review may offer referable information for the selection of various platforms for genome-wide analysis of DNA methylation.;Shizhao Li and Trygve O. Tollefsbol;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Molecular Biology (Q1)";627.0;316.0;United States;1990-2020;10.1016/j.ymeth.2020.10.002;0.0203;150.0;;10462023;10462023;10959130;10462023;3.608;DNA methylation, DNA hydroxymethylation, Next-generation sequencing, Bisulfite conversion, Endonuclease digestion, Affinity enrichment, Microarray;Academic Press Inc.;;5896.0;Northern America;2080.0;Q1;14129.0;Methods;Dna methylation methods: global dna methylation and methylomic analyses;26408.0;2617.0;219.0;676.0;12913.0;journal;article;2021
The use of clarifying questions (CQs) is a fairly new and useful technique to aid systems in recognizing the intent, context, and preferences behind user queries. Yet, understanding the extent of the effect of CQs on user behavior and the ability to identify relevant information remains relatively unexplored. In this work, we conduct a large user study to understand the interaction of users with CQs in various quality categories, and the effect of CQ quality on user search performance in terms of finding relevant information, search behavior, and user satisfaction. Analysis of implicit interaction data and explicit user feedback demonstrates that high-quality CQs improve user performance and satisfaction. By contrast, low- and mid-quality CQs are harmful, and thus allowing the users to complete their tasks without CQ support may be preferred in this case. We also observe that user engagement, and therefore the need for CQ support, is affected by several factors, such as search result quality or perceived task difficulty. The findings of this study can help researchers and system designers realize why, when, and how users interact with CQs, leading to a better understanding and design of search clarification systems.;Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Pera, Maria Soledad and Liu, Yiqun;"Business, Management and Accounting (miscellaneous) (Q1); Information Systems (Q1); Computer Science Applications (Q2)";126.0;710.0;United States;1983-2020;10.1145/3524110;0.00183;83.0;;10468188;10468188;15582868;10468188;4.797;"User Study; Information Seeking Systems; Clarifying Questions";Association for Computing Machinery (ACM);Association for Computing Machinery;7002.0;Northern America;672.0;Q1;18997.0;Acm transactions on information systems;Users meet clarifying questions: toward a better understanding of user interactions for search clarification;2193.0;850.0;44.0;126.0;3081.0;journal;article;2022
"ABSTRACT
Purpose
The use of predictive models in epidemiology is relatively narrow as most of the studies report results of traditional statistical models such as Linear, Logistic, or Cox regressions. In this study, a high-dimensional epidemiological cohort, collected within the Kuopio Ischemic Heart Disease Risk Factor Study in 1984–1989, was used to investigate the predictive ability of models with embedded variable selection.
Methods
Simple Logistic Regression with seven preselected risk factors was compared to k-Nearest Neighbors, Logistic Lasso Regression, Decision Tree, Random Forest, and Multilayer Perceptron in predicting cardiovascular death for the aged men from Kuopio Ischemic Heart Disease Risk Factor for the long horizon of 30 ± 3 years: 746 predictor variables were available for 2682 men (705 cardiovascular deaths were registered). We considered two scenarios of handling competing risks (removing subjects and treating them as non-cases).
Results
The best average AUC on the test sample was 0.8075 (95%CI, 0.8051–0.8099) in scenario 1 and 0.7155 (95%CI, 0.7128–0.7183) in scenario 2 achieved with Logistic Lasso Regression, which was 6.04% and 5.50% higher than the baseline AUC provided by Logistic Regression with manually preselected predictors.
Conclusions
In both scenarios Logistic Lasso Regression, Random Forest, and Multilayer Perceptron outperformed Simple Logistic Regression.";Christina Brester and Ari Voutilainen and Tomi-Pekka Tuomainen and Jussi Kauhanen and Mikko Kolehmainen;Epidemiology (Q2);361.0;300.0;United States;1990-2020;10.1016/j.annepidem.2022.03.010;0.01121;122.0;;10472797;18732585;10472797;18732585;3.797;Machine learning, Prediction of cardiovascular death, Population study, Epidemiology;Elsevier Inc.;;3734.0;Northern America;1644.0;Q2;19569.0;Annals of epidemiology;Epidemiological predictive modeling: lessons learned from the kuopio ischemic heart disease risk factor study☆;8616.0;1286.0;118.0;396.0;4406.0;journal;article;2022
With the growing accessibility and usability of internet there is a growing concern over content protection of digital images. Recently, to eliminate the traditional use of passwords and to ensure that the access to the image is restricted only to legitimate users, security solutions are increasingly combined with biometrics. Consequently, biometric-based watermarking algorithms, that involve embedding the identity of the owner, are proposed to solve ownership disputes. This paper presents a new scheme for protecting and authenticating invisibly watermarked digital images. It applies Independent Component Analysis to the cover image and enables the insertion of two independent watermarks based on fingerprint and iris biometrics. In this approach biometric techniques are used for watermarks generation and for owners authentication. The main advantage of proposed algorithm is construction of ICA based watermarking domain to enable insertion of two independent watermarks, that improve authentication accuracy and makes scheme more robust.;Wioletta Wójtowicz and Marek R. Ogiela;"Media Technology (Q1); Computer Vision and Pattern Recognition (Q2); Electrical and Electronic Engineering (Q2); Signal Processing (Q2)";764.0;355.0;United States;1990-2020;10.1016/j.jvcir.2016.02.006;0.007390000000000001;81.0;;10473203;10473203;10473203;10473203;2.678;Image watermarking, Image authentication, Biometric watermarking, Independent Component Analysis (ICA), Biometric verification, Multibiometrics, Fingerprint recognition, Iris recognition;Academic Press Inc.;;4385.0;Northern America;502.0;Q1;25592.0;Journal of visual communication and image representation;Digital images authentication scheme based on bimodal biometric watermarking in an independent domain;5331.0;2863.0;169.0;769.0;7411.0;journal;article;2016
This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.;Jean-Sébastien Lacam and David Salvetat;"Computer Science Applications (Q2); Information Systems and Management (Q2); Management of Technology and Innovation (Q2); Marketing (Q2); Strategy and Management (Q2)";56.0;476.0;United Kingdom;1990-2020;10.1016/j.hitech.2021.100406;;46.0;;10478310;10478310;10478310;10478310;;Big data, Smart data, Volume, Velocity, Variety, Automotive distribution;Elsevier BV;;6772.0;Western Europe;684.0;Q2;19713.0;Journal of high technology management research;Big data and smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop;;243.0;18.0;56.0;1219.0;journal;article;2021
Understanding the traits that define a leader is a perennial quest. An ongoing debate surrounds the complexity required to unravel the leader trait paradigm. With the advancement of machine learning, scholars are now better equipped to model leadership as an outcome of complex patterns in traits. However, interpreting those models is often harder. In this paper, we guide researchers in the application of machine learning techniques to uncover complex relationships. Specifically, we demonstrate how applying machine learning can help to assess the complexity of a relationship and show techniques that help interpret the outcomes of “black box” machine learning algorithms. While demonstrating techniques to uncover complex relationships, we are using the Big Five Inventory and need for cognition to predict leadership role occupancy. Among our sample (n = 3385), we find that the leader trait paradigm can benefit from modeling complexity beyond linear effects and generate several interpretable results.;Brian M. Doornenbal and Brian R. Spisak and Paul A. {van der Laken};"Applied Psychology (Q1); Business and International Management (Q1); Organizational Behavior and Human Resource Management (Q1); Sociology and Political Science (Q1)";142.0;994.0;United States;1990-2020;10.1016/j.leaqua.2021.101515;0.00895;151.0;;10489843;10489843;10489843;10489843;10.517;Leader trait paradigm, Machine learning, Complexity, Interpretability, Personality;Elsevier Inc.;;12072.0;Northern America;4989.0;Q1;21149.0;Leadership quarterly;Opening the black box: uncovering the leader trait paradigm through machine learning;14059.0;1576.0;78.0;150.0;9416.0;journal;article;2021
Symbiotic simulation systems that incorporate data-driven methods (such as machine/deep learning) are effective and efficient tools for just-in-time (JIT) operational decision making. With the growing interest on Digital Twin City, such systems are ideal for real-time microscopic traffic simulation. However, learning-based models are heavily biased towards the training data and could produce physically inconsistent outputs. In terms of microscopic traffic simulation, this could lead to unsafe driving behaviours causing vehicle collisions in the simulation. As for symbiotic simulation, this could severely affect the performance of real-time base simulation model resulting in inaccurate or unrealistic forecasts, which could, in turn, mislead JIT what-if analysis. To overcome this issue, a physics-guided data-driven modelling paradigm should be adopted so that the resulting model could capture both accurate and safe driving behaviours. However, very few works exist in the development of such a car-following model that can balance between simulation accuracy and physical consistency. Therefore, in this paper, a new “jointly-trained physics-guided Long Short-term Memory (JTPG-LSTM)” neural network, is proposed and integrated to a dynamic data-driven simulation system to capture dynamic car-following behaviours. An extensive set of experiments was conducted to demonstrate the advantages of the proposed model from both modelling and simulation perspectives.;Naing, Htet and Cai, Wentong and Nan, Hu and Tiantian, Wu and Liang, Yu;"Computer Science Applications (Q3); Modeling and Simulation (Q3)";77.0;233.0;United States;1991-2020;10.1145/3558555;0.00054;51.0;;10493301;15581195;10493301;15581195;1.075;"Data-driven modelling; car-following; physics-guided machine learning; online learning; just-in-time simulation; real-time simulation; symbiotic simulation; digital twin";Association for Computing Machinery (ACM);Association for Computing Machinery;3592.0;Northern America;380.0;Q3;23054.0;Acm transactions on modeling and computer simulation;Dynamic data-driven microscopic traffic simulation using jointly trained physics-guided long short-term memory;722.0;171.0;25.0;81.0;898.0;journal;article;2022
Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.;Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang;Software (Q2);58.0;494.0;United States;1992-2020;10.1145/3468854;0.00099;78.0;;1049331X;15577392;1049331X;15577392;2.674;deep learning, Machine learning, software security;Association for Computing Machinery (ACM);Association for Computing Machinery;7053.0;Northern America;597.0;Q2;18121.0;Acm transactions on software engineering and methodology;Spi: automated identification of security patches via commits;908.0;297.0;32.0;66.0;2257.0;journal;article;2021
;Krzysztof Drzymalski and Joshua Schulman-Marcus;Cardiology and Cardiovascular Medicine (Q1);144.0;249.0;United States;1991-2020;10.1016/j.tcm.2017.08.005;0.004229999999999999;98.0;;10501738;10501738;18732615;10501738;6.677;;Elsevier Inc.;;3968.0;Northern America;1599.0;Q1;20655.0;Trends in cardiovascular medicine;Editorial commentary: death after acute myocardial infarction, possible to predict?;3542.0;732.0;171.0;326.0;6786.0;journal;article;2018
Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available “big data” sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.;Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed;"Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cardiology and Cardiovascular Medicine (Q2)";715.0;186.0;United States;1990-2020;10.1016/j.jvir.2022.08.003;0.01078;133.0;;10510443;10510443;15357732;10510443;3.464;;Elsevier Inc.;;1689.0;Northern America;979.0;Q1;17243.0;Journal of vascular and interventional radiology;A practical guide to use of publicly available data sets for observational research in interventional radiology;11063.0;2111.0;401.0;1086.0;6771.0;journal;article;2022
We construct a new measure of aggregate housing wealth for the U.S. based on (1) home-value estimates derived from machine learning algorithms applied to detailed information on property characteristics and recent transaction prices, and (2) Census housing unit counts. According to our new measure, the timing and amplitude of the recent house-price cycle differs materially but plausibly from commonly-used measures, which are based on survey data or repeat-sales price indexes. Thus, our methodology generates estimates that should be of considerable value to researchers and policymakers interested in the dynamics of aggregate housing wealth.;Joshua Gallin and Raven Molloy and Eric Nielsen and Paul Smith and Kamila Sommer;Economics and Econometrics (Q2);110.0;157.0;United States;1991-2020;10.1016/j.jhe.2020.101734;0.00176;51.0;;10511377;10960791;10511377;10960791;1.705;Residential real estate, Consumer economics and finance, Data collection and estimation, Flow of funds;Academic Press Inc.;;4517.0;Northern America;819.0;Q2;13927.0;Journal of housing economics;Measuring aggregate housing wealth: new insights from machine learning ☆;1602.0;294.0;35.0;114.0;1581.0;journal;article;2021
Detecting weak celestial source signals from massive radio data is a very challenging task because the radiation received by radio telescope is very weak and prone to disturbances. In order to detect these weak signals, we propose a two-stage object detection method that performs more finely in computer vision tasks. The novelty of the proposed method is to combine traditional soft thresholding denoising methods with attention mechanisms in deep neural networks. We propose a channel attention shrinkage network as the backbone of the object detection model to extract the features of weak signals from celestial sources by removing noise-related information. Moreover, targeting the characteristics of celestial source fringes in phase images, we propose a cluster-based anchor boxes generation algorithm to improve the accuracy of fringes position detection. We also introduce the CIoU loss function to improve the performance of the model because of the large aspect ratio of the celestial source fringes in the phase image. We generate simulated celestial source fringes data based on the parameters of the observation system to train our model and conduct experiments to evaluate the performance of the proposed algorithm. Our model obtains satisfactory detection accuracy and accurate for the location of celestial source fringes.;Ruiqing Yan and Rong Ma and Wei Liu and Zongyao Yin and Zhengang Zhao and Siying Chen and Sheng Chang and Hui Zhu and Dan Hu and Xianchuan Yu;"Computer Vision and Pattern Recognition (Q1); Applied Mathematics (Q2); Artificial Intelligence (Q2); Computational Theory and Mathematics (Q2); Electrical and Electronic Engineering (Q2); Signal Processing (Q2); Statistics, Probability and Uncertainty (Q2)";587.0;424.0;United States;1991-2020;10.1016/j.dsp.2022.103663;;76.0;;10512004;10954333;10512004;10954333;;Deep learning, Weak signal detection, Celestial source fringe, Soft thresholding, Astronomical image processing;Elsevier Inc.;;4323.0;Northern America;631.0;Q1;24306.0;Digital signal processing: a review journal;Weak celestial source fringes detection based on channel attention shrinkage networks and cluster-based anchor boxes generation algorithm;;2366.0;194.0;593.0;8386.0;journal;article;2022
;Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren;"Anesthesiology and Pain Medicine (Q2); Cardiology and Cardiovascular Medicine (Q2)";1041.0;158.0;United Kingdom;1991-2020;10.1053/j.jvca.2019.11.012;0.00838;82.0;;10530770;10530770;15328422;10530770;2.628;;W.B. Saunders Ltd;;2892.0;Western Europe;678.0;Q2;23831.0;Journal of cardiothoracic and vascular anesthesia;Making sense of big data to improve perioperative care: learning health systems and the multicenter perioperative outcomes group;7080.0;2501.0;770.0;1616.0;22269.0;journal;article;2020
Electronic health records (EHR) have been implemented successfully in a majority of United States healthcare systems in some form. There has been a rise in secondary uses of EHR, especially for research. EHR data is large, heterogenous, incomplete, noisy, and primarily created for purposes other than research. This presents many challenges, many of which are beginning to be overcome with the application of computer science artificial intelligence techniques, such as natural language processing and machine learning. EHR are gradually being redesigned to facilitate future research, though we are still far from a “complete EHR.”;Ellen Kim and Samuel M. Rubinstein and Kevin T. Nead and Andrzej P. Wojcieszynski and Peter E. Gabriel and Jeremy L. Warner;"Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cancer Research (Q2)";118.0;529.0;United Kingdom;1991-2020;10.1016/j.semradonc.2019.05.010;0.00271;93.0;;10534296;10534296;15329461;10534296;5.934;;W.B. Saunders Ltd;;6438.0;Western Europe;1761.0;Q1;13121.0;Seminars in radiation oncology;The evolving use of electronic health records (ehr) for research;2837.0;654.0;40.0;129.0;2575.0;journal;article;2019
The current literature on the use of disruptive innovative technologies, such as artificial intelligence (AI) for human resource management (HRM) function, lacks a theoretical basis for understanding. Further, the adoption and implementation of AI-augmented HRM, which holds promise for delivering several operational, relational and transformational benefits, is at best patchy and incomplete. Integrating the technology, organisation and people (TOP) framework with core elements of the theory of innovation assimilation and its impact on a range of AI-Augmented HRM outcomes, or what we refer to as (HRM(AI)), this paper develops a coherent and integrated theoretical framework of HRM(AI) assimilation. Such a framework is timely as several post-adoption challenges, such as the dark side of processual factors in innovation assimilation and system-level factors, which, if unattended, can lead to the opacity of AI applications, thereby affecting the success of any HRM(AI). Our model proposes several testable future research propositions for advancing scholarship in this area. We conclude with implications for theory and practice.;Verma Prikshat and Ashish Malik and Pawan Budhwar;"Applied Psychology (Q1); Organizational Behavior and Human Resource Management (Q1)";119.0;686.0;United Kingdom;1991-2020;10.1016/j.hrmr.2021.100860;0.0041;92.0;;10534822;10534822;10534822;10534822;7.444;Technology-driven HRM, AI-adoption in HRM, AI-augmented HRM, Processual factors;Elsevier Ltd.;;11407.0;Western Europe;2549.0;Q1;24719.0;Human resource management review;Ai-augmented hrm: antecedents, assimilation and multilevel consequences;3952.0;1006.0;57.0;127.0;6502.0;journal;article;2021
The accumulation of multisite large-sample MRI datasets collected during large brain research projects in the last decade has provided critical resources for understanding the neurobiological mechanisms underlying cognitive functions and brain disorders. However, the significant site effects observed in imaging data and their derived structural and functional features have prevented the derivation of consistent findings across multiple studies. The development of harmonization methods that can effectively eliminate complex site effects while maintaining biological characteristics in neuroimaging data has become a vital and urgent requirement for multisite imaging studies. Here, we propose a deep learning-based framework to harmonize imaging data obtained from pairs of sites, in which site factors and brain features can be disentangled and encoded. We trained the proposed framework with a publicly available traveling subject dataset from the Strategic Research Program for Brain Sciences (SRPBS) and harmonized the gray matter volume maps derived from eight source sites to a target site. The proposed framework significantly eliminated intersite differences in gray matter volumes. The embedded encoders successfully captured both the abstract textures of site factors and the concrete brain features. Moreover, the proposed framework exhibited outstanding performance relative to conventional statistical harmonization methods in terms of site effect removal, data distribution homogenization, and intrasubject similarity improvement. Finally, the proposed harmonization network provided fixable expandability, through which new sites could be linked to the target site via indirect schema without retraining the whole model. Together, the proposed method offers a powerful and interpretable deep learning-based harmonization framework for multisite neuroimaging data that can enhance reliability and reproducibility in multisite studies regarding brain development and brain disorders.;Dezheng Tian and Zilong Zeng and Xiaoyi Sun and Qiqi Tong and Huanjie Li and Hongjian He and Jia-Hong Gao and Yong He and Mingrui Xia;"Cognitive Neuroscience (Q1); Neurology (Q1)";2767.0;682.0;United States;1970, 1992-2020;10.1016/j.neuroimage.2022.119297;0.10582000000000001;364.0;;10538119;10959572;10538119;10959572;6.556;Big data, Machine learning, Multicenter, Gray matter, Convolutional network, Site effect;Academic Press Inc.;;7504.0;Northern America;3259.0;Q1;17495.0;Neuroimage;A deep learning-based multisite neuroimage harmonization framework established with a traveling-subject dataset;119618.0;20526.0;981.0;2802.0;73619.0;journal;article;2022
Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.;Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre;"Ecology, Evolution, Behavior and Systematics (Q1); Genetics (Q1); Molecular Biology (Q2)";968.0;389.0;United States;1992-2020;10.1016/j.ympev.2018.05.007;0.02216;159.0;;10557903;10959513;10557903;10959513;4.286;Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis;Academic Press Inc.;;8166.0;Northern America;1612.0;Q1;18965.0;Molecular phylogenetics and evolution;Topological support and data quality can only be assessed through multiple tests in reviewing blattodea phylogeny;22497.0;3802.0;208.0;978.0;16985.0;journal;article;2018
The role of the healthcare organization is shifting and must overcome the challenges of fragmented, costly care, and lack of evidence in practice, to reduce cost, ensure quality, and deliver high-value care. Notable gaps exist within the expected quality and delivery of pediatric healthcare, necessitating a change in the role of the healthcare organization. To realize these goals, the use of collaborative networks that leverage massive datasets to provide information for the development of learning healthcare systems will become increasingly necessary as efforts are made to narrow the gap in healthcare quality for children. By building upon the lessons learned from early collaborative efforts and other industries, operationalizing new technologies, encouraging clinical–community partnerships, and improving performance through transparent pursuit of meaningful goals, pediatric surgery can increase the adoption of best practices by developing collaborative networks that provide evidence-based clinical decision support and accelerate progress toward a new culture of delivering high-quality, high-value, and evidenced-based pediatric surgical care.;Grace E. Hsiung and Fizan Abdullah;"Pediatrics, Perinatology and Child Health (Q1); Surgery (Q1)";174.0;264.0;United Kingdom;1992-2020;10.1053/j.sempedsurg.2015.08.008;0.00282;65.0;;10558586;10558586;15329453;10558586;2.754;Quality improvement, Health services research, Pediatric collaborative networks, Pediatric health, Learning healthcare systems;W.B. Saunders Ltd;;4803.0;Western Europe;848.0;Q1;22283.0;Seminars in pediatric surgery;Improving surgical care for children through multicenter registries and qi collaboratives;2233.0;598.0;76.0;191.0;3650.0;journal;article;2015
Introduction: Computational methods have been widely applied to toxicology across pharmaceutical, consumer product and environmental fields over the past decade. Progress in computational toxicology is now reviewed. Methods: A literature review was performed on computational models for hepatotoxicity (e.g. for drug-induced liver injury (DILI)), cardiotoxicity, renal toxicity and genotoxicity. In addition various publications have been highlighted that use machine learning methods. Several computational toxicology model datasets from past publications were used to compare Bayesian and Support Vector Machine (SVM) learning methods. Results: The increasing amounts of data for defined toxicology endpoints have enabled machine learning models that have been increasingly used for predictions. It is shown that across many different models Bayesian and SVM perform similarly based on cross validation data. Discussion: Considerable progress has been made in computational toxicology in a decade in both model development and availability of larger scale or ‘big data’ models. The future efforts in toxicology data generation will likely provide us with hundreds of thousands of compounds that are readily accessible for machine learning models. These models will cover relevant chemistry space for pharmaceutical, consumer product and environmental applications.;Sean Ekins;"Pharmacology (Q3); Toxicology (Q4)";413.0;75.0;United States;1992-2002, 2004-2020;10.1016/j.vascn.2013.12.003;0.00214;71.0;;10568719;10568719;10568719;10568719;1.950;Bayesian, Computational toxicology, Machine learning, Support Vector Machine;Elsevier Inc.;;3978.0;Northern America;353.0;Q3;25146.0;Journal of pharmacological and toxicological methods;Progress in computational toxicology;2757.0;451.0;69.0;416.0;2745.0;journal;article;2014
"To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the ""clogging point"" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for ""meeting the people's increasing needs for a better life"".";Yang Wang and Hong Zhang and Libing Liu;"Economics and Econometrics (Q2); Finance (Q2)";524.0;245.0;United States;1992-2021;10.1016/j.iref.2022.01.004;;54.0;;10590560;10590560;10590560;10590560;;Quality of life, Point of interest, Happiness;Elsevier Inc.;;5112.0;Northern America;781.0;Q2;22707.0;International review of economics and finance;Does city construction improve life quality?-evidence from poi data of china;;1462.0;186.0;529.0;9509.0;journal;article;2022
"Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.";Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper;"Medicine (miscellaneous) (Q1); Neurology (Q2); Neurology (clinical) (Q2)";718.0;272.0;United Kingdom;1992-2020;10.1016/j.seizure.2018.09.013;;85.0;;10591311;10591311;15322688;10591311;;Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection;W.B. Saunders Ltd;;3479.0;Western Europe;1158.0;Q1;19380.0;Seizure : the journal of the british epilepsy association;Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients;;2457.0;317.0;752.0;11029.0;journal;article;2019
In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.;Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza;"Accounting (Q2); Finance (Q2)";59.0;247.0;United Kingdom;1992-2020;10.1016/j.intaccaudtax.2020.100357;;41.0;;10619518;10619518;10619518;10619518;;Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit;Elsevier BV;;6547.0;Western Europe;444.0;Q2;29884.0;Journal of international accounting, auditing and taxation;Correlates of the internal audit function’s use of data analytics in the big data era: global evidence;;178.0;32.0;66.0;2095.0;journal;article;2021
Female entrepreneurship is important for business and economic development. However, women face greater obstacles than men in accessing financing and information, making it more difficult for them to engage in entrepreneurship. This paper examines the impact of digital financial inclusion on female entrepreneurship by using a national sample consisting of matched data from a digital financial inclusion index and a nationally representative survey. The results show that digital financial inclusion significantly promotes women’s entrepreneurial behavior. We find that digital financial inclusion can ease women’s financing constraints and provide business information to alleviate their information constraints. Furthermore, the development of digital financial inclusion improves women’s work flexibility, inspiring them to engage in entrepreneurship. In addition, digital financial inclusion has a greater effect on entrepreneurship among vulnerable women, such as those with less education or a lack of financial autonomy and those living in areas with high gender inequality, which supports the idea that digital financial inclusion can empower women.;Xiaolan Yang and Yidong Huang and Mei Gao;"Economics and Econometrics (Q2); Finance (Q2)";346.0;272.0;United States;1992-2020;10.1016/j.najef.2022.101800;0.00234;37.0;;10629408;10629408;10629408;10629408;2.772;Digital financial inclusion, Female entrepreneurship, CFPS;Elsevier Inc.;;4826.0;Northern America;607.0;Q2;24089.0;North american journal of economics and finance;Can digital financial inclusion promote female entrepreneurship? evidence and mechanisms;2130.0;911.0;268.0;348.0;12933.0;journal;article;2022
We evaluate and analyse a framework for evolutionary visual exploration EVE that guides users in exploring large search spaces. EVE uses an interactive evolutionary algorithm to steer the exploration of multidimensional data sets toward two-dimensional projections that are interesting to the analyst. Our method smoothly combines automatically calculated metrics and user input in order to propose pertinent views to the user. In this article, we revisit this framework and a prototype application that was developed as a demonstrator, and summarise our previous study with domain experts and its main findings. We then report on results from a new user study with a clearly predefined task, which examines how users leverage the system and how the system evolves to match their needs. While we previously showed that using EVE, domain experts were able to formulate interesting hypotheses and reach new insights when exploring freely, our new findings indicate that users, guided by the interactive evolutionary algorithm, are able to converge quickly to an interesting view of their data when a clear task is specified. We provide a detailed analysis of how users interact with an evolutionary algorithm and how the system responds to their exploration strategies and evaluation patterns. Our work aims at building a bridge between the domains of visual analytics and interactive evolution. The benefits are numerous, in particular for evaluating interactive evolutionary computation IEC techniques based on user study methodologies.;Boukhelifa, N. and Bezerianos, A. and Cancino, W. and Lutton, E.;Computational Mathematics (Q2);87.0;497.0;United States;1996-2020;10.1162/EVCO_a_00161;0.00164;82.0;;10636560;15309304;10636560;15309304;3.277;genetic programming, data mining, information visualization., interactive evolutionary computation, visual analytics, Interactive evolutionary algorithms;MIT Press Journals;MIT Press;5067.0;Northern America;732.0;Q2;24966.0;Evolutionary computation;Evolutionary visual exploration: evaluation of an iec framework for guided visual search;3275.0;478.0;12.0;88.0;608.0;journal;article;2017
Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.;Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)";652.0;540.0;United States;1993-2020;10.1109/TNET.2019.2944984;;174.0;;10636692;10636692;10636692;10636692;;;Institute of Electrical and Electronics Engineers Inc.;IEEE Press;3740.0;Northern America;1022.0;Q1;27237.0;Ieee/acm transactions on networking;Enabling data trustworthiness and user privacy in mobile crowdsensing;;3573.0;187.0;653.0;6994.0;journal;article;2019
Processing large-scale and highly interconnected Knowledge Graphs (KG) is becoming crucial for many applications such as recommender systems, question answering, etc. Profiling approaches have been proposed to summarize large KGs with the aim to produce concise and meaningful representation so that they can be easily managed. However, constructing profiles and calculating several statistics such as cardinality descriptors or inferences are resource expensive. In this paper, we present ABSTAT-HD, a highly distributed profiling tool that supports users in profiling and understanding big and complex knowledge graphs. We demonstrate the impact of the new architecture of ABSTAT-HD by presenting a set of experiments that show its scalability with respect to three dimensions of the data to be processed: size, complexity and workload. The experimentation shows that our profiling framework provides informative and concise profiles, and can process and manage very large KGs.;Alva Principe, Renzo Arturo and Maurino, Andrea and Palmonari, Matteo and Ciavotta, Michele and Spahiu, Blerina;"Hardware and Architecture (Q1); Information Systems (Q1)";113.0;446.0;United States;1992-2020;10.1007/s00778-021-00704-2;0.0023;90.0;;10668888;10668888;0949877X;10668888;2.868;Data quality, Data profiling, Distributed processing engine, Data management, Knowledge graph;Springer New York;Springer-Verlag;7538.0;Northern America;653.0;Q1;13646.0;Vldb journal;Abstat-hd: a scalable tool for profiling very large knowledge graphs;2106.0;544.0;64.0;117.0;4824.0;journal;article;2022
The use of electronic case report forms (CRF) to gather data in randomized clinical trials has grown to progressively replace paper-based forms. Computerized form designs must ensure the same data quality expected of paper CRF, by following Good Clinical Practice rules. Electronic data capture (EDC) tools must also comply with applicable statutory and regulatory requirements. Here the authors focus on the development of computerized systems for clinical trials implementing FDA and EU recommendations and regulations, and describe a laptop-based electronic CRF used in a randomized, multicenter clinical trial.;Bogdan Ene-Iordache and Sergio Carminati and Luca Antiga and Nadia Rubis and Piero Ruggenenti and Giuseppe Remuzzi and Andrea Remuzzi;Health Informatics (Q1);580.0;469.0;United States;1994-2020;10.1197/jamia.M2787;;150.0;;10675027;1527974X;10675027;1527974X;;;Oxford University Press;;3581.0;Northern America;1614.0;Q1;23600.0;Journal of the american medical informatics association : jamia;Developing regulatory-compliant electronic case report forms for clinical trials: experience with the demand trial;;3360.0;264.0;632.0;9454.0;journal;article;2009
The trend of collecting information about human activities to inform and influence actions and decisions poses a series of challenges to analyze this data deluge. The lack of ability to understand and interact with this amount of data prevents people and organizations from taking the best of this information. To investigate how people interact with data, a new area of study called “Human-Data Interaction” (HDI) is emerging. In this article, we conduct a thorough literature review to create the big picture about the subject. We carry out a variety of analyses and visual examinations to understand the characteristics of existing publications, detecting the most frequently addressed research topics and consolidating the research challenges. Based on the needs of HDI we found in the analyzed publications, we organize a set of recommendations and evaluate online systems that demand intensive human-data interaction. The obtained results indicate there are still many open questions for this interesting area, which is maturing with an increase number of publications in the last years, and that systems with large amount of data openly available poorly meet the proposed recommendations.;Eliane Zambon Victorelli and Julio Cesar {Dos Reis} and Heiko Hornung and Alysson Bolognesi Prado;"Engineering (miscellaneous) (Q1); Hardware and Architecture (Q1); Software (Q1); Education (Q2); Human-Computer Interaction (Q2); Human Factors and Ergonomics (Q2)";300.0;517.0;United States;1994-2021;10.1016/j.ijhcs.2019.09.004;;122.0;;10715819;10959300;10715819;10959300;;Human-Data Interaction, Literature review, Research challenges, Data deluge;Academic Press Inc.;;7580.0;Northern America;733.0;Q1;12960.0;International journal of human computer studies;Understanding human-data interaction: literature review and recommendations for design;;1599.0;92.0;308.0;6974.0;journal;article;2020
"Background
After several neutral telehealth trials, the positive findings and subsequent Food and Drug Administration approval of an implantable pulmonary arterial pressure monitor (PAPM) led to renewed interest in remote patient monitoring (RPM). Here we seek to provide contemporary guidance on the appropriate use of RPM technology.
Results
Although early trials of external RPM devices suggested benefit, subsequent multicenter trials failed to demonstrate improved outcomes. Monitoring features of cardiac implantable electronic devices (CIEDs) also did not deliver improved HF outcomes, newer, multisensor algorithms may be better. Earlier technologies using direct pressure measurement via implanted devices failed to show benefit owing to complications or failure. Recently, 1 PAPM showed benefit in a randomized controlled trial. Although not showing cost reduction, cost-benefit analysis of that device suggests that it may meet acceptable standards. Additional research is warranted and is in progress. Consumer-owned electronic devices are becoming more pervasive and hold hope for future benefit in HF management. Practical aspects around RPM technology include targeting of risk populations, having mechanisms to ensure patient adherence to monitoring, and health care team structures that act on the data.
Conclusions
Based on available evidence, routine use of external RPM devices is not recommended. Implanted devices that monitor pulmonary arterial pressure and/or other parameters may be beneficial in selected patients or when used in structured programs, but the value of these devices in routine care requires further study. Future research is also warranted to better understand the cost-effectiveness of these devices.";Michael G. Dickinson and Larry A. Allen and Nancy A. Albert and Thomas DiSalvo and Gregory A. Ewald and Amanda R. Vest and David J. Whellan and Michael R. Zile and Michael M. Givertz;Cardiology and Cardiovascular Medicine (Q1);311.0;233.0;United States;1994-2020;10.1016/j.cardfail.2018.08.011;0.00869;105.0;;10719164;10719164;15328414;10719164;5.712;Telehealth, CardioMEMS, heart failure, remote patient monitoring;Churchill Livingstone;;2097.0;Northern America;1674.0;Q1;23826.0;Journal of cardiac failure;Remote monitoring of patients with heart failure: a white paper from the heart failure society of america scientific statements committee;6604.0;1162.0;198.0;458.0;4153.0;journal;article;2018
This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor;Sambasivan, Nithya;Human-Computer Interaction (Q3);292.0;182.0;United States;1994-1995, 1997, 2006-2020;10.1145/3466160;;46.0;;10725520;10725520;10725520;10725520;;;Association for Computing Machinery (ACM);Association for Computing Machinery;594.0;Northern America;247.0;Q3;4000148705.0;Interactions;Seeing like a dataset from the global south;;503.0;125.0;292.0;742.0;trade journal;article;2021
;Richard L. Schilsky;Surgery (Q1);563.0;291.0;United States;1994-2020;10.1016/j.jamcollsurg.2016.10.025;0.023469999999999998;177.0;;10727515;18791190;10727515;18791190;6.113;;Elsevier Inc.;;1711.0;Northern America;2305.0;Q1;20440.0;Journal of the american college of surgeons;Finding the evidence in real-world evidence: moving from data to information to knowledge;20819.0;2723.0;368.0;914.0;6295.0;journal;article;2017
Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.;Burkart, Nadia and Huber, Marco F.;Artificial Intelligence (Q2);199.0;715.0;United States;1993, 1996-2020;10.1613/jair.1.12228;0.0033;123.0;;10769757;10769757;10769757;10769757;2.776;;Morgan Kaufmann Publishers, Inc.;AI Access Foundation;6297.0;Northern America;790.0;Q2;24330.0;Journal of artificial intelligence research;A survey on the explainability of supervised machine learning;5173.0;1285.0;75.0;199.0;4723.0;journal;article;2021
This paper introduces a novel transit data analytics platform for public transit planning, assessing service quality and revealing service problems in high spatiotemporal resolution for public transit systems based on Automatic Passenger Counting (APC) and Automatic Vehicle Location (AVL) technologies. The platform offers a systematic way for users and decision makers to understand system performance from many aspects of service quality, including passenger waiting time, stop-skipping frequency, bus bunching level, bus travel time, on-time performance, and bus fullness. The AVL-APC data from September 2012 to March 2016 were archived in a database to support the development of a user-friendly web application that allows both users and managers to interactively query bus performance metrics for any bus routes, stops, or trips for any time period. This paper demonstrates a case study using the platform to examine bus bunching in a transit system operated by the Port Authority of Allegheny County (PAAC) in Pittsburgh. It is found that the incidence of bus bunching is heavily impacted by the location on the route as well as the time of day, and the bunching problem is more severe for bus routes operating in mixed traffic than for bus rapid transit, which operates along a dedicated busway. Furthermore, a second case study is presented with a comprehensive analysis on a representative route in Pittsburgh under schedule changes. Suggestions for operation of this route to improve service quality are proposed based on the data analytics results.;Xidong Pi and Mark Egge and Jackson Whitmore and Zhen (Sean) Qian and Amy Silbermann;"Geography, Planning and Development (Q1); Urban Studies (Q1); Transportation (Q2)";30.0;230.0;United States;2007, 2009, 2011-2018, 2020;10.5038/2375-0901.21.2.2;0.00054;25.0;;1077291X;1077291X;1077291X;1077291X;2.529;Transit system, Automatic Vehicle Location, Automatic Passenger Counting, data analytics platform, performance metrics, bus bunching, service quality;National Center for Transit Research;;3250.0;Northern America;721.0;Q1;21100205751.0;Journal of public transportation;Understanding transit system performance using avl-apc data: an analytics platform with case studies for the pittsburgh region;1013.0;78.0;4.0;36.0;130.0;journal;article;2018
"ABSTRACT
High-throughput experimentation (HTE) is a well-established technique used in the pharmaceutical industry to accelerate compound synthesis and route optimization through automated chemical processes. A key part of any HTE workflow is the analytical component, through which the reaction outcome can be determined. The development of new analytical techniques capable of high-throughput data generation from nanoscale chemical reactions has been required to streamline the HTE process and address challenges generated through the recent move to miniaturize synthesis. In this Perspective, we review the currently available state-of-the-art analytical methods, discuss the challenges encountered in high-throughput analysis—with a particular focus on the analysis of nanoscale reactions, and provide a future outlook on potential developments in the field.";Rachel Grainger and Stuart Whibley;"Organic Chemistry (Q1); Physical and Theoretical Chemistry (Q1)";729.0;309.0;United States;1997-2020;10.1021/acs.oprd.0c00463;;109.0;;10836160;1520586X;10836160;1520586X;;analytical, HTE, mass spectrometry, nanoscale, optimization;American Chemical Society;;4117.0;Northern America;904.0;Q1;26400.0;Organic process research and development;A perspective on the analytical challenges encountered in high-throughput experimentation;;2405.0;309.0;741.0;12721.0;journal;article;2021
;Tessa M. Andermann and Jonathan U. Peled and Christine Ho and Pavan Reddy and Marcie Riches and Rainer Storb and Takanori Teshima and Marcel R.M. {van den Brink} and Amin Alousi and Sophia Balderman and Patrizia Chiusolo and William B. Clark and Ernst Holler and Alan Howard and Leslie S. Kean and Andrew Y. Koh and Philip L. McCarthy and John M. McCarty and Mohamad Mohty and Ryotaro Nakamura and Katy Rezvani and Brahm H. Segal and Bronwen E. Shaw and Elizabeth J. Shpall and Anthony D. Sung and Daniela Weber and Jennifer Whangbo and John R. Wingard and William A. Wood and Miguel-Angel Perales and Robert R. Jenq and Ami S. Bhatt;"Hematology (Q1); Transplantation (Q1)";978.0;416.0;United States;1995-2020;10.1016/j.bbmt.2018.02.009;0.026389999999999997;120.0;;10838791;15236536;10838791;15236536;5.742;Microbiome, Microbiota, Hematopoietic stem cell transplantation, Metagenomics, Prebiotic, Fecal microbiota transplant;Elsevier Inc.;;3638.0;Northern America;2301.0;Q1;25453.0;Biology of blood and marrow transplantation;The microbiome and hematopoietic cell transplantation: past, present, and future;17149.0;4502.0;409.0;1086.0;14881.0;journal;article;2018
"With the advancement in communication techniques and sensor technologies, mobile crowdsensing (MCS)—one of the most successful applications of crowdsourcing—has recently become a powerful tool to solve complex and scalable sensing problems. Generally, MCS is a location-aware crowdsourcing technique in which participating workers must physically move to a specific location to complete tasks. Hence, workers must disclose information regarding their current true location to service providers. However, location information may contain sensitive data; therefore, most workers are not comfortable—or are even reluctant—to provide their exact location information to service providers because of privacy concerns. This is perceived as the most significant challenge faced in MCS. Thus, guaranteeing location privacy is essential for attracting more participants to actively participate in MCS. Accordingly, extensive studies have been conducted in the past few years to protect the location privacy of participating workers in MCS. In this study, we comprehensively survey the state-of-the-art mechanisms for protecting the location privacy of workers in MCS. We divide the location protection mechanisms into three categories depending on the nature of their algorithm and compare them from the viewpoints of architecture, privacy, computational overhead, and utility. Moreover, we discuss certain promising future research directions to spur further research in this area.";Jong Wook Kim and Kennedy Edemacu and Beakcheol Jang;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1)";701.0;817.0;United States;1996-2020;10.1016/j.jnca.2021.103315;0.0115;105.0;;10848045;10958592;10848045;10958592;6.281;Mobile crowdsensing, Location privacy, Privacy, Security;Academic Press Inc.;;7442.0;Northern America;1145.0;Q1;27277.0;Journal of network and computer applications;Privacy-preserving mechanisms for location privacy in mobile crowdsensing: a survey;9700.0;5998.0;187.0;713.0;13916.0;journal;article;2022
;Mark Plazier and Vincent Raymaekers and Wim Duyvendak and Sacha Meeuws and Maarten Wissels and Steven Vanvolsem and Gert Roosen and Sven Bamps and Salah-Edine Achabar and Stefan Schu and Anna Keil and Björn Carsten Schultheis and Philipp Slotty and Dirk {De Ridder} and Jan Vesper;"Anesthesiology and Pain Medicine (Q1); Medicine (miscellaneous) (Q1); Neurology (clinical) (Q1); Neurology (Q2)";335.0;356.0;United Kingdom;1998-2020;10.1016/j.neurom.2022.08.222;0.00554;60.0;;10947159;15251403;10947159;15251403;4.722;;Wiley-Blackwell Publishing Ltd;;3813.0;Western Europe;1296.0;Q1;17973.0;Neuromodulation;Po048 / #665 the big change is comming: big data: e-poster viewing;4447.0;1477.0;253.0;378.0;9647.0;journal;article;2022
Firms are increasingly turning towards new-age technologies such as artificial intelligence (AI), the internet of things (IoT), blockchain, and drones, among others, to assist in interacting with their customers. Further, with the prominence of personalization and customer engagement as the go-to customer management strategies, it is essential for firms to understand how to integrate new-age technologies into their existing practices to aid seamlessly in the generation of actionable insights. Towards this end, this study proposes an organizing framework to understand how firms can use digital analytics, within the changing technology landscape, to generate consumer insights. The proposed framework begins by recognizing the forces that are external to the firm then lead to the generation of specific capabilities by the firm. Further, the firms capabilities can lead to the generation of insights for decision-making that can be data-driven and/or analytics-driven. Finally, the proposed framework identifies the creation of value-based outcomes for firms and customers resulting from the insights generated. Additionally, we identify moderators that influence: (a) the impact of external forces on the development of firm capabilities, and (b) the creation of insights and subsequent firm outcomes. This study also identifies questions for future research that combines the inclusion of new-age technologies, generation of strategic insights, and the achievement of established firm outcomes.;Shaphali Gupta and Agata Leszkiewicz and V. Kumar and Tammo Bijmolt and Dmitriy Potapov;"Business and International Management (Q1); Marketing (Q1)";95.0;661.0;United States;1997-2021;10.1016/j.intmar.2020.04.003;0.0029100000000000003;106.0;;10949968;10949968;15206653;10949968;6.258;Digital analytics, Internet of things, Artificial intelligence, Drones, Blockchain, Firm capabilities;Elsevier Ltd.;;8590.0;Northern America;2605.0;Q1;22928.0;Journal of interactive marketing;Digital analytics: modeling for insights and new methods;5433.0;850.0;31.0;96.0;2663.0;journal;article;2020
The SHEILA framework provides a policy and strategy framework informing the strategic implementation and use of learning analytics. However, as evidenced in several ‘ground-up’ implementations of tools, the institutional preparedness and the governance around use often comes secondary to the policy. In this paper we depart from familiar approaches and evaluate one such example of a tool's development (SRES – Student Relationship Engagement System). SRES' adoption and scaling across two institutions are evaluated using an auto-ethnographic approach scaffolded through the dimensions of the SHEILA framework, focusing on the individual perspectives of the institutional champions who have been central to this journey. This practical approach and the emerging insights may enable other institutions to identify areas of potential improvement and inform senior academic managers about the strategic requirements to scale the approach, accounting for aspects not considered in the initial ‘organic growth’ of the implementation.;Lorenzo Vigentini and Danny Y.T. Liu and Natasha Arthars and Mollie Dollinger;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Education (Q1); E-learning (Q1)";73.0;860.0;United Kingdom;1998-2020;10.1016/j.iheduc.2020.100728;0.00413;90.0;;10967516;10967516;10967516;10967516;7.178;Learning analytics, Adoption framework, Scalability and sustainability of LA, Institutional adoption, Analytic autoethnography;Elsevier BV;;6461.0;Western Europe;3143.0;Q1;16965.0;Internet and higher education;Evaluating the scaling of a la tool through the lens of the sheila framework: a comparison of two cases from tinkerers to institutional adoption;4427.0;740.0;23.0;73.0;1486.0;journal;article;2020
"Summary
The CRISPR-Cas system offers a programmable platform for eukaryotic genome and epigenome editing. The ability to perform targeted genetic and epigenetic perturbations enables researchers to perform a variety of tasks, ranging from investigating questions in basic biology to potentially developing novel therapeutics for the treatment of disease. While CRISPR systems have been engineered to target DNA and RNA with increased precision, efficiency, and flexibility, assays to identify off-target editing are becoming more comprehensive and sensitive. Furthermore, techniques to perform high-throughput genome and epigenome editing can be paired with a variety of readouts and are uncovering important cellular functions and mechanisms. These technological advances drive and are driven by accompanying computational approaches. Here, we briefly present available CRISPR technologies and review key computational advances and considerations for various CRISPR applications. In particular, we focus on the analysis of on- and off-target editing and CRISPR pooled screen data.";Kendell Clement and Jonathan Y. Hsu and Matthew C. Canver and J. Keith Joung and Luca Pinello;"Cell Biology (Q1); Molecular Biology (Q1)";1177.0;1389.0;United States;1997-2020;10.1016/j.molcel.2020.06.012;0.16184;395.0;;10972765;10974164;10972765;10974164;17.970;;Cell Press;;6040.0;Northern America;12615.0;Q1;18606.0;Molecular cell;Technologies and computational analysis strategies for crispr applications;86299.0;17825.0;413.0;1182.0;24946.0;journal;article;2020
"Objectives
This study aimed to showcase the potential and key concerns and risks of artificial intelligence (AI) in the health sector, illustrating its application with current examples, and to provide policy guidance for the development, assessment, and adoption of AI technologies to advance policy objectives.
Methods
Nonsystematic scan and analysis of peer-reviewed and gray literature on AI in the health sector, focusing on key insights for policy and governance.
Results
The application of AI in the health sector is currently in the early stages. Most applications have not been scaled beyond the research setting. The use in real-world clinical settings is especially nascent, with more evidence in public health, biomedical research, and “back office” administration. Deploying AI in the health sector carries risks and hazards that must be managed proactively by policy makers. For AI to produce positive health and policy outcomes, 5 key areas for policy are proposed, including health data governance, operationalizing AI principles, flexible regulation, skills among health workers and patients, and strategic public investment.
Conclusions
AI is not a panacea, but a tool to address specific problems. Its successful development and adoption require data governance that ensures high-quality data are available and secure; relevant actors can access technical infrastructure and resources; regulatory frameworks promote trustworthy AI products; and health workers and patients have the information and skills to use AI products and services safely, effectively, and efficiently. All of this requires considerable investment and international collaboration.";Tiago Cravo Oliveira Hashiguchi and Jillian Oderkirk and Luke Slawomirski;"Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)";532.0;367.0;United Kingdom;1998-2020;10.1016/j.jval.2021.11.1369;0.01786;103.0;;10983015;10983015;15244733;10983015;5.725;artificial intelligence, governance, machine learning, policy;Elsevier Ltd.;;3876.0;Western Europe;1859.0;Q1;22377.0;Value in health;Fulfilling the promise of artificial intelligence in the health sector: let’s get real;12642.0;2291.0;211.0;572.0;8179.0;journal;article;2022
Digital transformation of manufacturing is a hot topic among strategic managers of manufacturing companies. The crux of digital transformation lies in the digitalization of manufacturing supply chain (MSC). However, the digital transformation of the MSC is highly uncertain, owing to the dynamic and complex changes of its nodes and structure in response to growing customer demand and fierce market competition. To propel the MSC digital transformation, it is crucial to effectively identify and predict the risk factors in the course of digital transformation. Therefore, this paper attempts to help manufacturing companies in China to successfully switch to a digital MSC. Firstly, the risk sources of the MSC digitization were identified, and complied into an evaluation index system for the digital transformation of the MSC. Next, the principal component analysis (PCA) was performed to reduce the dimension of the original data by revealing the three key principal components, and then the characteristic parameters of risk prediction are selected, so as to simplify the structure of neural network and improve the speed and efficiency of network training. On this basis, a backpropagation neural network (BPNN) was constructed for predicting the risks in MSC digitization. The results of training the model based on some data show that the proposed BPNN model has a good predictive effect. Furthermore, our model was compared with the traditional artificial neural network (ANN) model on a test set. The comparison demonstrates that our model achieved better effect than the traditional model in risk prediction. The results also show that the selected three principal components are reasonable, and the evaluation index system is valuable. The research results provide new insights to the smooth digital transformation of the MSC.;Caihong Liu;Engineering (miscellaneous) (Q1);619.0;438.0;Egypt;2000-2020;10.1016/j.aej.2021.06.010;;58.0;;11100168;11100168;11100168;11100168;;Digital transformation, manufacturing supply chain (MSC), risk factor, backpropagation neural network (BPNN), principal component analysis (PCA);Alexandria University;;3924.0;Africa/Middle East;584.0;Q1;13907.0;Aej - alexandria engineering journal;Risk prediction of digital transformation of manufacturing supply chain based on principal component analysis and backpropagation artificial neural network;;2854.0;446.0;619.0;17503.0;journal;article;2022
The present study evaluates the performance of PNN models for porosity prediction using seismic attributes. Four seismic datasets and more than 20 wells from different sedimentary basins located in Libya, Iraq, Egypt and USA are employed to characterize the effective attributes for porosity prediction. Verification and testing error analysis is adopted for evaluating the prediction performance. Results indicated that the porosity prediction models are primarily dependent to the propagation related attributes with frequency attributes as the most effective parameters in porosity prediction. In addition, the data quality and processing history strongly control the prediction model performance with relatively limited effects to dataset dimensionality (2D versus 3D) and the number of wells utilized in model construction. Such results are important to better understand and evaluate the performance of PNN porosity prediction models using various seismic attributes.;Abdulaziz M. Abdulaziz;"Fuel Technology (Q1); Organic Chemistry (Q1); Process Chemistry and Technology (Q1); Catalysis (Q2); Geochemistry and Petrology (Q2); Renewable Energy, Sustainability and the Environment (Q2)";288.0;525.0;Egypt;2011-2020;10.1016/j.ejpe.2019.12.001;;38.0;;11100621;11100621;20902468;11100621;;Porosity prediction, Seismic attributes, PNN, Al Ghani field, Sooner field, Kifl field, Baltim field;Egyptian Petroleum Research Institute;;3891.0;Africa/Middle East;942.0;Q1;21100819607.0;Egyptian journal of petroleum;The effective seismic attributes in porosity prediction for different rock types: some implications from four case studies;;1586.0;33.0;288.0;1284.0;journal;article;2020
Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.;Haitham Ghallab and Hanan Fahmy and Mona Nasr;"Information Systems (Q1); Computer Science Applications (Q2); Management Science and Operations Research (Q2)";59.0;694.0;Egypt;2010-2020;10.1016/j.eij.2019.12.001;0.00093;34.0;;11108665;11108665;11108665;11108665;3.943;Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs;Faculty of Computers and Information, Cairo University;;3521.0;Africa/Middle East;728.0;Q1;19700182731.0;Egyptian informatics journal;Detection outliers on internet of things using big data technology;820.0;415.0;47.0;59.0;1655.0;journal;article;2020
Landslides are geological hazards that can have severe impacts, threatening both the people and the local environment of highlands or mountain slopes. Landslide susceptibility mapping is an essential tool for predicting landslides and mitigating landslide-associated damage in areas prone to these events. This study aims to investigate the combination of using an adaptive network-based fuzzy inference system (ANFIS) with metaheuristic optimization algorithms: gray wolf optimizer (GWO), particle swarm optimization algorithm (PSO), and the imperialist competitive algorithm (ICA) in mapping landslide potential. The study area was Pyeongchang-gun, South Korea, for which an accurate landslide inventory dataset is available. A landslide inventory map was organized, and the data were separated randomly into training data (70%) and validation data (30%). In addition, 16 landslide-related factors consisting of geo-environmental and topo-hydrological factors were considered as predictive variables. This landslide susceptibility model was be evaluated based on the value of the area under the receiver operating characteristic (ROC) curve (AUC) to measure its accuracy. Based on the maps, the validation results showed that the optimized models of ANFIS-ICA, ANFIS-PSO, and ANFIS-GWO had AUC accuracies of 0.927, 0.947, and 0.968, respectively. The result from the hybrid algorithms model of ANFIS with metaheuristic algorithms outperformed the standalone ANFIS model in terms of accuracy in predicting landslide potential. Therefore, the ML algorithm and optimization algorithm models proposed in this study are more suitable for landslide susceptibility mapping in the study area.;Muhammad Fulki Fadhillah and Wahyu Luqmanul Hakim and Mahdi Panahi and Fatemeh Rezaie and Chang-Wook Lee and Saro Lee;Earth and Planetary Sciences (miscellaneous) (Q1);116.0;485.0;Egypt;2003, 2010-2020;10.1016/j.ejrs.2022.03.008;;34.0;;11109823;11109823;11109823;11109823;;Susceptibility map, Hybrid algorithm, ANFIS, Metaheuristic algorithm;National Authority for Remote Sensing and Space Sciences;;3489.0;Africa/Middle East;1063.0;Q1;19700183014.0;Egyptian journal of remote sensing and space science;Mapping of landslide potential in pyeongchang-gun, south korea, using machine learning meta-based optimization algorithms;;697.0;57.0;118.0;1989.0;journal;article;2022
"Purpose
Fixed-field intensity modulated radiation therapy (FF-IMRT) or volumetric modulated arc therapy (VMAT) beams complexity is due to fluence fluctuation. Pre-treatment Quality Assurance (PTQA) failure could be linked to it. Several plan complexity metrics (PCM) have been published to quantify this complexity but in a heterogeneous formalism. This review proposes to gather different PCM and to discuss their eventual PTQA failure identifier abilities.
Methods and materials
A systematic literature search and outcome extraction from MEDLINE/PubMed (National Center for Biotechnology Information, NCBI) was performed. First, a list and a synthesis of available PCM is made in a homogeneous formalism. Second, main results relying on the link between PCM and PTQA results but also on other uses are listed.
Results
A total of 163 studies were identified and n = 19 were selected after inclusion and exclusion criteria application. Difference is made between fluence and degree of freedom (DOF)-based PCM. Results about the PCM potential as PTQA failure identifier are described and synthesized. Others uses are also found in quality, big data, machine learning and audit procedure.
Conclusions
A state of the art is made thanks to this homogeneous PCM classification. For now, PCM should be seen as a planning procedure quality indicator although PTQA failure identifier results are mitigated. However limited clinical use seems possible for some cases. Yet, addressing the general PTQA failure prediction case could be possible with the big data or machine learning help.";Mikaël Antoine and Flavien Ralite and Charles Soustiel and Thomas Marsac and Paul Sargos and Audrey Cugny and Jérôme Caron;"Physics and Astronomy (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Biophysics (Q2); Medicine (miscellaneous) (Q2)";779.0;256.0;Italy;1989-2020;10.1016/j.ejmp.2019.05.024;;44.0;;11201797;11201797;1724191X;11201797;;Modulation indices, Plan complexity, Volumetric modulated arc therapy;Associazione Italiana di Fisica Medica;;3856.0;Western Europe;883.0;Q1;17037.0;Physica medica;Use of metrics to quantify imrt and vmat treatment plan complexity: a systematic review and perspectives;;2126.0;307.0;792.0;11838.0;journal;article;2019
"Background
Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID).
Objectives
We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID.
Sources
References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019.
Content
We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%).
Implications
Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.";N. Peiffer-Smadja and T.M. Rawson and R. Ahmad and A. Buchard and P. Georgiou and F.-X. Lescure and G. Birgand and A.H. Holmes;"Infectious Diseases (Q1); Medicine (miscellaneous) (Q1); Microbiology (medical) (Q1)";683.0;504.0;United Kingdom;1995-2020;10.1016/j.cmi.2019.09.009;0.03171;154.0;;1198743X;1198743X;14690691;1198743X;8.067;Artificial intelligence, Clinical decision support system, Infectious diseases, Information technology, Machine learning;Elsevier Ltd.;;2529.0;Western Europe;2884.0;Q1;29316.0;Clinical microbiology and infection;Machine learning for clinical decision support in infectious diseases: a narrative review of current applications;24871.0;5153.0;503.0;1003.0;12723.0;journal;article;2020
;D. Harper;"Infectious Diseases (Q1); Medicine (miscellaneous) (Q1); Microbiology (medical) (Q1)";880.0;286.0;Netherlands;1996-2020;10.1016/j.ijid.2016.11.067;0.01806;89.0;;12019712;12019712;18783511;12019712;3.623;;Elsevier;;2651.0;Western Europe;1278.0;Q1;22380.0;International journal of infectious diseases;Sharing public health data saves lives;17784.0;3172.0;902.0;1019.0;23916.0;journal;article;2016
"The objective of this quality evaluation was to evaluate the changes in public health nursing (PHN) interventions after the implementation of an evidence-based family home visiting (EB-FHV) guideline encoded using the Omaha System.
Design and sample
This quality improvement evaluation was conducted using a secondary dataset of 27,910 PHN family home visiting interventions from visits to 129 adult clients enrolled in EB-FHV programs in a Midwestern PHN agency. The interventions were documented 12 months before and 14 months after EB-FHV Guideline implementation. The EB-FHV consisted of 94 PHN interventions for 10 Omaha System problems, with electronic health record (EHR) data generated by PHNs during routine clinical documentation. Standard descriptive and inferential statistics were employed in the analysis.
Measures
The Omaha System was used to compare PHN practice before and after the guideline implementation.
Results
Documentation patterns revealed that PHNs tailored interventions while also shifting toward the use of the EB-FHV guideline interventions. Ten EB-FHV problems accounted for 96.3% of interventions documented before and 98.5% of interventions documented after implementation. The proportion of interventions before and after EB-FHV by problem differed significantly for all problems except Substance use. Fewer interventions were provided after EB-FHV for the primary problems of Pregnancy and Postpartum, with a shift to more interventions for Caretaking/parenting.
Conclusion
The PHN documentation after guideline implementation demonstrated adherence to the EB-FHV guideline, while tailoring the evidence-based interventions differentially by problem. Further research is needed to extend this quality improvement approach to other guidelines and populations.";Karen A. Monsen and Sadie M. Swenson and Lisa V. Klotzbach and Michelle A. Mathiason and Karen E. Johnson;"Philosophy (Q2); Nursing (miscellaneous) (Q4); Public Health, Environmental and Occupational Health (Q4)";156.0;42.0;Czech Republic;2014-2020;10.1016/j.kontakt.2017.03.002;;8.0;;12124117;18047122;12124117;18047122;;Family home visiting, Omaha System, Intervention, Guideline, Evaluation;University of South Bohemia;;2933.0;Eastern Europe;167.0;Q2;21100456164.0;Kontakt;Empirical evaluation of the changes in public health nursing interventions after the implementation of an evidence-based family home visiting guideline;;75.0;43.0;168.0;1261.0;journal;article;2017
"The technological advancement heralded the arrival of precision radiotherapy (RT), thereby increasing the therapeutic ratio and decreasing the side effects from treatment. Contour of target volumes (TV) and organs at risk (OARs) in RT is a complicated process. In recent years, automatic contouring of TV and OARs has rapidly developed due to the advances in deep learning (DL). This technology has the potential to save time and to reduce intra- or inter-observer variability. In this paper, the authors provide an overview of RT, introduce the concept of DL, summarize the data characteristics of the included literature, summarize the possible challenges for DL in the future, and discuss the possible research directions.
Résumé
Le contour des volumes-cibles et des organes à risque en radiothérapie est un processus compliqué. Ces dernières années, la délinéation automatique des volumes-cibles et des organes à risque s’est rapidement développée en raison des progrès du deep learning. Cette technologie a le potentiel de gagner du temps et de réduire la variabilité pour un même – ou entre les – observateurs. Dans cet article, les auteurs donnent un aperçu de la radiothérapie, introduisent le concept de deep learning, résument les caractéristiques des données de la littérature incluse, résument les défis possibles pour le deep learning à l’avenir et discutent des directions de recherche possibles.";M. Chen and S. Wu and W. Zhao and Y. Zhou and Y. Zhou and G. Wang;"Radiology, Nuclear Medicine and Imaging (Q3); Oncology (Q4)";325.0;79.0;France;1997-2020;10.1016/j.canrad.2021.08.020;;31.0;;12783218;12783218;12783218;12783218;;Radiotherapy, Target volumes, Organs at risk, Artificial intelligence, Deep learning, Radiothérapie, Volumes cibles, Organes à risque, Intelligence artificielle, Apprentissage profond;Elsevier Masson;;2849.0;Western Europe;288.0;Q3;29181.0;Cancer radiotherapie : journal de la societe francaise de radiotherapie oncologique;Application of deep learning to auto-delineation of target volumes and organs at risk in radiotherapy;;301.0;164.0;407.0;4672.0;journal;article;2022
Thermal fluid processes are inherently multi-physics and multi-scale, involving mass-momentum-energy transport phenomena at multiple scales. Thermal fluid simulation (TFS) is based on solving conservative equations, for which – except for “first-principles” direct numerical simulation – closure relations (CRs) are required to provide microscopic interactions or so-called sub-grid-scale physics. In practice, TFS is realized through reduced-order modeling, and its CRs as low-fidelity models can be informed by observations and data from relevant and adequately evaluated experiments and high-fidelity simulations. This paper is focused on data-driven TFS models, specifically on their development using machine learning (ML). Five ML frameworks are introduced including physics-separated ML (PSML or Type I ML), physics-evaluated ML (PEML or Type II ML), physics-integrated ML (PIML or Type III ML), physics-recovered (PRML or Type IV ML), and physics-discovered ML (PDML or Type V ML). The frameworks vary in their performance for different applications depending on the level of knowledge of governing physics, source, type, amount and quality of available data for training. Notably, outlined for the first time in this paper, Type III models present stringent requirements on modeling, substantial computing resources for training, and high potential in extracting value from “big data” in thermal fluid research. The current paper demonstrates and investigates ML frameworks in three examples. First, we utilize the heat diffusion equation with a nonlinear conductivity model formulated by convolutional neural networks (CNNs) and feedforward neural networks (FNNs) to illustrate the applications of Type I, Type II, Type III, and Type V ML. The results indicate a preference for Type II ML under deficient data support. Type III ML can effectively utilize field data, potentially generating more robust predictions than Type I and Type II ML. CNN-based closures exhibit more predictability than FNN-based closures, but CNN-based closures require more training data to obtain accurate predictions. Second, we illustrate how to employ Type I ML and Type II ML frameworks for data-driven turbulence modeling using reference works. Third, we demonstrate Type I ML by building a deep FNN-based slip closure for two-phase flow modeling. The results show that deep FNN-based closures exhibit a bounded error in the prediction domain.;Chih-Wei Chang and Nam T. Dinh;"Condensed Matter Physics (Q1); Engineering (miscellaneous) (Q1)";1267.0;391.0;France;1973-1978, 1987, 1999-2021;10.1016/j.ijthermalsci.2018.09.002;0.01547;119.0;;12900729;12900729;12900729;12900729;3.744;Thermal fluid simulation, Closure relations, Multiscale modeling, Machine learning framework, Deep learning, Data driven, Convolutional neural networks, Classification;Elsevier Masson SAS;;4081.0;Western Europe;1208.0;Q1;13761.0;International journal of thermal sciences;Classification of machine learning frameworks for data-driven thermal fluid models;16386.0;4983.0;331.0;1268.0;13507.0;journal;article;2019
The ‘Evolved GE.N.ESIS Project’ highlights the underwater cultural heritage resources off the coast of Methoni, Greece that could locally drive sustainable socioeconomic growth. An integrated marine geophysical survey, a hydrographic survey, and a GNSS survey were conducted off Methoni, recording six historic wreck sites, artefacts, the ruins of a submerged prehistoric settlement, and the town's ancient harbour/breakwater, as well as the geophysical properties of the underwater environment. The preliminary project results present bathymetric surfaces, backscatter intensity and magnetic maps, drawings, and seismic reflection profiles of the underwater antiquities and of the seabed, all fused in a 3D geographical platform. The results also shed light on the archaeological potential of the site, the nearshore physical processes, and their effect on the underwater archaeological resources. The project outcomes have shown that the establishment of an underwater archaeological park and diving sites at the cultural heritage sites will support cultural tourism development in the area and will have a positive impact on local socioeconomic development. The underwater archaeological park should comply with the basic principles of a site management plan – one that is established in the context of an integrated coastal management plan that identifies the maritime synergies or conflicts among human activities, archaeological resources, and the local environment, and utilises the 3D synthesis of marine knowledge from the project outcomes as a decision-making tool.;Panagiotis Gkionis and George Papatheodorou and Maria Geraga and Elias Fakiris and Dimitris Christodoulou and Konstantinia Tranaka;"Anthropology (Q1); Archeology (arts and humanities) (Q1); Conservation (Q1); Economics, Econometrics and Finance (miscellaneous) (Q1); History (Q1); Chemistry (miscellaneous) (Q2); Computer Science Applications (Q2); Materials Science (miscellaneous) (Q2); Spectroscopy (Q2)";508.0;307.0;France;2000-2020;10.1016/j.culher.2019.08.009;0.00433;64.0;;12962074;12962074;12962074;12962074;2.955;Maritime archaeology, Marine geophysics, Hydrography, Bathymetry, Site evolution, Underwater cultural heritage;Elsevier Masson;;4511.0;Western Europe;663.0;Q1;57810.0;Journal of cultural heritage;A marine geoarchaeological investigation for the cultural anthesis and the sustainable growth of methoni, greece;4959.0;1619.0;203.0;517.0;9158.0;journal;article;2020
In this study, highly accurate particulate matter (PM10 and PM2.5) predictions were obtained using meteorological prediction data from the local data assimilation and prediction system (LDAPS) and tree-based machine learning (ML). The study area was Seoul, South Korea, and data from July 2018 to June 2021 as well as LDAPS 36-h predictions with 1-h intervals 4 times a day were used. The predicted PM values were then compared with the observed PM measurements to evaluate the prediction accuracy. The PM prediction performance of the Community Multi-Scale Air Quality (CMAQ)-based chemical transport model (CTM) was compared with that reported by this study. The experimental results report that, among tree-based ML algorithms, light gradient boosting (LGB) is the most suitable for PM prediction. The PM prediction results of the LGB algorithm for the hourly test data were: bias = −0.10 μg/m3, root mean square error (RMSE) = 13.15 μg/m3, and R2 = 0.86 for PM10 and bias = −0.02 μg/m3, RMSE = 7.48 μg/m3, and R2 = 0.83 for PM2.5, and for daily mean were: RMSE ≤1.16 μg/m3 and R2 = 0.996. The relative RMSE (%RMSE) is 21% lower than the results of the CTM model, and R2 is 0.20 higher. Even in the high PM concentration case prediction results, the algorithm showed good predictive performance with %RMSE = 8.91%–20.43% and R2 = 0.89–0.97. Therefore, in addition to the CTM, high-accuracy PM prediction results using ML can also be used for air quality monitoring and improvement.;Bu-Yo Kim and Yun-Kyu Lim and Joo Wan Cha;"Pollution (Q1); Waste Management and Disposal (Q1); Atmospheric Science (Q2)";455.0;450.0;Netherlands;2010-2020;10.1016/j.apr.2022.101547;0.00526;45.0;;13091042;13091042;13091042;13091042;4.352;Particulate matter prediction, PM, PM, Tree-based machine learning, Air quality monitoring, Light gradient boosting algorithm;Turkish National Committee for Air Pollution Research (TUNCAP);;5666.0;Western Europe;984.0;Q1;21100254615.0;Atmospheric pollution research;Short-term prediction of particulate matter (pm10 and pm2.5) in seoul, south korea using tree-based machine learning algorithms;4254.0;2090.0;249.0;455.0;14109.0;journal;article;2022
For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.;Samah Salem and Fouzia Benchikha;Computer Science (miscellaneous) (Q1);267.0;489.0;Saudi Arabia;2014-2020;10.1016/j.jksuci.2021.06.001;;33.0;;13191578;22131248;13191578;22131248;;Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia;King Saud University;;3987.0;Middle East;617.0;Q1;21100389724.0;Journal of king saud university - computer and information sciences;Lodquma: a free-ontology process for linked (open) data quality management;;1739.0;314.0;341.0;12519.0;journal;article;2022
This review highlights recent advances in atopic dermatitis (AD) and food allergy (FA), particularly on molecular mechanisms and disease endotypes, recent developments in global strategies for the management of patients, pipeline for future treatments, primary and secondary prevention and psychosocial aspects. During the recent years, there has been major advances in personalized/precision medicine linked to better understanding of disease pathophysiology and precision treatment options of AD. A greater understanding of the molecular and cellular mechanisms of AD through substantial progress in epidemiology, genetics, skin immunology and psychological aspects resulted in advancements in the precision management of AD. However, the implementation of precision medicine in the management of AD still requires the validation of reliable biomarkers, which will provide more tailored management, starting from prevention strategies towards targeted therapies for more severe diseases. Cutaneous exposure to food via defective barriers is an important route of sensitization to food allergens. Studies on the role of the skin barrier genes demonstrated their association with the development of IgE-mediated FA, and suggest novel prevention and treatment strategies for type 2 diseases in general because of their link to barrier defects not only in AD and FA, but also in asthma, chronic rhinosinusitis, allergic rhinitis and inflammatory bowel disease. The development of more accurate diagnostic tools, biomarkers for early prediction, and innovative solutions require a better understanding of molecular mechanisms and the pathophysiology of FA. Based on these developments, this review provides an overview of novel developments and advances in AD and FA, which are reported particularly during the last two years.;Kazunari Sugita and Cezmi A. Akdis;"Medicine (miscellaneous) (Q1); Immunology and Allergy (Q2)";201.0;316.0;Japan;1996-2020;10.1016/j.alit.2019.08.013;0.004520000000000001;58.0;;13238930;14401592;13238930;14401592;5.836;Atopic dermatitis, Barrier, Food allergy, Precision medicine, Mechanisms and pathophysiology;Japanese Society of Allergology;;3217.0;Asiatic Region;1490.0;Q1;20184.0;Allergology international;Recent developments and advances in atopic dermatitis and food allergy;3122.0;1245.0;111.0;335.0;3571.0;journal;article;2020
We focus on new gravity and gravity gradient data sets from modern satellite missions GOCE, GRACE and CHAMP, and their geophysical interpretation at passive continental margins of the South Atlantic. Both sides, South Africa and South America, have been targets of hydrocarbon exploration and academic research of the German Priority Program SAMPLE (South Atlantic Margin Processes and Links with onshore Evolution). The achievable spatial resolution, driven by GOCE, is 70–80km. Therefore, most of the geological structures, which cause a significant gravity effect (by both size and density contrast), can be resolved. However, one of the most important aspects is the evaluation of the omission error, which is not always in the focus of interpreters. It results from high-frequency signals of very rough topographic and bathymetric structures, which cannot be resolved by satellite gravimetry due to the exponential signal attenuation with altitude. The omission error is estimated from the difference of the combined gravity model EIGEN-6C4 and the satellite-only model GOCO05S. It can be significantly reduced by topographic reductions. Simple 2D density models and their related mathematical formulas provide insights in the magnitude of the gravity effect of masses that form a passive continental margin. They are contrasted with results from satellite-only and combined gravity models. Example geophysical interpretations are given for the western and eastern margin of the South Atlantic Ocean, where standard deviations vary from 25 to 16mGal and 21–11mGal, respectively. It could be demonstrated, that modern satellite gravity data provide significant added value in the geophysical gravity data processing domain and in the validation of heterogeneous terrestrial data bases. Combined models derived from high-resolution terrestrial gravity and homogeneous satellite data will lead to more detailed and better constrained lithospheric density models, and hence will improve our knowledge about structure, evolution and state of stress in the lithosphere.;Hans-Jürgen Götze and Roland Pail;Geology (Q1);517.0;581.0;United States;1997-2020;10.1016/j.gr.2017.04.015;0.01984;135.0;;1342937X;1342937X;1342937X;1342937X;6.051;Continental margins, Satellite gravity missions, Spatial resolution, Omission error, Interpretation gravity effects, Interpretation gravity gradients;Elsevier Inc.;;13149.0;Northern America;2859.0;Q1;22647.0;Gondwana research;Insights from recent gravity satellite missions in the density structure of continental margins – with focus on the passive margins of the south atlantic;18040.0;3698.0;243.0;535.0;31951.0;journal;article;2018
Hyperspectral imaging technology has evolved for over thirty years and is widely used for geologic mapping, environmental monitoring, vegetation analysis, atmospheric characterization, biological and chemical detection, etc. With advances in technology, hyperspectral imagery not only determines the presence of materials and objects, but more importantly, also quantifies the variability and abundance of the identified materials or objects. Airborne hyperspectral imagers still perform a vital role in remote sensing fields due to advantages of spatial resolution, performance capabilities in a cloudy atmosphere, and onboard maintenance as compared to similar imagers aboard spaceborne platforms. To date, hundreds of airborne hyperspectral systems have been designed, built, and operated. Here, a review of key technologies for airborne hyperspectral imaging technology during past three decades is presented. First discussed will be high throughput imaging modes, high quality spectroscopic subsystems, and high sensitivity detector technology used on current airborne hyperspectral imagers. Particularly, the importance of data-processing such as calibration, geometric rectification, and atmospheric correction are discussed. Next, several new and novel applications are presented on the basis of state-of-the-art airborne hyperspectral technology. Finally, an outlook of challenges and future technology directions is presented along with general advice for designing and realizing novel high-performance airborne hyperspectral systems in this rapidly evolving field. By illustrating the status and prospects of typical airborne hyperspectral imagers, this overview provides a comparison of the technologies employed in previous hyperspectral imaging systems, current imaging technology research programs and prospects for innovative technology in future airborne hyperspectral imaging platforms.;Jianxin Jia and Yueming Wang and Jinsong Chen and Ran Guo and Rong Shu and Jianyu Wang;"Atomic and Molecular Physics, and Optics (Q2); Condensed Matter Physics (Q2); Electronic, Optical and Magnetic Materials (Q2)";914.0;292.0;Netherlands;1994-2020;10.1016/j.infrared.2019.103115;;65.0;;13504495;13504495;13504495;13504495;;Airborne, Hyperspectral, Key technology, Surface reflectance, Application;Elsevier;;3501.0;Western Europe;542.0;Q2;12121.0;Infrared physics and technology;Status and application of advanced airborne hyperspectral imaging technology: a review;;2807.0;391.0;918.0;13688.0;journal;article;2020
The advent of high throughput next generation sequencing (NGS) has accelerated the pace of discovery of disease-associated genetic variants and genomewide profiling of expressed sequences and epigenetic marks, thereby permitting systems-based analyses of ocular development and disease. Rapid evolution of NGS and associated methodologies presents significant challenges in acquisition, management, and analysis of large data sets and for extracting biologically or clinically relevant information. Here we illustrate the basic design of commonly used NGS-based methods, specifically whole exome sequencing, transcriptome, and epigenome profiling, and provide recommendations for data analyses. We briefly discuss systems biology approaches for integrating multiple data sets to elucidate gene regulatory or disease networks. While we provide examples from the retina, the NGS guidelines reviewed here are applicable to other tissues/cell types as well.;Vijender Chaitankar and Gökhan Karakülah and Rinki Ratnapriya and Felipe O. Giuste and Matthew J. Brooks and Anand Swaroop;"Ophthalmology (Q1); Sensory Systems (Q1)";129.0;1969.0;United Kingdom;1990-1991, 1993-2020;10.1016/j.preteyeres.2016.06.001;0.01236;152.0;;13509462;13509462;18731635;13509462;21.198;ChIP-seq, Chromatin, Epigenetics, Gene regulatory network, Genomics, High throughput data, Network analysis, Photoreceptor, Retina, Retinal disease, RNA-Seq, Systems biology, Transcriptome, Vision, Whole exome sequencing, Whole genome sequencing, NGS data integration;Elsevier Ltd.;;27556.0;Western Europe;7198.0;Q1;15090.0;Progress in retinal and eye research;Next generation sequencing technology and genomewide data analysis: perspectives for retinal research;9869.0;2610.0;62.0;129.0;17085.0;journal;article;2016
The last decade has seen an explosion in data sources available for monitoring and prediction of environmental phenomena. While several inferential methods have been developed to make predictions on the underlying process by combining these data, an optimal sampling design for additional data collection in the presence of multiple heterogeneous sources has not yet been developed. Here, we provide an adaptive spatial design strategy based on a utility function that combines both prediction uncertainty and risk-factor criteria. Prediction uncertainty is obtained through a spatial data fusion approach based on fixed rank kriging that can tackle data with differing spatial supports and signal-to-noise ratios. We focus on the application of low-cost portable sensors, which tend to be relatively noisy, for air pollution monitoring, where data from regulatory stations as well as numeric modeling systems are also available. Although we find that spatial adaptive sampling designs can help to improve predictions and reduce prediction uncertainty, low-cost portable sensors are only likely to be beneficial if they are sufficient in number and quality. Our conclusions are based on a multi-factorial simulation experiment, and on a realistic simulation of pollutants in the Erie and Niagara counties in Western New York.;Eun-Hye Yoo and Andrew Zammit-Mangion and Michael G. Chipeta;"Atmospheric Science (Q1); Environmental Science (miscellaneous) (Q1)";2034.0;493.0;United Kingdom;1968-1989, 1994-2021;10.1016/j.atmosenv.2019.117091;0.03998;240.0;;13522310;13522310;18732844;13522310;4.798;Adaptive spatial sampling design, Change-of-support problem, Fixed rank kriging, Low-cost portable air sensors, Measurement uncertainty;Elsevier Ltd.;;5833.0;Western Europe;1400.0;Q1;23357.0;Atmospheric environment;Adaptive spatial sampling design for environmental field prediction using low-cost sensing technologies;68466.0;10479.0;609.0;2046.0;35523.0;journal;article;2020
A huge number of businesses and organisations of all sizes, from the world's biggest, to small, local companies manage themselves using data to direct their energies. Analytics is at the forefront of modern business management. These are the thoughts that Dave Wells, practice director for data management at Eckerson Group, puts forward in the report ‘The New Analytics Lifecycle’.1 Organisations of all sizes use data to direct their energies. Analytics is at the forefront of modern business management. Yet with the continuing snowballing of data resources, access privileges and the technology stack to support them, security and governance have not been kept top of mind within the analytics lifecycle. An organisation needs to understand from top to bottom the data owners, flows and processes it has and then map onto these new ways of doing business that will keep them compliant and customer-centric, says Nick Jewell of Alteryx.;Nick Jewell;"Computer Networks and Communications (Q4); Information Systems and Management (Q4); Safety, Risk, Reliability and Quality (Q4)";155.0;90.0;Netherlands;1994-2020;10.1016/S1353-4858(19)30047-9;;24.0;;13534858;13534858;13534858;13534858;;;Elsevier BV;;590.0;Western Europe;177.0;Q4;27334.0;Network security;The analytics lifecycle and the age of innovation;;145.0;50.0;155.0;295.0;trade journal;article;2019
PM2.5 pollution imposes substantial health risks on urban residents. Previous studies mainly focused on assessing peoples' exposures at static locations, such as homes or workplaces. There has been a scarcity of research that quantifies the dynamic PM2.5 exposures of people when they travel in cities. To address this gap, we use cellphone positioning data and PM2.5 concentration data collected from smart sensors along roads in Guangzhou, China, to assess personal travel exposure to on-road PM2.5. First, we extract the trips of cellphone users from their trajectories and use the shortest path algorithm to calculate their travel routes on the road network. Second, the travel exposure of each user is estimated by associating their movement patterns with PM2.5 concentrations on roads. The result shows that most users’ average travel exposures per hour fall within the range of 20 ug/m3 to 75 ug/m3. Travel exposure varies across users, and 54.0% of users experience low travel exposure throughout the day, 25.5% of users experience high travel exposure in the evening, and 20.5% of users experience high travel exposure in the afternoon. Furthermore, the impacts of on-road PM2.5 on urban populations are uneven across roads. More attention should be given to roads with high PM2.5 concentrations and traffic flows in each period, such as Huan Shi Middle Road in the morning, Inner Ring Road in the afternoon, and Xinjiao Middle Road in the evening. The findings in this study can contribute to a more in-depth understanding of the relationship between air pollution and the travel activities of urban populations.;Qiuping Li and Shen Liang and Yang Xu and Lin Liu and Suhong Zhou;"Geography, Planning and Development (Q1); Health (social science) (Q1); Life-span and Life-course Studies (Q1); Public Health, Environmental and Occupational Health (Q1); Sociology and Political Science (Q1)";444.0;397.0;United Kingdom;1995-2020;10.1016/j.healthplace.2022.102803;;109.0;;13538292;13538292;13538292;13538292;;Travel exposure, On-road PM concentrations, Cellphone positioning data, Mobile sensors;Elsevier Ltd.;;6251.0;Western Europe;1341.0;Q1;29102.0;Health and place;Assessing personal travel exposure to on-road pm2.5 using cellphone positioning data and mobile sensors;;2110.0;174.0;448.0;10877.0;journal;article;2022
Metal additive manufacturing is a disruptive technology that is revolutionizing the manufacturing industry. Despite its unrivaled capability for directly fabricating metal parts with complex geometries, the wide realization of the technology is currently limited by microstructural defects and anomalies, which could significantly degrade the structural integrity and service performance of the product. Accurate detection, characterization, and prediction of these defects and anomalies have an important and immediate impact in manufacturing fully-dense and defect-free builds. This review seeks to elucidate common defects/anomalies and their formation mechanisms in powder bed fusion additive manufacturing processes. They could arise from raw materials, processing conditions, and post-processing. While defects/anomalies in laser welding have been studied extensively, their formation and evolution remain unclear. Additionally, the existence of powder in powder bed fusion techniques may generate new types of defects, e.g., porosity transferring from powder to builds. Practical strategies to mitigate defects are also addressed through fundamental understanding of their formation. Such explorations enable the validation and calibration of models and ease the process qualification without costly trial-and-error experimentation.;Amir Mostafaei and Cang Zhao and Yining He and Seyed {Reza Ghiaasiaan} and Bo Shi and Shuai Shao and Nima Shamsaei and Ziheng Wu and Nadia Kouraytem and Tao Sun and Joseph Pauza and Jerard V. Gordon and Bryan Webler and Niranjan D. Parab and Mohammadreza Asherloo and Qilin Guo and Lianyi Chen and Anthony D. Rollett;Materials Science (miscellaneous) (Q1);75.0;1071.0;United Kingdom;1996-1999, 2001-2020;10.1016/j.cossms.2021.100974;;112.0;;13590286;13590286;13590286;13590286;;Additive manufacturing, Powder-related defects, Processing-related defects, Post-processing-related defects, Defect mitigation, Process-structure–property relationship;Elsevier Ltd.;;12267.0;Western Europe;2918.0;Q1;15378.0;Current opinion in solid state and materials science;Defects and anomalies in powder bed fusion metal additive manufacturing;;927.0;30.0;79.0;3680.0;journal;article;2022
Competence lack, inadequate social support at work leads to the inability of workers since they are suffering from occupational stress. This will cause distress, burnout or psychosomatic difficulties, decreases in quality of life and service provision. Some of them may connect to work in an individual's personal life, both as managers, recognize stressors in their department, and respond on a departmental basis or individually. Many workers say that their employee utilization monitoring is not sufficient until computer counting involves. In addition, the systems are associated with higher stress, health hazards, and work unhappiness among supervised personnel. Monitoring these problems can increase employee awareness of personal productivity, providing performance information more promptly and frequently. Interventions are based on an examination of the variables that impact the performance of health workers. The article for employee stress management and health monitoring using information technology (SMHM-IT) gives better working conditions, motivation, retention, etc. Evaluation of occupational risks is a framework introduced to manage health and safety implications associated with preventative measures for improving and protecting the highest physical, social, or emotional working skills. Statistical data analysis is introduced to compare a medical specialty which includes analysis of employee's details. Results are compared with assessments shows that architecture offers successful in-time accessibility of performance 98.12% is achieved.;Ming Chen and Bin Ran and Xiaoying Gao and Guilan Yu and Jing Wang and J. Jagannathan;"Clinical Psychology (Q1); Pathology and Forensic Medicine (Q1); Psychiatry and Mental Health (Q1)";281.0;432.0;United Kingdom;1996-2020;10.1016/j.avb.2021.101713;0.0069900000000000006;102.0;;13591789;13591789;13591789;13591789;4.382;Occupational stress, Improving performance, Productivity, Stress, Health, Information technology;Elsevier Ltd.;;8092.0;Western Europe;1586.0;Q1;29451.0;Aggression and violent behavior;Evaluation of occupational stress management for improving performance and productivity at workplaces by monitoring the health, well-being of workers;6869.0;1410.0;120.0;286.0;9710.0;journal;article;2021
Nuclear magnetic resonance (NMR) spectroscopy acts as the best tool that can be used in tissue engineering scaffolds to investigate unknown metabolites. Moreover, metabolomics is a systems approach for examining in vivo and in vitro metabolic profiles, which promises to provide data on cancer metabolic alterations. However, metabolomic profiling allows for the activity of small molecules and metabolic alterations to be measured. Furthermore, metabolic profiling also provides high-spectral resolution, which can then be linked to potential metabolic relationships. An altered metabolism is a hallmark of cancer that can control many malignant properties to drive tumorigenesis. Metabolite targeting and metabolic engineering contribute to carcinogenesis by proliferation, and metabolic differentiation. The resulting the metabolic differences are examined with traditional chemometric methods such as principal component analysis (PCA), and partial least squares-discriminate analysis (PLS-DA). In this review, we examine NMR-based activity metabolomic platforms that can be used to analyze various fluxomics and for multivariant statistical analysis in cancer. We also aim to provide the reader with a basic understanding of NMR spectroscopy, cancer metabolomics, target profiling, chemometrics, and multifunctional tools for metabolomics discrimination, with a focus on metabolic phenotypic diversity for cancer therapeutics.;Ganesan Raja and Youngmi Jung and Sang Hoon Jung and Tae-Jin Kim;"Applied Microbiology and Biotechnology (Q2); Bioengineering (Q2); Biochemistry (Q3)";960.0;366.0;United Kingdom;1950, 1953-1955, 1973-1975, 1979-2020;10.1016/j.procbio.2020.08.023;0.00836;157.0;;13595113;00329592;13595113;00329592;3.757;Cancer, Metabolomics, Metabolic engineering, Target profiling, Software, Therapeutics;Elsevier BV;;5291.0;Western Europe;689.0;Q2;16134.0;Process biochemistry;1h-nmr-based metabolomics for cancer targeting and metabolic engineering –a review;21378.0;3554.0;380.0;963.0;20104.0;journal;article;2020
While target-based drug discovery strategies rely on the precise knowledge of the identity and function of the drug targets, phenotypic drug discovery (PDD) approaches allow the identification of novel drugs based on knowledge of a distinct phenotype. Image-based high-content screening (HCS) is a potent PDD strategy that characterizes small-molecule effects through the quantification of features that depict cellular changes among or within cell populations, thereby generating valuable data sets for subsequent data analysis. However, these data can be complex, making image analysis from large HCS campaigns challenging. Technological advances in image acquisition, processing, and analysis as well as machine-learning (ML) approaches for the analysis of multidimensional data sets have rendered HCS as a viable technology for small-molecule drug discovery. Here, we discuss HCS concepts, current workflows as well as opportunities and challenges of image-based phenotypic screening and data analysis.;Sean Lin and Kenji Schorpp and Ina Rothenaigner and Kamyar Hadian;"Drug Discovery (Q1); Pharmacology (Q1)";630.0;717.0;United Kingdom;1996-2020;10.1016/j.drudis.2020.06.001;0.0174;175.0;;13596446;18785832;13596446;18785832;7.851;;Elsevier Ltd.;;7656.0;Western Europe;1778.0;Q1;21196.0;Drug discovery today;Image-based high-content screening in drug discovery;18695.0;5142.0;258.0;667.0;19752.0;journal;article;2020
Deep learning (DL), a subset of machine learning approaches, has emerged as a versatile tool to assimilate large amounts of heterogeneous data and provide reliable predictions of complex and uncertain phenomena. These tools are increasingly being used by the plant science community to make sense of the large datasets now regularly collected via high-throughput phenotyping and genotyping. We review recent work where DL principles have been utilized for digital image–based plant stress phenotyping. We provide a comparative assessment of DL tools against other existing techniques, with respect to decision accuracy, data size requirement, and applicability in various scenarios. Finally, we outline several avenues of research leveraging current and future DL tools in plant science.;Asheesh Kumar Singh and Baskar Ganapathysubramanian and Soumik Sarkar and Arti Singh;Plant Science (Q1);377.0;1139.0;United Kingdom;1996-2020;10.1016/j.tplants.2018.07.004;0.02277;263.0;;13601385;13601385;18784372;13601385;18.313;high throughput, phenomics, machine learning, diseases, transfer learning, imaging, smartphone app, automation;Elsevier Ltd.;;5820.0;Western Europe;4587.0;Q1;17809.0;Trends in plant science;Deep learning for plant stress phenotyping: trends and future perspectives;29531.0;4844.0;162.0;386.0;9428.0;journal;article;2018
Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.;Lena Maier-Hein and Matthias Eisenmann and Duygu Sarikaya and Keno März and Toby Collins and Anand Malpani and Johannes Fallert and Hubertus Feussner and Stamatia Giannarou and Pietro Mascagni and Hirenkumar Nakawala and Adrian Park and Carla Pugh and Danail Stoyanov and Swaroop S. Vedula and Kevin Cleary and Gabor Fichtinger and Germain Forestier and Bernard Gibaud and Teodor Grantcharov and Makoto Hashizume and Doreen Heckmann-Nötzel and Hannes G. Kenngott and Ron Kikinis and Lars Mündermann and Nassir Navab and Sinan Onogur and Tobias Roß and Raphael Sznitman and Russell H. Taylor and Minu D. Tizabi and Martin Wagner and Gregory D. Hager and Thomas Neumuth and Nicolas Padoy and Justin Collins and Ines Gockel and Jan Goedeke and Daniel A. Hashimoto and Luc Joyeux and Kyle Lam and Daniel R. Leff and Amin Madani and Hani J. Marcus and Ozanan Meireles and Alexander Seitel and Dogu Teber and Frank Ückert and Beat P. Müller-Stich and Pierre Jannin and Stefanie Speidel;"Computer Graphics and Computer-Aided Design (Q1); Computer Vision and Pattern Recognition (Q1); Health Informatics (Q1); Radiological and Ultrasound Technology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";411.0;1328.0;Netherlands;1996-2020;10.1016/j.media.2021.102306;0.018359999999999998;135.0;;13618415;13618415;13618423;13618415;8.545;Surgical data science, Artificial intelligence, Deep learning, Computer aided surgery, Clinical translation;Elsevier;;5729.0;Western Europe;2887.0;Q1;17271.0;Medical image analysis;Surgical data science – from concepts toward clinical translation;11568.0;7895.0;161.0;413.0;9223.0;journal;article;2022
Road grade is crucial in vehicle control and emission studies but challenging to obtain in large-scale road networks due to current methods’ expensive deployment costs or limited accuracy. This paper proposed a scale-deployable and cost-efficient road grade estimation solution based on the fuel consumption rate (FCR) difference between flat and graded roads. Real-world road grades from design drawings and 261,814 second-by-second vehicle operating data from 680 light-duty vehicles were collected to examine the proposed method’s performance. Sensitivity tests for vehicle types and sample sizes were conducted. Results show that (1) the proposed method acquired road grade with an accuracy of 0.12% mean absolute error (MAE), (2) in positive vehicle specific power (VSP) bins, a 1% road grade caused an average 16% FCR change, and (3) larger-scale fuel consumption data contributed to reducing estimation error which converged from 0.25% to 0.12% as the segment passes increased from 50 to 400.;Pengfei Fan and Guohua Song and Zijun Zhu and Yizheng Wu and Zhiqiang Zhai and Lei Yu;"Civil and Structural Engineering (Q1); Environmental Science (miscellaneous) (Q1); Transportation (Q1)";821.0;610.0;United Kingdom;1996-2020;10.1016/j.trd.2022.103262;;99.0;;13619209;13619209;13619209;13619209;;Road Grade, Fuel Consumption, Vehicle Specific Power, Connected Vehicle;Elsevier Ltd.;;5678.0;Western Europe;1600.0;Q1;20894.0;Transportation research, part d: transport and environment;Road grade estimation based on large-scale fuel consumption data of connected vehicles;;5627.0;370.0;834.0;21008.0;journal;article;2022
As the need to evaluate the energy performance of buildings has increased, the use of energy analysis tools has become more widespread and their results are now a key factor in building energy assessments. The current emphasis on the interpretation of building energy performance means that the enhancement of energy analysis tools and their ease of use is worthy of study. Based on analyses of building energy and the physical properties of materials, material properties essential to the analysis of building energy performance were selected. These properties were automatically extracted and stored using an algorithm to collect information from the internet. Based on the designed algorithms, we conducted a questionnaire-based survey and qualitative analysis to measure their convenience. From the analysis, the satisfaction level was found to exceed an average of 80%, resulting in a high level of satisfaction for practitioners using the energy analysis tool. It was also shown that the perceived convenience could be improved by reducing the duration of the search by at least 60% and by applying physical property information to the energy analysis application.;Sungwoong Yang and Seunghwan Wi and Ji Hun Park and Hyun Mi Cho and Sumin Kim;Renewable Energy, Sustainability and the Environment (Q1);3206.0;1630.0;United Kingdom;1997-2021;10.1016/j.rser.2019.109665;;295.0;;13640321;13640321;13640321;13640321;;Energy simulation, Building material, Crawler, Physical properties, Algorithm, Framework;Elsevier Ltd.;;11502.0;Western Europe;3522.0;Q1;27567.0;Renewable and sustainable energy reviews;Framework for developing a building material property database using web crawling to improve the applicability of energy simulation tools;;54087.0;643.0;3239.0;73956.0;journal;article;2020
Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.;Neil Stewart and Jesse Chandler and Gabriele Paolacci;"Cognitive Neuroscience (Q1); Experimental and Cognitive Psychology (Q1); Neuropsychology and Physiological Psychology (Q1)";292.0;1201.0;United Kingdom;1997-2020;10.1016/j.tics.2017.06.007;0.03627;313.0;;13646613;1879307X;13646613;1879307X;20.229;;Elsevier Ltd.;;6819.0;Western Europe;6857.0;Q1;15359.0;Trends in cognitive sciences;Crowdsourcing samples in cognitive science;33482.0;4345.0;121.0;348.0;8251.0;journal;article;2017
Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.;Demetris T. Christopoulos;"Geophysics (Q2); Atmospheric Science (Q3); Space and Planetary Science (Q3)";620.0;189.0;United Kingdom;1997-2020;10.1016/j.jastp.2015.03.009;0.005229999999999999;89.0;;13646826;13646826;13646826;13646826;1.735;Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis;Elsevier Ltd.;;4974.0;Western Europe;515.0;Q2;28436.0;Journal of atmospheric and solar-terrestrial physics;Extraction of the global absolute temperature for northern hemisphere using a set of 6190 meteorological stations from 1800 to 2013;7321.0;1256.0;180.0;628.0;8954.0;journal;article;2015
Multi-spectral spaceborne sensors with different spatial resolutions produce Earth observation (EO) time series (TS) with global coverage. The interactive visualization and interpretation of TS is essential to better understand changes in land-use and land-cover and to extract reference information for model calibration and validation. However, available software tools are often limited to specific sensors or optimized for application-specific visualizations. To overcome these limitations, we developed the EO Time Series Viewer, a free and open source QGIS plugin for user-friendly visualization, interpretation and labeling of multi-sensor TS data. The EO Time Series Viewer (i) combines advantages of spatial, spectral and temporal data visualization concepts that are so far not available in a single tool, (ii) provides maximum flexibility in terms of supported data formats, (iii) minimizes the user-interactions required to load and visualize multi-sensor TS data and (iv) speeds-up labeling of TS data based on enhanced GIS vector tools and formats.;Benjamin Jakimow and Sebastian {van der Linden} and Fabian Thiel and David Frantz and Patrick Hostert;"Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)";711.0;547.0;Netherlands;1997-2020;10.1016/j.envsoft.2020.104631;;136.0;;13648152;13648152;13648152;13648152;;Change detection, Training, Validation, Open-source, QGIS plugin, EO time series viewer;Elsevier BV;;6376.0;Western Europe;1828.0;Q1;23295.0;Environmental modelling and software;Visualizing and labeling dense multi-sensor earth observation time series: the eo time series viewer;;4373.0;223.0;717.0;14218.0;journal;article;2020
"Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call ‘exponentiated phase’. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.";Yuan, Yanhua O and Bozdağ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J;"Geochemistry and Petrology (Q1); Geophysics (Q1)";1517.0;277.0;United Kingdom;1922-1943, 1945, 1947-1957, 1988-2020;10.1093/gji/ggaa063;0.03134;168.0;;1365246X;0956540X;1365246X;0956540X;2.934;"Inverse theory;Time-series analysis;Seismic tomography";Oxford University Press;;5695.0;Western Europe;1302.0;Q1;27947.0;Geophysical journal international;The exponentiated phase measurement, and objective-function hybridization for adjoint waveform tomography;32388.0;4356.0;531.0;1529.0;30241.0;journal;article;2019
Data science and analytics are attracting more and more attention from researchers and practitioners in recent years. Due to the rapid development of advanced technologies nowadays, a massive amount of real time data regarding flight information, flight performance, airport conditions, air traffic conditions, weather, ticket prices, passengers comments, crew comments, etc., are all available from a diverse set of sources, including flight performance monitoring systems, operational systems of airlines and airports, and social media platforms. Development of data analytics in aviation and related applications is also growing rapidly. This paper concisely examines data science and analytics in aviation studies in several critical areas, namely big data analysis, air transport network management, forecasting, and machine learning. The papers featured in this special issue are also introduced and reviewed, and future directions for data science and analytics in aviation are discussed.;Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi;"Business and International Management (Q1); Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)";499.0;763.0;United Kingdom;1997-2020;10.1016/j.tre.2020.101837;;110.0;;13665545;13665545;13665545;13665545;;Data science, Aviation, Analytics, Flight, Air logistics;Elsevier Ltd.;;5633.0;Western Europe;2042.0;Q1;20909.0;Transportation research, part e: logistics and transportation review;Data science and analytics in aviation;;4018.0;233.0;507.0;13126.0;journal;article;2020
The central claim of the paper is that the development and control of Cyber-Physical Production Systems (CPPS) requires a systematic approach to handle and include explicit ethical considerations. Since the contribution of artificial intelligence (AI) technologies, and of agent-based models in particular, was instrumental in the evolution of CPPSs, approaches of ethical AI should be endorsed in CPPS development by design. The paper discusses recent advances for ethical AI and suggests a pathway from ethical norms towards standards. As it is argued, taking the responsible AI approach is promising when tackling the main ethic-related challenges of Cyber-Physical Production Systems. We expose a number of dilemmas to be resolved so that AI systems incorporated in CPPS cause no damages either in humans, equipment or in the environment and increase the trust in the users of current and future AI technologies.;István Mezgár and József Váncza;"Control and Systems Engineering (Q1); Software (Q1)";132.0;915.0;United Kingdom;1996-2020;10.1016/j.arcontrol.2022.04.002;0.0039;80.0;;13675788;13675788;13675788;13675788;6.091;Artificial intelligence, Cyber-physical production system, Agents, Ethics, Control, Trust;Elsevier Ltd.;;9236.0;Western Europe;1780.0;Q1;27843.0;Annual reviews in control;From ethics to standards – a path via responsible ai to cyber-physical production systems;2756.0;1435.0;55.0;145.0;5080.0;journal;article;2022
The Qaidam Basin was formed under the background of the continuous uplift of the Tibetan Plateau, and its tectonic location imparts unique geothermal regime. The geothermal regime of the Qaidam Basin was studied based on oil-test static temperatures from 60 wells, 195 thermal conductivities measured by the optical scanning method, and 142 radiogenic heat production values. The geothermal gradient in the Qaidam Basin is 17.1–47.6 °C/km, with an average of 31.3 °C/km, and the thermal conductivity is 0.523–4.379 W/m‧K. The heat flow is 28.3–83.1 mW/m2, with an average of 59.6 mW/m2, and it is “high in the west and low in the east.” The heat flow can exceed 70 mW/m2 in the northern marginal fold-and-thrust belt of western Kunlun and Nanyishan and generally exceeds 65 mW/m2 in the Mangya Depression. The average radiogenic heat production rate (HPR) is 2.53 μW/m3, which is close to the HPR of granite in northern Tibetan Plateau. The crustal and mantle heat flows in the Qaidam Basin are 32.9 mW/m2 and 26.7 mW/m2, respectively. The crustal contribution to the surface heat flow was approximately 55 %. The geothermal regime may be dominated by lithospheric thickness, HPR of sedimentary cover, and extra heat production related to late Cenozoic tectonic movement. The proven hydrocarbon reserves are primarily distributed around hydrocarbon-generating kitchens, and the existence of abnormally high-temperature zones significantly influences hydrocarbon distribution. The Qaidam Basin satisfies the fundamental temperature conditions for the development of low- and medium-temperature geothermal resources using abandoned oil and gas drilling wells.;Yumao Pang and Kaizhen Zou and Xingwei Guo and Yan Chen and Jian Zhao and Fei Zhou and Jun Zhu and Lifeng Duan and Guoxin Yang;"Earth-Surface Processes (Q1); Geology (Q1)";1177.0;348.0;United Kingdom;1997-2020;10.1016/j.jseaes.2022.105400;0.018340000000000002;125.0;;13679120;18785786;13679120;18785786;3.449;Qaidam Basin, Geothermal regime, Heat flow, Thermal structure, Petroleum distribution, Geothermal resources;Elsevier Ltd.;;8838.0;Western Europe;1317.0;Q1;27602.0;Journal of asian earth sciences;Geothermal regime and implications for basin resource exploration in the qaidam basin, northern tibetan plateau;20153.0;4424.0;347.0;1193.0;30668.0;journal;article;2022
Recent advances in artificial intelligence, computer science, communication, sensing and actuation technologies have resulted in the development of several novel intelligent systems. At the same time, the emergence of nanogenerators has opened a new research avenue with the overarching goal of developing self-powered sensing systems. The concepts of self-powered sensing, based on nanogenerators and intelligent systems can be fused together to open a new area of interdisciplinary research. In this article, we aim to show how these two emerging technologies have been combined to develop self-powered intelligent sensing systems. We first focus on the main keywords in the area of nanogenerators. Keyword co-occurrence network graphs are generated based on the most used keywords in the area of nanogenerators to select key concepts that are directly connected to the concept of intelligent systems. Thus, a detailed review is provided on different intelligent self-powered sensing systems based on nanogenerators. We also discuss the challenges presented by combining intelligent systems and self-powered sensing. As most of intelligent devices rely on machine learning techniques, a comprehensive section is allocated to this topic to focus on its applications in nanogenerator-based devices.;Hassan Askari and Nan Xu and Bruno Henrique {Groenner Barbosa} and Yanjun Huang and Longping Chen and Amir Khajepour and Hong Chen and Zhong Lin Wang;"Condensed Matter Physics (Q1); Materials Science (miscellaneous) (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1)";218.0;2564.0;Netherlands;1999, 2002-2020;10.1016/j.mattod.2021.11.027;0.02245;177.0;;13697021;13697021;13697021;13697021;31.041;Nanogenerators, Intelligent systems, Machine learning, Self-powered sensing, Artificial intelligence;Elsevier;;11044.0;Western Europe;8071.0;Q1;24769.0;Materials today;Intelligent systems using triboelectric, piezoelectric, and pyroelectric nanogenerators;20082.0;5938.0;156.0;319.0;17229.0;journal;article;2022
"When talking about automation, “autonomous vehicles”, often abbreviated as AVs, come to mind. In transitioning from the “driver” mode to the different automation levels, there is an inevitable need for modeling driving behavior. This often happens through data collection from experiments and studies, but also information extraction, a key step in behavioral modeling. Particularly, naturalistic driving studies and field operational trials are used to collect meaningful data on drivers’ interactions in real–world conditions. On the other hand, information extraction methods allow to predict or mimic driving behavior, by using a set of statistical learning methods. In simple words, the way to understand drivers’ needs and wants in the era of automation can be represented in a data–information cycle, starting from data collection, and ending with information extraction. To develop this cycle, this research reviews studies with keywords “data collection”, “information extraction”, “AVs”, while keeping the focus on driving behavior. The resulting review led to a screening of about 161 papers, out of which about 30 were selected for a detailed analysis. The analysis included an investigation of the methods and equipment used for data collection, the features collected, the size and frequency of the data along with the main problems associated with the different sensory equipment; the studies also looked at the models used to extract information, including various statistical techniques used in AV studies. This paved the way to the development of a framework for data analytics and fusion, allowing the use of highly heterogeneous data to reach the defined objectives; for this paper, the example of impacts of AVs on a network level and AV acceptance is given. The authors suggest that such a framework could be extended and transferred across the various transportation sectors.";Christelle {Al Haddad} and Constantinos Antoniou;"Applied Psychology (Q1); Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Transportation (Q1)";778.0;390.0;United Kingdom;1998-2020;10.1016/j.trf.2021.12.017;;94.0;;13698478;13698478;13698478;13698478;;Data collection, Information extraction, Impacts of AVs, Behavioral modeling, Data analytics, Data fusion;Elsevier Ltd.;;5356.0;Western Europe;1231.0;Q1;20897.0;Transportation research part f: traffic psychology and behaviour;A data–information–knowledge cycle for modeling driving behavior;;3204.0;218.0;784.0;11675.0;journal;article;2022
Wireless Rechargeable Sensor Network (WRSN) is largely used in monitoring of environment and traffic, video surveillance and medical care, etc., and helps to improve the quality of urban life. However, it is challenging to provide the sustainable energy for sensors deployed in buildings, soil or other places, where it is hard to harvest the energy from environment. To address this issue, we design a new wireless charging system, which levers the bus network assisted drone in urban areas. We formulate the drone scheduling problem based on this new wireless charging system to minimize the total time cost of drone subject to all sensors can be charged under the energy constraint of drone. Then, we propose an approximation algorithm DSA for the energy tightened drone scheduling problem. To make the tasks of WRSN sustainable, we further formulate the drone scheduling problem with deadlines of sensors, and present the approximation algorithm DDSA to find the drone schedule with the maximal number of sensors charged by the drone before deadlines. Through the extensive simulations, we demonstrate that DSA can reduce the total time cost by 84.83% compared with Greedy Replenished Energy algorithm, and uses at most 5.98 times of the total time cost of optimal solution on average. Then, we also demonstrate that DDSA can increase the survival rate of sensors by 51.95% compared with Deadline Greedy Replenished Energy algorithm, and can obtain 77.54% survival rate of optimal solution on average.;Yong Jin and Jia Xu and Sixu Wu and Lijie Xu and Dejun Yang and Kaijian Xia;"Hardware and Architecture (Q1); Software (Q2)";262.0;497.0;Netherlands;1996-2020;10.1016/j.sysarc.2021.102059;0.00137;51.0;;13837621;13837621;13837621;13837621;3.777;Wireless rechargeable sensor network, Bus network, Drone scheduling, Traveling salesman path problem, Submodular orienteering problem;Elsevier;;4339.0;Western Europe;598.0;Q1;12398.0;Journal of systems architecture;Bus network assisted drone scheduling for sustainable charging of wireless rechargeable sensor network;1570.0;1137.0;111.0;272.0;4816.0;journal;article;2021
We report the automatic detection of 11 transients (7 possible supernovae and 4 active galactic nuclei candidates) within the Zwicky Transient Facility fourth data release (ZTF DR4), all of them observed in 2018 and absent from public catalogs. Among these, three were not part of the ZTF alert stream. Our transient mining strategy employs 41 physically motivated features extracted from both real light curves and four simulated light curve models (SN Ia, SN II, TDE, SLSN-I). These features are input to a k-D tree algorithm, from which we calculate the 15 nearest neighbors. After pre-processing and selection cuts, our dataset contained approximately a million objects among which we visually inspected the 105 closest neighbors from seven of our brightest, most well-sampled simulations, comprising 89 unique ZTF DR4 sources. Our result illustrates the potential of coherently incorporating domain knowledge and automatic learning algorithms, which is one of the guiding principles directing the SNAD team. It also demonstrates that the ZTF DR is a suitable testing ground for data mining algorithms aiming to prepare for the next generation of astronomical data.;P.D. Aleo and K.L. Malanchev and M.V. Pruzhinskaya and E.E.O. Ishida and E. Russeil and M.V. Kornilov and V.S. Korolev and S. Sreejith and A.A. Volnova and G.S. Narayan;"Instrumentation (Q2); Astronomy and Astrophysics (Q3); Space and Planetary Science (Q3)";277.0;179.0;Netherlands;1996-2021;10.1016/j.newast.2022.101846;0.0016899999999999999;72.0;;13841076;13841092;13841076;13841092;1.325;Transient sources, Time domain astronomy, Supernovae, Active galactic nuclei;Elsevier;;5019.0;Western Europe;435.0;Q2;27730.0;New astronomy;Snad transient miner: finding missed transient events in ztf dr4 using k-d trees;1703.0;465.0;78.0;284.0;3915.0;journal;article;2022
In this study Vis/NIR spectroscopy was applied to evaluate soluble solids content (SSC) of tomato. A total of 168 tomato samples with five different maturity stages, were measured by two developed systems with the wavelength ranges of 500–930 nm and 900–1400 nm, respectively. The raw spectral data were pre-processed by first derivative and standard normal variate (SNV), respectively, and then the effective wavelengths were selected using competitive adaptive reweighted sampling (CARS) and random frog (RF). Partial least squares (PLS) and least square-support vector machines (LS-SVM) were employed to build the prediction models to evaluate SSC in tomatoes. The prediction results revealed that the best performance was obtained using the PLS model with the optimal wavelengths selected by CARS in the range of 900–1400 nm (Rp = 0.820 and RMSEP = 0.207 °Brix). Meanwhile, this best model yielded desirable results with Rp and RMSEP of 0.830 and 0.316 °Brix, respectively, in 60 samples of the independent set. The method proposed from this study can provide an effective and quick way to predict SSC in tomato.;Dongyan Zhang and Yi Yang and Gao Chen and Xi Tian and Zheli Wang and Shuxiang Fan and Zhenghua Xin;"Analytical Chemistry (Q2); Atomic and Molecular Physics, and Optics (Q2); Instrumentation (Q2); Spectroscopy (Q2)";2805.0;396.0;Netherlands;1995-2021;10.1016/j.saa.2020.119139;;123.0;;13861425;13861425;13861425;13861425;;Vis/NIR, Soluble solids content, Tomato, PLS, LS-SVM, Effective wavelength;Elsevier;;4511.0;Western Europe;606.0;Q2;24530.0;Spectrochimica acta - part a: molecular and biomolecular spectroscopy;Nondestructive evaluation of soluble solids content in tomato with different stage by using vis/nir technology and multivariate algorithms;;10524.0;1258.0;2813.0;56751.0;journal;article;2021
"Aim
The increasing availability of Big Biomedical Data is leading to large research data samples collected over long periods of time. We propose the analysis of the kinematics of data probability distributions over time towards the characterization of data temporal variability.
Methods
First, we propose a kinematic model based on the estimation of a continuous data temporal trajectory, using Functional Data Analysis over the embedding of a non-parametric statistical manifold which points represent data temporal batches, the Information Geometric Temporal (IGT) plot. This model allows measuring the velocity and acceleration of data changes. Next, we propose a coordinate-free method to characterize the oriented seasonality of data based on the parallelism of lagged velocity vectors of the data trajectory throughout the IGT space, the Auto-Parallelism of Velocity Vectors (APVV) and APVVmap. Finally, we automatically explain the maximum variance components of the IGT space coordinates by means of correlating data points with known temporal factors from the domain application.
Materials
Methods are evaluated on the US National Hospital Discharge Survey open dataset, consisting of 3,25M hospital discharges between 2000 and 2010.
Results
Seasonal and abrupt behaviours were present on the estimated multivariate and univariate data trajectories. The kinematic analysis revealed seasonal effects and punctual increments in data celerity, the latter mainly related to abrupt changes in coding. The APVV and APVVmap revealed oriented seasonal changes on data trajectories. For most variables, their distributions tended to change to the same direction at a 12-month period, with a peak of change of directionality at mid and end of the year. Diagnosis and Procedure codes also included a 9-month periodic component. Kinematics and APVV methods were able to detect seasonal effects on extreme temporal subgrouped data, such as in Procedure code, where Fourier and autocorrelation methods were not able to. The automated explanation of IGT space coordinates was consistent with the results provided by the kinematic and seasonal analysis. Coordinates received different meanings according to the trajectory trend, seasonality and abrupt changes.
Discussion
Treating data as a particle moving over time through a multidimensional probabilistic space and studying the kinematics of its trajectory has turned out to a new temporal variability methodology. Its results on the NHDS were aligned with the dataset and population descriptions found in the literature, contributing with a novel temporal variability characterization. We have demonstrated that the APVV and APVVmat are an appropriate tool for the coordinate-free and oriented analysis of trajectories or complex multivariate signals.
Conclusion
The proposed methods comprise an exploratory methodology for the characterization of data temporal variability, what may be useful for a reliable reuse of Big Biomedical Data repositories acquired over long periods of time.";Carlos Sáez and Juan M García-Gómez;Health Informatics (Q1);572.0;482.0;Ireland;1996-2020;10.1016/j.ijmedinf.2018.09.015;0.010440000000000001;106.0;;13865056;18728243;13865056;18728243;4.046;Temporal stability, Data quality, Time series, Data reuse, Big data, Seasonality, Coordinate-free, Trajectories, Functional data analysis, Statistical manifolds;Elsevier Ireland Ltd;;4320.0;Western Europe;1124.0;Q1;23689.0;International journal of medical informatics;Kinematics of big biomedical data to characterize temporal variability and seasonality of data repositories: functional data analysis of data temporal evolution over non-parametric statistical manifolds;7651.0;3102.0;210.0;579.0;9073.0;journal;article;2018
Delay Extraction (DE) in Single Particle Mass Spectrometry (SPMS) provides substantial resolution enhancement. However, DE has two main drawbacks which are discussed in our article. First, ion peak position in this case becomes very sensitive to initial ion coordinate. Therefore, substantial peak jitter is observed when switching between mass spectra of individual particles. This peak jitter obstructs correct mass spectra accumulation resulting in wide peaks with irregular shapes in the accumulated mass spectrum, which leads to the fact that isotopic pattern identification becomes difficult. In the present article two ways of the peak jitter compensation, Dynamic Calibration and Mass Spectra correction based on Spectra Correlation, are proposed. It was shown that both proposed techniques provide substantial resolution improvement in the summed mass spectrum especially for small peaks with complex isotopic patterns. Second, time delay distorts regular quadratic dependence between time-of-flight and m/z. In our paper we also propose calibration equation with variable exponent for SPMS modified with DE. It is shown that usage of an exponent as a fitting parameter allows to achieve mass accuracy comparable with 4th degree equation proposed earlier.;Alexey Chudinov and Lei Li and Zhen Zhou and Zhengxu Huang and Wei Gao and Jiajun Yu and Sergei Nikiforov and Alexander Pikhtelev and Aygul Bukharina and Viacheslav Kozlovskiy;"Condensed Matter Physics (Q2); Instrumentation (Q2); Physical and Theoretical Chemistry (Q2); Spectroscopy (Q3)";538.0;184.0;Netherlands;1998-2020;10.1016/j.ijms.2018.11.013;0.00471;112.0;;13873806;13873806;13873806;13873806;1.986;Delay extraction, Time-of-flight, Single particle, Aerosol, Resolution, Mass accuracy;Elsevier;;4312.0;Western Europe;511.0;Q2;12142.0;International journal of mass spectrometry;Improvement of peaks identification and dynamic range for bi-polar single particle mass spectrometer;7381.0;1046.0;112.0;551.0;4829.0;journal;article;2019
During the past decades, high-throughput approaches for analyzing different molecular classes such as nucleic acids, proteins, metabolites, and lipids have grown rapidly. These approaches became powerful tools for getting a fundamental understanding of biological systems. Considering each approach and its results separately, relations and causal connections between these classes have no chance to be revealed, since only separate molecular snapshots are provided. Only a combined approach, not fully established yet, with the integration of the corresponding data, might yield a comprehensive and complete understanding of biological processes, such as crosstalk and interactions in signaling pathways. Taking two or more omics-methods into consideration for analysis is referred to as a multi-omics approach, which is gradually evolving. In this critical note, we briefly discuss the relevance, challenges, current state, and potential of data integration from multi-omics approaches, with a special focus on lipidomics analysis, listing the advantages and gaps in this field. This article is part of a Special Issue entitled: BBALIP_Lipidomics Opinion Articles edited by Sepp Kohlwein.;Dominik Kopczynski and Cristina Coman and Rene P. Zahedi and Kristina Lorenz and Albert Sickmann and Robert Ahrends;"Cell Biology (Q2); Molecular Biology (Q2)";454.0;436.0;Netherlands;1977, 1985, 1998-2020;10.1016/j.bbalip.2017.02.003;;160.0;;13881981;13881981;18792618;13881981;;;Elsevier;;7952.0;Western Europe;1769.0;Q2;16831.0;Biochimica et biophysica acta - molecular and cell biology of lipids;Multi-omics: a critical technical perspective on integrative lipidomics approaches;;2373.0;195.0;468.0;15507.0;journal;article;2017
Magnetoencephalography (MEG) records weak magnetic fields outside the human head and thereby provides millisecond-accurate information about neuronal currents supporting human brain function. MEG and electroencephalography (EEG) are closely related complementary methods and should be interpreted together whenever possible. This manuscript covers the basic physical and physiological principles of MEG and discusses the main aspects of state-of-the-art MEG data analysis. We provide guidelines for best practices of patient preparation, stimulus presentation, MEG data collection and analysis, as well as for MEG interpretation in routine clinical examinations. In 2017, about 200 whole-scalp MEG devices were in operation worldwide, many of them located in clinical environments. Yet, the established clinical indications for MEG examinations remain few, mainly restricted to the diagnostics of epilepsy and to preoperative functional evaluation of neurosurgical patients. We are confident that the extensive ongoing basic MEG research indicates potential for the evaluation of neurological and psychiatric syndromes, developmental disorders, and the integrity of cortical brain networks after stroke. Basic and clinical research is, thus, paving way for new clinical applications to be identified by an increasing number of practitioners of MEG.;Riitta Hari and Sylvain Baillet and Gareth Barnes and Richard Burgess and Nina Forss and Joachim Gross and Matti Hämäläinen and Ole Jensen and Ryusuke Kakigi and François Mauguière and Nobukatzu Nakasato and Aina Puce and Gian-Luca Romani and Alfons Schnitzler and Samu Taulu;"Neurology (Q1); Neurology (clinical) (Q1); Physiology (medical) (Q1); Sensory Systems (Q1)";750.0;265.0;Ireland;1999-2020;10.1016/j.clinph.2018.03.042;0.018330000000000003;183.0;;13882457;13882457;18728952;13882457;3.708;Magnetoencephalography, Electroencephalography, Clinical neurophysiology, Evoked and event-related responses, Transient and steady-state responses, Spontaneous brain activity, Neural oscillations, Analysis and interpretation, Artifacts, Source modeling, Epilepsy, Preoperative evaluation, Stroke, Pain, Traumatic brain injury, Parkinson’s disease, Hepatic encephalopathy, Alzheimer’s disease and dementia, Neuropsychiatric disorders, Brain maturation and development, Dyslexia, Guidelines;Elsevier Ireland Ltd;;4259.0;Western Europe;1478.0;Q1;14900.0;Clinical neurophysiology;Ifcn-endorsed practical guidelines for clinical magnetoencephalography (meg);23593.0;3272.0;389.0;997.0;16568.0;journal;article;2018
IoT (Internet of Things) is a new paradigm which provides a set of new services for the next wave of technological innovations. IoT applications are nearly limitless while enabling seamless integration of the cyber-world with the physical world. However, despite the enormous efforts of standardization bodies, alliances, industries, researchers and others, there are still numerous problems to deal with in order to reach the full potential of IoT. These issues should be considered from various aspects such as enabling technologies, applications, business models, social and environmental impacts. In focus of this paper are open issues and challenges considered from the technological perspective. Just for clarification, we put in light different visions that stand behind this paradigm in order to facilitate a better understanding of the IoT's features. Furthermore, this exhaustive survey provides insights into the state-of-the-art of IoT enabling and emerging technologies. The most relevant among them are addressed with some details. The main scope is to deliver a comprehensive overview of open issues and challenges to be tackled by future research. We provide some insights into specific emerging ideas in order to facilitate future research. Also, this paper brings order in the existing literature by classifying contributions according to different research topics.;Alem Čolaković and Mesud Hadžialić;Computer Networks and Communications (Q1);877.0;593.0;Netherlands;1977-1984, 1989-1990, 1996-2020;10.1016/j.comnet.2018.07.017;0.00957;135.0;;13891286;13891286;13891286;13891286;4.474;IoT (Internet of Things), IoT vision, IoT features, IoT enabling technologies, Open issues and challenges, Future research direction;Elsevier;;5160.0;Western Europe;798.0;Q1;26811.0;Computer networks;Internet of things (iot): a review of enabling technologies, challenges, and open research issues;11644.0;4981.0;383.0;896.0;19762.0;journal;article;2018
"Study objectives
We present an automated sleep electroencephalogram (EEG) spectral analysis pipeline that includes an automated artifact detection step, and we test the hypothesis that spectral power density estimates computed with this pipeline are comparable to those computed with a commercial method preceded by visual artifact detection by a sleep expert (standard approach).
Methods
EEG data were analyzed from the C3-A2 lead in a sample of polysomnograms from 161 older women participants in a community-based cohort study. We calculated the sensitivity, specificity, accuracy, and Cohen's kappa measures from epoch-by-epoch comparisons of automated to visual-based artifact detection results; then we computed the average EEG spectral power densities in six commonly used EEG frequency bands and compared results from the two methods using correlation analysis and Bland–Altman plots.
Results
Assessment of automated artifact detection showed high specificity [96.8%–99.4% in non-rapid eye movement (NREM), 96.9%–99.1% in rapid eye movement (REM) sleep] but low sensitivity (26.7%–38.1% in NREM, 9.1–27.4% in REM sleep). However, large artifacts (total power > 99th percentile) were removed with sensitivity up to 87.7% in NREM and 90.9% in REM, with specificities of 96.9% and 96.6%, respectively. Mean power densities computed with the two approaches for all EEG frequency bands showed very high correlation (≥0.99). The automated pipeline allowed for a 100-fold reduction in analysis time with regard to the standard approach.
Conclusion
Despite low sensitivity for artifact rejection, the automated pipeline generated results comparable to those obtained with a standard method that included manual artifact detection. Automated pipelines can enable practical analyses of recordings from thousands of individuals, allowing for use in genetics and epidemiological research requiring large samples.";Sara Mariani and Leila Tarokh and Ina Djonlagic and Brian E. Cade and Michael G. Morrical and Kristine Yaffe and Katie L. Stone and Kenneth A. Loparo and Shaun M. Purcell and Susan Redline and Daniel Aeschbach;Medicine (miscellaneous) (Q1);838.0;296.0;Netherlands;2000-2020;10.1016/j.sleep.2017.11.1128;0.01619;122.0;;13899457;18785506;13899457;18785506;3.492;Large-scale spectral analysis, Sleep EEG, Artifact detection;Elsevier;;3829.0;Western Europe;1335.0;Q1;19834.0;Sleep medicine;Evaluation of an automated pipeline for large-scale eeg spectral analysis: the national sleep research resource;14810.0;3251.0;466.0;969.0;17844.0;journal;article;2018
Across a wide range of disciplines, mounting evidence points to solutions for addressing the global biodiversity and climate crisis through sustainable land use development. Managing ecosystem services offers promising potential of combining environmental, economic, and social interests in this process. Achieving sustainability, however, requires collaboration across disciplines, or in short “cross-disciplinary” approaches. Multi-, inter- and transdisciplinary approaches are often used as synonyms, although they are defined by different levels of integrating results and perspectives. We highlight challenges and opportunities related to these cross-disciplinary approaches by using research on bird- and bat-mediated ecosystem services as a case - with a focus on sustainable agricultural development. Examples from transdisciplinary collaborations show how more integrative and inclusive approaches promote the implementation of basic and applied ecological research into land use practices. Realizing this opportunity requires strong partnerships between science, practice and policy, as well as integration of diverse skills and perspectives. If appropriately funded and guided, this effort is rewarded by improved data quality, more targeted concepts, as well as improvement implementation and impact of sustainability research and practice. We outline a stepwise approach for developing these processes and highlight case studies from bird and bat research to inspire cross-disciplinary approaches within and beyond ecology.;Bea Maas and Carolina Ocampo-Ariza and Christopher J. Whelan;Ecology, Evolution, Behavior and Systematics (Q1);190.0;310.0;Germany;2000-2020;10.1016/j.baae.2021.06.010;0.0035;79.0;;14391791;16180089;14391791;16180089;3.414;Agricultural biodiversity, Collaborative conservation, Ecosystem functions, Ecosystem services, Knowledge co-production, Sustainable agriculture, Transdisciplinary research;Urban und Fischer Verlag Jena;;6126.0;Western Europe;1372.0;Q1;22800.0;Basic and applied ecology;Cross-disciplinary approaches for better research: the case of birds and bats;4102.0;725.0;65.0;195.0;3982.0;journal;article;2021
"Objectives
The rise of data analytics has been not only central to the digital transformation of many industries and governments, but is now ubiquitous in daily life. But what is it? Researchers in military human performance may very well ask themselves: What is new? After all, aren't they already collecting, analysing, interpreting, and presenting data? Do they need to adapt?
Discussion
Defence and security have often been at the forefront of new technologies, but has lagged other industries with respect to data analytics. Sports science is one of the industries that are on the leading edge and this presents an opportunity that researchers in military human performance must seize.
Conclusions
Researchers must embrace data analytics and seek opportunities to ‘operationalize’ their research via data science: responsible analytics respecting scientific development supporting decision making at the necessary speed of relevance.";Bohdan L. Kaluzny;"Orthopedics and Sports Medicine (Q1); Physical Therapy, Sports Therapy and Rehabilitation (Q1); Sports Science (Q1)";662.0;370.0;Netherlands;1998-2020;10.1016/j.jsams.2021.04.003;0.01434;99.0;;14402440;18781861;14402440;18781861;4.319;Data analytics, Operations research, Military human performance;Elsevier BV;;3087.0;Western Europe;1724.0;Q1;19874.0;Journal of science and medicine in sport;Data analytics in military human performance: getting in the game: summary of a keynote address;10210.0;2840.0;244.0;716.0;7533.0;journal;article;2021
Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.;Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree;"Cardiology and Cardiovascular Medicine (Q2); Pulmonary and Respiratory Medicine (Q2)";618.0;193.0;United Kingdom;2000-2020;10.1016/j.hlc.2021.04.023;0.0073;46.0;;14439506;14442892;14439506;14442892;2.975;Big data, Datasets, Cardiovascular disease, National platform;Elsevier Ltd.;;3057.0;Western Europe;770.0;Q2;23235.0;Heart lung and circulation;A versatile big data health system for australia: driving improvements in cardiovascular health;4050.0;1363.0;327.0;744.0;9995.0;journal;article;2021
Noting the growing literature on relational bibliometrics and prevailing methodological challenges in hospitality and tourism research – inadequate bibliometric-focused structure and methodological transparency – this study contributes to knowledge about applicable analytical procedures. The authors uncover methodological issues by content analyzing 85 relational bibliometric articles published in 19 hospitality and tourism journals. The findings provide a basis for best practice recommendations. Four guiding principles are proposed for the scholarly deployment of relational bibliometrics, namely: (1) using multiple relational techniques to ensure a rich and comprehensive coverage of the pertinent field, (2) providing sufficient methodological disclosure, particularly language selection, data extraction from the applicable sampling, data cleaning and supplemental materials provided, (3) following a best practice work flow, including relational study methodologies, and (4) ensuring methodological adherence to three desired attributes – structured, comprehensive and transparent. The latter can potentially improve the thoroughness, clarity, and trustworthiness of future studies. The study concludes with a discussion of the findings, and a future research agenda which presents significant insights offering encouragement for bibliometric analyses, as well as acknowledging potential limitations.;Mehmet Ali Koseoglu and Melissa Yan {Yee Yick} and Brian King and Hasan Evrim Arici;Tourism, Leisure and Hospitality Management (Q1);190.0;544.0;United Kingdom;2006-2020;10.1016/j.jhtm.2022.07.002;0.00232;34.0;;14476770;18395260;14476770;18395260;5.959;Bibliometrics, Co-citation, Co-word, Co-authorship, Tourism, Hospitality;Elsevier BV;;7926.0;Western Europe;1310.0;Q1;21100255484.0;Journal of hospitality and tourism management;Relational bibliometrics for hospitality and tourism research: a best practice guide;2467.0;1119.0;153.0;194.0;12127.0;journal;article;2022
AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.;Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui;"Geography, Planning and Development (Q1); Management, Monitoring, Policy and Law (Q1)";624.0;552.0;Netherlands;1998-2020;10.1016/j.envsci.2021.09.001;;115.0;;14629011;18736416;14629011;18736416;;;Elsevier BV;;6395.0;Western Europe;1716.0;Q1;21536.0;Environmental science and policy;Ai for social good: ai and big data approaches for environmental decision-making;;3655.0;262.0;635.0;16755.0;journal;article;2021
We use gravity information obtained from the XGM2016 global gravitational model together with topographic, bathymetric and seismic data to interpret the crustal structure beneath Cameroon and adjoined geological regions. For this purpose, we apply the regularized non-linear gravity inversion for a gravimetric determination of the Moho depth utilizing existing results of seismic data analysis as constraints. The estimated Moho model reflects regional tectonic configuration and geological structure of this region, mainly consisting of two major geological units, i.e. the Cameroon Volcanic Line and the Congo Craton. A validation of gravimetric result at sites of the Cameroon Broadband Seismic Experiment (CBSE) reveals overall similarities between gravimetric and seismic estimates. A comparison of our result is also conducted with previously published results. The cross-comparison of these results reveals a good agreement between them, particularly beneath the Cameroon Volcanic Line, the Adamawa Plateau and the Garoua Rift. Nevertheless, some relatively large inconsistencies roughly reaching ±10 km in estimated values of the Moho depth are identified in geological regions of the Congo Craton and the Yaoundé domain. The spatial correlation analysis between the Moho geometry and the topography indicates an isostatic state of particular geological units, suggesting their compensation stage. Our result closely agrees with the assumption that most of isostatically over compensated geological structures were formed during a compressional tectonic regime, except for the Garoua Rift that was likely formed during an extensional regime. We also computed the Bouguer gravity data at different constant elevations above sea level in order to supress a gravitational signature of shallower sources, while enhancing a gravitational signature from deeper crustal and lithospheric structures, focusing primarily on cores of major cratonic formations. The Bouguer gravity maps indicate that the Yaoundé domain likely represents the crustal manifestation of the suture zone between the Congo Craton and the Adamawa-Yadé domain, acting as a micro-continent.;Franck Eitel {Kemgang Ghomsi} and Nguiya Sévérin and Animesh Mandal and Françoise Enyegue A. Nyam and Robert Tenzer and Alain P. {Tokam Kamga} and Robert Nouayou;"Earth-Surface Processes (Q2); Geology (Q2)";912.0;199.0;United Kingdom;1983-2020;10.1016/j.jafrearsci.2019.103657;0.00628;76.0;;1464343X;1464343X;1464343X;1464343X;2.046;Cameroon, Crust, Gravity inversion, Moho, Seismic data;Elsevier Ltd.;;7282.0;Western Europe;572.0;Q2;31815.0;Journal of african earth sciences;Cameroon's crustal configuration from global gravity and topographic models and seismic data;8527.0;1920.0;277.0;922.0;20170.0;journal;article;2020
The increasing integration of computer technology for the processing of business transactions and the growing amount of financially relevant data in organizations create new challenges for external auditors. The availability of digital data opens up new opportunities for innovative audit procedures. Process mining can be used as a novel data analysis technique to support auditors in this context. Process mining algorithms produce process models by analyzing recorded event logs. Contemporary general purpose mining algorithms commonly use the temporal order of recorded events for determining the control flow in mined process models. The presented research shows how data dependencies related to the accounting structure of recorded events can be used as an alternative to the temporal order of events for discovering the control flow. The generated models provide accurate information on the control flow from an accounting perspective and show a lower complexity compared to those generated using timestamp dependencies. The presented research follows a design science research approach and uses three different real world data sets for evaluation purposes.;Michael Werner;"Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2)";50.0;556.0;United States;2000-2020;10.1016/j.accinf.2017.03.004;0.00056;53.0;;14670895;14670895;14670895;14670895;4.400;Process mining, Financial audits, Journal entries, Business process intelligence, Business process modeling, Control flow inference, Design science research, Enterprise resource planning systems;Elsevier Inc.;;6548.0;Northern America;897.0;Q1;29806.0;International journal of accounting information systems;Financial process mining - accounting data structure dependent control flow inference;1009.0;290.0;21.0;51.0;1375.0;journal;article;2017
Biodiversity databases are typically incomplete and biased. We identify their three main limitations for characterizing the geographic distributions of species: unknown levels of survey effort, unknown absences of a species from a region, and unknown level of repeated occurrence of a species in different samples collected at the same location. These limitations hinder our ability to distinguish between the actual absence of a species at a given location and its (erroneous) apparent absence as consequence of inadequate surveys. Good practice in biodiversity research requires knowledge of the number, location and degree of completeness of relatively well-surveyed inventories within territorial units. We herein present KnowBR, an application designed to simultaneously estimate the completeness of species inventories across an unlimited number of spatial units and different geographical extents, resolutions and unit expanses from any biodiversity database. We use the number of database records gathered in a territorial unit as a surrogate of survey effort, assuming that such number correlates positively with the probability of recording a species within such area. Consequently, KnowBR uses a “record-by-species” matrix to estimate the relationship between the accumulated number of species and the number of database records to characterize the degree of completeness of the surveys. The final slope of the species accumulation curves and completeness percentages are used to discriminate and map well-surveyed territorial units according to user criteria. The capacity and possibilities of KnowBR are demonstrated through two examples derived from data of varying geographic extent and numbers of records. Further, we identify the main advances that would improve the current functionality of KnowBR.;Jorge M. Lobo and Joaquín Hortal and José Luís Yela and Andrés Millán and David Sánchez-Fernández and Emilio García-Roselló and Jacinto González-Dacosta and Juergen Heine and Luís González-Vilas and Castor Guisande;"Decision Sciences (miscellaneous) (Q1); Ecology (Q1); Ecology, Evolution, Behavior and Systematics (Q1)";2408.0;500.0;Netherlands;2001-2021;10.1016/j.ecolind.2018.03.077;0.037880000000000004;127.0;;1470160X;1470160X;1470160X;1470160X;4.958;Spatial bias, Data limitations, Database records, Geographic distribution, Survey completeness, Wallacean shortfall;Elsevier;;6905.0;Western Europe;1315.0;Q1;20292.0;Ecological indicators;Knowbr: an application to map the geographical variation of survey effort and identify well-surveyed areas from biodiversity databases;32205.0;12850.0;1074.0;2432.0;74155.0;journal;article;2018
"Summary
In sub-Saharan Africa (SSA), urgent action is needed to curb a growing crisis in cancer incidence and mortality. Without rapid interventions, data estimates show a major increase in cancer mortality from 520 348 in 2020 to about 1 million deaths per year by 2030. Here, we detail the state of cancer in SSA, recommend key actions on the basis of analysis, and highlight case studies and successful models that can be emulated, adapted, or improved across the region to reduce the growing cancer crises. Recommended actions begin with the need to develop or update national cancer control plans in each country. Plans must include childhood cancer plans, managing comorbidities such as HIV and malnutrition, a reliable and predictable supply of medication, and the provision of psychosocial, supportive, and palliative care. Plans should also engage traditional, complementary, and alternative medical practices employed by more than 80% of SSA populations and pathways to reduce missed diagnoses and late referrals. More substantial investment is needed in developing cancer registries and cancer diagnostics for core cancer tests. We show that investments in, and increased adoption of, some approaches used during the COVID-19 pandemic, such as hypofractionated radiotherapy and telehealth, can substantially increase access to cancer care in Africa, accelerate cancer prevention and control efforts, increase survival, and save billions of US dollars over the next decade. The involvement of African First Ladies in cancer prevention efforts represents one practical approach that should be amplified across SSA. Moreover, investments in workforce training are crucial to prevent millions of avoidable deaths by 2030. We present a framework that can be used to strategically plan cancer research enhancement in SSA, with investments in research that can produce a return on investment and help drive policy and effective collaborations. Expansion of universal health coverage to incorporate cancer into essential benefits packages is also vital. Implementation of the recommended actions in this Commission will be crucial for reducing the growing cancer crises in SSA and achieving political commitments to the UN Sustainable Development Goals to reduce premature mortality from non-communicable diseases by a third by 2030.";Wilfred Ngwa and Beatrice W Addai and Isaac Adewole and Victoria Ainsworth and James Alaro and Olusegun I Alatise and Zipporah Ali and Benjamin O Anderson and Rose Anorlu and Stephen Avery and Prebo Barango and Noella Bih and Christopher M Booth and Otis W Brawley and Jean-Marie Dangou and Lynette Denny and Jennifer Dent and Shekinah N C Elmore and Ahmed Elzawawy and Diane Gashumba and Jennifer Geel and Katy Graef and Sumit Gupta and Serigne-Magueye Gueye and Nazik Hammad and Laila Hessissen and Andre M Ilbawi and Joyce Kambugu and Zisis Kozlakidis and Simon Manga and Lize Maree and Sulma I Mohammed and Susan Msadabwe and Miriam Mutebi and Annet Nakaganda and Ntokozo Ndlovu and Kingsley Ndoh and Jerry Ndumbalo and Mamsau Ngoma and Twalib Ngoma and Christian Ntizimira and Timothy R Rebbeck and Lorna Renner and Anya Romanoff and Fidel Rubagumya and Shahin Sayed and Shivani Sud and Hannah Simonds and Richard Sullivan and William Swanson and Verna Vanderpuye and Boateng Wiafe and David Kerr;Oncology (Q1);575.0;890.0;United Kingdom;2000-2020;10.1016/S1470-2045(21)00720-8;;324.0;;14702045;14702045;14745488;14702045;;;Lancet Publishing Group;;1416.0;Western Europe;13530.0;Q1;12354.0;Lancet oncology, the;Cancer in sub-saharan africa: a lancet oncology commission;;15824.0;491.0;1722.0;6955.0;journal;article;2022
"Microglia–astrocyte interactions represent a delicate balance affecting neural cell functions in health and disease. Tightly controlled to maintain homeostasis during physiological conditions, rapid and prolonged departures during disease, infection, and following trauma drive multiple outcomes: both beneficial and detrimental. Recent sequencing studies at the bulk and single-cell level in humans and rodents provide new insight into microglia–astrocyte communication in homeostasis and disease. However, the complex changing ways these two cell types functionally interact has been a barrier to understanding disease initiation, progression, and disease mechanisms. Single cell sequencing is providing new insights; however, many questions remain. Here, we discuss how to bridge transcriptional states to specific functions so we can develop therapies to mediate negative effects of altered microglia–astrocyte interactions.";Shane A. Liddelow and Samuel E. Marsh and Beth Stevens;"Immunology (Q1); Immunology and Allergy (Q1)";286.0;1067.0;United Kingdom;1987-1990, 1993-1994, 2001-2020;10.1016/j.it.2020.07.006;0.024569999999999998;226.0;;14714906;14714906;14714981;14714906;16.687;microglia, astrocytes, neuroimmune, single cell sequencing, neuroinflammation, neurodegeneration;Elsevier Ltd.;;7350.0;Western Europe;6349.0;Q1;21365.0;Trends in immunology;Microglia and astrocytes in disease: dynamic duo or partners in crime?;16915.0;3648.0;112.0;297.0;8232.0;journal;article;2020
"The biomedical field has witnessed remarkable advances in analytical tools and technologies that have expanded our understanding of healthy and diseased human tissue and, at the same time, enable extensive molecular characterization of living cells. The volume of scientific data generated is expanding in an unprecedented manner; however, these data remain scattered across research groups worldwide. Access to various data sources in a systematic fashion could hugely benefit the progress of nascent fields such as stem cell-based therapeutics. We explore here the currently available databases for stem cell research, and we propose creating a common portal to access these different databases that could act as a translational link between basic and clinical research to advance stem cell-based therapeutic development.";Andreas Kurtz and Magdi Elsallab and Ralf Sanzenbacher and Mohamed Abou-El-Enein;"Molecular Biology (Q1); Molecular Medicine (Q1)";298.0;774.0;United Kingdom;2000-2020;10.1016/j.molmed.2018.10.008;0.014719999999999999;178.0;;14714914;1471499X;14714914;1471499X;11.951;big data, stem cells, regulation, clinical trials, cell lines, registries, databases, translation;Elsevier Ltd.;;7878.0;Western Europe;3789.0;Q1;20205.0;Trends in molecular medicine;Linking scattered stem cell-based data to advance therapeutic development;13213.0;2703.0;110.0;302.0;8666.0;journal;article;2019
Molecular surveillance of antimalarial drug resistance markers has become an important part of resistance detection and containment. In the current climate of multidrug resistance, including resistance to the global front-line drug artemisinin, there is a consensus to upscale molecular surveillance. The most salient limitation to current surveillance efforts is that skill and infrastructure requirements preclude many regions. This includes sub-Saharan Africa, where Plasmodium falciparum is responsible for most of the global malaria disease burden. New molecular and data technologies have emerged with an emphasis on accessibility. These may allow surveillance to be conducted in broad settings where it is most needed, including at the primary healthcare level in endemic countries, and extending to the village health worker.;Christiane Prosser and Wieland Meyer and John Ellis and Rogan Lee;"Infectious Diseases (Q1); Parasitology (Q1)";322.0;549.0;United Kingdom;2001-2020;10.1016/j.pt.2018.01.001;0.01108;145.0;;14714922;14715007;14714922;14715007;9.014;antimalarial resistance surveillance, artemisinin resistance, molecular surveillance, mHealth;Elsevier Ltd.;;5293.0;Western Europe;2239.0;Q1;20863.0;Trends in parasitology;Evolutionary arms race: antimalarial resistance molecular surveillance;8666.0;2014.0;126.0;363.0;6669.0;journal;article;2018
A gap in informatics expertise amongst nursing students, practising staff and faculty has been noted globally, which reduces the potential for nurses to utilise technology to enhance patient care. National nursing education strategies and recommendations from professional associations have identified digital health as an area that needs investment. This case study describes how health informatics is being integrated into a Bachelor of Nursing programme in the United Kingdom. An international collaboration with a US-UK Fulbright Specialist Scholar enabled individual learning units corresponding to key health informatics competencies to be designed and incorporated into a pedagogic framework grounded in the spiral learning approach. This approach is proposed as one way to integrate informatics into nursing education, so students can become competent clinicians that are able to deliver technology enabled care in the health service.;Siobhan O'Connor and Elizabeth LaRue;"Education (Q1); Nursing (miscellaneous) (Q1); Medicine (miscellaneous) (Q2)";433.0;224.0;United Kingdom;2001-2020;10.1016/j.nepr.2020.102934;0.00456;47.0;;14715953;18735223;14715953;18735223;2.281;Nursing education, Informatics, Technology, Digital health;Churchill Livingstone;;3619.0;Western Europe;924.0;Q1;28805.0;Nurse education in practice;Integrating informatics into undergraduate nursing education: a case study using a spiral learning approach;3334.0;1193.0;193.0;486.0;6984.0;journal;article;2021
This study enriched the research horizon of the social sciences and contents of MOOCs by providing stakeholders with authentic data-driven recommendations to better conceptualize, design, develop and deliver MOOCs in today's higher education context. Key factors driving positive/negative learning experiences in business MOOCs were identified and explored. A topic modelling algorithm—Latent Dirichlet allocation (LDA)—was used to examine 144,946 online reviews of 729 business courses on Coursera between August 7, 2015 and August 16, 2021. Two major themes and 11 topics emerged as MOOC delivery (professor, information, comprehension, assessment and materials) and subject matter (finance, marketing, people management, computer skills, technology and project management). A textual salience-valence analysis was employed to analyze the factors driving positive/negative learning experiences regarding MOOC delivery. Findings suggested that 1) business MOOCs should value the importance of instructors' professional and celebrity image to appeal to learners, 2) course design and structure should be easy and simple to manage by learners, 3) course contents, information and assessment should be challenging rather than hard, and 4) the application and validation of peer reviews in both learning process and assessment should be more responsive to eliminate potential issues that could negatively impact learning experiences.;Xiaoxia Wei and Viriya Taecharungroj;"Education (Q1); Strategy and Management (Q1)";146.0;294.0;Netherlands;1970, 2012-2020;10.1016/j.ijme.2022.100675;0.00116;28.0;;14728117;14728117;14728117;14728117;2.707;MOOC learning experience, Online reviews, Sentiment analysis, Textual analysis, Coursera;Elsevier BV;;5643.0;Western Europe;1167.0;Q1;21100206607.0;International journal of management education;How to improve learning experience in moocs an analysis of online reviews of business courses on coursera;1035.0;579.0;47.0;148.0;2652.0;journal;article;2022
"Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.";Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke;Infectious Diseases (Q1);512.0;660.0;United Kingdom;2001-2020;10.1016/S1473-3099(21)00585-5;;235.0;;14733099;14744457;14733099;14744457;;;Lancet Publishing Group;;1427.0;Western Europe;7475.0;Q1;22471.0;Lancet infectious diseases, the;Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications;;9339.0;533.0;1381.0;7607.0;journal;article;2022
This study proposed a novel methodology that integrates complex network theory and multiple time series to enhance the systematic understanding of the daily settlement behavior in deep excavation. The original time series of ground surface, surrounding buildings, and structure settlement instrumentation data over an excavation time period were measured into a similarity matrix with correlation coefficients. A threshold was then determined and binarized into adjacent matrix to identify the optimal topology and structure of the complex network. The reconstructed settlement network has nodes corresponding to multiple settlement time series individually and edges regarded as nonlinear relationships between them. A deep excavation case study of the metro station project in the Wuhan Metro network, China, was applied to validate the feasibility and potential value of the proposed approach. Results of the topological analysis corroborate a small-world phenomenon with highly compacted interactions and provide the assessment of the significance among multiple settlement time series. This approach, which provides a new way to assess the safety monitoring data in underground construction, can be implemented as a tool for extracting macro- and micro-level decision information from multiple settlement time series in deep excavation from complex system perspectives.;Cheng Zhou and Lieyun Ding and Ying Zhou and Hanbin Luo;"Artificial Intelligence (Q1); Information Systems (Q1)";289.0;641.0;United Kingdom;2002-2020;10.1016/j.aei.2018.02.005;0.00428;81.0;;14740346;14740346;14740346;14740346;5.603;Deep excavation, Settlement time series, Complex network, Similarity matrix, Topological analysis, Node influence;Elsevier Ltd.;;5605.0;Western Europe;1107.0;Q1;23640.0;Advanced engineering informatics;Topological mapping and assessment of multiple settlement time series in deep excavation: a complex network perspective;4432.0;1973.0;146.0;295.0;8184.0;journal;article;2018
Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.;Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila;"Geochemistry and Petrology (Q2); Geophysics (Q2)";293.0;287.0;United Kingdom;1982, 1991-1992, 1995, 2002-2020;10.1016/j.pce.2022.103152;0.00279;82.0;;14747065;14747065;14747065;14747065;2.712;Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor, Wastewater treatment;Elsevier Ltd.;;4539.0;Western Europe;724.0;Q2;23315.0;Physics and chemistry of the earth;Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant;5794.0;923.0;111.0;303.0;5038.0;journal;article;2022
Technological progress and the development of laboratory techniques and bioinformatics tools have led to the availability of ever-increasing amounts of biological data including genomic, proteomic, and transcriptomic sequences and related information. These data have helped in understanding some of the complicated life process from a systematic level. Many diseases are generated by abnormalities in multiple regulating processes. In this study, we constructed a novel miRNA–gene–disease fusion (MGDF) algorithm by integrating three genome-wide networks, namely microRNA (miRNA), gene function, and disease similarity networks. The data fusion method was applied to construct a miRNA–gene–disease association network model from these networks to explore miRNA–disease associations mediated by genes with similar functions. mmiRNAs bind to their target genes and regulate their expression, so the miRNA–gene and gene–disease regulatory relationships were included in the network model to more accurately predict miRNA–disease associations. The proposed MGDF was used to predict miRNA–cancer associations and the results show that most of the predicted associations had evidence in existing databases.;Chunyu Wang and Kai Sun and Juexin Wang and Maozu Guo;"Biochemistry (Q3); Computational Mathematics (Q3); Organic Chemistry (Q3); Structural Biology (Q4)";543.0;277.0;United Kingdom;2003-2020;10.1016/j.compbiolchem.2020.107357;0.00295;60.0;;14769271;14769271;14769271;14769271;2.877;Network fusion, Random walk, miRNA, Disease;Elsevier Ltd.;;5310.0;Western Europe;416.0;Q3;24599.0;Computational biology and chemistry;Data fusion-based algorithm for predicting mirna–disease associations;2831.0;1376.0;189.0;543.0;10036.0;journal;article;2020
The main problem of ecological data modeling is their interpretation and its correct understanding. This problem cannot be solved solely by a big data collection. To sufficiently understand ecosystems we need to know how these processes behave and how they respond to internal and external factors. Similarly, we need to know the behavior of processes that are involved in the climate system and the biosphere of the earth. In order to characterize precisely the behavior of individual elements and ecosystems we need to use deterministic, stochastic and chaotic behavior. Unfortunately, the chaotic part of systems is typically completely ignored in almost all approaches. Ignoring of chaotical part leads to many biased outcomes. To overcome this gap we model chaotic system behavior by random iterated function system which provides a generic guideline for such data management. This also allows to replicate a complexity and chaos of ecosystem.;M. Stehlík and J. Dušek and J. Kiseľák;"Ecology, Evolution, Behavior and Systematics (Q2); Ecological Modeling (Q3)";172.0;178.0;Netherlands;2004-2020;10.1016/j.ecocom.2015.12.003;0.0017399999999999998;57.0;;1476945X;1476945X;1476945X;1476945X;1.882;Stochasticity, Determinism, Entropy, Chaos, Wetland ecosystem, Kullback–Leibler (KL) divergence;Elsevier;;5435.0;Western Europe;537.0;Q2;20289.0;Ecological complexity;Missing chaos in global climate change data interpreting?;2150.0;395.0;48.0;180.0;2609.0;journal;article;2016
Robotic Process Automation (RPA) has received growing attention within the digital transformation as this cutting-edge technology automates human behavior and promises high potentials. However, the adoption in purchasing and supply management (PSM) is still in its infancy and has hardly been explored, particularly in the public sector. Based on a multiple case study including 19 organizations of the public and private sector, this paper narrows that gap and presents comprehensive insights into potentials, barriers, suitable processes, and best practices and components for RPA implementation. The findings indicate that adoption depends on the organizations’ digital procurement readiness and maturity. Application areas of RPA enlarge with increasing experience and range from transactional and operative tasks within the procure-to-pay process to more strategic use cases in sourcing and supply relationship management. Potentials mainly comprise employee reliefs, cost savings, and increased operational efficiency and quality. We uncover multiple technical, organizational, and environmental barriers related to IT infrastructure and human resources, internal communication, financial resources, top management support, organizational structures, supplier-related issues, and government regulations. Furthermore, our study indicates several differences between the private and public sectors for RPA implementation. We outline implications for the emerging research on RPA and pivotal directions for organizational practice.;Christian Flechsig and Franziska Anslinger and Rainer Lasch;"Marketing (Q1); Strategy and Management (Q1)";101.0;576.0;United Kingdom;2003-2020;10.1016/j.pursup.2021.100718;0.0016;85.0;;14784092;14784092;14784092;14784092;5.500;Robotic process automation, Digital procurement, Implementation, Barriers, Digital readiness, Public sector;Elsevier Ltd.;;8779.0;Western Europe;1708.0;Q1;22972.0;Journal of purchasing and supply management;Robotic process automation in purchasing and supply management: a multiple case study on potentials, barriers, and implementation;2586.0;639.0;34.0;109.0;2985.0;journal;article;2022
"Background
Diabetic foot ulcers (DFUs) are common and disabling, necessitating lengthy hospitalizations. In this study we sought to identify potentially modifiable determinants of high-intensity hospital care use among adults with DFUs.
Methods
Three related case–control studies were conducted using Canada-wide cohorts of adults hospitalized with a DFU from 2011 to 2015. In study 1, cases comprised the top 10% with the highest cumulative 1-year acute care hospital costs; controls were randomly selected from those below the top 10%. Study 2 comprised cases/controls within/below the top 10% for cumulative acute care hospital length of stay (LOS). Study 3 included cases/controls within/below the top 10% for cumulative number of acute care hospitalizations. Using generalized linear models, predictor variables were tested between cases and controls, while adjusting for age and sex.
Results
In study 1, mean acute care costs among 8,971 cases and 3,174 controls were $71,757 and $13,687, respectively. Sepsis conferred the greatest excess cost (mean, $38,790; 95% confidence interval [CI], $34,597 to $43,508), followed by chronic kidney disease (mean, $30,607; 95% CI, $28,389 to $32,825) and major lower limb amputation (mean, $30,884; 95% CI, $28,613 to $33,155). In study 2, mean LOS was higher among 8,477 cases (69 days) than 3,467 controls (12 days). Lower limb amputation conferred the greatest adjusted excess in mean LOS (mean, 28 days; 95% CI, 27 to 28 days). In study 3, there was a mean of 3 hospitalizations among 10,341 cases and 1 among 5,509 controls. Peripheral artery disease conferred the greatest excess number of hospitalizations (1.3 more hospitalizations; 1.2 to 1.4).
Conclusions
Early aggressive treatment of chronic kidney disease and peripheral artery disease, alongside guideline-based amputation prevention strategies, may reduce high-intensity hospital care use among adults with DFUs.
Résumé
Introduction
Les ulcères du pied diabétique (UPD) sont fréquents et handicapants, et nécessitent de longues hospitalisations. Dans la présente étude, nous avons cherché à cerner les facteurs déterminants potentiellement modifiables du recours considérable aux soins hospitaliers chez les adultes atteints d’UPD.
Méthodes
Trois études cas témoins connexes ont été menées auprès de cohortes d’adultes atteints d’UPD et hospitalisés dans l’ensemble du Canada de 2011 à 2015. Dans l’étude 1, les cas représentaient les coûts hospitaliers cumulatifs les plus élevés en soins de courte durée pendant 1 année dans les 10 % supérieurs; les témoins étaient sélectionnés de façon aléatoire parmi ceux en dessous des 10 % supérieurs. L’étude 2 était composée de cas et de témoins dont la durée de séjour (DS) cumulative à l’hôpital en soins de courte durée était dans ou en dessous des 10 % supérieures. L’étude 3 portait sur des cas et des témoins dont le nombre cumulatif d’hospitalisations en soins de courte durée était dans ou en dessous des 10 % supérieurs. À l’aide des modèles linéaires généralisés, nous avons vérifié les variables de prédiction entre les cas et les témoins, tout en les ajustant selon l’âge et le sexe.
Résultats
Dans l’étude 1, les coûts moyens en soins de courte durée des 8971 cas et des 3174 témoins étaient respectivement de 71 757 $ et 13 687 $. La sepsie générait les coûts excédentaires les plus importants (moyenne, 38 790 $; intervalle de confiance [IC] à 95 %, de 34 597 $ à 43 508 $), puis suivaient l’insuffisance rénale chronique (moyenne, 30 607 $; IC à 95 %, de 28 389 $ à 32 825 $) et l’amputation majeure d’un membre inférieur (moyenne, 30 884 $; IC à 95 %, de 28 613 $ à 33 155 $). Dans l’étude 2, la DS moyenne des 8477 cas (69 jours) était plus élevée que chez les 3467 témoins (12 jours). L’amputation d’un membre inférieur générait l’allongement de la DS moyenne ajustée le plus important (moyenne, 28 jours; IC à 95 %, de 27 à 28 jours). Dans l’étude 3, il y avait une moyenne de 3 hospitalisations chez les 10 341 cas et 1 chez les 5509 témoins. La maladie artérielle périphérique générait le nombre excédentaire d’hospitalisations le plus important (1,3 hospitalisation excédentaire; de 1,2 à 1,4).
Conclusions
Le traitement vigoureux précoce de l’insuffisance rénale chronique et de la maladie artérielle périphérique, et les stratégies de prévention de l’amputation fondées sur les lignes directrices peuvent faire diminuer le recours considérable aux soins hospitaliers chez les adultes atteints d’UPD.";Muzammil H. Syed and Mohammed Al-Omran and Joel G. Ray and Muhammad Mamdani and Charles {de Mestral};"Endocrinology (Q2); Endocrinology, Diabetes and Metabolism (Q2); Internal Medicine (Q2); Medicine (miscellaneous) (Q2)";315.0;254.0;United States;2002-2020;10.1016/j.jcjd.2021.10.005;0.00466;37.0;;14992671;14992671;14992671;14992671;4.190;Canada, diabetic foot ulcers, epidemiology, health-care burden, population-based, Canada, ulcères du pied diabétique, épidémiologie, fardeau des soins de santé, populationnelle;Elsevier Inc.;;3629.0;Northern America;900.0;Q2;25440.0;Canadian journal of diabetes;High-intensity hospital utilization among adults with diabetic foot ulcers: a population-based study;2554.0;905.0;146.0;352.0;5299.0;journal;article;2022
Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.;James C.L. Chow;"Oncology (Q3); Radiology, Nuclear Medicine and Imaging (Q3); Cancer Research (Q4)";248.0;131.0;Poland;1998-2020;10.1016/j.rpor.2017.08.005;;23.0;;15071367;15071367;15071367;15071367;;Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data;Elsevier Sp. z o.o.;;3138.0;Eastern Europe;367.0;Q3;54509.0;Reports of practical oncology and radiotherapy;Internet-based computer technology on radiotherapy;;338.0;166.0;252.0;5209.0;journal;article;2017
Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.;Suchitra Kataria and Vinod Ravindran;"Medicine (miscellaneous) (Q1); Rheumatology (Q1)";185.0;348.0;United Kingdom;1995, 1999-2020;10.1016/j.berh.2019.101429;;100.0;;15216942;15216942;15321770;15216942;;Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health;Bailliere Tindall Ltd;;8121.0;Western Europe;1449.0;Q1;19160.0;Best practice and research in clinical rheumatology;Emerging role of ehealth in the identification of very early inflammatory rheumatic diseases;;872.0;70.0;203.0;5685.0;journal;article;2019
Background: The use of electronic health record (EHR) systems encourages and facilitates the use of data for the development and surveillance of quality indicators, including pain management. Aim: to conduct an integrative review on pain management research using data extracted from EHR in order to synthesize and analyze the following elements: pain management (assessments, interventions, and outcomes) and study results with potential clinical implications, data source, clinical sample characteristics, and method description. Design: An integrative review of the literature was undertaken to identify exemplars of scientific research studies that explore pain management using data from EHR, using Cooper’s framework. Results: Our search of 1,061 records from PubMed, Scopus, and Cinahl was narrowed down to 28 eligible articles to be analyzed. Conclusion: Results of this integrative review will make a critical contribution, assisting others in developing research proposals and sound research methods, as well as providing an overview of such studies over the past 10 years. Through this review it is therefore possible to guide new research on clinical pain management using EHR.;Aline Tsuma Gaedke Nomura and Lisiane Pruinelli and Luciana Nabinger Menna Barreto and Murilo dos Santos Graeff and Elizabeth A. Swanson and Thamiris Silveira and Miriam de Abreu Almeida;Advanced and Specialized Nursing (Q1);197.0;181.0;United Kingdom;2000-2020;10.1016/j.pmn.2021.01.016;0.0017699999999999999;48.0;;15249042;15249042;15328635;15249042;1.929;;W.B. Saunders Ltd;;3895.0;Western Europe;557.0;Q1;21937.0;Pain management nursing;Pain management in clinical practice research using electronic health records;1835.0;399.0;117.0;216.0;4557.0;journal;article;2021
;;"Molecular Medicine (Q1); Pathology and Forensic Medicine (Q1)";269.0;395.0;United States;1999-2020;10.1016/S1525-1578(20)30513-4;0.00965;95.0;;15251578;15251578;19437811;15251578;5.568;;Elsevier Inc.;;3540.0;Northern America;2420.0;Q1;15983.0;Journal of molecular diagnostics;Association for molecular pathology 2020 annual meeting abstracts;5407.0;1431.0;144.0;283.0;5097.0;journal;article;2020
;Güneş Koru;"Geriatrics and Gerontology (Q1); Health Policy (Q1); Medicine (miscellaneous) (Q1); Nursing (miscellaneous) (Q1)";612.0;355.0;United States;2001-2020;10.1016/j.jamda.2021.03.028;0.01873;91.0;;15258610;15258610;15389375;15258610;4.669;;Elsevier Inc.;;3227.0;Northern America;1840.0;Q1;12066.0;Journal of the american medical directors association;Bringing quality health care home via technology innovations;13307.0;3536.0;482.0;853.0;15552.0;journal;article;2021
Healthcare is a highly regulated domain. Seamless, online access to integrated electronic health records for citizens is still far from becoming a reality. The implementation of personally managed health data systems still needs to overcome several interoperability, usability, ethics, security, and regulatory issues to deliver the envisioned benefits. This paper offers a policy viewpoint on how the new European Interoperability Framework (EIF) may benefit the implementation of eHealth systems for the management of personal health information for citizens. Interoperability facilitates sharing of health and illness experiences, coordinated care and research for citizen empowerment and improved health outcomes. The adoption of principles relevant to core interoperability and generic user needs and expectations, as described in the new EIF, in line with European and national regulations are quite essential for the development of safe and secure patient access services to support mobility. An interoperability framework facilitates the creation of the appropriate context in which personal health record applications can be designed and implemented in support of disease specific solutions, such as chronic non-malignant pain, diabetes and cancer. It is evident that no solution will fit all circumstances. However, the new EIF, when adapted for personally managed health data, provides a useful and relevant framework to facilitate implementation and adoption of personal health record systems within a coordinated care environment. Practical implications of this work relate to the need of multi-disciplinary cooperation and European level compatibility and sustainability of the underlying infrastructures required to support reliable and secure access to and sharing of medical data, as well as the readiness to address continuously evolving functional and non-functional requirements for regional, national, and cross-border settings.;Angelina Kouroubali and Dimitrios G. Katehakis;"Computer Science Applications (Q1); Health Informatics (Q1)";527.0;705.0;United States;2001-2020;10.1016/j.jbi.2019.103166;0.01469;103.0;;15320464;15320480;15320464;15320480;6.317;European interoperability framework, Coordinated care, Electronic health record, National infrastructures, Cross-border healthcare, Personal health record;Academic Press Inc.;;4879.0;Northern America;1057.0;Q1;23706.0;Journal of biomedical informatics;The new european interoperability framework as a facilitator of digital transformation for citizen empowerment;12255.0;3884.0;203.0;591.0;9904.0;journal;article;2019
Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.;Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin;Computer Networks and Communications (Q1);140.0;414.0;United States;2001-2020;10.1145/3461839;0.00144;56.0;;15335399;15576051;15335399;15576051;3.135;information diffusion, tensor analysis, popularity prediction, Group level, online social networks, incremental approach;Association for Computing Machinery (ACM);Association for Computing Machinery;5044.0;Northern America;667.0;Q1;15773.0;Acm transactions on internet technology;Incremental group-level popularity prediction in online social networks;940.0;612.0;45.0;149.0;2270.0;journal;article;2021
"Summary
Extreme weather events can cause heat stress that decreases crop production. Recent studies have demonstrated that protein degradation and rRNA homeostasis as well as transcription factors are involved in the thermoresponse in plants. However, how RNA modifications contribute to temperature stress response in plant remains largely unknown. Herein, we identified OsNSUN2 as an RNA 5-methylcytosine (m5C) methyltransferase in rice. osnsun2 mutant displayed severe temperature- and light-dependent lesion-mimic phenotypes and heat-stress hypersensitivity. Heat stress enhanced the OsNSUN2-dependent m5C modification of mRNAs involved in photosynthesis and detoxification systems, such as β-OsLCY, OsHO2, OsPAL1, and OsGLYI4, which increased protein synthesis. Furthermore, the photosystem of osnsun2 mutant was vulnerable to high ambient temperature and failed to undergo repair under tolerable heat stress. Thus, OsNSUN2 mutation reduced photosynthesis efficiency and accumulated excessive reactive oxygen species upon heat treatment. Our findings demonstrate an important mechanism of mRNA m5C-dependent heat acclimation in rice.";Yongyan Tang and Chun-Chun Gao and Ying Gao and Ying Yang and Boyang Shi and Jia-Li Yu and Cong Lyu and Bao-Fa Sun and Hai-Lin Wang and Yunyuan Xu and Yun-Gui Yang and Kang Chong;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Cell Biology (Q1); Developmental Biology (Q1); Molecular Biology (Q1)";896.0;760.0;United States;2001-2020;10.1016/j.devcel.2020.03.009;0.05835;264.0;;15345807;15345807;18781551;15345807;12.270;OsNSUN2, mC, RNA modification, methyltransferase, rice, heat stress, photosynthesis, reactive oxygen species;Cell Press;;5732.0;Northern America;5284.0;Q1;18525.0;Developmental cell;Osnsun2-mediated 5-methylcytosine mrna modification enhances rice adaptation to high temperature;36177.0;7055.0;298.0;918.0;17082.0;journal;article;2020
"Summary
Patterns of genomic evolution between primary and metastatic breast cancer have not been studied in large numbers, despite patients with metastatic breast cancer having dismal survival. We sequenced whole genomes or a panel of 365 genes on 299 samples from 170 patients with locally relapsed or metastatic breast cancer. Several lines of analysis indicate that clones seeding metastasis or relapse disseminate late from primary tumors, but continue to acquire mutations, mostly accessing the same mutational processes active in the primary tumor. Most distant metastases acquired driver mutations not seen in the primary tumor, drawing from a wider repertoire of cancer genes than early drivers. These include a number of clinically actionable alterations and mutations inactivating SWI-SNF and JAK2-STAT3 pathways.";Lucy R. Yates and Stian Knappskog and David Wedge and James H.R. Farmery and Santiago Gonzalez and Inigo Martincorena and Ludmil B. Alexandrov and Peter {Van Loo} and Hans Kristian Haugland and Peer Kaare Lilleng and Gunes Gundem and Moritz Gerstung and Elli Pappaemmanuil and Patrycja Gazinska and Shriram G. Bhosle and David Jones and Keiran Raine and Laura Mudie and Calli Latimer and Elinor Sawyer and Christine Desmedt and Christos Sotiriou and Michael R. Stratton and Anieta M. Sieuwerts and Andy G. Lynch and John W. Martens and Andrea L. Richardson and Andrew Tutt and Per Eystein Lønning and Peter J. Campbell;"Cancer Research (Q1); Cell Biology (Q1); Oncology (Q1)";495.0;1960.0;United States;2002-2020;10.1016/j.ccell.2017.07.005;0.08104;335.0;;15356108;15356108;18783686;15356108;31.743;breast cancer, metastasis, relapse, genomics, somatic mutation;Cell Press;;4722.0;Northern America;13035.0;Q1;29093.0;Cancer cell;Genomic evolution of breast cancer metastasis and relapse;50839.0;10411.0;193.0;496.0;9114.0;journal;article;2017
Genetic and genomic research has greatly advanced our understanding of heart disease. Yet, comprehensive, in-depth, quantitative maps of protein expression in hearts of living humans are still lacking. Using samples obtained during valve replacement surgery in patients with mitral valve prolapse (MVP), we set out to define inter-chamber differences, the intersect of proteomic data with genetic or genomic datasets, and the impact of left atrial dilation on the proteome of patients with no history of atrial fibrillation (AF). We collected biopsies from right atria (RA), left atria (LA) and left ventricle (LV) of seven male patients with mitral valve regurgitation with dilated LA but no history of AF. Biopsy samples were analyzed by high-resolution mass spectrometry (MS), where peptides were pre-fractionated by reverse phase high-pressure liquid chromatography prior to MS measurement on a Q-Exactive-HF Orbitrap instrument. We identified 7,314 proteins based on 130,728 peptides. Results were confirmed in an independent set of biopsies collected from three additional individuals. Comparative analysis against data from post-mortem samples showed enhanced quantitative power and confidence level in samples collected from living hearts. Our analysis, combined with data from genome wide association studies suggested candidate gene associations to MVP, identified higher abundance in ventricle for proteins associated with cardiomyopathies and revealed the dilated LA proteome, demonstrating differential representation of molecules previously associated with AF, in non-AF hearts. This is the largest dataset of cardiac protein expression from human samples collected in vivo. It provides a comprehensive resource that allows insight into molecular fingerprints of MVP and facilitates novel inferences between genomic data and disease mechanisms. We propose that over-representation of proteins in ventricle is consequent not to redundancy but to functional need, and conclude that changes in abundance of proteins known to associate with AF are not sufficient for arrhythmogenesis.;Nora Linscheid and Pi Camilla Poulsen and Ida Dalgaard Pedersen and Emilie Gregers and Jesper Hastrup Svendsen and Morten Salling Olesen and Jesper Velgaard Olsen and Mario Delmar and Alicia Lundby;"Analytical Chemistry (Q1); Biochemistry (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)";564.0;543.0;United States;2002-2020;10.1074/mcp.RA119.001878;;187.0;;15359476;15359476;15359484;15359476;;Cardiovascular disease, cardiovascular function or biology, clinical proteomics, label-free quantification, tandem mass spectrometry, cardiac proteomics, heart physiology, quantitative proteomics;American Society for Biochemistry and Molecular Biology Inc.;;6045.0;Northern America;2757.0;Q1;14151.0;Molecular and cellular proteomics;Quantitative proteomics of human heart samples collected in vivo reveal the remodeled protein landscape of dilated left atrium without atrial fibrillation;;3143.0;156.0;573.0;9430.0;journal;article;2020
Long-term monitoring of animal activity can yield key information for both researchers in ethology and engineers in charge of developing precision livestock farming tools. First, a barn is segmented into delimited areas (e.g. cubicles) with which an activity can be associated (e.g. resting), then a real-time location system (RTLS) can be used to automatically convert cow position into behaviour. Working within the EU-PLF project, we tested a system already able to determine basic activities (resting, moving, eating…) and logged a “big data” set of billions of data points (123 days × 190 cows × 1 location-per-second readings). We then focused on integrating image analysis techniques to help visualise and analyse the dataset, first to validate the data and then to enrich the information extracted. The algorithm developed using freely available tools quickly confirmed the ability of the system to determine cows' main activities (except drinking behaviour), even with 11% of positions missing. The good localisation precision (16 cm) made it possible to enrich the time-budget with new activities such as using brushes and licking mineral blocks. For both activities, using visual observations as gold standard, activity profiles with excellent sensitivity (nearly 80%) were extracted. This validation procedure is both necessary and generalisable to other situations. The improvement of biological information contained in such data holds promise for people designing alarm devices and health and welfare indicators for farmers and/or vets.;Bruno Meunier and Philippe Pradel and Karen H. Sloth and Carole Cirié and Eric Delval and Marie M. Mialon and Isabelle Veissier;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Control and Systems Engineering (Q1); Food Science (Q1); Soil Science (Q1)";581.0;507.0;United States;2002-2020;10.1016/j.biosystemseng.2017.08.019;0.007370000000000001;110.0;;15375110;15375129;15375110;15375129;4.123;RTLS, Image analysis, Dairy cows, Behaviour, Precision livestock farming, Time-budget;Academic Press Inc.;;4150.0;Northern America;894.0;Q1;61490.0;Biosystems engineering;Image analysis to refine measurements of dairy cow behaviour from a real-time location system;9924.0;3137.0;236.0;588.0;9794.0;journal;article;2018
;Jane Englebright and Barbara Caspers;Leadership and Management (Q3);248.0;39.0;United States;2003-2020;10.1016/j.mnl.2016.01.001;;13.0;;15414612;15414612;15414620;15414612;;;Academic Press Inc.;;1104.0;Northern America;264.0;Q3;28813.0;Nurse leader;The role of the chief nurse executive in the big data revolution;;106.0;151.0;293.0;1667.0;journal;article;2016
Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.;Ludwig, Heiko;"Computer Networks and Communications; Computer Science Applications; Hardware and Architecture; Software; Theoretical Computer Science";86.0;77.0;United States;1998, 2014, 2019;10.1109/EDOC.2014.21;;16.0;;15417719;15417719;15417719;15417719;;"Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments";;;2183.0;Northern America;232.0;-;21100384313.0;Proceedings - ieee international enterprise distributed object computing workshop, edocw;Managing big data effectively - a cloud provider and a cloud consumer perspective;;87.0;18.0;95.0;393.0;conference and proceedings;inproceedings;2014
;C.J. Puranik and Sreenivasa Rao and S. Chennamaneni;Ophthalmology (Q1);204.0;381.0;United States;2003-2020;10.1016/j.jtos.2019.07.010;0.006229999999999999;65.0;;15420124;15420124;15420124;15420124;5.033;;Elsevier Inc.;;4913.0;Northern America;3505.0;Q1;130050.0;Ocular surface;The perils and pitfalls of big data analysis in medicine;4145.0;1932.0;120.0;234.0;5895.0;journal;article;2019
"Background & Aims
Population growth and changes in demographic structure are linked to trends in colorectal cancer (CRC) incidence. The aim of this study is to estimate future CRC incidence in the ageing population, and compare trends across developing and developed regions.
Methods
Cancer and population data were extracted from the International Agency for Research on Cancer. Annual incidence rates for the major types of cancer in 118 selected populations were extracted from 102 cancer registries in 39 countries worldwide. We selected 8 jurisdictions (from the United States, Europe, and Asia) that reported 20-year cancer incidence rates since 1988. Time series models were constructed to project cancer incidence, by sex and age, to 2030. Incidence rates for persons older than 65 years were combined and further adjusted for change of ageing population. We compared age-adjusted incidence rates among the jurisdictions.
Results
The total population older than 65 years old was 12,917,794 in 1988, and the number increased by almost 40% to 17,950,115 in 2007. In developed countries in the West CRC incidence is predicted to decrease by 16.3% in the United States, increase by 4.8% in the United Kingdom, and increase by 4.7% in Sweden by 2030. In developing countries, such as China (Shanghai), Croatia, and Costa Rica, CRC incidence is predicted to increase in a steep curve by 2030 because of the growing population and ageing effect; in 2030, the incidence increases were 60.5% for China, 47.0% for Croatia, and 18.5% for Costa Rica. We also predict CRC incidence will increase greatly by 2030 in Japan and Hong Kong, which are developed regions.
Conclusions
With the exception of the United States, the incidence of CRC is expected to continue to rise in most regions in the coming decades, due to population growth and changes in demographic structure. The predicted increases are more marked in developing regions with limited health care resources.";Kelvin K.F. Tsoi and Hoyee W. Hirai and Felix C.H. Chan and Sian Griffiths and Joseph J.Y. Sung;"Gastroenterology (Q1); Hepatology (Q1)";1097.0;393.0;United Kingdom;2003-2020;10.1016/j.cgh.2016.09.155;0.03998;169.0;;15423565;15427714;15423565;15427714;11.382;IARC, Old Age, Colon Cancer, Rectal Cancer, Cross-National Comparison;W.B. Saunders Ltd;;2019.0;Western Europe;2634.0;Q1;28273.0;Clinical gastroenterology and hepatology;Predicted increases in incidence of colorectal cancer in developed and developing regions, in association with ageing populations;24144.0;6091.0;694.0;1524.0;14009.0;journal;article;2017
"Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.";Diakopoulos, Nicholas;Computer Science (miscellaneous) (Q2);117.0;289.0;United States;2003-2020;10.1145/2857274.2886105;;44.0;;15427730;15427730;15427749;15427730;;;Association for Computing Machinery (ACM);Association for Computing Machinery;685.0;Northern America;457.0;Q2;19700186817.0;Queue;Accountability in algorithmic decision-making: a view from computational journalism;;265.0;39.0;131.0;267.0;journal;article;2015
Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.;Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat;"Applied Mathematics (Q2); Biotechnology (Q2); Genetics (Q3)";534.0;303.0;United States;2004-2020;10.1109/TCBB.2019.2937862;;71.0;;15455963;15579964;15455963;15579964;;;Institute of Electrical and Electronics Engineers Inc.;IEEE Computer Society Press;2919.0;Northern America;745.0;Q2;17971.0;Ieee/acm transactions on computational biology and bioinformatics;Use of electronic health data for disease prediction: a comprehensive literature review;;1935.0;273.0;572.0;7970.0;journal;article;2021
;Nadja Kadom and Paul Nagy;Radiology, Nuclear Medicine and Imaging (Q1);1041.0;220.0;Netherlands;2004-2020;10.1016/j.jacr.2015.09.031;0.01381;59.0;;15461440;1558349X;15461440;1558349X;5.532;;Elsevier BV;;2098.0;Western Europe;1022.0;Q1;130052.0;Journal of the american college of radiology;Data drives quality improvement;6584.0;3013.0;397.0;1321.0;8331.0;journal;article;2015
"Taxi service is one of the most important modes for urban transportation. In recent years, many taxi companies have been routinely collecting data to track the movement of each taxi for improving security, coordination, and service performance. This paper is intended to use the GPS vehicle positioning data to assess the route choice behavior of taxi drivers and explore if the routes selected by taxi drivers can be incorporated into a traveler information system. It is often perceived that taxi drivers have the ability to select quality routes assuming that: (1) they tend to be more knowledgeable about alternative routes and time-dependent traffic conditions than general public, including some publicly available route guidance systems due to the nature of their profession; and (2) they are typically more motivated to incorporate their knowledge about traffic conditions into their route choice decisions. An experimental study is conducted to examine the validity of these two assumptions. We have developed a framework that can effectively process the data into information about routes selected by taxi drivers and their associated travel times. The performance of the routes selected by taxi drivers is compared with the performance of those recommended by e-maps. Our results indicate that the routes selected by taxi drivers are generally more efficient than the routes recommended by some major e-maps, suggesting that taxi drivers are more active in selecting routes to avoid congestion.";Zheng Wang and Wei-Hua Lin and Wangtu Xu;"Aerospace Engineering (Q1); Applied Mathematics (Q1); Automotive Engineering (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems (Q1); Software (Q1)";129.0;438.0;United Kingdom;2004-2020;10.1080/15472450.2019.1617142;0.0016899999999999999;47.0;;15472450;15472450;15472450;15472450;4.277;Data driven approach, route choice, taxi service, travel time estimation;Taylor and Francis Ltd.;;4347.0;Western Europe;1321.0;Q1;144889.0;Journal of intelligent transportation systems;A data driven approach to assessing the reliability of using taxicab as probes for real-time route selections;1524.0;632.0;77.0;133.0;3347.0;journal;article;2021
"Summary
In this Minireview, we provide an epidemiologist’s perspective on the debate and recent advances in determining the relationship between diet and cardiovascular health. We conclude that, in order to reduce the global burden of cardiovascular disease, there should be a greater emphasis on improving overall diet quality and food sources of macronutrients, such as dietary fats and carbohydrates. In addition, building a strong evidence base through high-quality intervention and observational studies is crucial for effective policy changes, which can greatly improve the food environment and population health.";An Pan and Xu Lin and Elena Hemler and Frank B. Hu;"Cell Biology (Q1); Molecular Biology (Q1); Physiology (Q1)";661.0;1738.0;United States;2005-2020;10.1016/j.cmet.2018.02.017;0.091;266.0;;15504131;15504131;15504131;15504131;27.287;;Cell Press;;5589.0;Northern America;10326.0;Q1;146172.0;Cell metabolism;Diet and cardiovascular disease: advances and challenges in population-based studies;52192.0;12515.0;215.0;693.0;12016.0;journal;article;2018
Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.;Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta;Computer Networks and Communications (Q2);117.0;352.0;United States;2005-2020;10.1145/3085580;0.00099;67.0;;15504859;15504859;15504859;15504859;2.253;QoS, medical care, Body area networks, cloud computing, healthcare;Association for Computing Machinery (ACM);Association for Computing Machinery;5345.0;Northern America;598.0;Q2;4700152843.0;Acm transactions on sensor networks;Qos in body area networks: a survey;1365.0;447.0;44.0;118.0;2352.0;journal;article;2017
"Background
Perioperative myocardial infarction (PMI) is a feared complication after surgery. Bariatric surgery, due to its intraabdominal nature, is traditionally considered an intermediate risk procedure. However, there are limited data on MI rates and its predictors in patients undergoing bariatric surgery.
Objectives
To enumerate the prevalence of PMI after bariatric surgery and develop a risk assessment tool.
Setting
Bariatric surgery centers, United States.
Methods
Patients undergoing bariatric surgery were identified from the MBSAQIP participant use file (PUF) 2016. Preoperative characteristics, which correlated with PMI were identified by multivariable regression analysis. PUF 2015 was used to validate the scoring tool developed from PUF 2016.
Results
We identified 172,017 patients from PUF 2016. Event rate for MI within 30 days of the operation was .03%; with a mortality rate of 17.3% in patients with a PMI. Four variables correlated with PMI on regression, including history of a previous MI (odds ratio [OR] = 8.57, confidence interval [CI] = 3.4–21.0), preoperative renal insufficiency (OR = 3.83, CI = 1.2–11.4), hyperlipidemia (OR = 2.60, CI = 1.3–5.1), and age >50 (OR = 2.15, CI = 1.1–4.2). Each predicting variable was assigned a score and event rate for MI was assessed with increasing risk score in PUF 2015; the rate increased from 9.5 per 100,000 operations with a score of 0 to 3.2 per 100 with a score of 5.
Conclusion
The prevalence of MI after bariatric surgery is lower than other intraabdominal surgeries. However, mortality with PMI is high. This scoring tool can be used by bariatric surgeons to identify patients who will benefit from focused perioperative cardiac workup.";Amlish Bilal Gondal and Chiu-Hsieh Hsu and Rostam Khoubyari and Iman Ghaderi;Surgery (Q1);780.0;262.0;United States;2005-2020;10.1016/j.soard.2018.12.032;0.01665;86.0;;15507289;15507289;15507289;15507289;4.734;Bariatric surgery, Perioperative myocardial infarction, Mortality, Risk stratification;Elsevier Inc.;;2668.0;Northern America;1733.0;Q1;4100151501.0;Surgery for obesity and related diseases;Development of a bariatric surgery specific risk assessment tool for perioperative myocardial infarction;10541.0;3226.0;360.0;1125.0;9606.0;journal;article;2019
"Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.";Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.;"Hardware and Architecture (Q1); Computer Networks and Communications (Q2)";240.0;398.0;United States;2005-2020;10.1145/2808198;;49.0;;15516857;15516865;15516857;15516865;;recruitment, DTA, deployment, trajectory, Participatory sensing, tensor;Association for Computing Machinery (ACM);Association for Computing Machinery;5326.0;Northern America;558.0;Q1;4700151918.0;Acm transactions on multimedia computing, communications and applications;Launching an efficient participatory sensing campaign: a smart mobile device-based approach;;1130.0;91.0;252.0;4847.0;journal;article;2015
;Song Li and Gavin W. Hickey and Matthew M. Lander and Manreet K. Kanwar;"Cardiology and Cardiovascular Medicine (Q2); Medicine (miscellaneous) (Q2)";165.0;244.0;United States;2005-2020;10.1016/j.hfc.2021.11.005;0.00175;38.0;;15517136;15517136;15517136;15517136;3.179;Artificial intelligence, Machine learning, Mechanical circulatory support, Heart failure;Elsevier Inc.;;5469.0;Northern America;878.0;Q2;4000148704.0;Heart failure clinics;Artificial intelligence and mechanical circulatory support;1324.0;438.0;51.0;188.0;2789.0;journal;article;2022
"Clinical trials have been slow to incorporate e-technology (digital and electronic technology that utilizes mobile devices or the Internet) into the design and execution of studies. In the meantime, individuals and corporations are relying more on electronic platforms and most have incorporated such technology into their daily lives. This paper provides a general overview of the use of e-technologies in clinical trials research, specifically within the last decade, marked by rapid growth of mobile and Internet-based tools. Benefits of and challenges to the use of e-technologies in data collection, recruitment and retention, delivery of interventions, and dissemination are provided, as well as a description of the current status of regulatory oversight of e-technologies in clinical trials research. As an example of ways in which e-technologies can be used for intervention delivery, a summary of e-technologies for treatment of substance use disorders is presented. Using e-technologies to design and implement clinical trials has the potential to reach a wide audience, making trials more efficient while also reducing costs; however, researchers should be cautious when adopting these tools given the many challenges in using new technologies, as well as threats to participant privacy/confidentiality. Challenges of using e-technologies can be overcome with careful planning, useful partnerships, and forethought. The role of web- and smartphone-based applications is expanding, and the increasing use of those platforms by scientists and the public alike make them tools that cannot be ignored.";Carmen Rosa and Aimee N.C. Campbell and Gloria M. Miele and Meg Brunner and Erin L. Winstanley;"Medicine (miscellaneous) (Q1); Pharmacology (medical) (Q2)";521.0;199.0;United States;2005-2020;10.1016/j.cct.2015.07.007;0.01012;60.0;;15517144;15592030;15517144;15592030;2.226;Clinical trials, E-technology, Social media, Apps, Smartphones, Internet;Elsevier Inc.;;5445.0;Northern America;1067.0;Q1;130115.0;Contemporary clinical trials;Using e-technologies in clinical trials;4858.0;1147.0;207.0;529.0;11272.0;journal;article;2015
"Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.";Ingunn Björnsdottir and Guri Birgitte Verne;"Pharmacy (Q1); Pharmaceutical Science (Q2)";404.0;257.0;United States;2005-2020;10.1016/j.sapharm.2018.02.009;;47.0;;15517411;15517411;15517411;15517411;;;Elsevier Inc.;;4248.0;Northern America;710.0;Q1;4700151922.0;Research in social and administrative pharmacy;Exhibiting caution with use of big data: the case of amphetamine in iceland's prescription registry;;1395.0;306.0;514.0;12998.0;journal;article;2018
Cognitive function is an important end point of treatments in dementia clinical trials. Measuring cognitive function by standardized tests, however, is biased toward highly constrained environments (such as hospitals) in selected samples. Patient-powered real-world evidence using information and communication technology devices, including environmental and wearable sensors, may help to overcome these limitations. This position paper describes current and novel information and communication technology devices and algorithms to monitor behavior and function in people with prodromal and manifest stages of dementia continuously, and discusses clinical, technological, ethical, regulatory, and user-centered requirements for collecting real-world evidence in future randomized controlled trials. Challenges of data safety, quality, and privacy and regulatory requirements need to be addressed by future smart sensor technologies. When these requirements are satisfied, these technologies will provide access to truly user relevant outcomes and broader cohorts of participants than currently sampled in clinical trials.;Stefan Teipel and Alexandra König and Jesse Hoey and Jeff Kaye and Frank Krüger and Julie M. Robillard and Thomas Kirste and Claudio Babiloni;"Cellular and Molecular Neuroscience (Q1); Developmental Neuroscience (Q1); Epidemiology (Q1); Geriatrics and Gerontology (Q1); Health Policy (Q1); Neurology (clinical) (Q1); Psychiatry and Mental Health (Q1)";412.0;1368.0;United States;2005-2020;10.1016/j.jalz.2018.05.003;;118.0;;15525260;15525279;15525260;15525279;;;Elsevier Inc.;;5213.0;Northern America;6713.0;Q1;3600148102.0;Alzheimer's and dementia;Use of nonintrusive sensor-based information and communication technology for real-world evidence for clinical trials in dementia;;5971.0;182.0;448.0;9488.0;journal;article;2018
"ABSTRACT
Study Objective
To develop a prototype of a complex gene expression biomarker for the diagnosis of endometriosis on the basis of differences between the molecular signatures of the endometrium from women with and without endometriosis.
Design
Prospective observational cohort study. Evidence obtained from a well-designed, controlled trial without randomization.
Setting
Department of reproductive medicine and surgery, A.I. Evdokimov Moscow State University of Medicine and Dentistry.
Patients
A total of 33 women (aged 32–38 years) were included in this study. Patients with and without endometriosis were divided into 2 separate groups. The group composed of patients with endometriosis included 19 living patients with endometriosis who underwent laparoscopic excision of endometriosis. The control group included 6 living patients who underwent laparoscopic excision of incompetent uterine scar after cesarean section, with both surgically and histologically confirmed absence of endometriosis and adenomyosis. An additional control/verification group included various previously RNA-sequencing–profiled tissue samples (endocervix, ovarian surface epithelium) of 8 randomly selected healthy female cadaveric donors aged 32 to 38 years. The exclusion criteria for all patients were hormone therapy and any intrauterine device use for more than 1 year preceding surgery, as well as absence of other diseases of the uterus, fallopian tubes, and ovaries.
Interventions
Laparoscopic excision of endometriotic foci and hysteroscopy with endometrial sampling were performed. The cadaveric tissue samples included endocervix and ovarian surface epithelium. Endometrial sampling was obtained from the women in the control group. RNA sequencing was performed using Illumina HiSeq 3000 equipment (Illumina, Inc., San Diego, CA) for single-end sequencing. Unique bioinformatics algorithms were developed and validated using experimental and public gene expression datasets.
Measurements and Main Results
We generated a characteristic signature of 5 genes downregulated in the endometrium and endometriotic tissue of the patients with endometriosis, selected after comparison with the endometrium of the women without endometriosis. This gene signature showed a capacity for nearly perfect separation of all 52 analyzed tissue samples of the patients with endometriosis (endometrial as well as endometriotic samples) from the 14 tissue samples of both living and cadaveric donors without endometriosis (area under the curve = 0.982, Matthews correlation coefficient = 0.832).
Conclusion
The gene signature of the endometrium identified in this study may potentially serve as a nonsurgical diagnostic method for endometriosis detection. Our data also suggest that the statistical method of 5-fold cross-validation of differential gene expression analysis can be used to generate robust gene signatures using real-world clinical data.";Leila Adamyan and Yana Aznaurova and Assia Stepanian and Daniil Nikitin and Andrew Garazha and Maria Suntsova and Maxim Sorokin and Anton Buzdin;Obstetrics and Gynecology (Q2);720.0;179.0;Netherlands;2005-2020;10.1016/j.jmig.2021.03.011;0.00763;79.0;;15534650;15534669;15534650;15534669;4.137;Endometriosis, Molecular diagnostics, RNA sequencing, Gene expression signature, Big data in clinical medicine;Elsevier;;1664.0;Western Europe;929.0;Q2;144915.0;Journal of minimally invasive gynecology;Gene expression signature of endometrial samples from women with and without endometriosis;5911.0;1643.0;431.0;849.0;7172.0;journal;article;2021
"The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.";Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won;Electrical and Electronic Engineering (Q1);360.0;3755.0;United States;2005-2020;10.1109/COMST.2020.2973314;0.03684;197.0;;1553877X;1553877X;1553877X;1553877X;25.249;"Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks";Institute of Electrical and Electronics Engineers Inc.;;19942.0;Northern America;6605.0;Q1;17900156715.0;Ieee communications surveys and tutorials;The future of healthcare internet of things: a survey of emerging technologies;22146.0;15073.0;91.0;368.0;18147.0;journal;article;2020
;Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn;"Medicine (miscellaneous) (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine (Q1)";581.0;604.0;United States;2006-2020;10.1016/j.jtho.2022.02.010;0.04278;133.0;;15560864;15561380;15560864;15561380;15.609;;International Association for the Study of Lung Cancer;;2207.0;Northern America;4539.0;Q1;6400153137.0;Journal of thoracic oncology;Unaccounted confounders limit the ability to draw conclusions from big data analysis comparing radiotherapy fractionation regimens in nsclc;24405.0;6688.0;316.0;1028.0;6973.0;journal;article;2022
;Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline;"Clinical Psychology (Q1); Medicine (miscellaneous) (Q1); Neurology (clinical) (Q2); Neuropsychology and Physiological Psychology (Q2); Psychiatry and Mental Health (Q2)";147.0;271.0;United Kingdom;2006-2020;10.1016/j.jsmc.2016.01.009;;39.0;;1556407X;1556407X;15564088;1556407X;;Sleep-disordered breathing, Big data, Management, Sleep apnea;W.B. Saunders Ltd;;6681.0;Western Europe;989.0;Q1;17500155127.0;Sleep medicine clinics;The role of big data in the management of sleep-disordered breathing;;513.0;58.0;179.0;3875.0;journal;article;2016
The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.;Fugini, Mariagrazia and Finocchi, Jacopo;"Conservation (Q1); Computer Graphics and Computer-Aided Design (Q2); Computer Science Applications (Q3); Information Systems (Q3)";74.0;279.0;United States;2008-2020;10.1145/3461015;;25.0;;15564673;15564711;15564673;15564711;;machine learning, Big Data analytics, unstructured Big Data, data quality, content management, text analytics;Association for Computing Machinery (ACM);Association for Computing Machinery;4817.0;Northern America;371.0;Q1;19400157014.0;Journal on computing and cultural heritage;Data and process quality evaluation in a textual big data archiving system;;224.0;36.0;79.0;1734.0;journal;article;2022
Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.;Barlaug, Nils and Gulla, Jon Atle;Computer Science (miscellaneous) (Q1);168.0;454.0;United States;2007-2020;10.1145/3442200;0.00224;59.0;;15564681;15564681;15564681;15564681;2.713;entity matching, record linkage, entity resolution, data matching, Deep learning;Association for Computing Machinery (ACM);Association for Computing Machinery;5252.0;Northern America;728.0;Q1;5800173377.0;Acm transactions on knowledge discovery from data;Neural networks for entity matching: a survey;1740.0;816.0;71.0;169.0;3729.0;journal;article;2021
"ABSTRACT
Volatile driving, characterized by hard accelerations and braking, can contribute substantially to higher energy consumption, tailpipe emissions, and crash risks. Drivers’ decisions to maintain speed, accelerate, brake rapidly, or jerk their vehicle are largely constrained by their unique regional and metropolitan contexts. These contexts may be characterized by their geography, roadway structure, traffic management, driving population, etc. This study captures how people generally drive in a region using large-scale vehicle trajectory data, implying how energy is consumed and how emissions are produced in regional transportation systems. Specifically, driving performance in four U.S. metropolitan areas (Los Angeles, San Francisco, Sacramento, and Atlanta) is compared, taking advantage of large-scale behavioral data (78.7 million seconds of speed records), collected by in-vehicle global positioning systems (GPSs) as part of regional surveys. Comparative analysis shows significant regional differences in terms of volatile driving and time spent to accelerate, brake, and jerk the vehicle during daily trips. Correlates of higher volatility are also explored, e.g., battery electric vehicles show low volatility, as expected. This study proposes a novel way to compare regional driving performance by successfully turning GPS driving data into valuable knowledge that can be applied in practice by developing regional driving performance indices. The new indices can also be used to compare regional performance over time and to imply the levels of sustainability of regional transportation systems. This study contributes by proposing a way to extract useful information from large-scale driving data.";Jun Liu and Asad Khattak and Xin Wang;"Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)";191.0;417.0;United Kingdom;2007-2020;10.1080/15568318.2016.1230803;0.00292;41.0;;15568318;15568334;15568318;15568334;3.929;Driving indices, driving volatility, large-scale data, metropolitan region, mixed-effects model, sustainability;Taylor and Francis Ltd.;;5593.0;Western Europe;1254.0;Q1;19400158456.0;International journal of sustainable transportation;A comparative study of driving performance in metropolitan regions using large-scale vehicle trajectory data: implications for sustainable cities;2123.0;882.0;139.0;193.0;7774.0;journal;article;2016
;Navid Hasani and Faraz Farhadi and Michael A. Morris and Moozhan Nikpanah and Arman Rahmim and Yanji Xu and Anne Pariser and Michael T. Collins and Ronald M. Summers and Elizabeth Jones and Eliot Siegel and Babak Saboury;"Medicine (miscellaneous) (Q2); Radiation (Q2); Radiology, Nuclear Medicine and Imaging (Q2)";126.0;194.0;United States;2006-2020;10.1016/j.cpet.2021.09.009;;24.0;;15568598;15568598;18799809;15568598;;Artificial intelligence, Rare diseases, Positron emission tomography, Medical imaging;W.B. Saunders Ltd;;5624.0;Northern America;581.0;Q2;17500155129.0;Pet clinics;Artificial intelligence in medical imaging and its impact on the rare disease community: threats, challenges and opportunities;;284.0;59.0;162.0;3318.0;journal;article;2022
Light detection and ranging (LiDAR) provides a 3-D understanding of environment and plays an important role in autonomous driving. To study the influence of 3-D data quality on the environment perception and provide a theoretical basis for optimizing system design, a multi-beam LiDAR perception assessment model has been established to reveal the relationship between data quality and multi-parameters, including system and motion parameters. A novel ground segmentation algorithm was proposed with a combination of the grid elevation and the neighbor relationship, which was used to validate how the data quality influences the results of environment perception. By the way of down-sampling based on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, the experimental results showed that the proposed ground segmentation with combination of grid-elevation and neighbor-relationship (GSCGN) method was superior than other general ground segmentation methods in terms of accuracy and efficiency. It should be noted that the mean vertical angular resolution (MVAR), laser repetition frequency, and beam numbers were the dominant influencing parameters on the point density and the accuracy of ground segmentation. Based on the experimental results, the lower limits of system parameters were determined as 16-beam and 4-kHz repetition frequency, with the acceptable recall of 92.2% for ground and 93.5% for object, the accuracy of 92.9% and the runtime of 0.036 s, which can not only provide a reliable environment perception effect, but also reducing the computational burden to satisfy the real-time autonomous driving. This study offers a meaningful investigation to guide LiDAR system design with balancing the contradiction between the optimized system design and the high-degree environment perception.;Li, Xiaolu and Zhou, Yier and Hua, Baocheng;"Electrical and Electronic Engineering (Q1); Instrumentation (Q1)";1120.0;447.0;United States;1963-2020;10.1109/TIM.2021.3094230;0.013269999999999999;119.0;;15579662;00189456;15579662;00189456;4.016;"Laser radar;Three-dimensional displays;Solid modeling;System analysis and design;Laser modes;Laser beams;Autonomous vehicles;Evaluation indicator;lower limit;multi-beam light detection and ranging (LiDAR);perception assessment model;system design";Institute of Electrical and Electronics Engineers Inc.;;3425.0;Northern America;820.0;Q1;15361.0;Ieee transactions on instrumentation and measurement;Study of a multi-beam lidar perception assessment model for real-time autonomous driving;18199.0;5589.0;962.0;1170.0;32944.0;journal;article;2021
Intelligent Transportation Systems (ITS) is a smart-transportation system for road-side assistance and data exchange support by integrating cloud and wireless networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data exchanges for satisfying user demands. The rate of big data granting to the vehicular users is interrupted by the fundamental attributes such as mobility and link instability of the vehicles. To address the issues in vehicular data exchange big data, this article introduces displacement-aware service endowment scheme with the benefits of data offloading. Displacement-aware big data endowment ensures responsive availability of vehicle request information despite unfavorable location and density factors. The time congruency in V2V and V2X data exchanges are adopted for minimizing data exchange dropouts. In the data offloading phase, extraneous information and big data responses are detained based on data exchange relevance to improve congestion free big data endowment. The distinct methods work in a co-operative manner to improve big data quality of fast configuring smart vehicles to provide reliable big data in smart city environments.;Manogaran, Gunasekaran and Nguyen, Tu N.;"Automotive Engineering (Q1); Computer Science Applications (Q1); Mechanical Engineering (Q1)";1015.0;841.0;United States;2000-2020;10.1109/TITS.2021.3078753;0.02555;153.0;;15580016;15580016;15249050;15580016;6.492;"Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility prediction;time synchronized data exchanges;data offloading;V2X data exchange.";Institute of Electrical and Electronics Engineers Inc.;;3484.0;Northern America;1591.0;Q1;18378.0;Ieee transactions on intelligent transportation systems;Displacement-aware service endowment scheme for improving intelligent transportation systems data exchange;20072.0;9523.0;595.0;1026.0;20731.0;journal;article;2021
The recent advances in Internet of Things (IoT), computational analytics, processing power, and assimilation of Big Data (BD) are playing an important role in revolutionizing maintenance and operations regimes within the wider facilities management (FM) sector. The BD offers the potential for the FM to obtain valuable insights from a large amount of heterogeneous data collected through various sources and IoT allows for the integration of sensors. The aim of this article is to extend the exploratory studies conducted on Big Data analytics (BDA) implementation and empirically test and categorize the associated drivers and challenges. Using exploratory factor analysis (EFA), the researchers aim to bridge the current knowledge gap and highlight the principal factors affecting the BDA implementation. Questionnaires detailing 26 variables are sent to the FM organization in the U.K. who are in the process or have already implemented BDA initiatives within their FM operations. Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that driven by market competition and ambitious sustainability goals, the industry is moving to holistically integrate analytics into its decision making. However, data quality, technological barriers, inadequate preparedness, data management, and governance issues and skill gaps are posing to be significant barriers to the fulfillment of expected opportunities. The findings of this study have important implications for FM businesses that are evaluating the potential of the BDA and IoT applications for their operations. Most importantly, it addresses the role of the BD maturity in FM organizations and its implications for perception of drivers.;Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon;"Electrical and Electronic Engineering (Q1); Strategy and Management (Q2)";212.0;281.0;United States;1969-2020;10.1109/TEM.2019.2959914;0.00209;92.0;;15580040;00189391;15580040;00189391;6.146;"Frequency modulation;Organizations;Big Data;Maintenance engineering;Data mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology implementation";Institute of Electrical and Electronics Engineers Inc.;;2685.0;Northern America;702.0;Q1;17359.0;Ieee transactions on engineering management;Drivers and challenges associated with the implementation of big data within u.k. facilities management sector: an exploratory factor analysis approach;4148.0;668.0;252.0;223.0;6766.0;journal;article;2022
Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are most likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies or, even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique, which is utilized in astronomical imaging. In the proposed method, the source and receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements in data quality compared to the conventional plus–minus (PM) method. In general, this method provides a straightforward strategy to reedit the travel times in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfies the closure-phase conditions.;Yu, Han and Hanafy, Sherif M. and Liu, Lulu;"Earth and Planetary Sciences (miscellaneous) (Q1); Electrical and Electronic Engineering (Q1)";1908.0;687.0;United States;1980-2020;10.1109/TGRS.2022.3169519;0.043789999999999996;254.0;;15580644;15580644;01962892;15580644;5.600;"Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational modeling;Closure phase;first arrivals;interferometry;statics";Institute of Electrical and Electronics Engineers Inc.;;3747.0;Northern America;2141.0;Q1;17360.0;Ieee transactions on geoscience and remote sensing;A weighted closure-phase statics correction method: synthetic and field data examples;48898.0;13865.0;823.0;1908.0;30841.0;journal;article;2022
The heterogeneous network is the foundation of next-generation networks. It aims to explore the existing network resources effectively, and providing better QoS for every kind of traffic flow as far as possible. However, the diversity and dynamic nature of heterogeneous networks will bring a huge burden and big data to the network traffic control. Therefore, how to achieve efficient and intelligent network traffic control becomes the key problem of heterogeneous networks. In this article, an AI-inspired traffic control scheme is proposed. In order to realize fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e., inter-layer, intra-layer, and caching and pushing) network traffic control is introduced. It is worth noting that backpropagation in deep recurrent neural networks is applied in the intra-layer such that an intelligent traffic control scheme can be derived efficiently when facing the huge traffic load in heterogeneous networks. Moreover, DBSCAN is adopted in the inter-layer, which supports efficient classification in the inter-layer. In addition, caching and pushing is adopted to make full use of network resources and provide better QoS. Simulation results demonstrate the effectiveness and practicability of the proposed scheme.;Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li;"Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1)";375.0;1358.0;United States;1986-2020;10.1109/MNET.2018.1800120;0.011890000000000001;129.0;;1558156X;08908044;1558156X;08908044;10.693;"Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked control systems;Recurrent neural networks";Institute of Electrical and Electronics Engineers Inc.;;1340.0;Northern America;2546.0;Q1;27239.0;Ieee network;Artificial intelligence inspired multi-dimensional traffic control for heterogeneous networks;6526.0;4922.0;263.0;401.0;3525.0;journal;article;2018
This article summarizes the key findings of a Canadian Anonymization Network study of several large data custodians who utilize deidentification and similar privacy-enhancing processes prior to engaging in analytics, secondary uses, and disclosure of personal information.;Kardash, Adam and Morin, Suzanne;"Law (Q1); Computer Networks and Communications (Q2); Electrical and Electronic Engineering (Q2)";212.0;312.0;United States;2003-2020;10.1109/MSEC.2021.3126185;;76.0;;15584046;15584046;15407993;15584046;;"Data privacy;Big data;Computer security;Data quality";Institute of Electrical and Electronics Engineers Inc.;;1158.0;Northern America;530.0;Q1;28916.0;Ieee security and privacy;The practices and challenges of generating nonidentifiable data;;678.0;62.0;253.0;718.0;journal;article;2022
Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.;Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.;Computer Networks and Communications (Q2);71.0;446.0;United States;2007-2020;10.1145/3340294;0.0007700000000000001;44.0;;15591131;15591131;15591131;15591131;2.043;service-oriented computing, systematic review, end-user service composition, empirical studies, review framework, qualitative studies, design guideline, User studies, mapshups, web services;Association for Computing Machinery (ACM);Association for Computing Machinery;7272.0;Northern America;438.0;Q2;5800207369.0;Acm transactions on the web;User studies on end-user service composition: a literature review and a design framework;782.0;250.0;18.0;72.0;1309.0;journal;article;2019
Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.;Tong Meng and Xuyang Jing and Zheng Yan and Witold Pedrycz;"Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1)";312.0;1573.0;Netherlands;2000-2021;10.1016/j.inffus.2019.12.001;0.0126;107.0;;15662535;18726305;15662535;18726305;12.975;Data fusion, Machine learning, Fusion methods, Fusion criteria;Elsevier;;8130.0;Western Europe;2776.0;Q1;26099.0;Information fusion;A survey on machine learning for data fusion;9059.0;5599.0;168.0;318.0;13659.0;journal;article;2020
The remarkable upsurge of social media has dramatic impacts on health care research and practice. Social media are reshaping health information management in a variety of ways, ranging from providing cost-effective ways to improve clinician-patient communication and exchange health-related information and experience, to enabling the discovery of new medical knowledge and information. Despite some demonstrated initial success, social media use and analytics for improving health as a research field is still at its infancy. Information systems researchers can potentially play a key role in advancing the field. This study proposes a conceptual framework for social media-based health information management by drawing on multi-disciplinary research. With the guidance of the framework, this paper presents related research challenges, identifies important yet under-explored research issues, and discusses promising directions for future research.;Lina Zhou and Dongsong Zhang and Christopher C. Yang and Yu Wang;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Management of Technology and Innovation (Q1); Marketing (Q1)";188.0;727.0;Netherlands;2002-2020;10.1016/j.elerap.2017.12.003;0.00318;74.0;;15674223;15674223;15674223;15674223;6.014;Conceptual framework, Health information management, Data analytics, Social media;Elsevier;;5570.0;Western Europe;1184.0;Q1;15057.0;Electronic commerce research and applications;Harnessing social media for health information management;4026.0;1347.0;108.0;188.0;6016.0;journal;article;2018
"Background
The potential for global collaborations to better inform public health policy regarding major non-communicable diseases has been successfully demonstrated by several large-scale international consortia. However, the true public health impact of familial hypercholesterolaemia (FH), a common genetic disorder associated with premature cardiovascular disease, is yet to be reliably ascertained using similar approaches. The European Atherosclerosis Society FH Studies Collaboration (EAS FHSC) is a new initiative of international stakeholders which will help establish a global FH registry to generate large-scale, robust data on the burden of FH worldwide.
Methods
The EAS FHSC will maximise the potential exploitation of currently available and future FH data (retrospective and prospective) by bringing together regional/national/international data sources with access to individuals with a clinical and/or genetic diagnosis of heterozygous or homozygous FH. A novel bespoke electronic platform and FH Data Warehouse will be developed to allow secure data sharing, validation, cleaning, pooling, harmonisation and analysis irrespective of the source or format. Standard statistical procedures will allow us to investigate cross-sectional associations, patterns of real-world practice, trends over time, and analyse risk and outcomes (e.g. cardiovascular outcomes, all-cause death), accounting for potential confounders and subgroup effects.
Conclusions
The EAS FHSC represents an excellent opportunity to integrate individual efforts across the world to tackle the global burden of FH. The information garnered from the registry will help reduce gaps in knowledge, inform best practices, assist in clinical trials design, support clinical guidelines and policies development, and ultimately improve the care of FH patients.";Antonio J. Vallejo-Vaz and Asif Akram and Sreenivasa Rao {Kondapally Seshasai} and Della Cole and Gerald F. Watts and G. Kees Hovingh and John J.P. Kastelein and Pedro Mata and Frederick J. Raal and Raul D. Santos and Handrean Soran and Tomas Freiberger and Marianne Abifadel and Carlos A. Aguilar-Salinas and Fahad Alnouri and Rodrigo Alonso and Khalid Al-Rasadi and Maciej Banach and Martin P. Bogsrud and Mafalda Bourbon and Eric Bruckert and Josip Car and Richard Ceska and Pablo Corral and Olivier Descamps and Hans Dieplinger and Can T. Do and Ronen Durst and Marat V. Ezhov and Zlatko Fras and Dan Gaita and Isabel M. Gaspar and Jaques Genest and Mariko Harada-Shiba and Lixin Jiang and Meral Kayikcioglu and Carolyn S.P. Lam and Gustavs Latkovskis and Ulrich Laufs and Evangelos Liberopoulos and Jie Lin and Nan Lin and Vincent Maher and Nelson Majano and A. David Marais and Winfried März and Erkin Mirrakhimov and André R. Miserez and Olena Mitchenko and Hapizah Nawawi and Lennart Nilsson and Børge G. Nordestgaard and György Paragh and Zaneta Petrulioniene and Belma Pojskic and Željko Reiner and Amirhossein Sahebkar and Lourdes E. Santos and Heribert Schunkert and Abdullah Shehab and M. Naceur Slimane and Mario Stoll and Ta-Chen Su and Andrey Susekov and Myra Tilney and Brian Tomlinson and Alexandros D. Tselepis and Branislav Vohnout and Elisabeth Widén and Shizuya Yamashita and Alberico L. Catapano and Kausik K. Ray;"Cardiology and Cardiovascular Medicine (Q2); Internal Medicine (Q2); Medicine (miscellaneous) (Q2)";92.0;124.0;Ireland;2000-2019;10.1016/j.atherosclerosissup.2016.10.001;0.00108;50.0;;15675688;18785050;15675688;18785050;3.235;Familial hypercholesterolaemia, LDL-Cholesterol, Cardiovascular disease, Registry, Study design, Familial Hypercholesterolaemia Studies Collaboration;Elsevier Ireland Ltd;;2545.0;Western Europe;686.0;Q2;22485.0;Atherosclerosis supplements;Pooling and expanding registries of familial hypercholesterolaemia to assess gaps in care and improve disease management and outcomes: rationale and design of the global eas familial hypercholesterolaemia studies collaboration;849.0;195.0;11.0;94.0;280.0;journal;article;2016
In this study, we established a general framework to use PacBio full-length transcriptome sequencing for the investigation of mitochondrial RNAs. As a result, we produced the first full-length human mitochondrial transcriptome using public PacBio data and characterized the human mitochondrial genome with more comprehensive and accurate information. Other results included determination of the H-strand primary transcript, identification of the ND5/ND6AS/tRNAGluAS transcript, discovery of palindrome small RNAs (psRNAs) and construction of the “mitochondrial cleavage” model, etc. These results reported for the first time in this study fundamentally changed annotations of human mitochondrial genome and enriched knowledge in the field of animal mitochondrial studies. The most important finding was two novel long non-coding RNAs (lncRNAs) of MDL1 and MDL1AS exist ubiquitously in animal mitochondrial genomes.;Shan Gao and Xiaoxuan Tian and Hong Chang and Yu Sun and Zhenfeng Wu and Zhi Cheng and Pengzhi Dong and Qiang Zhao and Jishou Ruan and Wenjun Bu;"Cell Biology (Q2); Molecular Biology (Q2); Molecular Medicine (Q2)";270.0;372.0;Netherlands;2001-2020;10.1016/j.mito.2017.08.002;0.00469;88.0;;15677249;15677249;18728278;15677249;4.160;Mitochondrial transcriptome, Full-length transcriptome, lncRNA, psRNA, ltiRNA;Elsevier;;7151.0;Western Europe;1397.0;Q2;20296.0;Mitochondrion;Two novel lncrnas discovered in human mitochondrial dna using pacbio full-length transcriptome data;4982.0;1088.0;122.0;277.0;8724.0;journal;article;2018
Power transformers are an indispensable equipment in power transmission and distribution systems, and failures or hidden defects in power transformers can cause operational and downtime issues in power supply, resulting in economic and resource losses. Therefore, it is highly desirable to put in place intelligent preventive maintenance measures to diagnose and evaluate the condition of power transformers. Although conventional methods have achieved success in detecting problems associated with power transformers, their adoption rate in practical environments is still far from universal. The advent of Computational Intelligence (CI) models offers useful potential to complement the existing diagnostic practices of power transformers. In this paper, we provide a review on various computational intelligence techniques for fault detection and diagnosis pertaining to preventive maintenance of power transformers. An overview of each representative CI approach is presented to facilitate researchers in selecting an appropriate method for a specific problem at hand. We carry out a broad discussion on numerous concerns and challenges that are missing from the current literature, which, nevertheless, need to be addressed seriously. We identify the research gaps in the literature, and suggest the way forward in research that will in the long run enhance power system reliability by embracing CI approaches into business operations in an effort to realize the Sustainable Development Goal (SDGs) advocated by the United Nation, primarily SDG7: Clean and Affordable Energy and SDG9: Industry, Innovation and Infrastructure.;Shen Yuong Wong and Xiaofeng Ye and Fengkai Guo and Hui Hwang Goh;Software (Q1);2003.0;771.0;Netherlands;2001-2020;10.1016/j.asoc.2021.108129;;143.0;;15684946;15684946;15684946;15684946;;Power transformer, Computational Intelligence, Preventive maintenance, Fault detection and diagnosis;Elsevier BV;;5435.0;Western Europe;1290.0;Q1;18136.0;Applied soft computing journal;Computational intelligence for preventive maintenance of power transformers;;15975.0;831.0;2023.0;45163.0;journal;article;2022
Over the past decade, the global proliferation of cyanobacterial harmful algal blooms (CyanoHABs) have presented a major risk to the public and wildlife, and ecosystem and economic services provided by inland water resources. As a consequence, water resources, environmental, and healthcare agencies are in need of early information about the development of these blooms to mitigate or minimize their impact. Results from various components of a novel multi-cloud cyber-infrastructure referred to as “CyanoTRACKER” for initial detection and continuous monitoring of spatio-temporal growth of CyanoHABs is highlighted in this study. The novelty of the CyanoTRACKER framework is the collection and integration of combined community reports (social cloud), remote sensing data (sensor cloud) and digital image analytics (computation cloud) to detect and differentiate between regular algal blooms and CyanoHABs. Individual components of CyanoTRACKER include a reporting website, mobile application (App), remotely deployable solar powered automated hyperspectral sensor (CyanoSense), and a cloud-based satellite data processing and integration tool. All components of CyanoTRACKER provided important data related to CyanoHABs assessments for regional and global water bodies. Reports and data received via social cloud including the mobile App, Twitter, Facebook, and CyanoTRACKER website, helped in identifying the geographic locations of CyanoHABs affected water bodies. A significant increase (124.92%) in tweet numbers related to CyanoHABs was observed between 2011 (total relevant tweets = 2925) and 2015 (total relevant tweets = 6579) that reflected an increasing trend of the harmful phenomena across the globe as well as an increased awareness about CyanoHABs among Twitter users. The CyanoHABs affected water bodies extracted via the social cloud were categorized, and smaller water bodies were selected for the deployment of CyanoSense, and satellite data analysis was performed for larger water bodies. CyanoSense was able to differentiate between ordinary algae and CyanoHABs through the use of their characteristic absorption feature at 620 nm. The results and products from this infrastructure can be rapidly disseminated via the CyanoTRACKER website, social media, and direct communication with appropriate management agencies for issuing warnings and alerting lake managers, stakeholders and ordinary citizens to the dangers posed by these environmentally harmful phenomena.;Deepak R. Mishra and Abhishek Kumar and Lakshmish Ramaswamy and Vinay K. Boddula and Moumita C. Das and Benjamin P. Page and Samuel J. Weber;"Aquatic Science (Q1); Plant Science (Q1)";338.0;413.0;Netherlands;2002-2020;10.1016/j.hal.2020.101828;0.0057399999999999994;91.0;;15689883;15689883;15689883;15689883;4.273;CyanoHABs, Remote sensing, Wireless sensors, Social media, Satellite data, Cyberinfrastructure;Elsevier;;8483.0;Western Europe;1162.0;Q1;19723.0;Harmful algae;Cyanotracker: a cloud-based integrated multi-platform architecture for global observation of cyanobacterial harmful algal blooms;7904.0;1511.0;148.0;339.0;12555.0;journal;article;2020
The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.;Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud;"Immunology (Q1); Immunology and Allergy (Q1)";408.0;766.0;Netherlands;2002-2020;10.1016/j.autrev.2021.102864;0.01481;122.0;;15689972;15689972;18730183;15689972;9.754;Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine;Elsevier;;6852.0;Western Europe;2621.0;Q1;20689.0;Autoimmunity reviews;Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: where do we stand?;13493.0;3509.0;177.0;465.0;12128.0;journal;article;2021
"The goal of Internet of Things (IoT) is to bring any object online, thereby creating a massive volume of data that can overwhelm the existing computing and networking technologies. Therefore, centered cloud isn't ideal for rapidly expanding IoT environmental requirements. Fog computing (FC) moves some portion of the computing load (related to real-time services) from the cloud into edge fog devices. FC is expected to become the subsequent major computing transition and this one has ability to overcome existing cloud limitations. However the key obstacles facing FC are: wide distribution, isolated coupling, quality-of-service (QoS) regulation, adaptability to conditions, and particularly the standardization and normalization is still in phase of development. Software defined networking (SDN) will help fog to solve these obstacles. SDN means unified network control plane (which is separated from data plane), allowing the introduction for advanced traffic control and the orchestration mechanisms of networks and resources. On the grounds of SDN concept, and then combining it with FC, the network type can be modified to resolve all those cloud drawbacks and improve IoT system's QoS. Within this paper, architecture is developed through the combination of independently researched areas of SDN and FC to enhance the QoS in an IoT system. An algorithm (which is dependent on partition the SDN virtually) is presented to support the architecture whose purpose is to select the optimal access point and optimal place to process the data. The main objective of this algorithm is to provide improved QoS by partitioning the corresponding fog devices through the SDN controller. A use case dependent on the presented architecture and algorithm is then provided and assessed this use case's QoS parameter values (network usage, cost, latency and power consumption) using the iFogSim simulator. In contrast to cloud-only deployment, the result indicates a major enhancement of the mentioned QoS parameter values in the deployment of fog with SDN. In addition, once compared to a relative former identical use case; the findings of this paper show improved results for power consumption, network usage and latency. In fact, when compared to a former identical use case, the outcome of this paper shows around 3 times less latency and 2 times less network usage. Finally the ground (IoMT, Industry 4.0, Green IoT, and 5G) that is influenced by this QoS improvement is broadly illustrated in this paper.";Ishtiaq Ahammad and Md. Ashikur Rahman Khan and Zayed Us Salehin;"Hardware and Architecture (Q2); Modeling and Simulation (Q2); Software (Q2)";312.0;402.0;Netherlands;2002-2021;10.1016/j.simpat.2021.102292;0.00315;69.0;;1569190X;1569190X;1569190X;1569190X;3.272;Internet of Things, Software-Defined Networking, Fog Computing, Quality of Service, Modeling and Simulation, iFogSim Simulator;Elsevier;;4398.0;Western Europe;554.0;Q2;12189.0;Simulation modelling practice and theory;Qos performance enhancement policy through combining fog and sdn;3547.0;1227.0;132.0;316.0;5805.0;journal;article;2021
"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.";Yahya Zefri and Imane Sebari and Hicham Hajji and Ghassane Aniba;"Computers in Earth Sciences (Q1); Earth-Surface Processes (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)";520.0;662.0;Netherlands;1998-2020;10.1016/j.jag.2021.102652;0.01275;98.0;;15698432;15698432;15698432;15698432;5.933;Digital photogrammetry, Unmanned Aerial Vehicle, Thermography, Photovoltaics, Big imagery data, Deep learning;Elsevier;;6725.0;Western Europe;1623.0;Q1;39563.0;International journal of applied earth observation and geoinformation;Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big uav imagery data;11556.0;3348.0;16.0;523.0;1076.0;journal;article;2022
The previous research of clinical big data mining showed that stir-baking Semen Cuscuta with salt solution (YP) ranked the first in the usage rate of treating abortion caused by kidney deficiency. At the same time, pharmacodynamic studies also showed that YP has better effect on improving recurrent spontaneous abortion (RSA) compared to raw products of Semen Cuscuta (SP). However, there were few studies on the biomarkers of YP improving RSA. In this study, the chemical and metabonomic profiling were used to screen the quality markers of YP on improving RSA. Firstly, a metabolomics study was carried out to select representative biomarkers of RSA. The ultra-high performance liquid chromatography coupled with electrospray ionization-quadrupole-time of flight-mass spectrometry (UPLC-ESI-Q-TOF-MS) technique was used to investigate the components of exogenous and endogenous in serum of rats after administrated with YP and SP. As a result, 14 differential compounds were identified between the serum of rats administrated SP and YP. Compared to SP, there was an upward trend in YP of the compounds including kaempferol-3-glucuronide, iso-kaempferol-3-glucuronide, (1S) −11-hydroxyhexadecanoic acid and 3-phenylpropionic acid. Meanwhile, there was a reducing trend in YP of the compounds including kaempferol 3-arabinofuranoside, apigenin-3-O-glucoside, hyperoside, caffeic acid-β-D glucoside, dicaffeoylquinic acid, linoleic acid, 3,4-dicaffeoylquinic acid, caffeic acid, palmitic acid and methyl myristate. 12 biomarkers for RSA indication were identified. SP and YP have a certain effect on the endogenous biomarker. The regulation effect of YP was higher than that of SP. The main metabolic pathways included phenylalanine, tyrosine and tryptophan biosynthesis, glycerophospholipid metabolism, fatty acid biosynthesis, sphingolipid metabolism, biosynthesis of unsaturated fatty acids. This study demonstrated a promising way to elucidate the active chemical and endogenous material basis of TCM.;Xiaoli Wang and Haiyan Gao and Song Tan and Chao Xu and Fengqing Xu and Tongsheng Wang and Jijun Chu and Yanquan Han and Deling Wu and Chuanshan Jin;"Analytical Chemistry (Q2); Clinical Biochemistry (Q2); Medicine (miscellaneous) (Q2); Biochemistry (Q3); Cell Biology (Q3)";1525.0;299.0;Netherlands;2002-2020;10.1016/j.jchromb.2021.122727;;149.0;;15700232;1873376X;15700232;1873376X;;, Stir-baking with salt solution, Metabolomics, UHPLC-Q-TOF-MS;Elsevier;;3566.0;Western Europe;729.0;Q2;24172.0;Journal of chromatography b: analytical technologies in the biomedical and life sciences;An integrated approach to uncover quality markers of stir-baking semen cuscuta with salt solution preventing recurrent spontaneous abortion based on chemical and metabolomic profiling;;4732.0;354.0;1537.0;12622.0;journal;article;2021
In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.;Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li;"Chemical Engineering (miscellaneous); Computer Science Applications";0.0;73.0;Netherlands;1997, 2000-2019;10.1016/B978-0-323-85159-6.50280-3;;25.0;;15707946;15707946;15707946;15707946;;Virtual sensor, soft-sensor, big data quality prediction, hashing-based just-in-time modeling;Elsevier;Elsevier;974.0;Western Europe;;-;11200153545.0;Computer aided chemical engineering;Hashing-based just-in-time learning for big data quality prediction;;1080.0;345.0;1662.0;3359.0;book series;incollection;2022
Assessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessment procedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curate the data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas of quality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolution analysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality, and range constraints. For the completeness analysis, we use data profiling information from consecutive knowledge base releases to estimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the results of the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively and qualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach is evaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpedia KB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learning models over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the best performing model in our experimental setup is Random Forest, reaching an F1 score greater than 90% for minimum and maximum cardinality and 84% for range constraints.;Mohammad Rifat Ahmmad Rashid and Giuseppe Rizzo and Marco Torchiano and Nandana Mihindukulasooriya and Oscar Corcho and Raúl García-Castro;"Computer Networks and Communications (Q2); Human-Computer Interaction (Q2); Software (Q2)";79.0;387.0;Netherlands;2003-2020;10.1016/j.websem.2018.11.004;;83.0;;15708268;15708268;15708268;15708268;;Quality assessment, Evolution analysis, Validation, Knowledge base, RDF shape, Machine learning;Elsevier;;4154.0;Western Europe;502.0;Q2;14879.0;Web semantics;Completeness and consistency analysis for evolving knowledge bases;;341.0;26.0;83.0;1080.0;journal;article;2019
With the development of IIoT (Industrial Internet of Things), Artificial Intelligence technology is widely used in many research areas, such as image classification, speech recognition, and information retrieval. Traditional supervised machine learning obtains labels from high-quality oracles, which is high cost and time-consuming and does not consider security. Since multi-label active learning becomes a hot topic, it is more challenging to train efficient and secure classification models, and reduce the label cost in the field of IIoT. To address this issue, this research focuses on the secure multi-label active learning for IIoT using an economical and efficient strategy called crowdsourcing, which involves querying labels from multiple low-cost annotators with various expertise on crowdsourcing platforms rather than relying on a high-quality oracle. To eliminate the effects of annotation noise caused by imperfect annotators, we propose the Multi-label Active Learning from Crowds (MALC) method, which uses a probabilistic model to simultaneously compute the annotation consensus and estimate the classifier’s parameters while also taking instance similarity into account. Then, to actively choose the most informative instances and labels, as well as the most reliable annotators, an instance-label-annotator triplets selection technique is proposed. Experimental results on two real-world data sets show that the performance of MALC is superior to existing methods.;Ming Wu and Qianmu Li and Muhammad Bilal and Xiaolong Xu and Jing Zhang and Jun Hou;"Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)";481.0;542.0;Netherlands;2003-2020;10.1016/j.adhoc.2021.102594;0.00533;94.0;;15708705;15708705;15708705;15708705;4.111;Crowdsourcing, Secure IIoT, Annotation consensus, Multi-label learning, Active learning;Elsevier;;4177.0;Western Europe;781.0;Q1;26799.0;Ad hoc networks;Multi-label active learning from crowds for secure iiot;5667.0;2621.0;171.0;496.0;7142.0;journal;article;2021
Among the reforms to OTC derivative markets since the global financial crisis is a commitment to collateralize counterparty exposures and to clear standardized contracts via central counterparties (CCPs). The reforms aim to reduce interconnectedness and improve counterparty risk management in these important markets. At the same time, however, the reforms necessarily concentrate risk in one or a few nodes in the financial network and also increase institutions’ demand for high-quality assets to meet collateral requirements. This paper looks more closely at the implications of increased CCP clearing for both the topology and stability of the financial network. Building on Heath et al. (2013) and Markose (2012), the analysis supports the view that the concentration of risk in CCPs could generate instability if not appropriately managed. Nevertheless, maintaining CCP prefunded financial resources in accordance with international standards and dispersing any unfunded losses widely through the system can limit the potential for a CCP to transmit stress even in very extreme market conditions. The analysis uses the Bank for International Settlements Macroeconomic Assessment Group on Derivatives (MAGD) data set on the derivatives positions of the 41 largest bank participants in global OTC derivative markets in 2012.;Alexandra Heath and Gerard Kelly and Mark Manning and Sheri Markose and Ali Rais Shaghaghi;"Economics, Econometrics and Finance (miscellaneous) (Q1); Finance (Q1)";211.0;396.0;Netherlands;2004-2020;10.1016/j.jfs.2015.12.004;0.005529999999999999;50.0;;15723089;15723089;15723089;15723089;3.727;OTC derivatives reforms, Central counterparty (CCP), Netting efficiency, Collateralization;Elsevier;;5284.0;Western Europe;2272.0;Q1;144987.0;Journal of financial stability;Ccps and network stability in otc derivatives markets;3013.0;974.0;51.0;219.0;2695.0;journal;article;2016
The promise of technology development in agriculture is well publicised with some claiming that digital disruption will transform the way farming and food production is done in the future. For farm advisers, engaging in smart farming involves managing the proliferation of new forms of information, new knowledge and networks and new technical devices that produce digitised representations of farm performance. The nature and effects of digital practices in particular poses challenges for farm advisers as they seek to understand how digital tools and services can be integrated into their service delivery for improved farm decision making. In this paper we present insights from a co-design process with private farm advisers and ask: What enables farm advisers to engage with digital innovation? And, how can digital innovation be supported and practiced in smart farming contexts? Digital innovation presents challenges for farmers and advisers due to the new relationships, skills, arrangements, techniques and devices required to realise value for farm production and profitability from digital tools and services. We show how a co-design process supported farm advisers to adapt their routine advisory practices through recognising and engaging with the social, material and symbolic practices of digiware in smart farming. We demonstrate the need to recognise ‘digiware as constituted in and by heterogeneous practices from which possibilities for digital innovation emerge. These possibilities include the increased capacity of farm advisers to identify the value proposition of smart farming tools and services for theirs and their clients’ businesses, and the adaptation of advisory services in ways that harnass and mobilise diverse skills, knowledge/s, materials and representations for translating digital data, digital infrastructure and digital capacities into better decisions for farm management.;Margaret Ayre and Vivienne {Mc Collum} and Warwick Waters and Peter Samson and Anthony Curro and Ruth Nettle and Jana-Axinja Paschen and Barbara King and Nicole Reichelt;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Development (Q1); Food Science (Q1); Plant Science (Q1)";85.0;511.0;Netherlands;1988, 1996-2003, 2005-2020;10.1016/j.njas.2019.05.001;;43.0;;15735214;15735214;15735214;15735214;;Digital innovation, Smart farming, Farm advisers, Digiware;Elsevier;;5036.0;Western Europe;1023.0;Q1;130185.0;Njas - wageningen journal of life sciences;Supporting and practising digital innovation with advisers in smart farming;;392.0;25.0;86.0;1259.0;journal;article;2019
Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.;Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras;"Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)";61.0;1249.0;Ireland;2007-2020;10.1016/j.cosrev.2020.100341;0.00195;44.0;;15740137;15740137;15740137;15740137;7.872;Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid;Elsevier Ireland Ltd;;14424.0;Western Europe;1646.0;Q1;8000153138.0;Computer science review;Machine learning and deep learning in smart manufacturing: the smart grid paradigm;1249.0;891.0;54.0;61.0;7789.0;journal;article;2021
We designed and applied interactive visualisation techniques for investigating how social networks are embedded in time and space, using data collected from smartphone logs. Our interest in spatial aspects of social networks is that they may reveal associations between participants missed by simply making contact through smartphone devices. Four linked and co-ordinated views of spatial, temporal, individual and social network aspects of the data, along with demographic and attitudinal variables, helped add context to the behaviours we observed. Using these techniques, we were able to characterise spatial and temporal aspects of participants’ social networks and suggest explanations for some of them. This provides some validation of our techniques. Unexpected deficiencies in the data that became apparent prompted us to evaluate the dataset in more detail. Contrary to what we expected, we found significant gaps in participant records, particularly in terms of location, a poorly connected sample of participants and asymmetries in reciprocal call logs. Although the data captured are of high quality, deficiencies such as these remain and are likely to have a significant impact on interpretations relating to spatial aspects of the social network. We argue that appropriately-designed interactive visualisation techniques–afforded by our flexible prototyping approach–are effective in identifying and characterising data inconsistencies. Such deficiencies are likely to exist in other similar datasets, and although the visual approaches we discuss for identifying data problems may not be scalable, the categories of problems we identify may be used to inform attempts to systematically account for errors in larger smartphone datasets.;Aidan Slingsby and Roger Beecham and Jo Wood;"Computer Networks and Communications (Q1); Computer Science (miscellaneous) (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Applied Mathematics (Q2); Computer Science Applications (Q2)";342.0;467.0;Netherlands;2005-2020;10.1016/j.pmcj.2013.07.002;0.00335;64.0;;15741192;15741192;15741192;15741192;3.453;Big data, Human behaviour, Spatiotemporal, Social networks, Visual analysis;Elsevier;;4059.0;Western Europe;687.0;Q1;3200147819.0;Pervasive and mobile computing;Visual analysis of social networks in space and time using smartphone logs;2540.0;1558.0;68.0;350.0;2760.0;journal;article;2013
Recent advancements in consumer directed personal computing technology have led to the generation of biomedically-relevant data streams with potential health applications. This has catalyzed international interest in Patient Generated Health Data (PGHD), defined as “health-related data – including health history, symptoms, biometric data, treatment history, lifestyle choices, and other information-created, recorded, gathered, or inferred by or from patients or their designees (i.e. care partners or those who assist them) to help address a health concern.”(Shapiro et al., 2012) PGHD offers several opportunities to improve the efficiency and output of clinical trials, particularly within oncology. These range from using PGHD to understand mechanisms of action of therapeutic strategies, to understanding and predicting treatment-related toxicity, to designing interventions to improve adherence and clinical outcomes. To facilitate the optimal use of PGHD, methodological research around considerations related to feasibility, validation, measure selection, and modeling of PGHD streams is needed. With successful integration, PGHD can catalyze the application of “big data” to cancer clinical research, creating both “n of 1” and population-level observations, and generating new insights into the nature of health and disease.;William A. Wood and Antonia V. Bennett and Ethan Basch;"Cancer Research (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Medicine (Q1); Oncology (Q1)";421.0;591.0;Netherlands;2007-2020;10.1016/j.molonc.2014.08.006;0.01225;88.0;;15747891;15747891;18780261;15747891;6.603;Information technology, Patient reported outcomes, Quality of care, Clinical trials;John Wiley and Sons Ltd;;5459.0;Western Europe;2332.0;Q1;5800207508.0;Molecular oncology;Emerging uses of patient generated health data in clinical research;8378.0;2983.0;239.0;428.0;13048.0;journal;article;2015
iEcology is used to supplement traditional ecological data by sourcing large quantities of media from the internet. Images and their metadata are widely available online and can provide information on species occurrence, behaviour and visible traits. However, this data is inherently noisy and data quality varies significantly between sources. Many iEcology studies utilise data from a single source for simplicity and efficiency. Hence, a tool to compare the suitability of different media sources in addressing a particular research question is needed. We provide a simple, novel way to estimate the fraction of images within multiple unverified datasets that potentially depict a specified target fauna. Our method, the Sum of Tag Frequency Differences (STFD), uses any pretrained, general-purpose image classifier. One of the method's innovations is that it does not require training the classifier to recognise the target fauna. Instead, STFD analyses the frequency of the generic text-tags returned by a classifier for multiple datasets and compares them to the corresponding frequencies of an authoritative image dataset that depicts only the target organism. From this comparison, STFD allows us to deduce the fraction of images of the target in unverified datasets. To validate the STFD approach, we processed images from five sources: Flickr, iNaturalist, Instagram, Reddit and Twitter. For each media source, we conducted an STFD analysis of three fauna invasive to Australia: Cane toads (Rhinella marina), German wasps (Vespula germanica), and the higher-level colloquial taxonomic classification, “wild rabbits”. We found the STFD provided an accurate assessment of image source relevance across all data sources and target organisms. This was demonstrated by the consistent, very strong correlation (toads r ≥0.97, wasps r ≥0.95, wild rabbits≥ 0.95) between STFD predictions, and the fraction of target images in a source dataset observed by a human expert. The STFD provides a low-cost, simple and accurate comparison of the relevance of online image sources to specific fauna for iEcology applications. It does not require expertise in machine learning or training neural-network species-specific classifiers. The method enables researchers to assess multiple image sources to select those warranting detailed investigation for the development of tools for web-scraping, citizen science campaigns, further monitoring or analysis.;Hannah M. Burke and Reid Tingley and Alan Dorin;"Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)";271.0;343.0;Netherlands;2006-2020;10.1016/j.ecoinf.2022.101598;0.0033200000000000005;55.0;;15749541;15749541;15749541;15749541;3.142;Biodiversity monitoring, Computer vision, Data mining, iEcology, Social media;Elsevier;;5127.0;Western Europe;774.0;Q1;3100147401.0;Ecological informatics;Tag frequency difference: rapid estimation of image set relevance for species occurrence data using general-purpose image classifiers;2893.0;964.0;109.0;273.0;5588.0;journal;article;2022
This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.;Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang;"Computer Science Applications (Q2); Hardware and Architecture (Q2); Management Science and Operations Research (Q3)";276.0;287.0;United Kingdom;1997-2020;10.1007/s00779-019-01217-0;0.00196;88.0;;16174909;16174909;16174917;16174909;3.006;Collection cost, Emotional recognition, Edge devices, Data acquisition, Location, Data collection;Springer London;Springer-Verlag;4113.0;Western Europe;416.0;Q2;22315.0;Personal and ubiquitous computing;Data collection scheme with minimum cost and location of emotional recognition edge devices;2743.0;891.0;160.0;297.0;6581.0;journal;article;2019
The equity of urban park access has received great attention from studies on public service provision. However, individuals’ growing demands for recreational activities have brought diversity and complexity to park usages, drawing doubts on traditional measurements of park accessibility. To fill the gap, this study explores park equity issues with a dataset containing 12.03 million mobile phone users who accessed one of the 332 parks in Shanghai. We measured community-level park accessibility with two traditional place-based indicators – park area proportion and Gaussian-based 2SFCA accessibility, and three innovative activity-based indicators – park activity frequency, park activity trip length, and park activity duration. We then explored the geographic and social inequity by calculating Gini index and conducting correlation analysis. The results show that place-based and activity-based indicators presented citywide differences, indicating a significant impact of human activities on urban park accessibility. The geographic inequality of park distribution was undermined by people’s actual park usages. However, residents in communities with higher quality of built-environment had higher park activity frequency while shorter trip length, and social inequity of park access among the total population was more obvious than the low-recreation-demand population. Therefore, policy-makers should rethink how to provide park resources to address the inequity issues brought by human activities. Our study contributes to the existing literature in the following ways: (1) compared place-based park accessibility and activity-based park accessibility in the same context, and (2) identified low-recreation-demand population as a comparison group to explore impacts of recreation demand on park equity.;Xiyuan Ren and ChengHe Guan;"Ecology (Q1); Forestry (Q1); Soil Science (Q1)";651.0;463.0;Germany;2002-2020;10.1016/j.ufug.2022.127709;;74.0;;16188667;16188667;16188667;16188667;;Geographic and social inequity, Urban park, Mobile phone data, Recreational activity, Low-recreation-demand population;Urban und Fischer Verlag GmbH und Co. KG;;6431.0;Western Europe;1163.0;Q1;145301.0;Urban forestry and urban greening;Evaluating geographic and social inequity of urban parks in shanghai through mobile phone-derived human activities;;3276.0;280.0;658.0;18007.0;journal;article;2022
In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.;Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz;"Materials Science (miscellaneous) (Q2); Mechanics of Materials (Q2)";252.0;242.0;France;2002-2019;10.1016/j.crme.2019.11.009;;53.0;;16310721;18737234;16310721;18737234;;Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data;Academie des sciences;;3294.0;Western Europe;454.0;Q2;19731.0;Comptes rendus - mecanique;Data-driven modeling and learning in science and engineering;;580.0;34.0;268.0;1120.0;journal;article;2019
Population dynamics and health risk factors keep changing in the KSA, requiring continuous research and quality data. We aimed to review the current status of population health data, outline the available opportunities for data utilization, and provide recommendations for population data-related improvement initiatives. We provide practical solutions to support the collection, linkage, quality assurance, and governance of population health data.;Saleh A. Alessy and Maha Alattas and Mahmoud A. Mahmoud and Ali Alqarni and Suliman Alghnam;Medicine (miscellaneous) (Q3);253.0;120.0;Netherlands;2009-2020;10.1016/j.jtumed.2022.06.011;;19.0;;16583612;16583612;16583612;16583612;;Data, Epidemiology, Health research, KSA, Population health;Elsevier BV;;2739.0;Western Europe;269.0;Q3;19700188101.0;Journal of taibah university medical sciences;Population health data in ksa: status, challenges, and opportunities;;362.0;93.0;262.0;2547.0;journal;article;2022
"Introduction and Objectives
Viral hepatitis is an endemic and epidemic disease of relevance in public health. This study estimated the frequency of viral hepatitis by occupational and non-occupational infections and analyzed the factors associated with case notifications in Brazil from 2007 to 2014.
Material and methods
This was an exploratory epidemiological study using the Notifiable Diseases Information System database. Descriptive and multivariate analyses were performed.
Results
The frequency of viral hepatitis by occupational infections was 0.7%, of which 1.3% were due to hepatitis A virus (HAV), 45.1% hepatitis B virus (HBV), and 45.3% hepatitis C virus (HCV). There was a significant association of the disease with female sex [AOR=1.31; P=0.048], schooling [AOR=1.71; P<0.001], occupation [AOR=2.74; P<0.001], previous contact with an HBV or HCV-infected patient [AOR=5.77; P<0.001], exposure to accidents with biological materials [AOR=99.82; P<0.001], and hepatitis B vaccination [AOR=0.73; P=0.033].
Conclusion
While there was a low frequency of viral hepatitis by occupational infections in Brazil from 2007 to 2014, these findings might be underreported and have been associated with individual and occupational characteristics. This reinforces the need for the adoption of prevention strategies in the workplace and for completeness of case notifications.";Técia Maria S.C. Cordeiro and Raymundo P. {Ferreira Filho} and Argemiro {D’Oliveira Júnior};"Medicine (miscellaneous) (Q2); Hepatology (Q3)";378.0;194.0;Mexico;2002-2020;10.1016/j.aohep.2019.03.009;0.0033299999999999996;53.0;;16652681;16652681;16652681;16652681;2.400;Communicable diseases, Epidemiology, Disease notification, Work;Mexican Association of Hepatology;;3965.0;Latin America;705.0;Q2;38551.0;Annals of hepatology;Factors associated with occupational and non-occupational viral hepatitis infections in brazil between 2007–2014;2610.0;912.0;111.0;423.0;4401.0;journal;article;2019
Esophageal squamous cell carcinoma (ESCC) is a major histological subtype of esophageal cancer with a poor prognosis. Although several serum metabolomic investigations have been reported, ESCC tumor-associated metabolic alterations and predictive biomarkers in sera have not been defined. Here, we enrolled 34 treatment-naive patients with ESCC and collected their pre- and post-esophagectomy sera together with the sera from 34 healthy volunteers for a metabolomic survey. Our comprehensive analysis identified ESCC tumor-associated metabolic alterations as represented by a panel of 12 serum metabolites. Notably, postoperative abrosia and parenteral nutrition substantially perturbed the serum metabolome. Furthermore, we performed an examination using sera from carcinogen-induced mice at the dysplasia and ESCC stages and identified three ESCC tumor-associated metabolites conserved between mice and humans. Notably, among these metabolites, the level of pipecolic acid was observed to be progressively increasing in mouse sera from dysplasia to cancerization, and it could be used to accurately discriminate between mice at the dysplasia stage and healthy control mice. Furthermore, this metabolite is essential for ESCC cells to restrain oxidative stress-induced DNA damage and cell proliferation arrest. Together, this study revealed a panel of 12 ESCC tumor-associated serum metabolites with potential for monitoring therapeutic efficacy and disease relapse, presented evidence for refining parenteral nutrition composition, and highlighted serum pipecolic acid as an attractive biomarker for predicting ESCC tumorigenesis.;Lei Liu and Jia Wu and Minxin Shi and Fengying Wang and Haimin Lu and Jibing Liu and Weiqin Chen and Guanzhen Yu and Dan Liu and Jing Yang and Qin Luo and Yan Ni and Xing Jin and Xiaoxia Jin and Wen-Lian Chen;"Biochemistry (Q1); Computational Mathematics (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)";135.0;641.0;China;2003-2020;10.1016/j.gpb.2021.08.016;;49.0;;16720229;16720229;16720229;16720229;;Esophageal squamous cell carcinoma, Serum metabolome, Esophagectomy, Predictive potential, Pipecolic acid;Beijing Genomics Institute;;5206.0;Asiatic Region;3114.0;Q1;89440.0;Genomics, proteomics and bioinformatics;New metabolic alterations and predictive marker pipecolic acid in sera for esophageal squamous cell carcinoma;;1152.0;36.0;158.0;1874.0;journal;article;2022
How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.;Lin, Mengting and Zhao, Youping;"Computer Networks and Communications (Q2); Electrical and Electronic Engineering (Q2)";607.0;353.0;China;2008-2020;10.23919/JCC.2020.03.006;0.00373;42.0;;16735447;16735447;16735447;16735447;2.688;"Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management";China Institute of Communication;;2922.0;Asiatic Region;508.0;Q2;19700177325.0;China communications;Artificial intelligence-empowered resource management for future wireless communications: a survey;2891.0;2012.0;227.0;629.0;6632.0;journal;article;2020
Translational regulation, especially tissue- or cell type-specific gene regulation, plays essential roles in plant growth and development. Thermo-sensitive genic male sterile (TGMS) lines have been widely used for hybrid breeding in rice (Oryza sativa). However, little is known about translational regulation during reproductive stage in TGMS rice. Here, we use translating ribosome affinity purification (TRAP) combined with RNA sequencing to investigate the reproductive tissue-specific translatome of TGMS rice expressing FLAG-tagged ribosomal protein L18 (RPL18) from the germline-specific promoter MEIOSIS ARRESTED AT LEPTOTENE1 (MEL1). Differentially expressed genes at the transcriptional and translational levels are enriched in pollen and anther-related formation and development processes. These contain a number of genes reported to be involved in tapetum programmed cell death (PCD) and lipid metabolism during pollen development and anther dehiscence in rice, including several encoding transcription factors and key enzymes, as well as several long non-coding RNAs (lncRNAs) that potentially affect tapetum and pollen-related genes in male sterility. This study represents the comprehensive reproductive tissue-specific characterization of the translatome in TGMS rice. These results contribute to our understanding of the molecular basis of sterility in TGMS rice and will facilitate further genetic manipulation of TGMS rice in two-line breeding systems.;Wei Liu and Jing Sun and Ji Li and Chunyan Liu and Fuyan Si and Bin Yan and Zhen Wang and Xianwei Song and Yuanzhu Yang and Yuxian Zhu and Xiaofeng Cao;"Genetics (Q2); Molecular Biology (Q2)";143.0;226.0;China;2007-2020;10.1016/j.jgg.2022.01.002;0.0037600000000000003;58.0;;16738527;16738527;16738527;16738527;4.275;TGMS rice, Translatome, MEL1, Reproductive tissue, Translating ribosome affinity purification (TRAP), Fertility alternation;Institute of Genetics and Developmental Biology;;5874.0;Asiatic Region;1391.0;Q2;5300152707.0;Journal of genetics and genomics;Reproductive tissue-specific translatome of a rice thermo-sensitive genic male sterile line;2754.0;710.0;73.0;248.0;4288.0;journal;article;2022
Phenotyping has become the rate-limiting step in using large-scale genomic data to understand and improve agricultural crops. Here, the Bellwether Phenotyping Platform for controlled-environment plant growth and automated multimodal phenotyping is described. The system has capacity for 1140 plants, which pass daily through stations to record fluorescence, near-infrared, and visible images. Plant Computer Vision (PlantCV) was developed as open-source, hardware platform-independent software for quantitative image analysis. In a 4-week experiment, wild Setaria viridis and domesticated Setaria italica had fundamentally different temporal responses to water availability. While both lines produced similar levels of biomass under limited water conditions, Setaria viridis maintained the same water-use efficiency under water replete conditions, while Setaria italica shifted to less efficient growth. Overall, the Bellwether Phenotyping Platform and PlantCV software detected significant effects of genotype and environment on height, biomass, water-use efficiency, color, plant architecture, and tissue water status traits. All ∼79 000 images acquired during the course of the experiment are publicly available.;Noah Fahlgren and Maximilian Feldman and Malia A. Gehan and Melinda S. Wilson and Christine Shyu and Douglas W. Bryant and Steven T. Hill and Colton J. McEntee and Sankalpi N. Warnasooriya and Indrajit Kumar and Tracy Ficor and Stephanie Turnipseed and Kerrigan B. Gilbert and Thomas P. Brutnell and James C. Carrington and Todd C. Mockler and Ivan Baxter;"Molecular Biology (Q1); Plant Science (Q1)";380.0;960.0;United States;2008-2020;10.1016/j.molp.2015.06.005;0.026860000000000002;115.0;;16742052;16742052;17529867;16742052;13.164;abiotic/environmental stress, water relations, bioinformatics, development, phenotyping;Cell Press;;5799.0;Northern America;4588.0;Q1;17600155011.0;Molecular plant;A versatile phenotyping system and analytics platform reveals diverse temporal responses to water availability in setaria;15778.0;4792.0;153.0;474.0;8872.0;journal;article;2015
This study integrates different machine learning (ML) methods and 5-fold cross-validation (CV) method to estimate the ground maximal surface settlement (MSS) induced by tunneling. We further investigate the applicability of artificial intelligent (AI) based prediction through a comparative study of two tunnelling datasets with different sizes and features. Four different ML approaches, including support vector machine (SVM), random forest (RF), back-propagation neural network (BPNN), and deep neural network (DNN), are utilized. Two techniques, i.e. particle swarm optimization (PSO) and grid search (GS) methods, are adopted for hyperparameter optimization. To assess the reliability and efficiency of the predictions, three performance evaluation indicators, including the mean absolute error (MAE), root mean square error (RMSE), and Pearson correlation coefficient (R), are calculated. Our results indicate that proposed models can accurately and efficiently predict the settlement, while the RF model outperforms the other three methods on both datasets. The difference in model performance on two datasets (Datasets A and B) reveals the importance of data quality and quantity. Sensitivity analysis indicates that Dataset A is more significantly affected by geological conditions, while geometric characteristics play a more dominant role on Dataset B.;Libin Tang and SeonHong Na;Geotechnical Engineering and Engineering Geology (Q1);298.0;462.0;China;2013-2020;10.1016/j.jrmge.2021.08.006;0.00573;46.0;;16747755;16747755;16747755;16747755;4.338;Surface settlement, Tunnel construction, Machine learning (ML), Hyperparameter optimization, Cross-validation (CV);Chinese Academy of Sciences;;5061.0;Asiatic Region;1470.0;Q1;21100381006.0;Journal of rock mechanics and geotechnical engineering;Comparison of machine learning methods for ground settlement prediction with different tunneling datasets;3915.0;1614.0;115.0;306.0;5820.0;journal;article;2021
In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.;Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a};"Computer Networks and Communications (Q4); Electrical and Electronic Engineering (Q4); Signal Processing (Q4)";124.0;82.0;China;2015-2020;10.1016/j.jnlest.2020.100059;;5.0;;1674862X;1674862X;1674862X;1674862X;;Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home;University of Electronic Science and Technology of China;;2597.0;Asiatic Region;132.0;Q4;21100432792.0;Journal of electronic science and technology;Impact of coronavirus pandemic crisis on technologies and cloud computing applications;;74.0;37.0;134.0;961.0;journal;article;2021
The identification of landslide-prone areas is an essential step in landslide hazard assessment and mitigation of landslide-related losses. In this study, we applied two novel deep learning algorithms, the recurrent neural network (RNN) and convolutional neural network (CNN), for national-scale landslide susceptibility mapping of Iran. We prepared a dataset comprising 4069 historical landslide locations and 11 conditioning factors (altitude, slope degree, profile curvature, distance to river, aspect, plan curvature, distance to road, distance to fault, rainfall, geology and land-sue) to construct a geospatial database and divided the data into the training and the testing dataset. We then developed RNN and CNN algorithms to generate landslide susceptibility maps of Iran using the training dataset. We calculated the receiver operating characteristic (ROC) curve and used the area under the curve (AUC) for the quantitative evaluation of the landslide susceptibility maps using the testing dataset. Better performance in both the training and testing phases was provided by the RNN algorithm (AUC ​= ​0.88) than by the CNN algorithm (AUC ​= ​0.85). Finally, we calculated areas of susceptibility for each province and found that 6% and 14% of the land area of Iran is very highly and highly susceptible to future landslide events, respectively, with the highest susceptibility in Chaharmahal and Bakhtiari Province (33.8%). About 31% of cities of Iran are located in areas with high and very high landslide susceptibility. The results of the present study will be useful for the development of landslide hazard mitigation strategies.;Phuong Thao {Thi Ngo} and Mahdi Panahi and Khabat Khosravi and Omid Ghorbanzadeh and Narges Kariminejad and Artemi Cerda and Saro Lee;Earth and Planetary Sciences (miscellaneous) (Q1);382.0;609.0;China;2010-2021;10.1016/j.gsf.2020.06.013;0.00927;57.0;;16749871;16749871;16749871;16749871;6.853;CNN, RNN, Deep learning, Landslide, Iran;China University of Geosciences (Beijing) and Peking University;;9211.0;Asiatic Region;1842.0;Q1;19700182749.0;Geoscience frontiers;Evaluation of deep learning algorithms for national scale landslide susceptibility mapping of iran;5390.0;2198.0;165.0;400.0;15198.0;journal;article;2021
To enhance the credibility of human reliability analysis, various kinds of data have been recently collected and analyzed. Although it is obvious that the quality of data is critical, the practices or considerations for securing data quality have not been sufficiently discussed. In this work, based on the experience of the recent human reliability data extraction projects, which produced more than fifty thousand data-points, we derive a number of issues to be considered for generating meaningful data. As a result, thirteen considerations are presented here as pertaining to the four different data extraction activities: preparation, collection, analysis, and application. Although the lessons were acquired from a single kind of data collection framework, it is believed that these results will guide researchers to consider important issues in the process of extracting data.;Yochan Kim;Nuclear Energy and Engineering (Q2);602.0;240.0;South Korea;2008-2020;10.1016/j.net.2020.01.034;0.00525;40.0;;17385733;2234358X;17385733;2234358X;2.341;Data analytics, Human reliability analysis, HuREX framework, Lesson learned, Simulation data;Korean Nuclear Society;;2662.0;Asiatic Region;737.0;Q2;11700154337.0;Nuclear engineering and technology;Considerations for generating meaningful hra data: lessons learned from hurex data collection;3095.0;1465.0;404.0;604.0;10754.0;journal;article;2020
In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.;Lars Richter and Gerhard F. Ecker;"Biotechnology (Q1); Drug Discovery (Q1); Molecular Medicine (Q1)";89.0;564.0;United Kingdom;2004-2020;10.1016/j.ddtec.2015.06.001;;51.0;;17406749;17406749;17406749;17406749;;;Elsevier Ltd.;;6263.0;Western Europe;1676.0;Q1;21207.0;Drug discovery today: technologies;Medicinal chemistry in the era of big data;;521.0;16.0;99.0;1002.0;journal;article;2015
Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.;Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger;"Medicine (miscellaneous) (Q1); Surgery (Q1)";959.0;438.0;Netherlands;2003-2020;10.1016/j.ijsu.2019.07.036;0.01876;61.0;;17439191;17439191;17439159;17439191;6.071;Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data;Elsevier BV;;1762.0;Western Europe;1315.0;Q1;130156.0;International journal of surgery;Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives;16011.0;4685.0;698.0;1214.0;12301.0;journal;article;2019
Cardiovascular disease is a major illness that causes human death, especially in the elderly. Timely and accurate diagnosis of arrhythmia types is the key to early prevention and diagnosis of cardiovascular diseases. This paper proposed an arrhythmia classification algorithm based on multi-head self-attention mechanism (ACA-MA). First, an ECG signal preprocessing algorithm based on wavelet transform is put forward and implemented using db6 wavelet transform to focus on improving the data quality of ECG signals and reduce the noise of ECG signals. Second, a linear projection layer for acquiring semantic features of ECG signals is designed using the matching relationship between ECG tag and segmented ECG signals. Third, a position encoding-based spatiotemporal characterization method of ECG signal sequences is designed to integrate time series information into a matrix operation. Fourth, a multi-head self-attentive mechanism capable of capturing global contextual information is proposed to extract relationships and semantic features between ECG segments and achieve semantic association and information stitching of nonadjacent ECG signals. Finally, experimental results on the arrhythmia dataset MIT/BIH show that ACA-MA outperforms other state-of-the-art methods with an overall classification accuracy of 99.4%, a specific rate of 99.41%, and a sensitivity of 97.36%.;Yue Wang and Guanci Yang and Shaobo Li and Yang Li and Ling He and Dan Liu;"Health Informatics (Q2); Signal Processing (Q2)";780.0;490.0;Netherlands;2006-2021;10.1016/j.bspc.2022.104206;0.00713;72.0;;17468094;17468094;17468094;17468094;3.880;Arrhythmia classification, Electrocardiogram (ECG), Attention mechanism, Feature extraction;Elsevier BV;;4265.0;Western Europe;767.0;Q2;4700152237.0;Biomedical signal processing and control;Arrhythmia classification algorithm based on multi-head self-attention mechanism;6354.0;3928.0;425.0;783.0;18127.0;journal;article;2023
;Nader Ibrahim and John Gibson and Stephen Ali and Thomas Dobbs and Iain S. Whitaker;Surgery (Q1);627.0;129.0;United Kingdom;2006-2020;10.1016/j.bjps.2020.12.036;;94.0;;17486815;17486815;17486815;17486815;;;Churchill Livingstone;;1822.0;Western Europe;855.0;Q1;4000148817.0;Journal of plastic, reconstructive and aesthetic surgery;Is poor quality non-melanoma skin cancer data affecting high quality research and patient care?;;1601.0;511.0;1095.0;9311.0;journal;article;2021
Continued research into reservoir characterization along with offshore carbon dioxide (CO2) transportation and infrastructure assets is needed to facilitate development of safe and successful carbon capture, utilization, and storage (CCUS) projects. This paper outlines a multi-criteria evaluation methodology that incorporates disparate sets of quantitative, spatially variable data into a decision-making framework for screening the Gulf of Mexico (GOM) outer continental shelf (OCS) for potentially viable CO2 storage and enhanced oil recovery (EOR) sites. Criteria categories include favorable geologic characteristics, logistics, and potential risks. Data compiled for 14 criteria from several publicly available geographic information system (GIS) layers was aggregated over 2559 spatially balanced points across the study area using the National Energy Technology Laboratory (NETL)-developed Cumulative Spatial Impact Layers™ (CSIL) GIS tool. Criteria are weighted by qualitative expert opinion relative to their perceived importance to given scenarios— the output of combined criteria values and weights enables regional CO2 storage suitability differentiation. The methodology considers both technical and non-technical factors impacting CCUS decision-making. The flexible methodology enables a systematic approach to regional ranking at high spatial resolution over a large study domain. Additionally, the framework enables high-grading of priority sites that warrant further characterization and follow-on analysis. Areas along the Louisiana coast and Mississippi River Delta consistently rank high for all scenarios largely a result of the favorable geology with the potential for stacked storage, as well as the density of existing pipelines and platforms, and proximity to several onshore CO2 sources. High-graded regions for the CO2 EOR-related scenarios are typically located further offshore towards the middle and edge of the OCS compared to higher priority regions for the geologic storage scenarios which fall closer to the Louisiana coastline.;Anna Wendt and Alana Sheriff and Chung Yan Shih and Derek Vikara and Tim Grant;"Energy (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Management, Monitoring, Policy and Law (Q1); Pollution (Q1)";787.0;379.0;Netherlands;2007-2020;10.1016/j.ijggc.2022.103688;0.01181;114.0;;17505836;17505836;17505836;17505836;3.738;Geologic carbon storage, Multi-criteria evaluation, Site screening, Gulf of Mexico, Outer continental shelf, Offshore CO storage;Elsevier;;5386.0;Western Europe;1025.0;Q1;6200180161.0;International journal of greenhouse gas control;A multi-criteria ccus screening evaluation of the gulf of mexico, usa;12182.0;3310.0;220.0;789.0;11850.0;journal;article;2022
This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.;Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou;"Applied Mathematics (Q1); Computer Science Applications (Q1); Library and Information Sciences (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1); Statistics and Probability (Q1)";245.0;566.0;Netherlands;2007-2020;10.1016/j.joi.2014.05.001;0.00554;76.0;;17511577;17511577;17511577;17511577;5.107;Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data;Elsevier BV;;4792.0;Western Europe;1605.0;Q1;5100155103.0;Journal of informetrics;Knowledge diffusion path analysis of data quality literature: a main path analysis;4326.0;1696.0;79.0;276.0;3786.0;journal;article;2014
In animal sciences, the number of published meta-analyses is increasing at a rate of 15% per year. This current review focuses on the good practices and the potential pitfalls in the conduct of meta-analyses in animal sciences, nutrition in particular. Once the study objectives have been defined, several key phases must be considered when doing a meta-analysis. First, as a principle of traceability, criteria used to select or discard publications should be clearly stated in a way that one could reproduce the final selection of data. Then, the coding phase, aiming to isolate specific experimental factors for an accurate graphical and statistical interpretation of the database, is discussed. Following this step, the study of the levels of independence of factors and of the degree of data balance of the meta-design represents an essential phase to ensure the validity of statistical processing. The consideration of the study effect as fixed or random must next be considered. It appears based on several examples that this choice does not generally have any influence on the conclusions of a meta-analysis when the number of experiments is sufficient.;D. Sauvant and M.P. Letourneau-Montminy and P. Schmidely and M. Boval and C. Loncke and J.B. Daniel;Animal Science and Zoology (Q1);944.0;293.0;United Kingdom;2007-2020;10.1017/S1751731120001688;0.00949;76.0;;17517311;1751732X;17517311;1751732X;3.240;modeling, database, meta-analysis, random and fixed effects, nutrition;Cambridge University Press;;3840.0;Western Europe;889.0;Q1;5300152613.0;Animal;Review: use and misuse of meta-analysis in animal science;10396.0;2903.0;325.0;980.0;12479.0;journal;article;2020
The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.;Grégoire Rey and Karim Bounebache and Claire Rondet;"Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine (Q2)";407.0;157.0;United Kingdom;2007-2020;10.1016/j.jflm.2016.12.004;0.00305;47.0;;1752928X;1752928X;1752928X;1752928X;1.614;Causes of death data, Data linkages, Big data;Churchill Livingstone;;2926.0;Western Europe;569.0;Q1;5100154502.0;Journal of forensic and legal medicine;Causes of deaths data, linkages and big data perspectives;2328.0;668.0;134.0;428.0;3921.0;journal;article;2018
The use of data has been essential throughout the unfolding COVID-19 pandemic. We have needed it to populate our models, inform our understanding, and shape our responses to the disease. However, data has not always been easy to find and access, it has varied in quality and coverage, been difficult to reuse or repurpose. This paper reviews these and other challenges and recommends steps to develop a data ecosystem better able to deal with future pandemics by better supporting preparedness, prevention, detection and response.;Nigel Shadbolt and Alys Brett and Min Chen and Glenn Marion and Iain J. McKendrick and Jasmina Panovska-Griffiths and Lorenzo Pellis and Richard Reeve and Ben Swallow;"Epidemiology (Q1); Infectious Diseases (Q1); Microbiology (Q1); Parasitology (Q1); Public Health, Environmental and Occupational Health (Q1); Virology (Q1)";138.0;593.0;Netherlands;2009-2020;10.1016/j.epidem.2022.100612;0.00442;41.0;;17554365;17554365;18780067;17554365;4.396;Data and models, Data ecosystem, Data lifecycles, FAIR data, Pandemic preparedness, COVID-19;Elsevier;;4631.0;Western Europe;2023.0;Q1;17300154924.0;Epidemics;The challenges of data in future pandemics;1395.0;786.0;48.0;141.0;2223.0;journal;article;2022
"Artificial Neural Networks (ANNs) are increasingly used for discrete choice analysis. But, at present, it is unknown what sample size requirements are appropriate when using ANNs in this particular context. This paper fills this knowledge gap: we empirically establish a rule-of-thumb for ANN-based discrete choice analysis based on analyses of synthetic and real data. To investigate the effect of complexity of the data generating process on the minimum required sample size, we conduct extensive Monte Carlo analyses using a series of different model specifications with different levels of model complexity, including RUM and RRM models, with and without random taste parameters. Based on our analyses we advise to use a minimum sample size of fifty times the number of weights in the ANN; it should be noted, that the number of weights is generally much larger than the number of parameters in a discrete choice model. This rule-of-thumb is considerably more conservative than the rule-of-thumb that is most often used in the ANN community, which advises to use at least ten times the number of weights.";Ahmad Alwosheel and Sander {van Cranenburgh} and Caspar G. Chorus;"Modeling and Simulation (Q1); Statistics, Probability and Uncertainty (Q1)";87.0;307.0;Netherlands;2008-2020;10.1016/j.jocm.2018.07.002;0.0016600000000000002;26.0;;17555345;17555345;17555345;17555345;3.091;;Elsevier BV;;4931.0;Western Europe;1328.0;Q1;21100246506.0;Journal of choice modelling;Is your dataset big enough? sample size requirements when using artificial neural networks for discrete choice analysis;783.0;290.0;26.0;89.0;1282.0;journal;article;2018
Business process management (BPM) supports the management and transformation of organizational operations. This paper provides a structured guideline for improving data-based process development within the BPM life cycle. We show how Industry 4.0-induced tools and models can be integrated within the BPM life cycle to achieve more efficient process excellence and evidence-based decision-making. The paper demonstrates how standards of machine learning (CRISP-ML(Q)), BPM, and tools of design science research can support the redesign phases of Industry 4.0 development. The proposed methodology is carried out on an assembly company, where the proposed improvement steps are investigated by simulation and evaluated by relevant key performance indicators.;Tímea Czvetkó and Alex Kummer and Tamás Ruppert and János Abonyi;Industrial and Manufacturing Engineering (Q1);144.0;471.0;Netherlands;2008-2020;10.1016/j.cirpj.2021.12.002;0.0018100000000000002;49.0;;17555817;17555817;18780016;17555817;3.602;Business process management (BPM), Business process redesign (BPR), Industry 4.0 (I4.0), Digital technology, Discrete event simulation;Elsevier BV;;4051.0;Western Europe;1309.0;Q1;12300154704.0;Cirp journal of manufacturing science and technology;Data-driven business process management-based development of industry 4.0 solutions;1677.0;720.0;112.0;145.0;4537.0;journal;article;2022
;Katherine Pereira and Victoria Goode and Petra Brysiewicz;Emergency Nursing (Q1);180.0;192.0;United Kingdom;2008-2020;10.1016/j.ienj.2019.100809;0.00168;39.0;;1755599X;1878013X;1755599X;1878013X;2.142;;Elsevier BV;;3688.0;Western Europe;677.0;Q1;11700154303.0;International emergency nursing;Using clinical data to effect practice change;1164.0;398.0;66.0;193.0;2434.0;journal;article;2019
;Roberto Jun Arai and Irene {de Lourdes Noronha} and José Carlos Nicolau and Charles Schmidt and Gustavo Moreira {de Albuquerque} and Kenneth W Mahaffey and Eduardo Moacyr Krieger and José Otávio Costa Auler Júnior;Medicine (miscellaneous) (Q2);391.0;207.0;Brazil;1945-1946, 2005-2020;10.6061/clinics/2017/e515s;0.00345;61.0;;18075932;19805322;18075932;19805322;2.365;;University of Sao Paolo;;1842.0;Latin America;618.0;Q2;4000148010.0;Clinics;Academic health centers: integration of clinical research with healthcare and education. comments on a workshop;4903.0;875.0;221.0;409.0;4071.0;journal;article;2018
"Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.";Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr;"Education (Q2); Health Policy (Q3); Medicine (miscellaneous) (Q3)";221.0;81.0;Germany;2008-2020;10.1016/j.zefq.2020.11.002;;28.0;;18659217;22120289;18659217;22120289;;Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases;Urban und Fischer Verlag Jena;;2979.0;Western Europe;423.0;Q2;11300153728.0;Zeitschrift fur evidenz, fortbildung und qualitat im gesundheitswesen;Evaluation eines zukunftsszenarios zur nutzung von big-data-anwendungen für die verbesserung der versorgung von menschen mit seltenen erkrankungen;;267.0;73.0;244.0;2175.0;journal;article;2020
The development of digital farming gives bovine mastitis research and management tools access to large datasets. However, the quality of registered data on clinical mastitis cases or treatments may be inadequate (e.g. due to missing records). In automatic milking systems, the decision to divert milk from the bulk milk tank during milking is registered (i.e. milk diversion indicator) for every milking and could potentially indicate a clinical mastitis case. This study accordingly estimated the diagnostic performance of a milk diversion indicator in relation to farmer-recorded clinical mastitis cases in the absence of a “gold standard”. Data on milk diversion and farmer-reported clinical mastitis from 3,443 lactations in 13 herds were analyzed. Each cow lactation was split into 30-DIM periods in which it was registered whether milk was diverted and whether clinical mastitis was reported. One 30-DIM period was randomly sampled for each lactation and this was the unit of analysis, this procedure was repeated 300 times, resulting in 300 datasets to create autocorrelation-robust results during analysis. We used Bayesian latent class analysis to assess the diagnostic properties of milk diversion and farmer-reported clinical status. We analyzed different episode lengths of milk diversion of 1 or more milk diversion days until 10 or more milk diversion days for two scenarios: farmers with poor-quality (51% sensitivity, 99% specificity) and high-quality (90% sensitivity, 99% specificity) mastitis registrations. The analysis was done for all 300 datasets. The results showed that for the scenario where the quality of clinical mastitis reporting was high, the sensitivity was similar for milk-diversion threshold durations of 1–4 days (0.843 to 0.793 versus 0.893). Specificity increased when the number of days of milk diversion increased and was ≥98% at a milk-diversion threshold durations of 8 or more consecutive milk diversion days. In the scenario where the quality of clinical mastitis reporting was low, the sensitivity of milk diversion and reported clinical mastitis cases was similar at milk-diversion threshold durations of 1–7 days (0.687 to 0.448 versus 0.503 to 0.504) while specificity exceeded the 98% at milk-diversion threshold durations of 7 or more consecutive milk diversion days. In both scenarios, a milk diversion threshold duration of 4–7 days achieved the most desirable combined sensitivity and specificity. This study concluded that milk diversion can be a valid alternative to farmer-reported clinical mastitis as it performs similarly in indicating actual clinical mastitis.;John Bonestroo and Nils Fall and Mariska {van der Voort} and Ilka Christine Klaas and Henk Hogeveen and Ulf Emanuelson;"Veterinary (miscellaneous) (Q1); Animal Science and Zoology (Q2)";756.0;191.0;Netherlands;2006-2020;10.1016/j.livsci.2021.104698;0.005520000000000001;111.0;;18711413;18711413;18711413;18711413;1.943;antibiotic treatment, proxy, Automatic milking system, Milk withdrawal, Latent class analysis;Elsevier;;4360.0;Western Europe;622.0;Q1;3300147807.0;Livestock science;Diagnostic properties of milk diversion and farmer-reported mastitis to indicate clinical mastitis status in dairy cows using bayesian latent class analysis;9037.0;1646.0;358.0;757.0;15608.0;journal;article;2021
"Background and aims
With no approved vaccines for treating COVID-19 as of August 2020, many health systems and governments rely on contact tracing as one of the prevention and containment methods. However, there have been instances when the infected person forgets his/her contact-persons and does not have their contact details. Therefore, this study aimed at analyzing possible opportunities and challenges of integrating emerging technologies into COVID-19 contact tracing.
Methods
The study applied literature search from Google Scholar, Science Direct, PubMed, Web of Science, IEEE and WHO COVID-19 reports and guidelines analyzed.
Results
While the integration of technology-based contact tracing applications to combat COVID-19 and break transmission chains promise to yield better results, these technologies face challenges such as technical limitations, dealing with asymptomatic individuals, lack of supporting ICT infrastructure and electronic health policy, socio-economic inequalities, deactivation of mobile devices’ WIFI, GPS services, interoperability and standardization issues, security risks, privacy issues, political and structural responses, ethical and legal risks, consent and voluntariness, abuse of contact tracing apps, and discrimination.
Conclusion
Integrating emerging technologies into COVID-19 contact tracing is seen as a viable option that policymakers, health practitioners and IT technocrats need to seriously consider in mitigating the spread of coronavirus. Further research is also required on how best to improve efficiency and effectiveness in the utilisation of emerging technologies in contact tracing while observing the security and privacy of people in fighting the COVID-19 pandemic.";Elliot Mbunge;"Internal Medicine (Q2); Medicine (miscellaneous) (Q2); Endocrinology, Diabetes and Metabolism (Q3)";900.0;246.0;Netherlands;2007-2020;10.1016/j.dsx.2020.08.029;;40.0;;18714021;18780334;18714021;18780334;;COVID-19, Contact tracing, Emerging technologies;Elsevier BV;;3117.0;Western Europe;684.0;Q2;5700165201.0;Diabetes and metabolic syndrome: clinical research and reviews;Integrating emerging technologies into covid-19 contact tracing: opportunities, challenges and pitfalls;;2232.0;403.0;910.0;12562.0;journal;article;2020
"Background
Without a standard terminology to classify models of maternity care, it is problematic to compare and evaluate clinical outcomes across different models. The Maternity Care Classification System is a novel system developed in Australia to classify models of maternity care based on their characteristics and an overarching broad model descriptor (Major Model Category).
Aim
This study aimed to assess the extent of variability in the defining characteristics of models of care grouped to the same Major Model Category, using the Maternity Care Classification System.
Method
All public hospital maternity services in New South Wales, Australia, were invited to complete a web-based survey classifying two local models of care using the Maternity Care Classification System. A descriptive analysis of the variation in 15 attributes of models of care was conducted to evaluate the level of heterogeneity within and across Major Model Categories.
Results
Sixty-nine out of seventy hospitals responded, classifying 129 models of care. There was wide variation in a number of important attributes of models classified to the same Major Model Category. The category of ‘Public hospital maternity care’ contained the most variation across all characteristics.
Conclusion
This study demonstrated that although models of care can be grouped into a distinct set of Major Model Categories, there are significant variations in models of the same type. This could result in seemingly ‘like’ models of care being incorrectly compared if grouped only by the Major Model Category.";Natasha R. Donnolley and Georgina M. Chambers and Kerryn A. Butler-Henderson and Michael G. Chapman and Elizabeth A. Sullivan;"Maternity and Midwifery (Q1); Medicine (miscellaneous) (Q1); Obstetrics and Gynecology (Q1)";344.0;289.0;Netherlands;2006-2020;10.1016/j.wombi.2017.01.005;0.00489;39.0;;18715192;18715192;18715192;18715192;3.172;Classification, Delivery of health care, Models of care, Maternity care, Health care evaluation mechanisms;Elsevier BV;;4147.0;Western Europe;1058.0;Q1;4700152255.0;Women and birth;More than a name: heterogeneity in characteristics of models of maternity care reported from the australian maternity care classification system validation study;2771.0;1123.0;245.0;382.0;10160.0;journal;article;2017
Italy has the third largest bioeconomy in Europe (€330 billion annual turnover, 2 million employees), making it a core pillar of the national economy. Its sectors of excellence are food and biobased products, and it is a consistent presence in research and innovation projects funded by the EU Horizon 2020 programme (Societal Challenges 2) and the European Public Private Partnership “Biobased industry” (BBI-JU). The bioeconomy reduces dependence on fossil fuels and finite materials, loss of biodiversity and changing land use. It contributes to environmental regeneration, spurs economic growth and supports jobs in rural, coastal and abandoned industrial areas, leveraging local contexts and traditions. In 2017 the Italian government promoted the development of a national Bioeconomy Strategy (BIT), recently updated (BIT II) to interconnect more efficiently the pillars of the national bioeconomy: production of renewable biological resources, their conversion into valuable food/feed, biobased products and bio-energy, and transformation and valorization of bio-waste streams. BIT II aims to improve coordination between Ministries and Italian regions in alignment of policies, regulations, R&I funding programmes and infrastructures investment. The goal is a 15 % increase in turnover and employment in the Italian bioeconomy by 2030. Based on Italy’s strategic geopolitical position in the Mediterranean basin, BIT II also includes actions to improve sustainable productivity, social cohesion and political stability through the implementation of bioeconomy strategies in this area. This paper provides an insight into these strategies and discusses the strengths and weaknesses of the sectors involved and the measures, regulatory initiatives and monitoring actions undertaken.;Fabio Fava and Lucia Gardossi and Patrizia Brigidi and Piergiuseppe Morone and Daniela A.R. Carosi and Andrea Lenzi;"Bioengineering (Q1); Biotechnology (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q2)";262.0;503.0;Netherlands;2008-2021;10.1016/j.nbt.2020.11.009;0.0045899999999999995;85.0;;18716784;18716784;18764347;18716784;5.079;Bioeconomy strategy, Circular bioeconomy, Italian bioeconomy;Elsevier;;5477.0;Western Europe;1163.0;Q1;11700154315.0;New biotechnology;The bioeconomy in italy and the new national strategy for a more competitive and sustainable country;4198.0;1497.0;69.0;276.0;3779.0;journal;article;2021
Next-generation Sequencing (NGS) is a rapidly evolving technology with demonstrated benefits for forensic genetic applications, and the strategies to analyze and manage the massive NGS datasets are currently in development. Here, the computing, data storage, connectivity, and security resources of the Cloud were evaluated as a model for forensic laboratory systems that produce NGS data. A complete front-to-end Cloud system was developed to upload, process, and interpret raw NGS data using a web browser dashboard. The system was extensible, demonstrating analysis capabilities of autosomal and Y-STRs from a variety of NGS instrumentation (Illumina MiniSeq and MiSeq, and Oxford Nanopore MinION). NGS data for STRs were concordant with standard reference materials previously characterized with capillary electrophoresis and Sanger sequencing. The computing power of the Cloud was implemented with on-demand auto-scaling to allow multiple file analysis in tandem. The system was designed to store resulting data in a relational database, amenable to downstream sample interpretations and databasing applications following the most recent guidelines in nomenclature for sequenced alleles. Lastly, a multi-layered Cloud security architecture was tested and showed that industry standards for securing data and computing resources were readily applied to the NGS system without disadvantageous effects for bioinformatic analysis, connectivity or data storage/retrieval. The results of this study demonstrate the feasibility of using Cloud-based systems for secured NGS data analysis, storage, databasing, and multi-user distributed connectivity.;Sarah F. Bailey and Melissa K. Scheible and Christopher Williams and Deborah S.B.S. Silva and Marina Hoggan and Christopher Eichman and Seth A. Faith;"Pathology and Forensic Medicine (Q1); Genetics (Q2)";449.0;412.0;Ireland;2007-2020;10.1016/j.fsigen.2017.08.008;;75.0;;18724973;18724973;18724973;18724973;;Cloud, Bioinformatics, Microsatellite, Database, Sequencing, Security;Elsevier Ireland Ltd;;3722.0;Western Europe;1144.0;Q1;5700191205.0;Forensic science international: genetics;Secure and robust cloud computing for high-throughput forensic microsatellite sequence analysis and databasing;;2288.0;173.0;564.0;6439.0;journal;article;2017
"Myopia is generally regarded as a failure of normal emmetropization process, however, its underlying molecular mechanisms are unclear. To investigate the retinal protein profile changes during emmetropization, we studied differential protein expressions of ocular growth in young guinea pigs at 3 and 21 days old respectively, when significant axial elongation was detected (P < 0.001, n = 10). Independent pooled retinal samples of both eyes were subjected to SWATH mass spectrometry (MS) followed by bioinformatics analysis using cloud-based platforms. A comprehensive retina SWATH ion-library consisting of 3138 (22,871) unique proteins (peptides) at 1% FDR was constructed. 40 proteins were found to be significantly up-regulated and 8 proteins down-regulated during emmetropization (≥log2 of 0.43 with ≥2 peptides matched per protein; P < 0.05). Using pathway analysis, the most significant pathway identifiable was ‘phototransduction’ (P = 1.412e−4). Expression patterns of 7 proteins identified in this pathway were further validated and confirmed (P < 0.05) with high-resolution Multiple Reaction Monitoring (MRM-HR) MS. Combining discovery and targeted proteomics approaches, this study for the first time comprehensively profiled protein changes in the guinea pig retina during normal emmetropization-associated eye growth. The findings of this study are also relevant to the myopia development, which is the result of failed emmetropization.
Significance
Myopia is considered as a failure of emmetropization. However, the underlying biochemical mechanism of emmetropization, a visually guided process in which eye grows towards the optimal optical state of clear vision during early development, is not well understood. Retina is known as the key tissue to regulate this active eye growth. we studied eye growth of young guinea pigs and harvested their retinal tissues. A comprehensive SWATH ion library with identification of a total 3138 unique proteins were established, in which 48 proteins exhibited significant differential expressions between 3 and 21 days old. After MRM-HR confirmation, ‘phototransduction’ were found as the most active pathway during emmetropic eye growth. This study is the first in discovering key retinal protein players and pathways which are presumably orchestrated by biological mechanism(s) underlying emmetropization.";Sze Wan Shan and Dennis Yan-yin Tse and Bing Zuo and Chi Ho To and Quan Liu and Sally A. McFadden and Rachel Ka-Man Chun and Jingfang Bian and King Kit Li and Thomas Chuen Lam;"Biophysics (Q1); Biochemistry (Q2)";823.0;380.0;Netherlands;2008-2021;10.1016/j.jprot.2018.03.023;0.01438;105.0;;18743919;18743919;18767737;18743919;4.044;Emmetropization, Retina, SWATH-MS, Myopia, Guinea pigs;Elsevier;;5999.0;Western Europe;1067.0;Q1;11700154304.0;Journal of proteomics;Integrated swath-based and targeted-based proteomics provide insights into the retinal emmetropization process in guinea pig;12152.0;3191.0;282.0;839.0;16918.0;journal;article;2018
Visible light communication (VLC) has emerged as a viable complement to traditional radio frequency (RF) based systems and as an enabler for high data rate communications for beyond-5G (B5G) indoor communication systems. In particular, the emergence of new B5G-based applications with quality of service (QoS) requirements and massive connectivity has recently led to research on the required service-levels and the development of improved physical (PHY) layer methods. As part of recent VLC standards development activities, the IEEE has formed the 802.11bb “Light Communications (LC) for Wireless Local Area Networking” standardization group. This paper investigates the network requirements of 5G indoor services such as virtual reality (VR) and high-definition (HD) video for residential environments using VLC. In this paper, we consider such typical VLC scenarios with additional impairments such as light-emitting diode (LED) nonlinearity and imperfect channel feedback, and propose hyperparameter-free mitigation techniques using Reproducing Kernel Hilbert Space (RKHS) methods. In this context, we also propose using a direct current biased optical orthogonal frequency division multiplexing (DCO-OFDM)-based adaptive VLC transmission method that uses precomputed bit error rate (BER) expressions for these RKHS-based detection methods and performs adaptive BER-based modulation-order switching. Simulations of channel impulse responses (CIRs) show that the adaptive transmission method provides significantly improved error rate performance, which makes it promising for high data rate VLC-based 5G indoor services.;Farshad Miramirkhani and Mehdi Karbalayghareh and Engin Zeydan and Rangeet Mitra;Electrical and Electronic Engineering (Q2);443.0;232.0;Netherlands;2008-2020;10.1016/j.phycom.2022.101679;0.0015199999999999999;33.0;;18744907;18744907;18744907;18744907;1.810;Visible light communication (VLC), Ray-tracing, Adaptive transmission, 5G services;Elsevier;;3496.0;Western Europe;354.0;Q2;11300153720.0;Physical communication;Enabling 5g indoor services for residential environment using vlc technology;1412.0;969.0;197.0;447.0;6888.0;journal;article;2022
In modern Smart Grids (SGs) ruled by advanced computing and networking technologies, condition monitoring relies on secure cyberphysical connectivity. Due to this connection, a portion of transported data, containing confidential information, must be protected as it is vulnerable and subject to several cyber threats. SG cyberspace adversaries attempt to gain access through networking platforms to commit several criminal activities such as disrupting or malicious manipulation of whole electricity delivery process including generation, distribution, and even customer services such as billing, leading to serious damage, including financial losses and loss of reputation. Therefore, human awareness training and software technologies are necessary precautions to ensure the reliability of data traffic and power transmission. By exploring the available literature, it is undeniable that Machine Learning (ML) has become the latest in the timeline and one of the leading artificial intelligence technologies capable of detecting, identifying, and responding by mitigating adversary attacks in SGs. In this context, the main objective of this paper is to review different ML tools used in recent years for cyberattacks analysis in SGs. It also provides important guidelines on ML model selection as a global solution when building an attack predictive model. A detailed classification is therefore developed with respect to data security triad, i.e., Confidentiality, Integrity, and Availability (CIA) within different types of cyber threats, systems, and datasets. Furthermore, this review highlights the various encountered challenges, drawbacks, and possible solutions as future prospects for ML cybersecurity applications in SGs.;Tarek Berghout and Mohamed Benbouzid and S.M. Muyeen;"Computer Science Applications (Q2); Information Systems and Management (Q2); Modeling and Simulation (Q2); Safety, Risk, Reliability and Quality (Q2)";94.0;362.0;Netherlands;2008-2020;10.1016/j.ijcip.2022.100547;0.00082;37.0;;18745482;18745482;18745482;18745482;2.865;Cybersecurity, Cyberattacks, Machine learning, Model selection, Smart grids;Elsevier;;4388.0;Western Europe;650.0;Q2;17300154971.0;International journal of critical infrastructure protection;Machine learning for cybersecurity in smart grids: a comprehensive review-based study on methods, solutions, and prospects;668.0;392.0;25.0;110.0;1097.0;journal;article;2022
Plants are ideal systems to teach core biology concepts due to their unique physiological and developmental features. Advances in DNA sequencing technology and genomics have allowed scientists to generate genome sequences and transcriptomics data for numerous model plant species. This information is publicly available and presents a valuable tool to introduce undergraduate students to the fundamental concepts of gene expression in the context of modern quantitative biology and bioinformatics. Modern biology classrooms must provide authentic research experiences to allow developing core competencies such as scientific inquiry, critical interpretation of experimental results, and quantitative analyses of large dataset using computational approaches. Recent educational research has shown that undergraduate students struggle when connecting gene expression concepts to classic genetics, phenotypic analyses, and overall flow of biological information in living organisms, suggesting that novel approaches are necessary to enhance learning of gene expression and regulation. This review describes different strategies and resources available to instructors willing to incorporate authentic research experiences, genomic tools, and bioinformatics analyses when teaching transcriptional regulation and gene expression in undergraduate courses. A variety of laboratory exercises and pedagogy materials developed to teach gene expression using plants are discussed. This article is part of a Special Issue entitled: Plant Gene Regulatory Mechanisms and Networks, edited by Dr. Erich Grotewold and Dr. Nathan Springer.;Irina Makarevitch and Betsy Martinez-Vaz;"Biochemistry (Q1); Biophysics (Q1); Genetics (Q1); Molecular Biology (Q1); Structural Biology (Q2)";318.0;412.0;Netherlands;2008-2020;10.1016/j.bbagrm.2016.04.012;;126.0;;18749399;18749399;18764320;18749399;;Regulation of gene expression, Undergraduate classroom, Student research experiences;Elsevier;;9683.0;Western Europe;1808.0;Q1;11200153524.0;Biochimica et biophysica acta - gene regulatory mechanisms;Killing two birds with one stone: model plant systems as a tool to teach the fundamental concepts of gene expression while analyzing biological data;;1306.0;102.0;324.0;9877.0;journal;article;2017
Geologic CO2 sequestration (GCS) has been identified as the most viable option for effectively reducing greenhouse gases emissions to mitigate global warming and worldwide climate change. However, CO2 injection into subsurface can induce reservoir expansion and fault reactivation, which ultimately result in near-surface infrastructure damage and personnel insecurity. Distributed fiber optic sensing (DFOS) technologies function one single fiber as an array of sensors to in-situ monitor multi-parameters, such as geomechanical deformation (i.e., strain), temperature, acoustics and pressure along the entire fiber or cable length. Due to its superiority over conventional geophone and detector, DFOS tool possesses great potential to sense geofluid injection-induced small disturbances in deep subsurface. Here we begin by highlighting recent research efforts in available monitoring tools employed in GCS sites. Given the increasing attentions of optical sensing, we present a first-hand review of DFOS categories, sensing principles, and advantages for GCS related investigations from both laboratory and field scales. We discuss in detail three typical DFOS-deployed GCS projects and explore the implicit findings to guide subsequent GCS field applications. Finally, we summarize the major challenges and going forward in developing, utilizing, and extending DFOS systems to widely apply for the future large-scale all-optical GCS monitoring sites.;Yankun Sun and Jinquan Liu and Ziqiu Xue and Qi Li and Chengkai Fan and Xu Zhang;"Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1)";1066.0;531.0;Netherlands;2009-2020;10.1016/j.jngse.2020.103751;0.02206;68.0;;18755100;18755100;18755100;18755100;4.965;Distributed fiber-optic sensing, Geologic CO sequestration, Brillouin- Rayleigh backscattering, Strain response, Temperature profile, Microseismicity detection;Elsevier;;5751.0;Western Europe;1079.0;Q1;19400158619.0;Journal of natural gas science and engineering;A critical review of distributed fiber optic sensing for real-time monitoring geologic co2 sequestration;14458.0;5865.0;542.0;1073.0;31172.0;journal;article;2021
Technology of human motion capture has been widely used in digital entertainment field. Editing the existing large amount of human motion capture data, correcting and eliminating motion distortion caused by noise and other defects have important value and significance for data reuse. In this paper, data processing is carried out based on convolutional automatic encoder and manifold learning. The popular structure of human motion data was learned by a one-dimensional time domain convolution automatic encoder, in which the hidden unit of the automatic encoder represents motion data. Three constraints were used to overcome the problem that the hidden unit has too much motion editing range. The data to be processed in this paper has no limit on the number of motions. The proposed method can process large data sets in parallel and automatically perform manifold learning without manual labelling and segmentation. In the final, comparative experiments based on a variety of damaged motion data have been carried out. The results showed that the proposed method can effectively reduce the error of the original motion data, and has achieved good results in both objective evaluation and subjective evaluation.;Dongsheng Zhou and Xinzhu Feng and Xin Yang and Qiang Zhang and Xiaopeng Wei and Xiaoyong Fang and Deyun Yang;"Computer Science Applications (Q2); Human-Computer Interaction (Q2); Software (Q2)";120.0;222.0;Netherlands;2009-2021;10.1016/j.entcom.2019.100300;0.0007;29.0;;18759521;18759521;18759521;18759521;1.455;Human motion data, Convolution automatic encoder, Manifold learning, Motion editing;Elsevier;;4469.0;Western Europe;473.0;Q2;19400158708.0;Entertainment computing;Human motion data editing based on a convolutional automatic encoder and manifold learning;530.0;318.0;32.0;125.0;1430.0;journal;article;2019
Despite a newfound wealth of data and information, the healthcare sector is lacking in actionable knowledge. This is largely because healthcare data, though plentiful, tends to be inherently complex and fragmented. Health data analytics, with an emphasis on predictive analytics, is emerging as a transformative tool that can enable more proactive and preventative treatment options. This review considers the ways in which predictive analytics has been applied in the for-profit business sector to generate well-timed and accurate predictions of key outcomes, with a focus on key features that may be applicable to healthcare-specific applications. Published medical research presenting assessments of predictive analytics technology in medical applications are reviewed, with particular emphasis on how hospitals have integrated predictive analytics into their day-to-day healthcare services to improve quality of care. This review also highlights the numerous challenges of implementing predictive analytics in healthcare settings and concludes with a discussion of current efforts to implement healthcare data analytics in the developing country, Saudi Arabia.;Hana Alharthi;"Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1); Infectious Diseases (Q2)";441.0;332.0;Netherlands;2008-2020;10.1016/j.jiph.2018.02.005;0.00603;35.0;;18760341;18760341;1876035X;18760341;3.718;Predictive analytics, Healthcare analytics, Data mining, Saudi Arabia;Elsevier BV;;3146.0;Western Europe;983.0;Q1;16800154711.0;Journal of infection and public health;Healthcare predictive analytics: an overview with a focus on saudi arabia;3870.0;1684.0;336.0;498.0;10571.0;journal;article;2018
;Neeraj Tandon and Rajiv Tandon;"Medicine (miscellaneous) (Q2); Psychiatry and Mental Health (Q2); Psychology (miscellaneous) (Q2)";488.0;182.0;Netherlands;2008-2020;10.1016/j.ajp.2019.09.009;0.0042899999999999995;37.0;;18762018;18762018;18762026;18762018;3.543;;Elsevier;;2186.0;Western Europe;793.0;Q2;12300154709.0;Asian journal of psychiatry;Machine learning in psychiatry- standards and guidelines;5338.0;1550.0;572.0;851.0;12504.0;journal;article;2019
The immunization schedule recommended by the U.S. Advisory Committee on Immunization Practices (ACIP) provides a structure for how 10 different vaccine series should be administered to children in the first 18 months of life. Progress toward US early childhood immunization goals has largely focused on measuring vaccination coverage at age 24 months. However, standard vaccination coverage measures do not reflect whether children received vaccine doses by recommended ages, or whether vaccines were given concomitantly, per the schedule. In this paper, we describe innovations in population-level measurement of immunization schedule adherence through quantifying vaccination timeliness and undervaccination patterns. Measuring vaccination timeliness involves comparing when children received vaccine doses relative to ACIP age recommendations. To assess undervaccination patterns, children's vaccination histories are analyzed to determine whether they were vaccinated consistent with the ACIP schedule. Some patterns, such as spreading out vaccines across visits, are indicative of parental hesitancy. Other patterns, such as starting all recommended series but missing doses, are largely indicative of other immunization services delivery challenges. Since 2003, at least 12 studies have used National Immunization Survey-Child, immunization information system, or integrated health plan data to measure vaccination timeliness or undervaccination patterns at national or state levels. Moving forward, these novel measures can be leveraged for population-based surveillance of vaccine confidence, and for distinguishing undervaccination due to parental vaccine hesitancy from undervaccination due to other causes. Broader adoption of these measures can facilitate identification of targeted strategies for improving timely and routine early childhood vaccination uptake across the United States.;Sophia R. Newcomer and Jason M. Glanz and Matthew F. Daley;Pediatrics, Perinatology and Child Health (Q1);381.0;244.0;United States;2009-2020;10.1016/j.acap.2022.08.003;0.00946;75.0;;18762859;18762859;18762867;18762859;3.107;immunization schedule, vaccines, vaccine hesitancy;Elsevier Inc.;;2677.0;Northern America;1324.0;Q1;16800154743.0;Academic pediatrics;Beyond vaccination coverage: population-based measurement of early childhood immunization schedule adherence;4049.0;1340.0;225.0;482.0;6023.0;journal;article;2022
Aiming at the actual demands of petroleum exploration and development, this paper describes the research progress and application of artificial intelligence (AI) in petroleum exploration and development, and discusses the applications and development directions of AI in the future. Machine learning has been preliminarily applied in lithology identification, logging curve reconstruction, reservoir parameter estimation, and other logging processing and interpretation, exhibiting great potential. Computer vision is effective in picking of seismic first breaks, fault identification, and other seismic processing and interpretation. Deep learning and optimization technology have been applied to reservoir engineering, and realized the real-time optimization of waterflooding development and prediction of oil and gas production. The application of data mining in drilling, completion, and surface facility engineering etc. has resulted in intelligent equipment and integrated software. The potential development directions of artificial intelligence in petroleum exploration and development are intelligent production equipment, automatic processing and interpretation, and professional software platform. The highlights of development will be digital basins, fast intelligent imaging logging tools, intelligent seismic nodal acquisition systems, intelligent rotary-steering drilling, intelligent fracturing technology and equipment, real-time monitoring and control of zonal injection and production.;Lichun KUANG and He LIU and Yili REN and Kai LUO and Mingyu SHI and Jian SU and Xin LI;"Economic Geology (Q1); Geology (Q1); Energy Engineering and Power Technology (Q2); Geochemistry and Petrology (Q2); Geotechnical Engineering and Engineering Geology (Q2)";346.0;313.0;Netherlands;2008-2020;10.1016/S1876-3804(21)60001-0;0.0044;50.0;;18763804;18763804;18763804;18763804;3.803;artificial intelligence, logging interpretation, seismic exploration, reservoir engineering, drilling and completion, surface facility engineering;Elsevier;;3299.0;Western Europe;759.0;Q1;17300154720.0;Petroleum exploration and development;Application and development trend of artificial intelligence in petroleum exploration and development;4738.0;1113.0;121.0;346.0;3992.0;journal;article;2021
While a significant part of the population is concentrated in urban areas, the influence of cityscape parameters on human heat stress remain poorly understood. Yet we agree to develop urban spaces (street, square, district ...) in a way to provide best possible quality of life. In order to do so, quantitative and qualitative references are required. To fill this gap the HES-SO††University of Applied Sciences and Arts of Western Switzerland - hepia/leea‡‡Haute école du paysage, d’ingénierie et d’architecture de Genève / Laboratory for energy, environment and architecture has developed an innovative portable monitoring system that can be easily deployed in various outdoor and indoor environments. The monitoring equipment is embedded into a backpack that is carried during ‘climatic urban walks’ that can be reproduced at different times of the day or seasons so to yield a detailed and dynamic description of the climatic context of a portion of the city from the pedestrian point of view.;Peter Gallinelli and Reto Camponovo and Victor Guillot;Energy (miscellaneous);7685.0;189.0;United Kingdom;2009-2019;10.1016/j.egypro.2017.07.427;;81.0;;18766102;18766102;18766102;18766102;;climate mitigation, micro climate monitoring, urban design, open data;Elsevier BV;;0.0;Western Europe;474.0;-;17700156736.0;Energy procedia;Cityfeel - micro climate monitoring for climate mitigation and urban design;;15629.0;0.0;7789.0;0.0;conference and proceedings;article;2017
;;Computer Science (miscellaneous);6359.0;209.0;Netherlands;2010-2020;10.1016/S1877-0509(19)31798-3;;76.0;;18770509;18770509;18770509;18770509;;;Elsevier BV;;2019.0;Western Europe;334.0;-;19700182801.0;Procedia computer science;Contents;;13722.0;1907.0;6483.0;38511.0;conference and proceedings;article;2019
The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.;Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley;Orthopedics and Sports Medicine (Q4);160.0;28.0;United Kingdom;2009-2020;10.1016/j.mporth.2021.01.004;;27.0;;18771327;18771327;18771327;18771327;;Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy;Churchill Livingstone;;2119.0;Western Europe;194.0;Q4;16800154701.0;Orthopaedics and trauma;Introduction to big data in trauma and orthopaedics;;82.0;68.0;208.0;1441.0;journal;article;2021
Monitoring socioecological impacts of policy interventions aimed at changing land-use practices is a major challenge in sustainable development and conservation. Reducing emissions from deforestation and forest degradation (REDD+) intends to compensate local stakeholders for demonstrated carbon emission reduction and increased removals accounted for internationally, while promoting social and environmental benefits locally. Thus, monitoring REDD+ inherently requires the use of interdisciplinary data at different scales. Forest carbon monitoring, central to REDD+, is considerably advanced, yet the progress on social and environmental monitoring systems is uneven. We argue that scalar and interdisciplinary integration of REDD+ monitoring is crucial to uncover and understand trade-offs and synergies on which effectiveness, efficiency and equity of REDD+ may depend. We review previous efforts in integrating environmental and social monitoring, as well as efforts specific to REDD+, and discuss how old and new knowledge can contribute towards integrated monitoring. We observe that there are many challenges, but strong advantages, in an integrated monitoring approach. The current emergence of diverging standards and methodologies with narrow focus can inform future integrative efforts but could, in the long run, hinder coherence in national processes. We conclude that recent technological advances open new opportunities to integrate information across scale and disciplines, leveraging and combining existing data with targeted additional measures. The application of mixed methods in data collection can foster integration, in particular from the local level upwards. However, this requires greater coordination at higher levels to efficiently upscale multiple data streams. The unequal standpoint of carbon, social and environmental monitoring efforts provide a timely opportunity to promote integration, learn from advances in carbon monitoring, and build on existing and emerging platforms and tools that are locally to globally relevant.;Claudio {de Sassi} and Shijo Joseph and Astrid B Bos and Amy E Duchelle and Ashwin Ravikumar and Martin Herold;"Environmental Science (miscellaneous) (Q1); Social Sciences (miscellaneous) (Q1)";282.0;600.0;Netherlands;2009-2021;10.1016/j.cosust.2015.04.003;0.00898;87.0;;18773435;18773435;18773435;18773435;6.984;;Elsevier;;5494.0;Western Europe;2029.0;Q1;19400158343.0;Current opinion in environmental sustainability;Towards integrated monitoring of redd+;6722.0;2032.0;89.0;301.0;4890.0;journal;article;2015
During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.;Kim B. Stevens and Dirk U. Pfeiffer;"Geography, Planning and Development (Q1); Health, Toxicology and Mutagenesis (Q2); Infectious Diseases (Q2); Epidemiology (Q3)";88.0;163.0;United Kingdom;2009-2020;10.1016/j.sste.2015.04.003;;26.0;;18775845;18775845;18775853;18775845;;Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information;Elsevier Ltd.;;3918.0;Western Europe;726.0;Q1;19700167025.0;Spatial and spatio-temporal epidemiology;Sources of spatial animal and human health data: casting the net wide to deal more effectively with increasingly complex disease problems;;169.0;40.0;90.0;1567.0;journal;article;2015
Based on efficient continuous parallel query series algorithm supporting multi-objective optimization, by using visual graphics technology for traffic data streams for efficient real-time graphical visualization, it improve human-computer interaction, to realize real-time and visual data analysis and to improve efficiency and accuracy of the analysis. This paper employs data mining processing and statistical analysis on real-time traffic data stream, based on the parameters standards of various data mining algorithms, and by using computer graphics and image processing technology, converts graphics or images and make them displayed on the screen according to the system requirements, in order to track, forecast and maintain the operating condition of all traffic service systems effectively.;Jia Chaolong and Wang Hanning and Wei Lili;Engineering (miscellaneous);5804.0;188.0;Netherlands;2009-2019;10.1016/j.proeng.2016.01.308;;74.0;;18777058;18777058;18777058;18777058;;Smart Transportation, Visualization, Cloud Computing, Parallel Query, Big Data;Elsevier BV;;0.0;Western Europe;320.0;-;18700156717.0;Procedia engineering;Research on visualization of multi-dimensional real-time traffic data stream based on cloud computing;;9870.0;0.0;5873.0;0.0;conference and proceedings;article;2016
Though many ecosystem states are physically observable, the number of measured variables is limited owning to the constraints of practical environments and onsite sensors. It is therefore beneficial to only measure fundamental variables that determine the behavior of the whole ecosystem, and to simulate other variables with the measured ones. This paper proposes an approach to extract fundamental variables from simulated or observed ecosystem data, and to synthesize the other variables using the fundamental variables. Because the relation of variables in the ecosystem depends on sampling time and frequencies, a region of interest (ROI) is determined using a sliding window on time series with a predefined sampling point and frequency. Within each ROI, system variables are clustered in accordance with a group of selective features by a combination of Affinity Propagation and k-Nearest-Neighbor. In each cluster, the unobserved variables are synthesized from selected fundamental variables using a linear fitting model with ARIMA errors. In the experiment, we studied the performance of variable clustering and data synthesis under a community-land-model based simulation platform. The performance of data synthesis is evaluated by data fitting errors in prediction and forecasting, and the change of system dynamics when synthesized data are in the loop. The experiment proves the high accuracy of the proposed approach in time-series analysis and synthesis for ecosystem simulation.;Hongsheng He and Dali Wang and Yang Xu and Jindong Tan;"Computer Science (miscellaneous) (Q1); Modeling and Simulation (Q2); Theoretical Computer Science (Q2)";465.0;497.0;Netherlands;2010-2020;10.1016/j.jocs.2016.01.005;0.00489;46.0;;18777503;18777503;18777503;18777503;3.976;Data synthesis, Data analysis, Machine learning, Affinity Propagation, ARIMA model;Elsevier;;4512.0;Western Europe;704.0;Q1;19700174607.0;Journal of computational science;Data synthesis in the community land model for ecosystem simulation;3198.0;2325.0;123.0;485.0;5550.0;journal;article;2016
"Background
Africa and the Caribbean are projected to have greater increases in Head and neck cancer (HNC) burden in comparison to North America and Europe. The knowledge needed to reinforce prevention in these populations is limited. We compared for the first time, incidence rates of HNC in black populations from African, the Caribbean and USA.
Methods
Annual age-standardized incidence rates (IR) and 95% confidence intervals (95%CI) per 100,000 were calculated for 2013–2015 using population-based cancer registry data for 14,911 HNC cases from the Caribbean (Barbados, Guadeloupe, Trinidad & Tobago, N = 443), Africa (Kenya, Nigeria, N = 772) and the United States (SEER, Florida, N = 13,696). We compared rates by sub-sites and sex among countries using data from registries with high quality and completeness.
Results
In 2013–2015, compared to other countries, HNC incidence was highest among SEER states (IR: 18.2, 95%CI = 17.6–18.8) among men, and highest in Kenya (IR: 7.5, 95%CI = 6.3–8.7) among women. Nasopharyngeal cancer IR was higher in Kenya for men (IR: 3.1, 95%CI = 2.5–3.7) and women (IR: 1.5, 95%CI = 1.0–1.9). Female oral cavity cancer was also notably higher in Kenya (IR = 3.9, 95%CI = 3.0–4.9). Blacks from SEER states had higher incidence of laryngeal cancer (IR: 5.5, 95%CI = 5.2–5.8) compared to other countries and even Florida blacks (IR: 4.4, 95%CI = 3.9–5.0).
Conclusion
We found heterogeneity in IRs for HNC among these diverse black populations; notably, Kenya which had distinctively higher incidence of nasopharyngeal and female oral cavity cancer. Targeted etiological investigations are warranted considering the low consumption of tobacco and alcohol among Kenyan women. Overall, our findings suggest that behavioral and environmental factors are more important determinants of HNC than race.";Aviane Auguste and Samuel Gathere and Paulo S. Pinheiro and Clement Adebamowo and Adeola Akintola and Kellie Alleyne-Mike and Simon G. Anderson and Kimlin Ashing and Fred Kwame Awittor and Baffour Awuah and Bernard Bhakkan and Jacqueline Deloumeaux and Maira du Plessis and Ima-Obong A. Ekanem and Uwemedimbuk Ekanem and Emmanuel Ezeome and Nkese Felix and Andrew K. Gachii and Stanie Gaete and Tracey Gibson and Robert Hage and Sharon Harrison and Festus Igbinoba and Kufre Iseh and Evans Kiptanui and Ann Korir and Heather-Dawn Lawson-Myers and Adana Llanos and Daniele Luce and Dawn McNaughton and Michael Odutola and Abidemi Omonisi and Theresa Otu and Jessica Peruvien and Nasiru Raheem and Veronica Roach and Natasha Sobers and Nguundja Uamburu and Camille Ragin;"Cancer Research (Q2); Epidemiology (Q2); Oncology (Q2)";449.0;270.0;Netherlands;2009-2020;10.1016/j.canep.2021.102053;0.00808;75.0;;18777821;18777821;1877783X;18777821;2.984;Head and neck cancer, Incidence, Blacks, Tobacco smoking, Alcohol drinking, HPV, Caribbean, Africa, USA, Population-based cancer registry;Elsevier BV;;3385.0;Western Europe;1156.0;Q2;17700155032.0;Cancer epidemiology;Heterogeneity in head and neck cancer incidence among black populations from africa, the caribbean and the usa: analysis of cancer registry data by the ac3;4347.0;1241.0;163.0;465.0;5517.0;journal;article;2021
"Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.";Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms;"Neurology (clinical) (Q2); Surgery (Q2)";6927.0;178.0;United States;2010-2020;10.1016/j.wneu.2022.02.113;0.04375;95.0;;18788750;18788769;18788750;18788769;2.104;Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS;Elsevier Inc.;;2422.0;Northern America;734.0;Q2;19700171401.0;World neurosurgery;The national inpatient sample: a primer for neurosurgical big data research and systematic review;23506.0;14126.0;2675.0;7491.0;64788.0;journal;article;2022
The Adolescent Brain Cognitive Development (ABCD) study is designed to be the largest study of brain development and child health in the United States, performing comprehensive assessments of 11,500 children repeatedly for 10 years. An endeavor of this magnitude requires an organized framework of governance and communication that promotes collaborative decision-making and dissemination of information. The ABCD consortium structure, built upon the Matrix Management approach of organizational theory, facilitates the integration of input from all institutions, numerous internal workgroups and committees, federal partners, and external advisory groups to make use of a broad range of expertise to ensure the study’s success.;Allison M. Auchter and Margie {Hernandez Mejia} and Charles J. Heyser and Paul D. Shilling and Terry L. Jernigan and Sandra A. Brown and Susan F. Tapert and Gayathri J. Dowling;Cognitive Neuroscience (Q1);304.0;636.0;Netherlands;2011-2020;10.1016/j.dcn.2018.04.003;0.01116;64.0;;18789293;18789293;18789307;18789293;6.464;Adolescence, Development, Neuroimaging, Longitudinal, Organizational framework, Governance;Elsevier BV;;7408.0;Western Europe;2662.0;Q1;19700187601.0;Developmental cognitive neuroscience;A description of the abcd organizational structure and communication framework;4477.0;2037.0;123.0;313.0;9112.0;journal;article;2018
;;"Geriatrics and Gerontology (Q2); Oncology (Q2)";309.0;286.0;United Kingdom;2010-2020;10.1016/S1879-4068(16)30162-X;0.00454;36.0;;18794068;18794068;18794076;18794068;3.599;;Elsevier Ltd.;;3386.0;Western Europe;1032.0;Q2;19700175259.0;Journal of geriatric oncology;Siog 2016 – abstract submission – poster presentations;2422.0;1040.0;270.0;347.0;9142.0;journal;article;2016
"Purpose
A database in which patient data are compiled allows analytic opportunities for continuous improvements in treatment quality and comparative effectiveness research. We describe the development of a novel, web-based system that supports the collection of complex radiation treatment planning information from centers that use diverse techniques, software, and hardware for radiation oncology care in a statewide quality collaborative, the Michigan Radiation Oncology Quality Consortium (MROQC).
Methods and materials
The MROQC database seeks to enable assessment of physician- and patient-reported outcomes and quality improvement as a function of treatment planning and delivery techniques for breast and lung cancer patients. We created tools to collect anonymized data based on all plans.
Results
The MROQC system representing 24 institutions has been successfully deployed in the state of Michigan. Since 2012, dose-volume histogram and Digital Imaging and Communications in Medicine-radiation therapy plan data and information on simulation, planning, and delivery techniques have been collected. Audits indicated >90% accurate data submission and spurred refinements to data collection methodology.
Conclusions
This model web-based system captures detailed, high-quality radiation therapy dosimetry data along with patient- and physician-reported outcomes and clinical data for a radiation therapy collaborative quality initiative. The collaborative nature of the project has been integral to its success. Our methodology can be applied to setting up analogous consortiums and databases.";Jean M. Moran and Mary Feng and Lisa A. Benedetti and Robin Marsh and Kent A. Griffith and Martha M. Matuszak and Michael Hess and Matthew McMullen and Jennifer H. Fisher and Teamour Nurushev and Margaret Grubb and Stephen Gardner and Daniel Nielsen and Reshma Jagsi and James A. Hayman and Lori J. Pierce;"Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)";402.0;238.0;Netherlands;2011-2020;10.1016/j.prro.2016.10.002;0.00629;35.0;;18798500;18798500;18798519;18798500;3.539;;Elsevier BV;;2387.0;Western Europe;1142.0;Q1;19900194500.0;Practical radiation oncology;Development of a model web-based system to support a statewide quality consortium in radiation oncology;2610.0;1179.0;150.0;444.0;3580.0;journal;article;2017
Agriculture remains a vital sector for most countries. It presents the main source of food for the population of the world. However, it faces a big challenge: producing more and better while increasing the sustainability with a reasonable use of natural resources, reducing environmental degradation as well as adapting to climate change. Hence, it is extremely important to switch from traditional agricultural methods to modern agriculture. Smart Agriculture is one of the solutions to deal with the growing demand for food while meeting sustainability requirements. In Smart Agriculture, the role of information is increasing. Information on weather conditions, soils, diseases, insects, seeds, fertilizers, etc. constitutes an important contribution to the economic and sustainable development of this sector. Smart management consists of collecting, transmitting, selecting and analyzing data. As the amount of agricultural data increases significantly, robust analytical techniques capable of processing and analyzing large amounts of data to obtain more reliable information and much more accurate predictions are essential. Data Mining is expected to play an important role in Smart Agriculture for managing real-time data analysis with massive data. The aim of this paper is to review ongoing studies and research on smart agriculture using the recent practice of Data Mining, to solve a variety of agricultural problems.;Hassina {Ait Issad} and Rachida Aoudjit and Joel J.P.C. Rodrigues;"Chemical Engineering (miscellaneous) (Q3); Food Science (Q3); Industrial and Manufacturing Engineering (Q3)";133.0;179.0;Japan;2008-2020;10.1016/j.eaef.2019.11.003;;18.0;;18818366;18818366;18818366;18818366;;Data mining, Smart agriculture, Precision agriculture, IoT, WSN;Asian Agricultural and Biological Engineering Association;;2400.0;Asiatic Region;267.0;Q3;19700201518.0;Engineering in agriculture, environment and food;A comprehensive review of data mining techniques in smart agriculture;;241.0;3.0;133.0;72.0;journal;article;2019
;Jessica Spence and C. David Mazer;"Anesthesiology and Pain Medicine (Q2); Medicine (miscellaneous) (Q2)";168.0;139.0;United States;2006-2020;10.1016/j.anclin.2019.08.008;;53.0;;19322275;19322275;19322275;19322275;;Cardiothoracic anesthesia, Research, Trial design, Future;W.B. Saunders Ltd;;5121.0;Northern America;596.0;Q2;4700152761.0;Anesthesiology clinics;The future directions of research in cardiac anesthesiology;;379.0;70.0;204.0;3585.0;journal;article;2019
Recent technologic advancements have enabled the creation of portable, low-cost, and unobtrusive sensors with tremendous potential to alter the clinical practice of rehabilitation. The application of wearable sensors to track movement has emerged as a promising paradigm to enhance the care provided to patients with neurologic or musculoskeletal conditions. These sensors enable quantification of motor behavior across disparate patient populations and emerging research shows their potential for identifying motor biomarkers, differentiating between restitution and compensation motor recovery mechanisms, remote monitoring, telerehabilitation, and robotics. Moreover, the big data recorded across these applications serve as a pathway to personalized and precision medicine. This article presents state-of-the-art and next-generation wearable movement sensors, ranging from inertial measurement units to soft sensors. An overview of clinical applications is presented across a wide spectrum of conditions that have potential to benefit from wearable sensors, including stroke, movement disorders, knee osteoarthritis, and running injuries. Complementary applications enabled by next-generation sensors that will enable point-of-care monitoring of neural activity and muscle dynamics during movement also are discussed.;Franchino Porciuncula and Anna Virginia Roto and Deepak Kumar and Irene Davis and Serge Roy and Conor J. Walsh and Louis N. Awad;"Medicine (miscellaneous) (Q2); Physical Therapy, Sports Therapy and Rehabilitation (Q2); Rehabilitation (Q2); Neurology (Q3); Neurology (clinical) (Q3); Sports Science (Q3)";620.0;164.0;United States;2009-2020;10.1016/j.pmrj.2018.06.013;;66.0;;19341482;19341482;19341563;19341482;;;Elsevier Inc.;;3019.0;Northern America;617.0;Q2;15800154701.0;Pm and r;Wearable movement sensors for rehabilitation: a focused review of technological and clinical advances;;1273.0;253.0;686.0;7638.0;journal;article;2018
"Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.";De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao;"Information Systems (Q3); Information Systems and Management (Q3)";73.0;289.0;United States;2009-2020;10.1145/2992787;;23.0;;19361955;19361963;19361955;19361963;;offline and online cleaning, Data quality, statistical data cleaning;Association for Computing Machinery (ACM);Association for Computing Machinery;4388.0;Northern America;273.0;Q3;19700186884.0;Journal of data and information quality;Bayeswipe: a scalable probabilistic framework for improving data quality;;197.0;17.0;76.0;746.0;journal;article;2016
;Suzanne McDermott and Margaret A. Turk;"Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)";278.0;229.0;United States;2008-2020;10.1016/j.dhjo.2015.04.003;0.00403;36.0;;19366574;19366574;18767583;19366574;2.554;;Elsevier Inc.;;3460.0;Northern America;967.0;Q1;11300153406.0;Disability and health journal;What are the implications of the big data paradigm shift for disability and health?;2139.0;796.0;132.0;307.0;4567.0;journal;article;2015
;;"Cardiology and Cardiovascular Medicine (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";422.0;457.0;United States;2008-2020;10.1016/S1936-878X(20)31073-1;;120.0;;1936878X;1936878X;18767591;1936878X;;;Elsevier Inc.;;2028.0;Northern America;5790.0;Q1;11300153407.0;Jacc: cardiovascular imaging;Full issue pdf;;4889.0;481.0;1051.0;9756.0;journal;article;2021
Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.;Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems and Management (Q1)";261.0;491.0;United States;2008-2020;10.1109/TSC.2016.2633264;0.0057799999999999995;70.0;;19391374;19391374;19391374;19391374;8.216;"Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks";Institute of Electrical and Electronics Engineers Inc.;;2618.0;Northern America;1207.0;Q1;18300156728.0;Ieee transactions on services computing;Learning the evolution regularities for bigservice-oriented online reliability prediction;3719.0;1950.0;141.0;364.0;3692.0;journal;article;2019
Artificial intelligence (AI) has the potential to impact almost every aspect of health care, from detection to prediction and prevention. The adoption of new technologies in health care, however, lags far behind the emergence of new technologies. Health care professionals and organizations must be prepared to change and evolve to adopt these new technologies. A basic understanding of emerging AI technologies will be essential for all health care professionals. These technologies include expert systems, robotic process automation, natural language processing, machine learning, and deep learning. Health care professionals and organizations must build their capacity and capabilities to understand and appropriately adopt these technologies. This understanding starts with basic AI literacy, including data governance principles, basic statistics, data visualization, and the impact on clinical processes. Health care professionals and organizations will need to overcome several challenges and tackle core structural issues, such as access to data and the readiness of algorithms for clinical practice. However, health care professionals have an opportunity to shape the way that AI will be used and the outcomes that will be achieved. There is an urgent and emerging need for education and training so that appropriate technologies can be rapidly adopted, resulting in a healthier world for our patients and our communities.;David Wiljer and Zaki Hakim;"Radiological and Ultrasound Technology (Q3); Radiology, Nuclear Medicine and Imaging (Q3)";179.0;113.0;United States;2008-2020;10.1016/j.jmir.2019.09.010;;12.0;;19398654;18767982;19398654;18767982;;Artificial intelligence, digital healthcare, AI-enabled health professions education, professional development, AI literacy;Elsevier Inc.;;1669.0;Northern America;297.0;Q3;12100155517.0;Journal of medical imaging and radiation sciences;Developing an artificial intelligence–enabled health care practice: rewiring health care professions for better care;;298.0;175.0;244.0;2921.0;journal;article;2019
Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.;Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime;"Aerospace Engineering (Q1); Applied Mathematics (Q1); Automotive Engineering (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1)";3060.0;780.0;United States;1967-2020;10.1109/TVT.2020.2991372;0.05223;178.0;;19399359;00189545;19399359;00189545;5.978;"Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet";Institute of Electrical and Electronics Engineers Inc.;;3495.0;Northern America;1365.0;Q1;17393.0;Ieee transactions on vehicular technology;Trading: traffic aware data offloading for big data enabled intelligent transportation system;36492.0;23276.0;1427.0;3074.0;49867.0;journal;article;2020
"ABSTRACT
To develop effective strategies for the supply of shared parking and study various theoretical choice models under uncertainty, this paper investigates private parking space owners’ propensity to engage in shared parking schemes using a stated choice experiment that involves an uncertain key attribute. A hybrid expected utility-regret model incorporating rejoice is specified to explore the participation behavior. Equivalent models considering the perception of attribute differences are also estimated. Results show that socio-demographic characteristics, social influence, government’s role, media attention, platform fee, and revenues are all important factors explaining private parking owners’ propensity to engage in shared parking schemes. Besides, the model incorporating all these components, including the emotions of regret and rejoice and the perception of attribute differences, yields the best results. These findings could help promote the policy development toward increasing people’s engagement in shared parking.";Qianqian Yan and Tao Feng and Harry Timmermans;Transportation (Q2);104.0;337.0;United Kingdom;2009-2020;10.1080/19427867.2022.2088568;;23.0;;19427867;19427867;19427875;19427867;;Shared parking, owners, propensity, hybrid expected utility-regret-rejoice models, perception;Maney Publishing;;3851.0;Western Europe;818.0;Q2;21100255479.0;Transportation letters;Private owners’ propensity to engage in shared parking schemes under uncertainty: comparison of alternate hybrid expected utility-regret-rejoice choice models;;367.0;127.0;107.0;4891.0;journal;article;2022
DNA methylation (5mC) and hydroxymethylation (5hmC) are chemical modifications of cytosine bases which play a crucial role in epigenetic gene regulation. However, cost, data complexity and unavailability of comprehensive analytical tools is one of the major challenges in exploring these epigenetic marks. Hydroxymethylation-and Methylation-Sensitive Tag sequencing (HMST-seq) is one of the most cost-effective techniques that enables simultaneous detection of 5mC and 5hmC at single base pair resolution. We present HMST-Seq-Analyzer as a comprehensive and robust method for performing simultaneous differential methylation analysis on 5mC and 5hmC data sets. HMST-Seq-Analyzer can detect Differentially Methylated Regions (DMRs), annotate them, give a visual overview of methylation status and also perform preliminary quality check on the data. In addition to HMST-Seq, our tool can be used on whole-genome bisulfite sequencing (WGBS) and reduced representation bisulfite sequencing (RRBS) data sets as well. The tool is written in Python with capacity to process data in parallel and is available at (https://hmst-seq.github.io/hmst/).;Amna Farooq and Sindre Grønmyr and Omer Ali and Torbjørn Rognes and Katja Scheffler and Magnar Bjørås and Junbai Wang;"Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2)";255.0;789.0;Sweden;2012-2020;10.1016/j.csbj.2020.09.038;0.00677;45.0;;20010370;20010370;20010370;20010370;7.271;Methylation analysis, Hydroxy methylation, Differential methylation, Hydroxymethylation-and methylation-sensitive tag sequencing, Whole genome bisulfite sequencing;Research Network of Computational and Structural Biotechnology;;7869.0;Western Europe;1908.0;Q1;21100318415.0;Computational and structural biotechnology journal;Hmst-seq-analyzer: a new python tool for differential methylation and hydroxymethylation analysis in various dna methylation sequencing data;3620.0;2022.0;357.0;255.0;28093.0;journal;article;2020
With advanced artificial intelligence and deep learning techniques, a growing number of data sources are playing more and more critical roles in planning and operating transportation services. The General Transit Feed Specification (GTFS), with standard open-source data in both static and real-time formats, is being widely used in public transport planning and operation management. However, compared to other extensively studied data sources such as smart card data and GPS trajectory data, the GTFS data lacks proper investigation yet. Utilization of the GTFS data is challenging for both transport planners and researchers due to its difficulty and complexity of understanding, processing, and leveraging the raw data. In this paper, a GTFS data acquisition and processing framework is proposed to offer an efficient and effective benchmark tool for converting and fusing the GTFS data to a ready-to-use format. To validate and test the proposed framework, a multivariate multistep Long Short-Term Memory is developed to predict train delay with minor anomaly in Sydney as a case study. The contribution of this new framework will render great potential for broader applications and deeper research.;Jianqing Wu and Bo Du and Zengyang Gong and Qiang Wu and Jun Shen and Luping Zhou and Chen Cai;"Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Management, Monitoring, Policy and Law (Q1); Transportation (Q1)";85.0;329.0;Netherlands;2012-2020;10.1016/j.ijtst.2022.01.005;;18.0;;20460430;20460430;20460449;20460430;;General transit feed specification, Delay prediction, Train delay, Long short-term memory, Data fusion;Elsevier BV;;3558.0;Western Europe;1133.0;Q1;21100934655.0;International journal of transportation science and technology;A gtfs data acquisition and processing framework and its application to train delay prediction;;384.0;50.0;92.0;1779.0;journal;article;2022
"ABSTRACT
The European Society for Medical Oncology (ESMO) and the American Society of Clinical Oncology (ASCO) are publishing a new edition of the ESMO/ASCO Global Curriculum (GC) thanks to contribution of 64 ESMO-appointed and 32 ASCO-appointed authors. First published in 2004 and updated in 2010, the GC edition 2016 answers to the need for updated recommendations for the training of physicians in medical oncology by defining the standard to be fulfilled to qualify as medical oncologists. At times of internationalisation of healthcare and increased mobility of patients and physicians, the GC aims to provide state-of-the-art cancer care to all patients wherever they live. Recent progress in the field of cancer research has indeed resulted in diagnostic and therapeutic innovations such as targeted therapies as a standard therapeutic approach or personalised cancer medicine apart from the revival of immunotherapy, requiring specialised training for medical oncology trainees. Thus, several new chapters on technical contents such as molecular pathology, translational research or molecular imaging and on conceptual attitudes towards human principles like genetic counselling or survivorship have been integrated in the GC. The GC edition 2016 consists of 12 sections with 17 subsections, 44 chapters and 35 subchapters, respectively. Besides renewal in its contents, the GC underwent a principal formal change taking into consideration modern didactic principles. It is presented in a template-based format that subcategorises the detailed outcome requirements into learning objectives, awareness, knowledge and skills. Consecutive steps will be those of harmonising and implementing teaching and assessment strategies.";Christian Dittrich and Michael Kosty and Svetlana Jezdic and Doug Pyle and Rossana Berardi and Jonas Bergh and Nagi El-Saghir and Jean-Pierre Lotz and Pia Österlund and Nicholas Pavlidis and Gunta Purkalne and Ahmad Awada and Susana Banerjee and Smita Bhatia and Jan Bogaerts and Jan Buckner and Fatima Cardoso and Paolo Casali and Edward Chu and Julia Lee Close and Bertrand Coiffier and Roisin Connolly and Sarah Coupland and Luigi {De Petris} and Maria {De Santis} and Elisabeth G.E. {de Vries} and Don S. Dizon and Jennifer Duff and Linda R. Duska and Alexandru Eniu and Marc Ernstoff and Enriqueta Felip and Martin F. Fey and Jill Gilbert and Nicolas Girard and Andor W.J.M. Glaudemans and Priya K. Gopalan and Axel Grothey and Stephen M. Hahn and Diana Hanna and Christian Herold and Jørn Herrstedt and Krisztian Homicsko and Dennie V. Jones and Lorenz Jost and Ulrich Keilholz and Saad Khan and Alexander Kiss and Claus-Henning Köhne and Rainer Kunstfeld and H.einz-Josef Lenz and Stuart Lichtman and Lisa Licitra and Thomas Lion and Saskia Litière and Lifang Liu and Patrick J. Loehrer and Merry Jennifer Markham and Ben Markman and Marius Mayerhoefer and Johannes G. Meran and Olivier Michielin and Elizabeth Charlotte Moser and Giannis Mountzios and Timothy Moynihan and Torsten Nielsen and Yuichiro Ohe and Kjell Öberg and Antonio Palumbo and Fedro Alessandro Peccatori and Michael Pfeilstöcker and Chandrajit Raut and Scot C. Remick and Mark Robson and Piotr Rutkowski and Roberto Salgado and Lidia Schapira and Eva Schernhammer and Martin Schlumberger and Hans-Joachim Schmoll and Lowell Schnipper and Cristiana Sessa and Charles L. Shapiro and Julie Steele and Cora N. Sternberg and Friedrich Stiefel and Florian Strasser and Roger Stupp and Richard Sullivan and Josep Tabernero and Luzia Travado and Marcel Verheij and Emile Voest and Everett Vokes and Jamie {Von Roenn} and Jeffrey S. Weber and Hans Wildiers and Yosef Yarden;"Cancer Research (Q1); Oncology (Q1)";235.0;443.0;United Kingdom;2016-2020;10.1136/esmoopen-2016-000097;0.00688;31.0;;20597029;20597029;20597029;20597029;6.540;Global curriculum, clinical training, medical oncology, didactic principles, learning objectives;BMJ Publishing Group;;2891.0;Western Europe;2409.0;Q1;21100873339.0;Esmo open;Esmo / asco recommendations for a global curriculum in medical oncology edition 2016;2452.0;1372.0;223.0;301.0;6446.0;journal;article;2016
"ABSTRACT
Sports injuries, trauma and the globally ageing and obese population require increasing levels of knee surgery. Shared decision making has replaced the paternalistic approach to patient management. Evidence-based medicine underpins surgical treatment strategies, from consenting an individual patient to national healthcare system design. The evolution of successful knee-related registries starting from specific arthroplasty registries has given rise to ligament reconstruction, osteotomy and cartilage surgery registries developing as platforms for surgical outcome data collection. Stakeholders include surgeons and their patients, researchers, healthcare systems, as well as the funding insurers and governments. Lately, implant manufacturers have also been mandated to perform postmarket surveillance with some hoping to base that on registry data. Aiming to assess the current status of knee-related registries, we performed a comprehensive literature and web search, which yielded 23 arthroplasty, 8 ligament, 4 osteotomy and 3 articular cartilage registries. Registries were evaluated for their scope, measured variables, impact and limitations. Registries have many advantages as they aim to increase awareness of outcomes; identify trends in practice over time, early failing implants, outlier surgeon or institution performance; and assist postmarketing surveillance. International collaborations have highlighted variations in practice. The limitations of registries are discussed in detail. Inconsistencies are found in collected data and measured variables. Potential measurement and selection biases are outlined. Without mandated data collection and with apparent issues such as unverified patient reporting of complications, registries are not designed to replace adverse event recording in place of a proper safety and efficacy study, as demanded by regulators. Registry ‘big data’ can provide evidence of associations of problems. However, registries cannot provide evidence of causation. Hence, without careful consideration of the data and its limitations, registry data are at risk of incorrectly drawn conclusions and the potential of misuse of the results. That must be guarded against. Looking at the future, registry operators benefit from a collective experience of running registries as they mature, allowing for improvements across specialties. Large-scale registries are not only of merit, improving with stakeholder acceptance, but also are critical in furthering our understanding of our patients’ outcomes. In doing so, they are a critical element for our future scientific discourse.";Eran {Beit Ner} and Norimasa Nakamura and Christian Lattermann and Michael James McNicholas;"Surgery (Q2); Orthopedics and Sports Medicine (Q3)";16.0;72.0;United Kingdom;2019-2020;10.1136/jisakos-2021-000625;;3.0;;20597754;20597762;20597754;20597762;;knee injuries, anterior cruciate ligament, arthroplasty, replacement, patient outcome assessment, osteotomy, articular cartilage restoration, registry, post marketing surveillance;BMJ Publishing Group;;3840.0;Western Europe;539.0;Q2;21100921027.0;Journal of isakos;Knee registries: state of the art;;13.0;57.0;18.0;2189.0;journal;article;2022
Rolling bearing fault detection is critical for improving production efficiency and lowering accident rates in complicated mechanical systems, as well as huge monitoring data, posing significant challenges to present fault diagnostic technology. Deep Learning is now an extraordinarily popular research topic in the field and a promising approach for detecting intelligent bearing faults. This paper aims to give a comprehensive overview of Deep Learning (DL) based on bearing fault diagnosis. The most widely used DL algorithms for detecting bearing faults include Convolutional Neural Network, Recurrent neural network, Autoencoder, and Generative Adversarial Network. It discusses a variety of transfer learning architectures and relevant theories while summarises, classifies, and explains several publications on the subject. The research area’s applications and problems are also addressed.;Mohammed Hakim and Abdoulhdi A. Borhana Omran and Ali Najah Ahmed and Muhannad Al-Waily and Abdallah Abdellatif;Engineering (miscellaneous) (Q2);475.0;376.0;Egypt;2010-2020;10.1016/j.asej.2022.101945;0.00343;46.0;;20904479;20904479;20904479;20904479;3.180;Rolling bearing, Deep learning, Transfer learning, Fault diagnosis, Systematic review;Ain Shams University;;4083.0;Africa/Middle East;505.0;Q2;19700200705.0;Ain shams engineering journal;A systematic review of rolling bearing fault diagnoses based on deep learning and transfer learning: taxonomy, overview, application, open challenges, weaknesses and recommendations;3403.0;1849.0;169.0;475.0;6901.0;journal;article;2022
This paper acknowledges the contemporary neoliberal mode of operation of Smart Cities. The pitfalls of Smart Cities concerning its propensity towards techno-centric and efficiency-focused governance are identified, with diminutive emphasis on social equity and human-centric urban growth. Thus, the paper elaborates upon an alternative mode of person-environment-interaction based approach towards placemaking: Empathic Cities. This approach implies embracing a shift from efficiency to sufficiency and wellbeing embedded regenerative perspective for conceiving the built environment. First, the variable dimensions of urban growth and governance, which gave rise to the smart city, are contextualized. The embedded neoliberal operational agenda of smart cities are established. On this basis, the underpinnings of an empathic city are established by acknowledging the shift from techno-centric to human-centric and from product-based to context-based smart city and wellbeing perspectives. Strategies toward urban development are proposed, such as embracing a regenerative perspective wherein the city and its constituents need to be understood as interdependent systemic elements while embracing a human-centric and ethical approach. Additionally, a transition from efficiency to sufficiency-oriented practices and a shift towards inclusive modes of participatory governance are proposed as fundamental principles for an empathic future of the built environment.;Nimish Biloria;"Archeology (Q1); Architecture (Q1); Building and Construction (Q2); Urban Studies (Q2)";146.0;181.0;Netherlands;2012-2020;10.1016/j.foar.2020.10.001;;24.0;;20952635;20952635;20952635;20952635;;Empathic city, Smart city, Wellbeing, Neoliberalism, Regenerative model;Elsevier BV;;4635.0;Western Europe;444.0;Q1;21100413834.0;Frontiers of architectural research;From smart to empathic cities;;342.0;51.0;146.0;2364.0;journal;article;2021
Traditional Chinese medicine, as a complementary and alternative medicine, has been practiced for thousands of years in China and possesses remarkable clinical efficacy. Thus, systematic analysis and examination of the mechanistic links between Chinese herbal medicine (CHM) and the complex human body can benefit contemporary understandings by carrying out qualitative and quantitative analysis. With increasing attention, the approach of network pharmacology has begun to unveil the mystery of CHM by constructing the heterogeneous network relationship of “herb-compound-target-pathway,” which corresponds to the holistic mechanisms of CHM. By integrating computational techniques into network pharmacology, the efficiency and accuracy of active compound screening and target fishing have been improved at an unprecedented pace. This review dissects the core innovations to the network pharmacology approach that were developed in the years since 2015 and highlights how this tool has been applied to understanding the coronavirus disease 2019 and refining the clinical use of CHM to combat it.;Yi-xuan Wang and Zhen Yang and Wen-xiao Wang and Yu-xi Huang and Qiao Zhang and Jia-jia Li and Yu-ping Tang and Shi-jun Yue;Complementary and Alternative Medicine (Q1);175.0;271.0;Singapore;2013-2020;10.1016/j.joim.2022.09.004;;31.0;;20954964;20954964;20954964;20954964;;Chinese traditional medicine, Herbal medicine, Network pharmacology, Compound identification, COVID-19;Science Press (China);;5410.0;Asiatic Region;564.0;Q1;21100258397.0;Journal of integrative medicine;Methodology of network pharmacology for research on chinese herbal medicine against covid-19: a review;;540.0;67.0;182.0;3625.0;journal;article;2022
Soil erosion is one of the most severe global environmental problems, and soil erosion surveys are the scientific basis for planning soil conservation and ecological development. To improve soil erosion sampling survey methods and accurately and rapidly estimate the actual rates of soil erosion, a Pan-Third Pole region was taken as an example to study a methodology of soil erosion sampling survey based on high-spatial-resolution remote sensing images. The sampling units were designed using a stratified variable probability systematic sampling method. The spatiotemporal characteristics of soil erosion and conservation were taken into account, and finer-resolution freely available and accessible images in Google Earth were used. Through the visual interpretation of the free high-resolution remote sensing images, detailed information on land use and soil conservation measures was obtained. Then, combined with the regional soil erosion factor data products, such as rainfall-runoff erosivity factor (R), soil erodibility factor (K), and slope length and steepness factor (LS), the soil loss rates of some sampling units were calculated. The results show that, based on these high-resolution remote sensing images, the land use and soil conservation measures of the sampling units can be quickly and accurately extracted. The interpretation accuracy in 4 typical cross sections was more than 80%, and sampling accuracy, described by histogram similarity in 11 large sampling sites, show that the landuse of sampling uints can represent the structural characteristics of regional land use. Based on the interpretation of data from the sample survey and the regional soil erosion factor data products, the calculation of the soil erosion rate can be completed quickly. The calculation results can reflect the actual conditions of soil erosion better than the potential soil erosion rates calculated by using the coarse-resolution remote sensing method.;Qinke Yang and Mengyang Zhu and Chunmei Wang and Xiaoping Zhang and Baoyuan Liu and Xin Wei and Guowei Pang and Chaozhen Du and Lihua Yang;"Agronomy and Crop Science (Q1); Nature and Landscape Conservation (Q1); Soil Science (Q1); Water Science and Technology (Q1)";113.0;660.0;Netherlands;2013-2020;10.1016/j.iswcr.2020.07.005;0.00235;29.0;;20956339;20956339;20956339;20956339;6.027;Pan -third pole area, Land use, Soil conservation measures, Remote sensing, Variable probability sampling;;;5291.0;Western Europe;1541.0;Q1;21100817619.0;International soil and water conservation research;Study on a soil erosion sampling survey in the pan-third pole region based on higher-resolution images;1612.0;775.0;47.0;113.0;2487.0;journal;article;2020
Road accidents are one of the most relevant causes of injuries and death worldwide, and therefore, they constitute a significant field of research on the use of advanced algorithms and techniques to analyze and predict traffic accidents and determine the most relevant elements that contribute to road accidents. The research of road accident prediction aims to respond to the challenge of offer tools to generate a more secure mobility environment, and ultimately, save lives. This paper aims to provide an overview of the state of the art in the prediction of road accidents through machine learning algorithms and advanced techniques for analyzing information, such as convolutional neural networks and long short-term memory networks, among other deep learning architectures. Furthermore, in this article, a compendium and study of the most used data sources for the road accident forecast is made. And a classification is proposed according to its origin and characteristics, such as open data, measurement technologies, onboard equipment and social media data. For the analysis of the information, the different algorithms employed to make predictions about road accidents are listed and compared, as well as their applicability depending on the types of data being analyzed, along with the results obtained and their ease of interpretation and analysis. The best results reported by the authors are obtained when two or more analytic techniques are combined, in such a way that analysis of the obtained results is strengthened. Among the future challenges in road traffic forecasting lies the enhancement of the scope of the proposed models and predictions by the incorporation of heterogeneous data sources, that include geo spatial data, information from traffic volume, traffic statistics, video, sound, text and sentiment from social media, that many authors concur that can improve the precision and accuracy of the analysis and predictions.;Camilo Gutierrez-Osorio and César Pedraza;"Civil and Structural Engineering (Q2); Transportation (Q2)";144.0;357.0;Netherlands;2014-2020;10.1016/j.jtte.2020.05.002;;26.0;;20957564;20957564;20957564;20957564;;Traffic engineering, Data analysis, Machine learning, Road accident forecasting, Traffic accident prediction;Elsevier BV;;5660.0;Western Europe;656.0;Q2;21100784267.0;Journal of traffic and transportation engineering (english edition);Modern data sources and techniques for analysis and forecast of road accidents: a review;;542.0;73.0;147.0;4132.0;journal;article;2020
The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t∙d−1, the coke ratio decreased 13.50 kg∙t−1, and the silicon content decreased by an average of 0.047%.;Heng Zhou and Chunjie Yang and Youxian Sun;"Chemical Engineering (miscellaneous) (Q1); Computer Science (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Materials Science (miscellaneous) (Q1)";324.0;673.0;United Kingdom;2015-2020;10.1016/j.eng.2021.04.022;0.00715;45.0;;20958099;20958099;20958099;20958099;7.553;Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation;Elsevier Ltd.;;5252.0;Western Europe;1376.0;Q1;21100780794.0;Engineering;Intelligent ironmaking optimization service on a cloud computing platform by digital twin;4023.0;3152.0;186.0;372.0;9769.0;journal;article;2021
;Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang;Multidisciplinary (Q1);616.0;721.0;Netherlands;2015-2020;10.1016/j.scib.2019.09.011;0.0164;112.0;;20959273;20959273;20959281;20959273;11.780;;Elsevier BV;;3508.0;Western Europe;1983.0;Q1;21100405003.0;Science bulletin;Opportunities and challenges of using big data for global health;8832.0;5639.0;365.0;833.0;12806.0;journal;article;2019
"In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the ""misjudgment rate"". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.";Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei;"Electrical and Electronic Engineering (Q4); Electronic, Optical and Magnetic Materials (Q4); Energy (miscellaneous) (Q4)";148.0;18.0;United States;2020;10.17775/CSEEJPES.2020.04080;0.0027;4.0;;20960042;20960042;20960042;20960042;3.938;"Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest";Institute of Electrical and Electronics Engineers Inc.;;3800.0;Northern America;118.0;Q4;21101017898.0;Csee journal of power and energy systems;A big data cleaning method based on improved clof and random forest for distribution network;1205.0;27.0;43.0;148.0;1634.0;journal;article;2020
Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.;Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang;"Control and Systems Engineering; Electrical and Electronic Engineering; Energy Engineering and Power Technology";0.0;0.0;United States;2020;10.23919/CJEE.2019.000025;;2.0;;20961529;20961529;20961529;20961529;;"Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality";Institute of Electrical and Electronics Engineers Inc.;;3180.0;Northern America;;-;21101018551.0;Chinese journal of electrical engineering;A missing power data filling method based on improved random forest algorithm;;0.0;35.0;0.0;1113.0;journal;article;2019
The hydrocarbon industry is considering a range of digital technologies to improve productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environmental risks, and variability in oil and gas project life cycles. Due to the emergence of industry 4.0 the improvement in performance, efficiency, and cost reduction, the hydrocarbon industry is gradually shifting towards solutions that are data-oriented. Understanding such complex systems involves the analysis of data from various sources at the same time. Digital Twin (DT) modelling is the foundation for the next generation of real-time production monitoring and optimization systems. It is a solution that boosts productivity by combining information, simulation, and visualization throughout the entire value chain of an operational firm, from subsurface equipment to central production plants. Oil and gas companies can majorly benefit from Hydrocarbon Exploration with the right use of such advanced technologies. This study focuses on the advancements in technology in the context of DT and how it has been used by the hydrocarbon industry. The study discusses about the emergence of the DT concept, various types, 5D representation, and tools for DT. Further, the study tries to implement fields of DT in hydrocarbon industry especially in the domains of exploration, drilling, and production. Challenges associated with DT strategy like accessibility, confidentiality integration, and maintenance are also discussed.;Anirbid Sircar and Abhishek Nair and Namrata Bist and Kriti Yadav;"Energy Engineering and Power Technology (Q2); Geology (Q2); Geochemistry and Petrology (Q3)";93.0;162.0;China;2016-2020;10.1016/j.ptlrs.2022.04.001;;8.0;;20962495;20962495;25241729;20962495;;Digital twin technology, Hydrocarbon exploration, Industry 4.0, Oil and gas, Digitalization, Automation;KeAi Publishing Communications Ltd.;;3013.0;Asiatic Region;478.0;Q2;21100981739.0;Petroleum research;Digital twin in hydrocarbon industry;;157.0;38.0;93.0;1145.0;journal;article;2022
Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.;Whang, Steven Euijong and Lee, Jae-Gil;Computer Science (miscellaneous) (Q1);566.0;550.0;United States;2008-2020;10.14778/3415478.3415562;0.00891;134.0;;21508097;21508097;21508097;21508097;2.047;;;VLDB Endowment;5041.0;Northern America;946.0;Q1;21100199855.0;Proceedings of the vldb endowment;Data collection and quality challenges for deep learning;7198.0;3759.0;246.0;598.0;12401.0;journal;article;2020
We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.;Palacio, Ana León and López, Óscar Pastor;"Computer Science Applications; Information Systems; Software";146.0;81.0;United States;2011, 2012, 2013, 2014, 2015;10.1109/RCIS.2019.8877003;;19.0;;21511357;21511357;21511349;21511357;;"Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method";;;0.0;Northern America;174.0;-;20300195007.0;Proceedings - international conference on research challenges in information science;Infoxication in the genomic data era and implications in the development of information systems;;116.0;0.0;150.0;0.0;conference and proceedings;inproceedings;2019
"Background: Pathologists and informaticians are becoming increasingly interested in electronic clinical decision support for pathology, laboratory medicine and clinical diagnosis. Improved decision support may optimize laboratory test selection, improve test result interpretation and permit the extraction of enhanced diagnostic information from existing laboratory data. Nonetheless, the field of pathology decision support is still developing. To facilitate the exchange of ideas and preliminary studies, we convened a symposium entitled: Pathology data integration and clinical decision support. Methods: The symposium was held at the Massachusetts General Hospital, on May 10, 2013. Participants were selected to represent diverse backgrounds and interests and were from nine different institutions in eight different states. Results: The day included 16 plenary talks and three panel discussions, together covering four broad areas. Summaries of each presentation are included in this manuscript. Conclusions: A number of recurrent themes emerged from the symposium. Among the most pervasive was the dichotomy between diagnostic data and diagnostic information, including the opportunities that laboratories may have to use electronic systems and algorithms to convert the data they generate into more useful information. Differences between human talents and computer abilities were described; well-designed symbioses between humans and computers may ultimately optimize diagnosis. Another key theme related to the unique needs and challenges in providing decision support for genomics and other emerging diagnostic modalities. Finally, many talks relayed how the barriers to bringing decision support toward reality are primarily personnel, political, infrastructural and administrative challenges rather than technological limitations.";Jason M. Baron and Anand S. Dighe and Ramy Arnaout and Ulysses J. Balis and W. Stephen Black-Schaffer and Alexis B. Carter and Walter H. Henricks and John M. Higgins and Brian R. Jackson and JiYeon Kim and Veronica E. Klepeis and Long P. Le and David N. Louis and Diana Mandelker and Craig H. Mermel and James S. Michaelson and Rakesh Nagarajan and Mihae E. Platt and Andrew M. Quinn and Luigi Rao and Brian H. Shirts and John R. Gilbertson;"Computer Science Applications (Q1); Health Informatics (Q1); Pathology and Forensic Medicine (Q1)";121.0;323.0;India;2012, 2016-2020;10.4103/2153-3539.126145;;17.0;;21533539;22295089;21533539;22295089;;Clinical decision support, genomics, interpretive reporting, machine learning, test utilization;Wolters Kluwer Medknow Publications;;2220.0;Asiatic Region;1009.0;Q1;21100791701.0;Journal of pathology informatics;The 2013 symposium on pathology data integration and clinical decision support and the current state of field;;412.0;15.0;133.0;333.0;journal;article;2014
Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.;Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel;"Hardware and Architecture; Software";23.0;304.0;United States;2012, 2013, 2019;10.1109/MiSE.2015.21;;19.0;;21567891;21567891;21572305;21567891;;"Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering";;;0.0;Northern America;197.0;-;21100211110.0;Icse workshop on software engineering for adaptive and self-managing systems;Dice: quality-driven development of data-intensive cloud applications;;79.0;0.0;26.0;0.0;conference and proceedings;inproceedings;2015
;Luxia Zhang and Ming-Hui Zhao and Li Zuo and Yue Wang and Feng Yu and Hong Zhang and Haibo Wang and Rui Chen and Hong Chu and Xinwei Deng and Lanxia Gan and Bixia Gao and Yifang Jiang and Lili Liu and Jianyan Long and Ying Shi and Zaiming Su and Xiaoyu Sun and Wen Tang and Fang Wang and Huai-Yu Wang and Jinwei Wang and Song Wang and Chao Yang and Dongliang Zhang and Xinju Zhao and Liren Zheng and Zhiye Zhou;Nephrology (Q1);22.0;854.0;United States;2011-2020;10.1016/j.kisu.2020.09.001;0.00197;38.0;;21571716;21571716;21571724;21571716;10.545;;Elsevier Inc.;;3917.0;Northern America;2855.0;Q1;21100258425.0;Kidney international supplements;China kidney disease network (ck-net) 2016 annual data report;3207.0;195.0;12.0;26.0;470.0;journal;article;2020
We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.;Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng;"Artificial Intelligence (Q1); Theoretical Computer Science (Q1)";192.0;980.0;United States;2010-2020;10.1145/3428080;0.00463;63.0;;21576904;21576912;21576904;21576912;4.654;charging scheduling, charging pattern, MDP, Electric bus, data driven;Association for Computing Machinery (ACM);Association for Computing Machinery;5920.0;Northern America;914.0;Q1;19700190323.0;Acm transactions on intelligent systems and technology;Pricing-aware real-time charging scheduling and charging station expansion for large-scale electric buses;4104.0;1489.0;70.0;199.0;4144.0;journal;article;2020
The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.;Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong;"Computer Vision and Pattern Recognition; Signal Processing";246.0;60.0;United States;2011, 2012, 2017, 2019;10.1109/WHISPERS.2015.8075416;;16.0;;21586276;21586276;21586276;21586276;;"Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data";;;0.0;Northern America;174.0;-;21000195601.0;Workshop on hyperspectral image and signal processing, evolution in remote sensing;Multiple stratified sampling strategy for assessing the big remote sensing products;;126.0;0.0;251.0;0.0;conference and proceedings;inproceedings;2015
Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.;Lukyanenko, Roman and Samuel, Binny M.;"Computer Science (miscellaneous) (Q1); Management Information Systems (Q2)";48.0;603.0;United States;2010-2020;10.1145/3131780;;29.0;;2158656X;21586578;2158656X;21586578;;Conceptual modeling, database design;Association for Computing Machinery (ACM);Association for Computing Machinery;5750.0;Northern America;603.0;Q1;21100200804.0;Acm transactions on management information systems;Are all classes created equal? increasing precision of conceptual modeling grammars;;243.0;26.0;50.0;1495.0;journal;article;2017
Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.;Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei;"Artificial Intelligence; Information Systems; Software";483.0;253.0;United States;2013, 2014, 2019;10.1109/CLOUD.2015.167;;29.0;;21596190;21596182;21596190;21596182;;"Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer architecture";;;2080.0;Northern America;352.0;-;21100291859.0;Ieee international conference on cloud computing, cloud;Big saas: the next step beyond big data;;1024.0;92.0;496.0;1914.0;conference and proceedings;inproceedings;2015
Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.;Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo;"Artificial Intelligence; Computational Theory and Mathematics; Computer Networks and Communications; Human-Computer Interaction";219.0;50.0;United States;2011, 2012, 2013, 2014, 2019;10.1109/ICMLC.2016.7872993;;18.0;;21601348;21601348;2160133X;21601348;;"Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining";;;0.0;Northern America;149.0;-;20300195054.0;Proceedings - international conference on machine learning and cybernetics;A dynamic data correction algorithm based on polynomial smooth support vector machine;;114.0;0.0;227.0;0.0;conference and proceedings;inproceedings;2016
With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.;Yang, Qiang;"Artificial Intelligence (Q3); Human-Computer Interaction (Q3)";78.0;337.0;United States;2011-2020;10.1145/3485875;0.00173;34.0;;21606455;21606463;21606455;21606463;2.137;privacy-preserving computing, Federated learning, blockchain, user privacy, responsible AI, data security, decentralized AI, machine learning;Association for Computing Machinery (ACM);Association for Computing Machinery;6913.0;Northern America;381.0;Q3;21100301601.0;Acm transactions on interactive intelligent systems;Toward responsible ai: an overview of federated learning for user-centered privacy-preserving computing;722.0;304.0;23.0;81.0;1590.0;journal;article;2021
Modern mobile devices are capable of running sophisticated, network-enabled applications exploiting a variety of sensors on a single low-cost piece of hardware. The electrical industry can benefit from these new platforms to automate existing processes and provide engineers and field crew with access to large amounts of complex data in real-time, anywhere in the world. The development of a standards-based application decouples the mobile client application from a single vendor or existing enterprise system, but requires a complex data integration architecture to support the use and exploitation of large amounts of data spread across multiple existing systems. The integration with a mobile application introduces new challenges when dealing with remote devices where data network communications cannot be relied on, especially under storm conditions, and the devices themselves are at risk of being lost or stolen. Addressing these challenges offers the potential to improve data quality, enable access to accurate, up-to-date information in the field and ultimately save a utility time and money.;McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough, N. and Stewart, E. M.;"Electrical and Electronic Engineering; Energy (miscellaneous); Engineering (miscellaneous)";211.0;86.0;United States;1994, 1996, 1999-2003, 2005-2006, 2012, 2014, 2016, 2018;10.1109/TDC.2014.6863306;;65.0;;21608563;21608563;21608555;21608563;;"Computer integrated manufacturing;Mobile communication;IEC standards;Logic gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data handling;Data visualization;CIM;Data integration;Big Data";Institute of Electrical and Electronics Engineers Inc.;;1256.0;Northern America;279.0;-;87075.0;Proceedings of the ieee power engineering society transmission and distribution conference;Data integration challenges for standards-compliant mobile applications;;182.0;160.0;212.0;2010.0;conference and proceedings;inproceedings;2014
Along with the development of the sensor system and sensor network, the wide applications of sensor networks have arisen at the historic moment. In reality, all kinds of sensors monitor every aspect of our life, which provides various services and brings the challenge: how to effectively integrate those distributed sensor resources and then can be used to find more advanced information or implement the sharing of resources are the big problems to be solved. Based on the framework of Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined with the function of web crawler, we study and find Sensor Observation Service (SOS) service which is the core components of the SWE then we design a system based on the web crawler technology and the Istituto Scienze della Terra Sensor Observation Service (Istsos) architecture. The design of sensor network technology integration architecture includes three parts. The layer of data access which is the lowest layer encapsulates the access to the database or other source of resources. The layer of business logic it provides the core operation of component which was named Request Operator, this layer is used for processing various requests from the lowest layer in order to return the classes of listening. The layer of web and the client is connected, which can provide some thin client of SOS. The published server includes the ability of new services creation, addition of new sensors and relative metadata, visualization, and manipulation of stored observations, registration of new measures and setting of system properties like observable properties and data quality codes. In order to get sensor data, web crawler technology is used in our research, which can make us get sensor data from the target website, and the standardized sensor data is gotten by filtering the original data and then the data is uploaded to the database of Istsos with the standardized format. At last, the implementation of SOS architecture has been configured. The test's results show that the integrated architecture of services can effectively obtain the required sensor data and display them graphically.;Yan Zhou and Haitian Xie;"Computer Networks and Communications (Q4); Computer Vision and Pattern Recognition (Q4); Electrical and Electronic Engineering (Q4); Geography, Planning and Development (Q4); Information Systems (Q4); Software (Q4)";209.0;30.0;United States;2013, 2016-2018;10.1109/GEOINFORMATICS.2015.7378670;;8.0;;2161024X;2161024X;21610258;2161024X;;"Service-oriented architecture;web crawler;sensor network;Sensor Observation Service (SOS);Tomact;Istsos";International Association of Chinese Professionals in Geographic Information Sciences;;0.0;Northern America;132.0;Q4;21100285411.0;International conference on geoinformatics;The integration technology of sensor network based on web crawler;;76.0;0.0;212.0;0.0;journal;inproceedings;2015
Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.;Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.;"Computer Networks and Communications; Computer Science Applications; Control and Systems Engineering; Electrical and Electronic Engineering; Hardware and Architecture; Signal Processing";507.0;62.0;United States;2001, 2011, 2013, 2019;10.1109/AICCSA47632.2019.9035250;;18.0;;21615330;21615330;21615322;21615330;;"Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark";;;2344.0;Northern America;164.0;-;21100198533.0;Proceedings of ieee/acs international conference on computer systems and applications, aiccsa;Assessing context-aware data consistency;;308.0;59.0;517.0;1383.0;conference and proceedings;inproceedings;2019
This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.;Hao, Jiao and Jinming, Chen and Yajuan, Guo;"Control and Systems Engineering; Electrical and Electronic Engineering; Energy Engineering and Power Technology";519.0;47.0;United States;2012, 2014;10.1109/CICED.2018.8592556;;10.0;;2161749X;2161749X;21617481;2161749X;;"Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis";;;0.0;Northern America;133.0;-;21100241614.0;China international conference on electricity distribution, ciced;Data-driven lean management for distribution network;;245.0;0.0;520.0;0.0;conference and proceedings;inproceedings;2018
Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.;Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon;"Computer Networks and Communications; Information Systems";0.0;0.0;United States;2012-2014;10.1109/ICTC52510.2021.9620761;;18.0;;21621233;21621241;21621233;21621241;;"Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection";IEEE Computer Society;;1137.0;Northern America;;-;21100226431.0;International conference on ict convergence;Mechanism of a big-data platform for residential heat energy consumption;;0.0;493.0;0.0;5606.0;conference and proceedings;inproceedings;2021
Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.;Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.;"Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Computer Science Applications (Q1); Software (Q1)";1107.0;1251.0;United States;2012-2020;10.1109/TNNLS.2020.3001602;0.04868;212.0;;21622388;2162237X;21622388;2162237X;10.451;"Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model";IEEE Computational Intelligence Society;;3746.0;Northern America;2882.0;Q1;21100235616.0;Ieee transactions on neural networks and learning systems;A wide-deep-sequence model-based quality prediction method in industrial process analysis;36361.0;14914.0;609.0;1117.0;22815.0;journal;article;2020
We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.;Wang, Weina and Ying, Lei and Zhang, Junshan;"Computational Mathematics (Q2); Computer Science (miscellaneous) (Q2); Economics and Econometrics (Q2); Marketing (Q2); Statistics and Probability (Q3)";51.0;433.0;United States;2013-2020;10.1145/3232863;;18.0;;21678375;21678375;21678383;21678375;;differential privacy, randomized response, Data collection;Association for Computing Machinery (ACM);Association for Computing Machinery;3758.0;Northern America;519.0;Q2;21100853526.0;Acm transactions on economics and computation;The value of privacy: strategic data subjects, incentive mechanisms, and fundamental limits;;196.0;24.0;55.0;902.0;journal;article;2018
Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.;Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton;"Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Human-Computer Interaction (Q1); Information Systems (Q1); Software (Q1)";997.0;1119.0;United States;2013-2020;10.1109/TCYB.2017.2685521;0.05214;124.0;;21682275;21682275;21682267;21682275;11.448;"Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service";IEEE Advancing Technology for Humanity;;3458.0;Northern America;3109.0;Q1;21100274221.0;Ieee transactions on cybernetics;Incorporation of efficient second-order solvers into latent factor models for accurate prediction of missing qos data;24753.0;13312.0;542.0;1065.0;18740.0;journal;article;2018
"Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. Lots of applications, across disparate domains, operate on huge amounts of data and offer great advantages both for business and research. According to analysts, cloud computing adoption is steadily increasing to support big data analyses and Spark is expected to take a prominent market position for the next decade. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This article presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The solution relies on an initial non-linear programming model formulation and a search space exploration based on simulation-optimization procedures. Spark application execution times are estimated by relying on a gamut of techniques, including machine learning, approximated analyses, and simulation. The benefits of the approach are evaluated on Microsoft Azure HDInsight and on a private cloud cluster based on POWER8 by considering the TPC-DS industry benchmark and SparkBench. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is in the range 4–29 percent while literature-based techniques present an average error in the range 6–63 percent. Moreover, in the second scenario, the proposed algorithms can address complex problems like computing the optimal redistribution of resources among tens of applications in less than a minute with an error of 8 percent on average. On the same considered tests, literature-based approaches obtain an average error of about 57 percent.";Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna, Danilo;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1)";223.0;485.0;United States;2013-2020;10.1109/TCC.2020.2985682;0.00415;49.0;;21687161;21687161;21687161;21687161;5.938;"Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational modeling;Optimization;Big data;quality of service;elastic resource provisioning;cluster management";Institute of Electrical and Electronics Engineers Inc.;;2307.0;Northern America;1075.0;Q1;21100338351.0;Ieee transactions on cloud computing;Optimal resource allocation of cloud-based spark applications;2658.0;1406.0;173.0;278.0;3991.0;journal;article;2022
Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.;Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin;"Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)";24200.0;448.0;United States;2013-2020;10.1109/ACCESS.2019.2896226;0.15395999999999999;127.0;;21693536;21693536;21693536;21693536;3.367;"Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs)";Institute of Electrical and Electronics Engineers Inc.;;4275.0;Northern America;587.0;Q1;21100374601.0;Ieee access;A survey of task allocation: contrastive perspectives from wireless sensor networks and mobile crowdsensing;105968.0;116691.0;18036.0;24267.0;771081.0;journal;article;2019
"Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.";Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon;"Computer Science (miscellaneous) (Q1); Electrical and Electronic Engineering (Q2)";169.0;450.0;United States;2011-2020;10.1016/j.suscom.2017.10.009;;27.0;;22105379;22105379;22105379;22105379;;Big data analytics, IoT, Energy harvesting;Elsevier USA;;4291.0;Northern America;591.0;Q1;19700201455.0;Sustainable computing: informatics and systems;Energy-harvesting based on internet of things and big data analytics for smart health monitoring;;766.0;78.0;178.0;3347.0;journal;article;2018
Fleet standardization, which refers to the homogeneity and harmonization of the fleet in terms of the number of manufacturers and models, as a strategy has been deployed across many sectors, including airline operations. Given that engines are substantial aircraft components in terms of both capital and operating cost, representing nearly 50% of the aerospace aftermarket, we investigate whether – and to what extent – standardization strategies in the context of engines can improve cost efficiency of the airline industry. Using engine data of 12,305 aircraft and financial data of years with high (2013/14) and low (2016/17) fuel prices, we apply bootstrapped Data Envelopment Analysis (DEA) followed by random effects panel regression analysis to 84 airlines across the globe. Our quantitative findings are validated by qualitative stakeholder interviews. Our results suggest that both airframe and engine commonality impact on airline cost efficiency, but that engine cost effects are significantly larger in magnitude than standard airframe cost effects. In contrast to current management practices focusing exclusively on airframe commonality and tactical activities (such as renegotiating supplier and maintenance contracts), we demonstrate that an engine standardization strategy (through procurement, storage, retirement, maintenance and spare part optimization) improves the cost competitiveness and efficiency of airlines in an OEM servitized world.;Rico Merkert;"Business and International Management (Q1); Decision Sciences (miscellaneous) (Q1); Economics, Econometrics and Finance (miscellaneous) (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1); Management Science and Operations Research (Q2); Transportation (Q2)";152.0;321.0;Netherlands;2011-2020;10.1016/j.rtbm.2022.100797;0.0018;32.0;;22105395;22105395;22105395;22105395;2.740;Fleet/engine standardization, Airline performance, DEA, Cost efficiency, Competitive advantage;Elsevier BV;;6101.0;Western Europe;835.0;Q1;20500195043.0;Research in transportation business and management;The impact of engine standardization on the cost efficiency of airlines;1337.0;557.0;118.0;163.0;7199.0;journal;article;2022
"A group of individual truckers can be regarded as a swarm intelligence system without central management. With the development of autonomous driving technology, trucker groups will be replaced by driverless vehicles. At that point, a swarm of truckers will become a swarm robotics system. Therefore, considering the design and control of an efficient swarm robotics system, it is essential to investigate the properties and model the behaviors of a swarm of truckers in advance. In this study, we probe the characteristics of both individual truckers and a swarm of truckers using trajectory data of truckers. First, the trajectory data were map matched based on the geographic scale of cities and administrative regions. Then, the properties of the division of labor, pattern formation, and swarm synchronization were obtained through an analysis of the spatiotemporal distribution of radius of gyration, travel distance, and the number of visited places. Because predicting the next visit locations of individuals of a swarm is a measure for modeling swarm behaviors, the prediction model can be used to predict future swarm robotics (driverless trucks) behaviors. Thus, we apply several machine learning models to predict the next locations of truckers. The results show that there are common characteristics and routines embodied in the behavior of the truckers; the swarm shows consistency and regularity. Moreover, the peak predictability of the entire group reached 94%, indicating that our model can predict the behavior of groups and individuals. Our findings provide basis supporting to the future efficient swarm robotics system.";Mi Gan and Qiujun Qian and Dandan Li and Yi Ai and Xiaobo Liu;"Computer Science (miscellaneous) (Q1); Mathematics (miscellaneous) (Q1)";368.0;860.0;Netherlands;2011-2021;10.1016/j.swevo.2021.100845;0.00662;65.0;;22106502;22106502;22106502;22106502;7.177;Swarm Intelligence, Prediction, Trucker Trajectory, Machine learning;Elsevier BV;;6271.0;Western Europe;1460.0;Q1;19900192513.0;Swarm and evolutionary computation;Capturing the swarm intelligence in truckers: the foundation analysis for future swarm robotics in road freight;5442.0;3354.0;100.0;374.0;6271.0;journal;article;2021
Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.;Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq;"Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)";1284.0;853.0;Netherlands;2011-2020;10.1016/j.scs.2018.04.014;0.01684;61.0;;22106707;22106707;22106707;22106707;7.587;Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city;Elsevier BV;;6215.0;Western Europe;1645.0;Q1;19700194105.0;Sustainable cities and society;Privacy preserving data by conceptualizing smart cities using midr-angelization;14373.0;10974.0;705.0;1286.0;43818.0;journal;article;2018
"Observational studies investigate a wide range of topics in multiple sclerosis research. This paper presents an overview of the various observational designs and their applications in clinical studies. Observational studies are well suited for making discoveries and assessing new explanations of phenomena, but less so for establishing causal relationships, due to confounding by indication (selection bias), co-morbidity, socio-economic or other factors. Whether observational findings are demonstrative, indicative or only suggestive, depends on the research question, whether and how the design fits this question, analytical techniques, and the quality of data. Observational studies may be cross-sectional vs. longitudinal, and prospective vs. retrospective. The term ‘retrograde’ is proposed to explicate that cross-sectional studies may obtain data that cover (long) preceding periods. Case reports and case series are usually based on accidental observations or routinely collected data. Cross-sectional studies, by simultaneously assessing clinical phenomena and external factors, enable the discovery and quantification of associations. In ecological studies the unit of analysis is population or group, and relationships on patient level cannot be established. A cohort study is a longitudinal study that investigates patients with a defining characteristic, e.g. diagnosis or specific treatment, by analyzing data acquired at various intervals. Prospective cohort studies use (some) data that are not yet available at the time the research is conceived, whereas in retrospective studies the data already exist. In a case-control study a representative group of patients with a specific clinical feature is compared with controls, and the frequencies at which an external factor, e.g. infection, has occurred in each group is compared; in a nested case-control study controls are drawn from a fully known cohort. Randomized controlled trial (RCT)-extension studies are informative because, due to RCT randomization, they are free from confounding by indication. Patient or disease registries are organised systems for the long-term collection of uniform data on a population that is defined by a particular disease, condition or exposure, with the purpose to study changes over time. In pharmacotherapeutic research, accidental observations of unexpected beneficial effects may lead to further research into a drug's efficacy in other conditions. Uncontrolled phase 1 studies investigate safety and dosing aspects. Observational studies are alternatives to RCTs when these are not feasible for ethical or practical reasons. Phase 4 observational studies play a crucial role in the evaluation of the effectiveness of treatments in daily practice, the validation of RCT-based side effect profiles, and the discovery of late occurring or rare, potentially life-threatening side effects. Combinations of multidisciplinary longitudinal data bases into large data sets enable the development of algorithms for personalized treatments. To improve the reporting of observational findings on treatment effectiveness, it is proposed that abstracts define the research question(s) the study was meant to answer, study design and analytical methods, and identify and quantify the patient population, treatment of interest, relevant outcomes and the study's strengths and limitations. The development of guidelines for Strengthening the Reporting of Observational Studies in Effectiveness Research (STROBER), as an extension of the guidelines used in epidemiology, is wanted.";Peter Joseph Jongen;"Medicine (miscellaneous) (Q2); Neurology (Q2); Neurology (clinical) (Q2)";838.0;305.0;United States;2012-2020;10.1016/j.msard.2019.07.006;0.008879999999999999;38.0;;22110348;22110356;22110348;22110356;4.339;Multiple sclerosis, Observational, Effectiveness, Disease-modifying drug, Pharmacotherapy;Elsevier;;2957.0;Northern America;870.0;Q2;20000195097.0;Multiple sclerosis and related disorders;Observational designs in clinical multiple sclerosis research: particulars, practices and potentialities;5292.0;2908.0;701.0;930.0;20727.0;journal;article;2019
Emerging techniques in deep learning have created exciting opportunities for next-generation electrochemical technologies. While deep learning has been revolutionizing many research fields, strategies for its implementation for electrocatalysis remain nascent. This Opinion calls on the electrocatalysis community to join together and introduce a paradigm shift by establishing standards for reporting and sharing data from electrocatalysis investigations. We speculate on a possible future where crowd-sourced and standardized data from experimental and computational researchers can be analyzed collectively to better understand fundamental electrochemistry, yielding unprecedented insights for the development of new electrocatalysts. We identify key barriers to realizing this opportunity and how they might be overcome.;John A Keith and James R McKone and Joshua D Snyder and Maureen H Tang;Energy (miscellaneous) (Q1);213.0;479.0;Netherlands;2011-2020;10.1016/j.coche.2022.100824;0.00398;47.0;;22113398;22113398;22113398;22113398;5.163;;Elsevier BV;;5279.0;Western Europe;1072.0;Q1;20500195210.0;Current opinion in chemical engineering;Deeper learning in electrocatalysis: realizing opportunities and addressing challenges;2795.0;1088.0;72.0;235.0;3801.0;journal;article;2022
Traditional Chinese medicine (TCM) has been an indispensable source of drugs for curing various human diseases. However, the inherent chemical diversity and complexity of TCM restricted the safety and efficacy of its usage. Over the past few decades, the combination of liquid chromatography with mass spectrometry has contributed greatly to the TCM qualitative analysis. And novel approaches have been continuously introduced to improve the analytical performance, including both the data acquisition methods to generate a large and informative dataset, and the data post-processing tools to extract the structure-related MS information. Furthermore, the fast-developing computer techniques and big data analytics have markedly enriched the data processing tools, bringing benefits of high efficiency and accuracy. To provide an up-to-date review of the latest techniques on the TCM qualitative analysis, multiple data-independent acquisition methods and data-dependent acquisition methods (precursor ion list, dynamic exclusion, mass tag, precursor ion scan, neutral loss scan, and multiple reaction monitoring) and post-processing techniques (mass defect filtering, diagnostic ion filtering, neutral loss filtering, mass spectral trees similarity filter, molecular networking, statistical analysis, database matching, etc.) were summarized and categorized. Applications of each technique and integrated analytical strategies were highlighted, discussion and future perspectives were proposed as well.;Yang Yu and Changliang Yao and De-an Guo;Pharmacology, Toxicology and Pharmaceutics (miscellaneous) (Q1);262.0;1062.0;Netherlands;2012, 2014-2020;10.1016/j.apsb.2021.02.017;9e-05;51.0;;22113835;22113843;22113835;22113843;11.413;Liquid chromatography−mass spectrometry, Qualitative analysis, Traditional Chinese medicine, Data acquisition, Data post-processing;Elsevier BV;;7633.0;Western Europe;1912.0;Q1;20700195026.0;Acta pharmaceutica sinica b;Insight into chemical basis of traditional chinese medicine based on the state-of-the-art techniques of liquid chromatography−mass spectrometry;6314.0;2616.0;202.0;278.0;15419.0;journal;article;2021
Environmental and ecological degradation are more prominent within lake-wetland ecosystem than any other ecosystem on Earth, especially in the highlands. The continued pressures of population growth and rapid urbanization on cultural ecosystem services (CES) provided by lake-wetland ecosystem in highlands present ongoing challenges to decision-makers and managers. In this paper, Social Values for Ecosystem Services model (SolVES) was used to map, quantify and assess CES in Dianchi lake basin (DLB) and Erhai lake basin (ELB) to understand the spatial dynamics of CES perceived by residents and tourists. After combining the field survey data with four environmental variables, our analysis shows that (1) The Maximum Value Index (M-VI) ranking of the three CES in descending order within DLB was aesthetic (M-VI = 10), recreation (M-VI = 10), cultural (M-VI = 8). (2) Different stakeholders (residents and tourists) had different perceptions of CES. Recreation value between residents and tourists had the same M-VI (M-VI = 10), but the M-VI of aesthetic value (M-VI = 10) perceived by tourists was higher than those perceived by residents. (3) The four environmental variables significantly influenced CES, especially distance to water and dominant landcover. (4) CES were transferable from DLB to ELB due to the potential transferability of Maxent's DLB models for each CES, and CES hotspots in ELB generated from SolVES were highly consistent with high kernel-density areas. In conclusion, SolVES incorporating CES can benefit the basin resource managers when seeking to integrate a social perspective into the resource management decision-making process. In particular, the involvement of various stakeholders ensures that they are not marginalized in environmental planning and management.;Jianfeng Pan and Yuewei Ma and Siqing Cai and Yan Chen and Yumei Chen;"Geography, Planning and Development (Q1); Management, Monitoring, Policy and Law (Q2)";130.0;324.0;Netherlands;2012-2020;10.1016/j.envdev.2022.100754;0.00193;31.0;;22114645;22114645;22114645;22114645;3.326;Cultural ecosystem services, Lake-wetland ecosystem, SolVES, Transfer value, Environmental variables;Elsevier BV;;6174.0;Western Europe;791.0;Q1;21100202110.0;Environmental development;Distribution patterns of lake-wetland cultural ecosystem services in highland;1702.0;509.0;80.0;158.0;4939.0;journal;article;2022
Climate change mitigation strategies are multifaceted and require collaboration among a range of stakeholder groups. The objective of this paper was to develop an overarching Renewable Energy and Energy Conservation Area Policy (REECAP) framework. The framework was developed based on a comprehensive literature review, in which seven principles for Renewable Energy and Energy Conservation Policies were identified. The paper also includes a case study to demonstrate an application of the REECAP framework. The novelty of the framework stems from its integration of carbon-energy-cash flows among different decision-making spheres, scales and area specific characteristics. The framework provides a mathematical understanding of how energy strategies can be transformed and optimised in a cost-effective manner by integrating stakeholders under a shared vision.;Abel S. Vieira and Rodney A. Stewart and Roberto Lamberts and Cara D. Beal;Energy (miscellaneous) (Q1);245.0;719.0;Netherlands;2012-2020;10.1016/j.esr.2020.100544;0.0034799999999999996;33.0;;2211467X;2211467X;2211467X;2211467X;6.425;REECAP framework, Carbon emission reduction, Renewable energy, Energy conservation, Carbon-energy-cash flows;Elsevier;;6401.0;Western Europe;1639.0;Q1;21100199818.0;Energy strategy reviews;Renewable energy and energy conservation area policy (reecap) framework: a novel methodology for bottom-up and top-down principles integration;2011.0;1645.0;98.0;247.0;6273.0;journal;article;2020
The National Database for Autism Research (NDAR) is a human-subject data repository on tens of thousands of research participants. Approved researchers have access to an unprecedented volume of item-level clinical, genomic, and imaging data. Data are shared quickly using both a common data standard and innovative tools for experiment definition, which provide the level of detail needed for efficient use of the repository. As described, early adopters have used it to conduct secondary data analysis. Now, with an ever-increasing volume of research data being made available, and new methods for data query, data download, and computation in place, this initiative is becoming vital to those interested in scientific discovery in autism or is being used as a model by other research communities.;S.I. Novikova and D.M. Richman and K. Supekar and L. Barnard-Brak and D. Hall;"Developmental and Educational Psychology (Q3); Psychiatry and Mental Health (Q3)";34.0;79.0;Netherlands;2011-2020;10.1016/B978-0-12-407760-7.00003-7;0.00032;14.0;;22116095;22116095;22116095;22116095;0.889;Secondary analysis, Data respository, Database, Autism, Developmental disabilities;Elsevier BV;Academic Press;7643.0;Western Europe;345.0;Q3;21100216328.0;International review of research in developmental disabilities;Chapter three - ndar: a model federal system for secondary analysis in developmental disabilities research;205.0;45.0;14.0;44.0;1070.0;book series;incollection;2013
The evaluation of the spatial similarity of two observed point patterns is an important issue in spatial data quality assessment. In this work we propose a formal procedure that takes advantage of the joint use of space-filling curves and the multinomial model in order to establish a statistical test to compare spatial point patterns. In this mix, the space-filling curves offer a mechanism to order the 2D, 3D or n-D space and the multinomial distribution the statistical approach for testing homogeneity. A simulation method is proposed in order to analyze the applied performance of this idea.;M.V. Alba-Fernández and F.J. Ariza-López and M. Dolores Jiménez-Gamero and J. Rodríguez-Avi;"Management, Monitoring, Policy and Law (Q1); Computers in Earth Sciences (Q2); Statistics and Probability (Q2)";168.0;253.0;Netherlands;2012-2020;10.1016/j.spasta.2016.07.004;0.00244;27.0;;22116753;22116753;22116753;22116753;2.060;Space-filling curve, Multinomial distribution, Simulation;Elsevier BV;;4025.0;Western Europe;888.0;Q1;21100206605.0;Spatial statistics;On the similarity analysis of spatial patterns;923.0;469.0;64.0;172.0;2576.0;journal;article;2016
;Michael M. Engelgau and Muin J. Khoury and Rebecca A. Roper and Jennifer S. Curry and George A. Mensah;"Community and Home Care (Q1); Cardiology and Cardiovascular Medicine (Q2); Epidemiology (Q2)";115.0;146.0;United Kingdom;2011-2020;10.1016/j.gheart.2019.02.003;0.0027199999999999998;37.0;;22118160;22118179;22118160;22118179;3.426;;"Elsevier Science &amp; Technology";;3937.0;Western Europe;1012.0;Q1;21100212900.0;Global heart;Predictive analytics: helping guide the implementation research agenda at the national heart, lung, and blood institute;1287.0;289.0;75.0;150.0;2953.0;journal;article;2019
;Denis Horgan and Mário Romão and Richard Torbett and Angela Brand;"Biomedical Engineering (Q3); Health Policy (Q3)";151.0;184.0;Netherlands;2012-2020;10.1016/j.hlpt.2014.10.007;0.0009;21.0;;22118837;22118845;22118837;22118845;1.931;;;;3628.0;Western Europe;393.0;Q3;21100201770.0;Health policy and technology;European data-driven economy: a lighthouse initiative on personalised medicine;630.0;325.0;86.0;168.0;3120.0;journal;article;2014
Data sharing is often hindered by a number of real word challenges caused by a mixture of technological and social factors. To date, the agri-food sector significantly lags behind other sectors in overcoming these challenges. However, the benefits of data sharing are too great to be ignored as they have a potential to address many historical failings such as issues related to food safety, traceability and transparency, and must be carefully considered as the sector is undergoing a widespread digitalisation. In this article, we explore the potential of different technologies in addressing the challenges presented by data sharing in the agri-food sector, and how the use of these technologies in the narrative of a Data Trust may address many of these obstacles. We argue the importance of utilising semantic web technologies, distributed ledger technologies, machine learning, and privacy preserving technologies to enable future transformative data sharing infrastructures in the agri-food sector. The utilisation of holistic statistical analysis of the shared data is also discussed, vital in supporting many of the sectors optimisation and sustainability goals.;Aiden Durrant and Milan Markovic and David Matthews and David May and Georgios Leontidis and Jessica Enright;"Ecology (Q1); Food Science (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)";177.0;767.0;United States;2012-2020;10.1016/j.gfs.2021.100493;;46.0;;22119124;22119124;22119124;22119124;;Data Trusts, Data sharing, AI Technologies, Agri-food supply chains;Elsevier;;6608.0;Northern America;2350.0;Q1;21100218328.0;Global food security;How might technology rise to the challenge of data sharing in agri-food?;;1608.0;124.0;177.0;8194.0;journal;article;2021
Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.;Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei;Tourism, Leisure and Hospitality Management (Q1);274.0;677.0;United States;2012-2020;10.1016/j.tmp.2020.100715;0.00445;43.0;;22119736;22119736;22119736;22119736;6.586;Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic;Elsevier USA;;8644.0;Northern America;1454.0;Q1;21100202157.0;Tourism management perspectives;Knowledge mapping of tourism demand forecasting research;3902.0;1824.0;140.0;277.0;12102.0;journal;article;2020
The concept of local climate zones (LCZ) has emerged to identify the nature of urban climate, air quality, and temperature at local levels. Thus, this study reviews the literature on methodologies and data sources used in LCZs empirical research and identifies recurrent themes. A systematic review was conducted using bibliometric analysis and the PRISMA framework. Web of Science and Scopus databases were used to extract relevant datasets, and records were screened and extracted. Descriptive analyses reveal that most LCZ empirical research has been done on Chinese cities. Numerous data sources and analytical methods have been used, but Landsat and WUDAPT methodology is generally favored in the LCZ research due to its simplicity and freely available global datasets. Similarly, the review also shows that various software and methodologies are available to identify climate-sensitive areas of urban settlements with varying functionalities, accuracy, and visualizations. The thematic analysis indicates that the LCZ framework and its associated processes are being used in crosscutting phenomena such as thermal comfort, urban planning, climate change adaptation, and energy use. The review also suggests incorporating institutional and social aspects in local climate zones. LCZs can integrate the philosophies of climate change adaptation, urban resilience, and sustainability.;Ayman Aslam and Irfan Ahmad Rana;"Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1); Atmospheric Science (Q2)";264.0;616.0;Netherlands;2012-2020;10.1016/j.uclim.2022.101120;0.00413;43.0;;22120955;22120955;22120955;22120955;5.731;Climate change adaptation, Climate risk mapping, Sustainable development, Urban planning;Elsevier BV;;5784.0;Western Europe;1151.0;Q1;21100220478.0;Urban climate;The use of local climate zones in the urban environment: a systematic review of data sources, methods, and themes;3116.0;1563.0;165.0;265.0;9544.0;journal;article;2022
The explosion of increasingly sophisticated mobile phone technologies can usefully be harnessed by disaster risk reduction (DRR) as a means of enhancing inclusivity and local relevance of knowledge production and resilience building. However, much new technology is designed on an ad hoc basis without considering user needs – especially mobile applications (apps), which often terminate at the proof-of-concept stage. Here, we examine best practice by marshalling learnings from 45 workers representing 20 organisations working globally across the disaster risk management (DRM) lifecycle, including physical and social science, NGOs, technological developers, and (inter)governmental regulatory bodies. We present a series of generalisable and scalable guidelines that are novel in being independent of any specific natural hazard or development setting, designed to maximise the positive societal impact of exploiting mobile technologies. Specifically, the local context, dynamics, and needs must be carefully interrogated a priori, while any product should ideally be co-developed with local stakeholders through a user-centered design approach.;Jonathan D. Paul and Emma Bee and Mirianna Budimir;"Atmospheric Science (Q1); Geography, Planning and Development (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)";113.0;436.0;Netherlands;2014-2020;10.1016/j.crm.2021.100296;0.0024;30.0;;22120963;22120963;22120963;22120963;4.090;Citizen science, Disaster risk management (DRM), Disaster risk reduction (DRR), Mobile phone technologies, Natural hazards, User-centered design (UCD);Elsevier BV;;5894.0;Western Europe;1846.0;Q1;21100312211.0;Climate risk management;Mobile phone technologies for disaster risk reduction;1248.0;813.0;48.0;114.0;2829.0;journal;article;2021
"Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.";Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu;"Economics, Econometrics and Finance (miscellaneous) (Q2); Pharmacology, Toxicology and Pharmaceutics (miscellaneous) (Q2); Health Policy (Q3)";197.0;116.0;United States;2012-2020;10.1016/j.vhri.2021.05.002;;19.0;;22121099;22121099;22121102;22121099;;administrative claims, data access, electronic health records, real-world data;Elsevier USA;;3596.0;Northern America;395.0;Q2;21100218529.0;Value in health regional issues;Real-world data for healthcare research in china: call for actions;;265.0;79.0;207.0;2841.0;journal;article;2022
The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.;Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas;"Geology (Q1); Geotechnical Engineering and Engineering Geology (Q1); Safety Research (Q1)";844.0;478.0;United Kingdom;2012-2020;10.1016/j.ijdrr.2022.103138;0.010029999999999999;45.0;;22124209;22124209;22124209;22124209;4.320;Urban fire risk, Fire risk management, Big data technologies, Data governance, Socio-economic factors, City-wide analysis;Elsevier Ltd.;;6377.0;Western Europe;1161.0;Q1;21100228018.0;International journal of disaster risk reduction;Effects of governmental data governance on urban fire risk: a city-wide analysis in china;6931.0;4363.0;562.0;856.0;35838.0;journal;article;2022
"Background/objectives
The Internet of Things (IoT) can create disruptive innovation in healthcare. Thus, during COVID-19 Pandemic, there is a need to study different applications of IoT enabled healthcare. For this, a brief study is required for research directions.
Methods
Research papers on IoT in healthcare and COVID-19 Pandemic are studied to identify this technology’s capabilities. This literature-based study may guide professionals in envisaging solutions to related problems and fighting against the COVID-19 type pandemic.
Results
Briefly studied the significant achievements of IoT with the help of a process chart. Then identifies seven major technologies of IoT that seem helpful for healthcare during COVID-19 Pandemic. Finally, the study identifies sixteen basic IoT applications for the medical field during the COVID-19 Pandemic with a brief description of them.
Conclusions
In the current scenario, advanced information technologies have opened a new door to innovation in our daily lives. Out of these information technologies, the Internet of Things is an emerging technology that provides enhancement and better solutions in the medical field, like proper medical record-keeping, sampling, integration of devices, and causes of diseases. IoT’s sensor-based technology provides an excellent capability to reduce the risk of surgery during complicated cases and helpful for COVID-19 type pandemic. In the medical field, IoT’s focus is to help perform the treatment of different COVID-19 cases precisely. It makes the surgeon job easier by minimising risks and increasing the overall performance. By using this technology, doctors can easily detect changes in critical parameters of the COVID-19 patient. This information-based service opens up new healthcare opportunities as it moves towards the best way of an information system to adapt world-class results as it enables improvement of treatment systems in the hospital. Medical students can now be better trained for disease detection and well guided for the future course of action. IoT’s proper usage can help correctly resolve different medical challenges like speed, price, and complexity. It can easily be customised to monitor calorific intake and treatment like asthma, diabetes, and arthritis of the COVID-19 patient. This digitally controlled health management system can improve the overall performance of healthcare during COVID-19 pandemic days.";Mohd Javaid and Ibrahim Haleem Khan;"Dentistry (miscellaneous) (Q2); Otorhinolaryngology (Q3)";175.0;245.0;Netherlands;2011-2020;10.1016/j.jobcr.2021.01.015;;18.0;;22124268;22124276;22124268;22124276;;Internet of things (IoT), COVID-19, Information technology applications, Healthcare, Smart hospital;Elsevier BV;;2921.0;Western Europe;454.0;Q2;21100461912.0;Journal of oral biology and craniofacial research;Internet of things (iot) enabled healthcare helps to take the challenges of covid-19 pandemic;;407.0;149.0;179.0;4352.0;journal;article;2021
;Vinod Krishnan;Orthodontics (Q3);74.0;73.0;Netherlands;1970, 2012-2020;10.1016/j.ejwf.2022.06.002;;8.0;;22124438;22124438;22124438;22124438;;;Elsevier BV;;2824.0;Western Europe;320.0;Q3;21100206609.0;Journal of the world federation of orthodontists;A “fair” approach to open research;;58.0;42.0;88.0;1186.0;journal;article;2022
This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.;Matthias Fuchs and Wolfram Höpken and Maria Lexhagen;"Business and International Management (Q1); Marketing (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1)";237.0;714.0;United Kingdom;2012-2020;10.1016/j.jdmm.2014.08.002;;39.0;;2212571X;2212571X;2212571X;2212571X;;Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP);Elsevier Ltd.;;8211.0;Western Europe;1703.0;Q1;21100228536.0;Journal of destination marketing and management;Big data analytics for knowledge generation in tourism destinations – a case from sweden;;1757.0;90.0;237.0;7390.0;journal;article;2014
The process controller in a precision grinder for bearing rings puts high performance demands on the machine to achieve desired quality in production. This paper presents a unique approach of adding additional sensors for machine condition monitoring for the purpose of learning and using high fidelity condition indicators. The consolidation of real-time sensor data and the process control signals yields high-dimensional dataset. Automatic segmentation helps optimize the amount of data for processing and data mining ahead of fault diagnosis. The proposed setup is state of the art for prognostics as part of condition-based maintenance in a production machine.;Muhammad Ahmer and Pär Marklund and Martin Gustafsson and Kim Berglund;"Control and Systems Engineering; Industrial and Manufacturing Engineering";3145.0;240.0;Netherlands;2012-2020;10.1016/j.procir.2020.04.094;;65.0;;22128271;22128271;22128271;22128271;;Analytics, Automation, Condition monitoring, Grinding, Machining, Maintenance, Manufacturing, Measurement, Process Monitoring, Sensor;Elsevier BV;;2091.0;Western Europe;683.0;-;21100243809.0;Procedia cirp;A unified approach towards performance monitoring and condition-based maintenance in grinding machines;;8225.0;1456.0;3191.0;30451.0;conference and proceedings;article;2020
"Background
The Longitudinal Epidemiologic Assessment of Diabetes Risk (LEADR) study uses a novel Electronic Health Record (EHR) data approach as a tool to assess the epidemiology of known and new risk factors for type 2 diabetes mellitus (T2DM) and study how prevention interventions affect progression to and onset of T2DM. We created an electronic cohort of 1.4 million patients having had at least 4 encounters with a healthcare organization for at least 24-months; were aged ≥18 years in 2010; and had no diabetes (i.e., T1DM or T2DM) at cohort entry or in the 12 months following entry. EHR data came from patients at nine healthcare organizations across the U.S. between January 1, 2010–December 31, 2016.
Results
Approximately 5.9% of the LEADR cohort (82,922 patients) developed T2DM, providing opportunities to explore longitudinal clinical care, medication use, risk factor trajectories, and diagnoses for these patients, compared with patients similarly matched prior to disease onset.
Conclusions
LEADR represents one of the largest EHR databases to have repurposed EHR data to examine patients’ T2DM risk. This paper is first in a series demonstrating this novel approach to studying T2DM.
Implications
Chronic conditions that often take years to develop can be studied efficiently using EHR data in a retrospective design.
Level of evidence
While much is already known about T2DM risk, this EHR's cohort's 160 M data points for 1.4 M people over six years, provides opportunities to investigate new unique risk factors and evaluate research hypotheses where results could modify public health practice for preventing T2DM.";Howard A. Fishbein and Rebecca Jeffries Birch and Sunitha M. Mathew and Holly L. Sawyer and Gerald Pulver and Jennifer Poling and David Kaelber and Russell Mardon and Maurice C. Johnson and Wilson Pace and Keith D. Umbel and Xuanping Zhang and Karen R. Siegel and Giuseppina Imperatore and Sundar Shrestha and Krista Proia and Yiling Cheng and Kai {McKeever Bullard} and Edward W. Gregg and Deborah Rolka and Meda E. Pavkov;Health Policy (Q1);124.0;158.0;Netherlands;2013-2020;10.1016/j.hjdsi.2020.100458;0.0043;19.0;;22130764;22130772;22130764;22130772;2.645;Chronic disease, Diabetes mellitus, Epidemiologic methods, Epidemiologic research design, Big data, Electronic health records, Public health informatics, Public health practice;Elsevier BV;;2527.0;Western Europe;917.0;Q1;21100361200.0;Healthcare;The longitudinal epidemiologic assessment of diabetes risk (leadr): unique 1.4 m patient electronic health record cohort;2175.0;259.0;74.0;126.0;1870.0;journal;article;2020
"Nature recreation and tourism is a substantial ecosystem service of Europe's countryside that has a substantial economic value and contributes considerably to income and employment of local communities. Highlighting the recreational value and economic contribution of nature areas can be used as a strong argument for the funding of protected and recreational areas. The total number of recreational visits of a nature area has been recognised as a major determinant of its economic recreational value and its contribution to local economies. This paper presents an international geo-database on recreational visitor numbers to non-urban ecosystems, containing 1267 observations at 518 separate case study areas throughout Europe. The monitored sites are described by their centroid coordinates and shape files displaying the exact extension of the sites. Therefore, the database illustrates the spatial distribution of visitor counting throughout Europe and can be used for secondary research, such as for validation of spatially explicit recreational ecosystem service models and for identifying relevant drivers of recreational ecosystem services. To develop the database, we review visitor monitoring literature throughout Europe and give an overview of such activities with special attention to visitor counting. We identify one major shortcoming in the available literature, which relates to the presentation, study area definition and methodological reporting of conducted visitor counting studies. Insufficient reporting hampers the identification of the study area, the comparability of different studies and the evaluation of the studies' quality. Based on our findings, we propose a standardised reporting template for visitor counting studies and advanced data sharing for recreational visitor data. Researchers and institutions are invited to report on their visitor counting studies via our web interface at rris.biopama.org/visitor-reporting and thereby contribute to a global visitor database that will be shared via the ESP Visualisation tool.
Management implications
The total annual visitor number is the most important variable for defining the relative importance and the economic recreational value of different recreational areas. Due to the importance of visitor counting and its increased attention in the scientific literature we: •present a geo-database on recreational visitor statistics for nature areas, which allows identifying sites for which visitor statistics exist and which can be used for secondary research•review current practice in recreational visitor counting across nature areas in Europe and give guidance for future applications,•identify shortcomings in the methodological reporting of recent visitor monitoring and counting studies and•present and recommend reporting standard for all future visitor counting studies in order to improve their comparability and to allow assessing their quality.•The reporting standard is translated into a web interface for visitor data collection, which allows for data sharing via a global map-browser.";Jan Philipp Schägner and Joachim Maes and Luke Brander and Maria-Luisa Paracchini and Volkmar Hartje and Gregoire Dubois;Tourism, Leisure and Hospitality Management (Q2);114.0;288.0;United Kingdom;2013-2020;10.1016/j.jort.2017.02.004;;21.0;;22130780;22130780;22130780;22130780;;Nature recreation, Visitor monitoring, Visitor counting review, Reporting standard, Visitor statistics, Visitor data sharing;Elsevier BV;;6277.0;Western Europe;686.0;Q2;21100255547.0;Journal of outdoor recreation and tourism;Monitoring recreation across european nature areas: a geo-database of visitor counts, a review of literature and a call for a visitor counting reporting standard;;381.0;65.0;116.0;4080.0;journal;article;2017
Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ∼1% for simulated and ∼2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely thousands of galaxies. Our software is made publicly available as a user-friendly package developed in Python, R and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope.;J. Elliott and R.S. {de Souza} and A. Krone-Martins and E. Cameron and E.E.O. Ishida and J. Hilbe;"Astronomy and Astrophysics (Q2); Computer Science Applications (Q2); Space and Planetary Science (Q2)";132.0;353.0;Netherlands;2013-2020;10.1016/j.ascom.2015.01.002;0.0025;31.0;;22131337;22131337;22131337;22131337;1.927;Techniques: photometric, Methods: statistical, Methods: analytical, Galaxies: distances and redshifts;Elsevier BV;;4374.0;Western Europe;692.0;Q2;21100241218.0;Astronomy and computing;The overlooked potential of generalized linear models in astronomy-ii: gamma regression and photometric redshifts;796.0;475.0;46.0;133.0;2012.0;journal;article;2015
Solar irradiation maps are fundamental geospatial datasets that have been used in a variety of research fields. However, it is difficult to estimate the continuous distribution of solar irradiation over large areas accurately by using conventional interpolation or extrapolation methods based on only a few observation stations. To tackle this problem, this study proposed a method to estimate spatially continuous land surface solar irradiation based on four machine learning models, namely, Gradient Boosting Machine (GBM), Random Forest (RF), Support Vector Regression (SVR), and Multilayer Perceptron (MLP). Clear-sky solar irradiation data were computed based on time and location, cloud optical thickness (COT) and aerosol optical thickness (AOT) that significantly influence solar irradiation were retrieved from Himawari-8 meteorological satellite images, and land surface solar irradiation data were obtained from observation stations for training and evaluation. To explore the weather effects on land surface solar irradiation, air temperatures, humidity, wind, and atmospheric pressure were also quantified and integrated into the models. As a comparative study, this study collected six-year historical data and estimated solar distribution at a 5-km spatial resolution in Australia and China. Based on the coefficient of determination (R2), normalized Root Mean Square Error (nRMSE), normalized mean bias error (nMBE), and consumption of time (t), the results show that GBM achieved the highest accuracy with R2 >0.7 at all stations, followed by RF, SVR, and MLP. It suggests that the proposed method can provide an accurate and reliable estimation of land surface solar irradiation, compared with the theoretical solar irradiation without the obstacle of the atmosphere. The annual solar distribution maps created by the built methods indicate that the proposed method is simple and effective for large geographical regions and can be used worldwide when similar datasets are obtained.;Xuan Liao and Rui Zhu and Man Sing Wong;"Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)";331.0;573.0;United Kingdom;2013-2020;10.1016/j.seta.2022.102323;0.0034799999999999996;39.0;;22131388;22131388;22131388;22131388;5.353;Land surface solar irradiation, Machine learning, Cloud optical thickness, Aerosol optical thickness, Himawari-8 satellite images, Meteorological data;Elsevier Ltd.;;4930.0;Western Europe;1040.0;Q1;21100239262.0;Sustainable energy technologies and assessments;Simplified estimation modeling of land surface solar irradiation: a comparative study in australia and china;3234.0;1833.0;270.0;333.0;13311.0;journal;article;2022
Major depression, currently the world's primary cause of disability, leads to profound personal suffering and increased risk of suicide. Unfortunately, the success of antidepressant treatment varies amongst individuals and can take weeks to months in those who respond. Electroconvulsive therapy (ECT), generally prescribed for the most severely depressed and when standard treatments fail, produces a more rapid response and remains the most effective intervention for severe depression. Exploring the neurobiological effects of ECT is thus an ideal approach to better understand the mechanisms of successful therapeutic response. Though several recent neuroimaging studies show structural and functional changes associated with ECT, not all brain changes associate with clinical outcome. Larger studies that can address individual differences in clinical and treatment parameters may better target biological factors relating to or predictive of ECT-related therapeutic response. We have thus formed the Global ECT-MRI Research Collaboration (GEMRIC) that aims to combine longitudinal neuroimaging as well as clinical, behavioral and other physiological data across multiple independent sites. Here, we summarize the ECT sample characteristics from currently participating sites, and the common data-repository and standardized image analysis pipeline developed for this initiative. This includes data harmonization across sites and MRI platforms, and a method for obtaining unbiased estimates of structural change based on longitudinal measurements with serial MRI scans. The optimized analysis pipeline, together with the large and heterogeneous combined GEMRIC dataset, will provide new opportunities to elucidate the mechanisms of ECT response and the factors mediating and predictive of clinical outcomes, which may ultimately lead to more effective personalized treatment approaches.;Leif Oltedal and Hauke Bartsch and Ole Johan Evjenth Sørhaug and Ute Kessler and Christopher Abbott and Annemieke Dols and Max L Stek and Lars Ersland and Louise Emsell and Philip {van Eijndhoven} and Miklos Argyelan and Indira Tendolkar and Pia Nordanskog and Paul Hamilton and Martin Balslev Jorgensen and Iris E Sommer and Sophie M Heringa and Bogdan Draganski and Ronny Redlich and Udo Dannlowski and Harald Kugel and Filip Bouckaert and Pascal Sienaert and Amit Anand and Randall Espinoza and Katherine L Narr and Dominic Holland and Anders M Dale and Ketil J Oedegaard;"Cognitive Neuroscience (Q1); Neurology (Q1); Neurology (clinical) (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";1226.0;467.0;Netherlands;2012-2020;10.1016/j.nicl.2017.02.009;;68.0;;22131582;22131582;22131582;22131582;;Electroconvulsive therapy, MRI, Longitudinal, Mega analysis, Multi-site;Elsevier BV;;6509.0;Western Europe;1772.0;Q1;21100222534.0;Neuroimage: clinical;The global ect-mri research collaboration (gemric): establishing a multi-site investigation of the neural mechanisms underlying response to electroconvulsive therapy;;6246.0;419.0;1226.0;27271.0;journal;article;2017
Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.;Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider;Immunology and Allergy (Q1);1067.0;368.0;United States;2013-2020;10.1016/j.jaip.2022.01.047;;58.0;;22132198;22132198;22132201;22132198;;Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education;Elsevier;;2732.0;Northern America;1731.0;Q1;21100239250.0;Journal of allergy and clinical immunology: in practice;A framework for augmented intelligence in allergy and immunology practice and research—a work group report of the aaaai health informatics, technology, and education committee;;4937.0;700.0;1357.0;19123.0;journal;article;2022
"Summary
Background
The large amount of clinical signals in intensive care units can easily overwhelm health-care personnel and can lead to treatment delays, suboptimal care, or clinical errors. The aim of this study was to apply deep machine learning methods to predict severe complications during critical care in real time after cardiothoracic surgery.
Methods
We used deep learning methods (recurrent neural networks) to predict several severe complications (mortality, renal failure with a need for renal replacement therapy, and postoperative bleeding leading to operative revision) in post cardiosurgical care in real time. Adult patients who underwent major open heart surgery from Jan 1, 2000, to Dec 31, 2016, in a German tertiary care centre for cardiovascular diseases formed the main derivation dataset. We measured the accuracy and timeliness of the deep learning model's forecasts and compared predictive quality to that of established standard-of-care clinical reference tools (clinical rule for postoperative bleeding, Simplified Acute Physiology Score II for mortality, and the Kidney Disease: Improving Global Outcomes staging criteria for acute renal failure) using positive predictive value (PPV), negative predictive value, sensitivity, specificity, area under the curve (AUC), and the F1 measure (which computes a harmonic mean of sensitivity and PPV). Results were externally retrospectively validated with 5898 cases from the published MIMIC-III dataset.
Findings
Of 47 559 intensive care admissions (corresponding to 42 007 patients), we included 11 492 (corresponding to 9269 patients). The deep learning models yielded accurate predictions with the following PPV and sensitivity scores: PPV 0·90 and sensitivity 0·85 for mortality, 0·87 and 0·94 for renal failure, and 0·84 and 0·74 for bleeding. The predictions significantly outperformed the standard clinical reference tools, improving the absolute complication prediction AUC by 0·29 (95% CI 0·23–0·35) for bleeding, by 0·24 (0·19–0·29) for mortality, and by 0·24 (0·13–0·35) for renal failure (p<0·0001 for all three analyses). The deep learning methods showed accurate predictions immediately after patient admission to the intensive care unit. We also observed an increase in performance in our validation cohort when the machine learning approach was tested against clinical reference tools, with absolute improvements in AUC of 0·09 (95% CI 0·03–0·15; p=0·0026) for bleeding, of 0·18 (0·07–0·29; p=0·0013) for mortality, and of 0·25 (0·18–0·32; p<0·0001) for renal failure.
Interpretation
The observed improvements in prediction for all three investigated clinical outcomes have the potential to improve critical care. These findings are noteworthy in that they use routinely collected clinical data exclusively, without the need for any manual processing. The deep machine learning method showed AUC scores that significantly surpass those of clinical reference tools, especially soon after admission. Taken together, these properties are encouraging for prospective deployment in critical care settings to direct the staff's attention towards patients who are most at risk.
Funding
No specific funding.";Alexander Meyer and Dina Zverinski and Boris Pfahringer and Jörg Kempfert and Titus Kuehne and Simon H Sündermann and Christof Stamm and Thomas Hofmann and Volkmar Falk and Carsten Eickhoff;Pulmonary and Respiratory Medicine (Q1);247.0;676.0;United Kingdom;2013-2020;10.1016/S2213-2600(18)30300-X;;113.0;;22132600;22132619;22132600;22132619;;;Elsevier Ltd.;;1606.0;Western Europe;9030.0;Q1;21100220495.0;Lancet respiratory medicine,the;Machine learning for real-time prediction of complications in critical care: a retrospective study;;5271.0;330.0;813.0;5300.0;journal;article;2018
"This paper presents an overview of the scientific evidence providing insights into long term ecosystem and social dynamics across the northern Tanzania and southern Kenya borderlands. The data sources covered a range from palaeoenvironmental records and archaeological information to remote sensing and social science studies that examined human-environmental interactions and land use land cover changes (LULCC) in the region. This knowledge map of published LULCC research contributes to current debates about the drivers and dynamics of LULCC. The review aims to facilitate both multidisciplinary LULCC research and evidence-based policy analyses to improve familiarity and engagement between LULCC knowledge producers and end-users and to motivate research integration for land management policy formulation. Improving familiarity among researchers and non-academic stakeholders through the collation and synthesis of the scientific literature is among the challenges hindering policy formulation and land management decision-making by various stakeholders along the Kenya-Tanzania borderlands. Knowledge syntheses are necessary; yet, do not fully bridge the gap between knowledge and policy action. Cooperation across the science-policy interface is fundamental for the co-production of research questions by academics, policy makers and diverse stakeholders aimed at supporting land management decision making. For improved co-development and co-benefitting outcomes, the LULCC scientific community needs to mobilise knowledge for a broader audience and to advance co-development of relevant and meaningful LULCC products.";Colin J. {Courtney Mustaphi} and Claudia Capitani and Oliver Boles and Rebecca Kariuki and Rebecca Newman and Linus Munishi and Rob Marchant and Paul Lane;"Earth and Planetary Sciences (miscellaneous) (Q1); Ecology (Q1); Global and Planetary Change (Q2)";85.0;356.0;United Kingdom;2013-2020;10.1016/j.ancene.2019.100228;0.0021100000000000003;33.0;;22133054;22133054;22133054;22133054;3.964;Landscape, Multidisciplinary, Science-policy interface, Serengeti, Socio-ecological systems, Policy support;Elsevier BV;;7089.0;Western Europe;1212.0;Q1;21100255737.0;Anthropocene;Integrating evidence of land use and land cover change for land management policy formulation along the kenya-tanzania borderlands;1112.0;377.0;37.0;86.0;2623.0;journal;article;2019
Massive parallel DNA sequencing combined with chromatin immunoprecipitation and a large variety of DNA/RNA-enrichment methodologies is at the origin of data resources of major importance. Indeed these resources, available for multiple genomes, represent the most comprehensive catalogue of (i) cell, development and signal transduction-specified patterns of binding sites for transcription factors (‘cistromes’) and for transcription and chromatin modifying machineries and (ii) the patterns of specific local post-translational modifications of histones and DNA (‘epigenome’) or of regulatory chromatin binding factors. In addition, (iii) the resources specifying chromatin structure alterations are emerging. Importantly, these types of “omics” datasets populate increasingly public repositories and provide highly valuable resources for the exploration of general principles of cell function in a multi-dimensional genome–transcriptome–epigenome–chromatin structure context. However, data mining is critically dependent on the data quality, an issue that, surprisingly, is still largely ignored by scientists and well-financed consortia, data repositories and scientific journals. So what determines the quality of ChIP-seq experiments and the datasets generated therefrom and what refrains scientists from associating quality criteria to their data? In this ‘opinion’ we trace the various parameters that influence the quality of this type of datasets, as well as the computational efforts that were made until now to qualify them. Moreover, we describe a universal quality control (QC) certification approach that provides a quality rating for ChIP-seq and enrichment-related assays. The corresponding QC tool and a regularly updated database, from which at present the quality parameters of more than 8000 datasets can be retrieved, are freely accessible at www.ngs-qc.org.;Marco Antonio Mendoza-Parra and Hinrich Gronemeyer;"Biotechnology (Q2); Biochemistry (Q3); Genetics (Q3); Molecular Medicine (Q3)";102.0;0.0;United States;2013-2017;10.1016/j.gdata.2014.08.002;;20.0;;22135960;22135960;22135960;22135960;;ChIP sequencing, Massive parallel sequencing, Quality control, Omics data mining;Elsevier Inc.;;0.0;Northern America;549.0;Q2;21100283759.0;Genomics data;Assessing quality standards for chip-seq and related massive parallel sequencing-generated datasets: when rating goes beyond avoiding the crisis;;196.0;0.0;102.0;0.0;journal;article;2014
The advanced internationalisation of markets and production processes continuously adds to the complexity of supply chains. At the same time improving the sustainability of the related international freight transport processes and optimising their efficiency is becoming a topic of central relevance. International freight transport models are an important tool to simulate impacts of measures taken to achieve such improvements of transport processes. Yet, the requirements towards international freight transport models are complex: they need to include various modes of transport, they need to cover different industries and their dynamics, they need to consider seasonality of supply and demand of goods, demographic parameters, economic developments, technological developments including their impact on production processes and structures, and many other aspects. Furthermore, international freight transport models need to include freight flows within countries as well as freight flows between the considered countries. This paper discusses the challenges which need to be confronted when developing international freight transport models which are able to correspond to the described complexity of international freight transport. Furthermore, it maps out the most important research gaps which need to be addressed by international freight transport modelling research in order to ensure that the challenges identified are captured within the models developed to improve international freight transport.;Hilde Meersman and Verena Charlotte Ehrler and Dirk Bruckmann and Ming Chen and Jan Francke and Peter Hill and Clare Jackson and Jens Klauenberg and Martin Kurowski and Saskia Seidel and Inge Vierth;"Geography, Planning and Development (Q1); Urban Studies (Q1); Transportation (Q2)";241.0;267.0;Netherlands;2013-2020;10.1016/j.cstp.2015.12.002;;19.0;;2213624X;22136258;2213624X;22136258;;International freight transport, Freight transport model, Data sourcing;Elsevier BV;;4283.0;Western Europe;693.0;Q1;21100283790.0;Case studies on transport policy;Challenges and future research needs towards international freight transport modelling;;635.0;148.0;247.0;6339.0;journal;article;2016
"The real-time conflict prediction model using traffic flow characteristics is much less studied than the crash-based model. This study aims at exploring the relationship between conflicts and traffic flow features with the consideration of heterogeneity and developing predictive models to identify conflict-prone conditions in a real-time manner. The high-resolution trajectory data from the HighD dataset is used as empirical data. A novel method with the virtual detector approach for traffic feature extraction and a two-step framework is proposed for the trajectory data analysis. The framework consists of an exploratory study by random parameter logit model with heterogeneity in means and variances and a comparative study on several machine learning methods, including eXtreme Gradient Boosting (Boosting), Random Forest (Bagging), Support Vector Machine (Single-classifier), and Multilayer-Perceptron (Deep neural network). Results indicate that (1) traffic flow characteristics have significant impacts on the probability of conflict occurrence; (2) the statistical model considering mean heterogeneity outperforms the counterpart and lane differences variables are found to significantly impact the means of random parameters for both lane variables and lane differences variables; (3) eXtreme Gradient Boosting trained on an under-sampled dataset turns out to be the best model with the highest AUC of 0.871 and precision of 0.867, showing that re-sampling techniques can significantly improve the model performance. The proposed model is found to be sensitive to the conflict threshold. Sensitivity analysis on feature selection further confirms that the conflict risk prediction should consider both subject lane features and lane difference features, which verifies the consistency with exploratory analysis based on the statistical model. The consistency between statistical models and machine learning methods improves the interpretability of results for the latter one.";Chen Yuan and Ye Li and Helai Huang and Shiqi Wang and Zhenhao Sun and Yan Li;"Safety Research (Q1); Transportation (Q1)";57.0;1256.0;United Kingdom;2014-2020;10.1016/j.amar.2022.100217;0.00268;35.0;;22136657;22136657;22136657;22136657;11.806;Real-time conflict risk, Heterogeneity, Random parameter, Machine learning;Elsevier BV;;6600.0;Western Europe;6221.0;Q1;21100261712.0;Analytic methods in accident research;Using traffic flow characteristics to predict real-time conflict risk: a novel method for trajectory data analysis;1445.0;766.0;27.0;57.0;1782.0;journal;article;2022
Transcriptional profiling is a powerful tool commonly used to benchmark stem cells and their differentiated progeny. As the wealth of stem cell data builds in public repositories, we highlight common data traps, and review approaches to combine and mine this data for new cell classification and cell prediction tools. We touch on future trends for stem cell profiling, such as single-cell profiling, long-read sequencing, and improved methods for measuring molecular modifications on chromatin and RNA that bring new challenges and opportunities for stem cell analysis.;Christine A. Wells and Jarny Choi;"Biochemistry (Q1); Cell Biology (Q1); Developmental Biology (Q1); Genetics (Q1)";756.0;664.0;United States;2013-2020;10.1016/j.stemcr.2019.07.008;0.029289999999999997;76.0;;22136711;22136711;22136711;22136711;7.765;pluripotent stem cell, reprogramming, transcriptome, single-cell sequencing, bioinformatics;Cell Press;;4640.0;Northern America;3207.0;Q1;21100248838.0;Stem cell reports;Transcriptional profiling of stem cells: moving from descriptive to predictive paradigms;10762.0;5305.0;191.0;769.0;8863.0;journal;article;2019
The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.;Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare;"Industrial and Manufacturing Engineering (Q1); Mechanics of Materials (Q1)";122.0;553.0;United States;2013-2020;10.1016/j.mfglet.2018.09.002;;26.0;;22138463;22138463;22138463;22138463;;Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems;Society of Manufacturing Engineers;;2032.0;Northern America;1072.0;Q1;21100283796.0;Manufacturing letters;Industrial artificial intelligence for industry 4.0-based manufacturing systems;;646.0;84.0;123.0;1707.0;journal;article;2018
"Summary
Background
Diabetes, particularly type 1 diabetes, at younger ages can be a largely preventable cause of death with the correct health care and services. We aimed to evaluate diabetes mortality and trends at ages younger than 25 years globally using data from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2019.
Methods
We used estimates of GBD 2019 to calculate international diabetes mortality at ages younger than 25 years in 1990 and 2019. Data sources for causes of death were obtained from vital registration systems, verbal autopsies, and other surveillance systems for 1990–2019. We estimated death rates for each location using the GBD Cause of Death Ensemble model. We analysed the association of age-standardised death rates per 100 000 population with the Socio-demographic Index (SDI) and a measure of universal health coverage (UHC) and described the variability within SDI quintiles. We present estimates with their 95% uncertainty intervals.
Findings
In 2019, 16 300 (95% uncertainty interval 14 200 to 18 900) global deaths due to diabetes (type 1 and 2 combined) occurred in people younger than 25 years and 73·7% (68·3 to 77·4) were classified as due to type 1 diabetes. The age-standardised death rate was 0·50 (0·44 to 0·58) per 100 000 population, and 15 900 (97·5%) of these deaths occurred in low to high-middle SDI countries. The rate was 0·13 (0·12 to 0·14) per 100 000 population in the high SDI quintile, 0·60 (0·51 to 0·70) per 100 000 population in the low-middle SDI quintile, and 0·71 (0·60 to 0·86) per 100 000 population in the low SDI quintile. Within SDI quintiles, we observed large variability in rates across countries, in part explained by the extent of UHC (r2=0·62). From 1990 to 2019, age-standardised death rates decreased globally by 17·0% (−28·4 to −2·9) for all diabetes, and by 21·0% (–33·0 to −5·9) when considering only type 1 diabetes. However, the low SDI quintile had the lowest decline for both all diabetes (−13·6% [–28·4 to 3·4]) and for type 1 diabetes (−13·6% [–29·3 to 8·9]).
Interpretation
Decreasing diabetes mortality at ages younger than 25 years remains an important challenge, especially in low and low-middle SDI countries. Inadequate diagnosis and treatment of diabetes is likely to be major contributor to these early deaths, highlighting the urgent need to provide better access to insulin and basic diabetes education and care. This mortality metric, derived from readily available and frequently updated GBD data, can help to monitor preventable diabetes-related deaths over time globally, aligned with the UN's Sustainable Development Targets, and serve as an indicator of the adequacy of basic diabetes care for type 1 and type 2 diabetes across nations.
Funding
Bill & Melinda Gates Foundation.";Ewerton Cousin and Bruce B Duncan and Caroline Stein and Kanyin Liane Ong and Theo Vos and Cristiana Abbafati and Mohsen Abbasi-Kangevari and Michael Abdelmasseh and Amir Abdoli and Rami Abd-Rabu and Hassan Abolhassani and Eman Abu-Gharbieh and Manfred Mario Kokou Accrombessi and Qorinah Estiningtyas Sakilah Adnani and Muhammad Sohail Afzal and Gina Agarwal and Krishna K Agrawaal and Marcela Agudelo-Botero and Bright Opoku Ahinkorah and Sajjad Ahmad and Tauseef Ahmad and Keivan Ahmadi and Sepideh Ahmadi and Ali Ahmadi and Ali Ahmed and Yusra {Ahmed Salih} and Wuraola Akande-Sholabi and Tayyaba Akram and Hanadi {Al Hamad} and Ziyad Al-Aly and Jacqueline Elizabeth Alcalde-Rabanal and Vahid Alipour and Syed Mohamed Aljunid and Rajaa M Al-Raddadi and Nelson Alvis-Guzman and Saeed Amini and Robert Ancuceanu and Tudorel Andrei and Catalina Liliana Andrei and Ranjit Mohan Anjana and Adnan Ansar and Ippazio Cosimo Antonazzo and Benny Antony and Anayochukwu Edward Anyasodor and Jalal Arabloo and Damian Arizmendi and Benedetta Armocida and Anton A Artamonov and Judie Arulappan and Zahra Aryan and Samaneh Asgari and Tahira Ashraf and Thomas Astell-Burt and Prince Atorkey and Maha Moh'd Wahbi Atout and Martin Amogre Ayanore and Ashish D Badiye and Atif Amin Baig and Mohan Bairwa and Jennifer L Baker and Ovidiu Constantin Baltatu and Palash Chandra Banik and Anthony Barnett and Mark Thomaz Ugliara Barone and Francesco Barone-Adesi and Amadou Barrow and Neeraj Bedi and Rebuma Belete and Uzma Iqbal Belgaumi and Arielle Wilder Bell and Derrick A Bennett and Isabela M Bensenor and David Beran and Akshaya Srikanth Bhagavathula and Sonu Bhaskar and Krittika Bhattacharyya and Vijayalakshmi S Bhojaraja and Ali Bijani and Boris Bikbov and Setognal Birara and Virginia Bodolica and Aime Bonny and Hermann Brenner and Nikolay Ivanovich Briko and Zahid A Butt and Florentino Luciano {Caetano dos Santos} and Luis Alberto Cámera and Ismael R Campos-Nonato and Yin Cao and Chao Cao and Ester Cerin and Promit Ananyo Chakraborty and Joht Singh Chandan and Vijay Kumar Chattu and Simiao Chen and Jee-Young Jasmine Choi and Sonali Gajanan Choudhari and Enayet Karim Chowdhury and Dinh-Toi Chu and Barbara Corso and Omid Dadras and Xiaochen Dai and Albertino Antonio Moura Damasceno and Lalit Dandona and Rakhi Dandona and Claudio Alberto Dávila-Cervantes and Jan-Walter {De Neve} and Edgar Denova-Gutiérrez and Deepak Dhamnetiya and Daniel Diaz and Sanam Ebtehaj and Hisham Atan Edinur and Sahar Eftekharzadeh and Iman {El Sayed} and Islam Y Elgendy and Muhammed Elhadi and Mohamed A Elmonem and Mohammed Faisaluddin and Umar Farooque and Xiaoqi Feng and Eduarda Fernandes and Florian Fischer and David Flood and Marisa Freitas and Peter Andras Gaal and Mohamed M Gad and Piyada Gaewkhiew and Lemma Getacher and Mansour Ghafourifard and Reza {Ghanei Gheshlagh} and Ahmad Ghashghaee and Nermin Ghith and Ghozali Ghozali and Paramjit Singh Gill and Ibrahim Abdelmageed Ginawi and Ekaterina Vladimirovna Glushkova and Mahaveer Golechha and Sameer Vali Gopalani and Rafael Alves Guimarães and Rajat Das Gupta and Rajeev Gupta and Vivek Kumar Gupta and Veer Bala Gupta and Sapna Gupta and Tesfa Dejenie Habtewold and Nima Hafezi-Nejad and Rabih Halwani and Asif Hanif and Graeme J Hankey and Shafiul Haque and Ahmed I Hasaballah and Syed Shahzad Hasan and Abdiwahab Hashi and Soheil Hassanipour and Simon I Hay and Khezar Hayat and Mohammad Heidari and Mohammad Bellal Hossain Hossain and Sahadat Hossain and Mostafa Hosseini and Soodabeh Hoveidamanesh and Junjie Huang and Ayesha Humayun and Rabia Hussain and Bing-Fang Hwang and Segun Emmanuel Ibitoye and Kevin S Ikuta and Leeberk Raja Inbaraj and Usman Iqbal and Md Shariful Islam and Sheikh Mohammed Shariful Islam and Rakibul M Islam and Nahlah Elkudssiah Ismail and Gaetano Isola and Ramaiah Itumalla and Masao Iwagami and Ihoghosa Osamuyi Iyamu and Mohammad Ali Jahani and Mihajlo Jakovljevic and Ranil Jayawardena and Ravi Prakash Jha and Oommen John and Jost B Jonas and Tamas Joo and Ali Kabir and Rohollah Kalhor and Ashwin Kamath and Tanuj Kanchan and Himal Kandel and Neeti Kapoor and Gbenga A Kayode and Sewnet Adem Kebede and Pedram Keshavarz and Mohammad Keykhaei and Yousef Saleh Khader and Himanshu Khajuria and Moien AB Khan and Md Nuruzzaman Khan and Maseer Khan and Amir M Khater and Tawfik Ahmed Muthafer Khoja and Jagdish Khubchandani and Min Seo Kim and Yun Jin Kim and Ruth W Kimokoti and Sezer Kisa and Adnan Kisa and Mika Kivimäki and Vladimir Andreevich Korshunov and Oleksii Korzh and Ai Koyanagi and Kewal Krishan and Barthelemy {Kuate Defo} and G Anil Kumar and Nithin Kumar and Dian Kusuma and Carlo {La Vecchia} and Ben Lacey and Anders O Larsson and Savita Lasrado and Wei-Chen Lee and Chiachi Bonnie Lee and Paul H Lee and Shaun Wen Huey Lee and Ming-Chieh Li and Stephen S Lim and Lee-Ling Lim and Giancarlo Lucchetti and Azeem Majeed and Ahmad Azam Malik and Borhan Mansouri and Lorenzo Giovanni Mantovani and Santi Martini and Prashant Mathur and Colm McAlinden and Nafiul Mehedi and Teferi Mekonnen and Ritesh G Menezes and Amanual Getnet Mersha and Junmei {Miao Jonasson} and Tomasz Miazgowski and Irmina Maria Michalek and Andreea Mirica and Erkin M Mirrakhimov and Agha Zeeshan Mirza and Prasanna Mithra and Abdollah Mohammadian-Hafshejani and Reza Mohammadpourhodki and Arif Mohammed and Ali H Mokdad and Mariam Molokhia and Lorenzo Monasta and Mohammad Ali Moni and Farhad Moradpour and Rahmatollah Moradzadeh and Ebrahim Mostafavi and Ulrich Otto Mueller and Christopher J L Murray and Ahmad Mustafa and Gabriele Nagel and Vinay Nangia and Atta Abbas Naqvi and Biswa Prakash Nayak and Javad Nazari and Rawlance Ndejjo and Ruxandra Irina Negoi and Sandhya {Neupane Kandel} and Cuong Tat Nguyen and Huong Lan Thi Nguyen and Jean Jacques Noubiap and Christoph Nowak and Bogdan Oancea and Oluwakemi Ololade Odukoya and Ayodipupo Sikiru Oguntade and Temitope T Ojo and Andrew T Olagunju and Obinna E Onwujekwe and Alberto Ortiz and Mayowa O Owolabi and Raffaele Palladino and Songhomitra Panda-Jonas and Seithikurippu R Pandi-Perumal and Shahina Pardhan and Tarang Parekh and Mojtaba Parvizi and Veincent Christian Filipino Pepito and Arokiasamy Perianayagam and Ionela-Roxana Petcu and Manju Pilania and Vivek Podder and Roman V Polibin and Maarten J Postma and Akila Prashant and Navid Rabiee and Mohammad Rabiee and Vafa Rahimi-Movaghar and Muhammad Aziz Rahman and Md. Mosfequr Rahman and Mosiur Rahman and Setyaningrum Rahmawaty and Nazanin Rajai and Pradhum Ram and Juwel Rana and Kamal Ranabhat and Priyanga Ranasinghe and Chythra R Rao and Satish Rao and Salman Rawaf and David Laith Rawaf and Lal Rawal and Andre M N Renzaho and Nima Rezaei and Aziz Rezapour and Seyed Mohammad Riahi and Daniela Ribeiro and Jefferson Antonio Buendia Rodriguez and Leonardo Roever and Peter Rohloff and Godfrey M Rwegerera and Paul MacDaragh Ryan and Maha Mohamed Saber-Ayad and Siamak Sabour and Basema Saddik and Sahar {Saeedi Moghaddam} and Amirhossein Sahebkar and Harihar Sahoo and KM Saif-Ur-Rahman and Hamideh Salimzadeh and Mehrnoosh Samaei and Juan Sanabria and Milena M Santric-Milicevic and Brijesh Sathian and Thirunavukkarasu Sathish and Markus P Schlaich and Abdul-Aziz Seidu and Mario Šekerija and Nachimuthu {Senthil Kumar} and Allen Seylani and Masood Ali Shaikh and Hina Shamshad and Md Shajedur Rahman Shawon and Sara Sheikhbahaei and Jeevan K Shetty and Rahman Shiri and K M Shivakumar and Kerem Shuval and Jasvinder A Singh and Ambrish Singh and Valentin Yurievich Skryabin and Anna Aleksandrovna Skryabina and Ahmad Sofi-Mahmudi and Amin Soheili and Jing Sun and Viktória Szerencsés and Miklós Szócska and Rafael Tabarés-Seisdedos and Hooman Tadbiri and Eyayou Girma Tadesse and Md. Tariqujjaman and Kavumpurathu Raman Thankappan and Rekha Thapar and Nihal Thomas and Binod Timalsina and Ruoyan Tobe-Gai and Marcello Tonelli and Marcos Roberto Tovani-Palone and Bach Xuan Tran and Jaya Prasad Tripathy and Lorainne {Tudor Car} and Biruk Shalmeno Tusa and Riaz Uddin and Era Upadhyay and Sahel {Valadan Tahbaz} and Pascual R Valdez and Tommi Juhani Vasankari and Madhur Verma and Victor E Villalobos-Daniel and Sergey Konstantinovitch Vladimirov and Bay Vo and Giang Thu Vu and Rade Vukovic and Yasir Waheed and Richard G Wamai and Andrea Werdecker and Nuwan Darshana Wickramasinghe and Andrea Sylvia Winkler and Befikadu Legesse Wubishet and Xiaoyue Xu and Suowen Xu and Seyed Hossein {Yahyazadeh Jabbari} and Hiroshi Yatsuya and Sanni Yaya and Taklo Simeneh Yazie Yazie and Siyan Yi and Naohiro Yonemoto and Ismaeel Yunusa and Siddhesh Zadey and Sojib Bin Zaman and Maryam Zamanian and Nelson Zamora and Mikhail Sergeevich Zastrozhin and Anasthasia Zastrozhina and Zhi-Jiang Zhang and Chenwen Zhong and Mohammad Zmaili and Alimuddin Zumla and Mohsen Naghavi and Maria Inês Schmidt;"Endocrinology (Q1); Endocrinology, Diabetes and Metabolism (Q1); Internal Medicine (Q1)";220.0;805.0;Netherlands;2013-2020;10.1016/S2213-8587(21)00349-1;;114.0;;22138587;22138595;22138587;22138595;;;Elsevier BV;;2283.0;Western Europe;8596.0;Q1;21100237403.0;Lancet diabetes and endocrinology,the;Diabetes mortality and trends before 25 years of age: an analysis of the global burden of disease study 2019;;5391.0;218.0;632.0;4977.0;journal;article;2022
"Summary
Benchmarking and monitoring of urban design and transport features is crucial to achieving local and international health and sustainability goals. However, most urban indicator frameworks use coarse spatial scales that either only allow between-city comparisons, or require expensive, technical, local spatial analyses for within-city comparisons. This study developed a reusable, open-source urban indicator computational framework using open data to enable consistent local and global comparative analyses. We show this framework by calculating spatial indicators—for 25 diverse cities in 19 countries—of urban design and transport features that support health and sustainability. We link these indicators to cities’ policy contexts, and identify populations living above and below critical thresholds for physical activity through walking. Efforts to broaden participation in crowdsourcing data and to calculate globally consistent indicators are essential for planning evidence-informed urban interventions, monitoring policy effects, and learning lessons from peer cities to achieve health, equity, and sustainability goals.";Geoff Boeing and Carl Higgs and Shiqin Liu and Billie Giles-Corti and James F Sallis and Ester Cerin and Melanie Lowe and Deepti Adlakha and Erica Hinckson and Anne Vernez Moudon and Deborah Salvo and Marc A Adams and Ligia V Barrozo and Tamara Bozovic and Xavier Delclòs-Alió and Jan Dygrýn and Sara Ferguson and Klaus Gebel and Thanh Phuong Ho and Poh-Chin Lai and Joan C Martori and Kornsupha Nitvimol and Ana Queralt and Jennifer D Roberts and Garba H Sambo and Jasper Schipperijn and David Vale and Nico {Van de Weghe} and Guillem Vich and Jonathan Arundel;Medicine (miscellaneous) (Q1);299.0;617.0;United Kingdom;2013-2020;10.1016/S2214-109X(22)00072-9;;88.0;;2214109X;2214109X;2214109X;2214109X;;;Elsevier BV;;1506.0;Western Europe;7970.0;Q1;21100265444.0;Lancet global health,the;Using open data and open-source software to develop spatial indicators of urban design and transport features for achieving healthy and sustainable cities;;6192.0;332.0;972.0;4999.0;journal;article;2022
"Introduction
The rapidly evolving COVID-19 pandemic has dramatically reshaped urban travel patterns. In this research, we explore the relationship between “social distancing,” a concept that has gained worldwide familiarity, and urban mobility during the pandemic. Understanding social distancing behavior will allow urban planners and engineers to better understand the new norm of urban mobility amid the pandemic, and what patterns might hold for individual mobility post-pandemic or in the event of a future pandemic.
Methods
There are still few efforts to obtain precise information on social distancing patterns of pedestrians in urban environments. This is largely attributed to numerous burdens in safely deploying any effective field data collection approaches during the crisis. This paper aims to fill that gap by developing a data-driven analytical framework that leverages existing public video data sources and advanced computer vision techniques to monitor the evolution of social distancing patterns in urban areas. Specifically, the proposed framework develops a deep-learning approach with a pre-trained convolutional neural network to mine the massive amount of public video data captured in urban areas. Real-time traffic camera data collected in New York City (NYC) was used as a case study to demonstrate the feasibility and validity of using the proposed approach to analyze pedestrian social distancing patterns.
Results
The results show that microscopic pedestrian social distancing patterns can be quantified by using a generalized real-distance approximation method. The estimated distance between individuals can be compared to social distancing guidelines to evaluate policy compliance and effectiveness during a pandemic. Quantifying social distancing adherence will provide decision-makers with a better understanding of prevailing social contact challenges. It also provides insights into the development of response strategies and plans for phased reopening for similar future scenarios.";Fan Zuo and Jingqin Gao and Abdullah Kurkcu and Hong Yang and Kaan Ozbay and Qingyu Ma;"Health Policy (Q1); Pollution (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1); Public Health, Environmental and Occupational Health (Q2); Transportation (Q2)";402.0;253.0;Netherlands;2013-2020;10.1016/j.jth.2021.101032;;30.0;;22141405;22141405;22141405;22141405;;Social distancing, COVID-19, Close contact, Pedestrian, Deep learning, Computer vision;Elsevier BV;;4720.0;Western Europe;898.0;Q1;21100310028.0;Journal of transport and health;Reference-free video-to-real distance approximation-based urban social distancing analytics amid covid-19 pandemic;;1290.0;140.0;436.0;6608.0;journal;article;2021
This paper examines and analyses weather data in Oman to re-encode the appropriate climate dimension to generate solar energy. It also suggested prediction models that could accurately predict future weather information. The present study aims to help decision-makers take the necessary measures to address the demand for renewable energy generation and solutions to environmental problems by taking advantage of long daylight hours in Oman to increase the production of alternative and clean electricity. There is no doubt that different environmental factors such as temperature, humidity, wind intensity and rain have a significant impact on the amount of solar cells produced. However, accurate forecasting of temperature and humidity helps to select the best weather conditions that can help raise the generation of solar energy and reduce the cost of production, leading to an increase in the economic income of countries. This paper presents various mathematical prediction models based on a multi-boundary score (2, 3, 4), which has the value of the R2 determination factor equal to (0.9335, 0.9603, 0.9977), respectively. The column test results (Prob> F) proved that the null hypothesis was accepted and rejected the alternative hypothesis. Thus, all the results are less than the significant value (0.5), and each variable has an average value or less than the mean value of the test (26). Therefore, there are no significant differences or unusual cases in historical temperature data in Oman from 1991 to 2015. Also, the prediction values corresponding to the actual temperature in the future, which helps to predict and analyze the temperature data at any time.;Jabar H. Yousif and Haitham A. Al-Balushi and Hussein A. Kazem and Miqdam T. Chaichan;"Engineering (miscellaneous) (Q1); Fluid Flow and Transfer Processes (Q1)";400.0;492.0;United Kingdom;2013-2020;10.1016/j.csite.2018.11.006;0.00345;37.0;;2214157X;2214157X;2214157X;2214157X;4.724;Climate data, Forecasting models, Renewable energy, Data analysis and visualization, Environment monitoring;Elsevier BV;;3153.0;Western Europe;913.0;Q1;21100262580.0;Case studies in thermal engineering;Analysis and forecasting of weather conditions in oman for renewable energy applications;2844.0;1987.0;224.0;400.0;7063.0;journal;article;2019
The number of chemicals with potential to reach the environment is still largely unknown, which poses great challenges for both environmental scientists and analytical chemists. Liquid chromatography coupled to high-resolution mass spectrometry (LC-HRMS) is currently the instrumentation of choice for identification of wide-scope polar chemicals of concern (CECs) in water. This review critically evaluates all steps involved in screening for polar CECs in water, including sampling and extraction, analysis by LC-HRMS, data (pre-)treatment, evaluation and reporting. Passive samplers and direct injection, in combination with LC-HRMS, provide new opportunities compared with conventional grab water sampling, as do instrumental advances such as ion-mobility spectrometry coupled to HRMS (IM-HRMS). In this paper, we argue that target, suspect and non-target screening should not be viewed as three separate principles, but rather as conceptual approaches to general data treatment strategies that can be linked together. Due to the large amount of data generated, smart prioritisation strategies are needed, in particular for non-target screening, to reduce complexity and focus on data of high interest. We critically evaluate existing strategies and consider that each prioritisation step will result in data loss (as any other step in a screening study), requiring compromises depending on the research question to be tackled. Many different data treatment strategies have been developed in recent years, but structure elucidation remains a challenging and time-consuming task. We discuss current and potential future trends, e.g. effect-based methods that can be used as future prioritisation tools, technological advances like IM-HRMS and improved software solutions that can enable new data treatment strategies.;Frank Menger and Pablo Gago-Ferrero and Karin Wiberg and Lutz Ahrens;"Analytical Chemistry (Q1); Environmental Chemistry (Q1)";31.0;940.0;Netherlands;2014-2020;10.1016/j.teac.2020.e00102;0.0008;25.0;;22141588;22141588;22141588;22141588;9.600;Aquatic environment, Organic, Emerging micropollutant, Non-Target screening, Suspect screening, LC, HRMS, Water analysis;Elsevier BV;;10439.0;Western Europe;1568.0;Q1;21100316072.0;Trends in environmental analytical chemistry;Wide-scope screening of polar contaminants of concern in water: a critical review of liquid chromatography-high resolution mass spectrometry-based strategies;647.0;287.0;31.0;31.0;3236.0;journal;article;2020
As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.;Suresh Neethirajan and Bas Kemp;"Electrical and Electronic Engineering (Q1); Electronic, Optical and Magnetic Materials (Q1); Biotechnology (Q2); Signal Processing (Q2)";140.0;412.0;Netherlands;2014-2020;10.1016/j.sbsr.2021.100408;;28.0;;22141804;22141804;22141804;22141804;;Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture;Elsevier BV;;3995.0;Western Europe;770.0;Q1;21100356802.0;Sensing and bio-sensing research;Digital livestock farming;;713.0;80.0;140.0;3196.0;journal;article;2021
Cooperative Intelligent Transport System (C-ITS) is a key enabler of future road traffic management systems. The core component of C-ITS includes vehicles, road side units and traffic command centers. They generate a large amount of traffic that is made up of both mobility and service-related data. To extract useful and relevant information out of this data, data analytics will play a crucial role in future C-ITS applications. We present a review of how data analytics can benefit C-ITS applications. We describe the various types of data generated by C-ITS applications and potential dissemination techniques using various wireless technologies. We demonstrate how meaningful results from this data could be beneficial to C-ITS. We also demonstrate the improved reliability of C-ITS applications that can be achieved with data analytics using simulation results. Finally, we discuss future possible applications of data analytics in C-ITS.;Muhammad Awais Javed and Sherali Zeadally and Elyes Ben Hamida;"Automotive Engineering (Q1); Communication (Q1); Electrical and Electronic Engineering (Q1)";112.0;842.0;United States;2014-2020;10.1016/j.vehcom.2018.10.004;0.0016600000000000002;32.0;;22142096;22142096;22142096;22142096;6.910;Intelligent transport systems, Data analytics, Route guidance application;Elsevier Inc.;;5956.0;Northern America;1298.0;Q1;21100324363.0;Vehicular communications;Data analytics for cooperative intelligent transport systems;1167.0;965.0;48.0;116.0;2859.0;journal;article;2019
Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.;Roberto O Andrade and Sang Guun Yoo;"Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)";292.0;543.0;United Kingdom;2013-2020;10.1016/j.jisa.2019.06.008;0.00209;40.0;;22142126;22142126;22142134;22142126;3.872;Cognitive security, Cognitive science, Situation awareness, Cyber operations;Elsevier Ltd.;;4677.0;Western Europe;610.0;Q2;21100332403.0;Journal of information security and applications;Cognitive security: a comprehensive study of cognitive science in cybersecurity;1526.0;1526.0;183.0;297.0;8559.0;journal;article;2019
Earth Observation data acquired by the Landsat missions are of immense value to the global community and constitute the world’s longest continuous civilian Earth Observation program. However, because of the costs of data storage infrastructure these data have traditionally been stored in raw form on tape storage infrastructures which introduces a data retrieval and processing overhead that limits the efficiency of use of this data. As a consequence these data have become ‘dark data’ with only limited use in a piece-meal and labor intensive manner. The Unlocking the Landsat Archive project was set up in 2011 to address this issue and to help realize the true value and potential of these data. The key outcome of the project was the migration of the raw Landsat data that was housed in tape archives at Geoscience Australia to High Performance Data facilities hosted by the National Computational Infrastructure (a super computer facility located at the Australian National University). Once this migration was completed the data were calibrated to produce a living and accessible archive of sensor and scene independent data products derived from Landsat-5 and Landsat-7 data for the period 1998–2012. The calibrated data were organized into High Performance Data structures, underpinned by ISO/OGC standards and web services, which have opened up a vast range of opportunities to efficiently apply these data to applications across multiple scientific domains.;Matthew B.J. Purss and Adam Lewis and Simon Oliver and Alex Ip and Joshua Sixsmith and Ben Evans and Roger Edberg and Glenn Frankish and Lachlan Hurst and Tai Chan;Earth and Planetary Sciences (miscellaneous) (Q3);27.0;0.0;United Kingdom;2014-2017;10.1016/j.grj.2015.02.010;;14.0;;22142428;22142428;22142428;22142428;;Big data, Landsat, High Performance Data, High Performance Computing, Data rescue, Earth Observation;Elsevier Ltd.;;0.0;Western Europe;348.0;Q3;21100324361.0;Georesj;Unlocking the australian landsat archive – from dark data to high performance data infrastructures;;63.0;0.0;27.0;0.0;journal;article;2015
Holistic information systems for climate-smart agriculture demands the seamless integration of various categories of climate, meteorological and weather data. Any actor in the agricultural value chain may harness weather forecasts at the short and medium-range, local weather history, and prevailing climatic conditions, to inform decision-making. Weather is fundamental to many day-to-day operations, especially at farm-level, influencing decision-making at various spatial and temporal scales. Many operational decisions ideally require hyper-localized service provision. In practice, integrating weather information into decision-support services demands a comprehensive understanding of various categories of weather-related data, their genesis, as well as the specific standards and data formats used by the meteorological community. This paper considers the weather as a crucial context for the delivery of farm-level operational services in smart agriculture, highlighting critical issues for reflection by system designers during the service design and implementation phases.;Michael O'Grady and David Langton and Francesca Salinari and Peter Daly and Gregory O'Hare;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Aquatic Science (Q1); Forestry (Q1); Computer Science Applications (Q2)";127.0;641.0;China;2014-2020;10.1016/j.inpa.2020.07.003;;27.0;;22143173;22143173;22143173;22143173;;Smart agriculture, Climate services, Agrometeorology, Precision agriculture;Elsevier BV;;4635.0;Asiatic Region;769.0;Q1;21100804573.0;Information processing in agriculture;Service design for climate-smart agriculture;;943.0;78.0;127.0;3615.0;journal;article;2021
Due to the comparatively low frequency of long-distance and overnight travel, it can be challenging to measure using traditional travel surveys. In response to this dearth of data, several recent surveys have included self-reported frequency questions. However, the value and accuracy of these questions is unclear. This study leverages data from 628 panel members who completed a year-long, online survey in 2013–2014 by comparing their one-time self-assessment of typical overnight trip frequency to those reported in 12 subsequent monthly surveys. The self-assessed frequency of overnight tours, airplane tours and tours to non-North American destinations are consistent for only 68% and 70% of respondents for work and personal tours respectively. For most tour types, consistency is highest for low frequency (never and <1 per year) suggesting, unsurprisingly, that individuals are good at knowing they do not travel. Inconsistent estimators both over- and under-estimated trip-making suggest that not only is recall an issue but that prestige bias and the complexity and variability of long-distance travel are factors in recall as well. Few demographic factors were statistically significant in estimating whether participants were consistent reporters. Only 9% of participants consistently either over- or under-estimated both work and personal trips suggesting that there is simply a general inaccuracy in these survey questions for measuring long-distance travel. The aggregate trip rate across people and trip types was accurate suggesting this crude frequency measure may be acceptable for total frequency but not for understanding relative patterns or details in overnight long-distance travel.;Jonathan Dowds and Lisa Aultman-Hall and Jeffrey J. LaMondia;Transportation (Q1);151.0;545.0;Netherlands;2014-2021;10.1016/j.tbs.2019.12.004;0.0022199999999999998;26.0;;2214367X;2214367X;2214367X;2214367X;4.983;Travel survey, Long-distance travel, Trip recall, Overnight travel;Elsevier BV;;5671.0;Western Europe;1695.0;Q1;21100316002.0;Travel behaviour and society;Comparing alternative methods of collecting self-assessed overnight long-distance travel frequencies;1218.0;822.0;94.0;155.0;5331.0;journal;article;2020
High-throughput maize phenotyping at both organ and plant levels plays a key role in molecular breeding for increasing crop yields. Although the rapid development of light detection and ranging (LiDAR) provides a new way to characterize three-dimensional (3D) plant structure, there is a need to develop robust algorithms for extracting 3D phenotypic traits from LiDAR data to assist in gene identification and selection. Accurate 3D phenotyping in field environments remains challenging, owing to difficulties in segmentation of organs and individual plants in field terrestrial LiDAR data. We describe a two-stage method that combines both convolutional neural networks (CNNs) and morphological characteristics to segment stems and leaves of individual maize plants in field environments. It initially extracts stem points using the PointCNN model and obtains stem instances by fitting 3D cylinders to the points. It then segments the field LiDAR point cloud into individual plants using local point densities and 3D morphological structures of maize plants. The method was tested using 40 samples from field observations and showed high accuracy in the segmentation of both organs (F-score =0.8207) and plants (F-score =0.9909). The effectiveness of terrestrial LiDAR for phenotyping at organ (including leaf area and stem position) and individual plant (including individual height and crown width) levels in field environments was evaluated. The accuracies of derived stem position (position error =0.0141 m), plant height (R2 >0.99), crown width (R2 >0.90), and leaf area (R2 >0.85) allow investigating plant structural and functional phenotypes in a high-throughput way. This CNN-based solution overcomes the major challenges in organ-level phenotypic trait extraction associated with the organ segmentation, and potentially contributes to studies of plant phenomics and precision agriculture.;Zurui Ao and Fangfang Wu and Saihan Hu and Ying Sun and Yanjun Su and Qinghua Guo and Qinchuan Xin;"Agronomy and Crop Science (Q1); Plant Science (Q1)";202.0;455.0;China;1970, 2013-2020;10.1016/j.cj.2021.10.010;0.0038399999999999997;29.0;;22145141;20955421;22145141;20955421;4.407;Terrestrial LiDAR, Phenotype, Organ segmentation, Convolutional neural networks;Institute of Crop Sciences (ICS);;5278.0;Asiatic Region;1437.0;Q1;21100791247.0;Crop journal;Automatic segmentation of stem and leaf components and individual maize plants in field terrestrial lidar data using convolutional neural networks;2168.0;1024.0;127.0;205.0;6703.0;journal;article;2021
The size of molecular datasets has been growing exponentially since the mid 1980s, and new technologies have now dramatically increased the slope of this increase. New datasets include genomes, transcriptomes, and hybrid capture data, producing hundreds or thousands of loci. With these datasets, we are approaching a consensus on the higher level insect phylogeny. Huge datasets can produce new challenges in interpreting branch support, and new opportunities in developing better models and more sophisticated partitioning schemes. Dating analyses are improving as we recognize the importance of careful fossil calibration selection. With thousands of genes now available, coalescent methods have come of age. Barcode libraries continue to expand, and new methods are being developed for incorporating them into phylogenies with tens of thousands of individuals.;Karl Kjer and Marek L Borowiec and Paul B Frandsen and Jessica Ware and Brian M Wiegmann;"Ecology, Evolution, Behavior and Systematics (Q1); Insect Science (Q1)";310.0;446.0;Netherlands;2014-2021;10.1016/j.cois.2016.09.006;0.00861;41.0;;22145745;22145745;22145753;22145745;5.186;;Elsevier BV;;5622.0;Western Europe;2068.0;Q1;21100325079.0;Current opinion in insect science;Advances using molecular data in insect systematics;3455.0;1636.0;98.0;347.0;5510.0;journal;article;2016
We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.;Christina G. Skarpathiotaki and Konstantinos E. Psannis;"Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)";69.0;406.0;United States;2014-2020;10.1016/j.bdr.2021.100274;0.00126;25.0;;22145796;22145796;22145796;22145796;3.578;Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes;Elsevier Inc.;;4973.0;Northern America;565.0;Q2;21100356018.0;Big data research;Cross-industry process standardization for text analytics;586.0;324.0;11.0;76.0;547.0;journal;article;2022
"A series of weaknesses in creativity, research design, and quality of writing continue to handicap energy social science. Many studies ask uninteresting research questions, make only marginal contributions, and lack innovative methods or application to theory. Many studies also have no explicit research design, lack rigor, or suffer from mangled structure and poor quality of writing. To help remedy these shortcomings, this Review offers suggestions for how to construct research questions; thoughtfully engage with concepts; state objectives; and appropriately select research methods. Then, the Review offers suggestions for enhancing theoretical, methodological, and empirical novelty. In terms of rigor, codes of practice are presented across seven method categories: experiments, literature reviews, data collection, data analysis, quantitative energy modeling, qualitative analysis, and case studies. We also recommend that researchers beware of hierarchies of evidence utilized in some disciplines, and that researchers place more emphasis on balance and appropriateness in research design. In terms of style, we offer tips regarding macro and microstructure and analysis, as well as coherent writing. Our hope is that this Review will inspire more interesting, robust, multi-method, comparative, interdisciplinary and impactful research that will accelerate the contribution that energy social science can make to both theory and practice.";Benjamin K. Sovacool and Jonn Axsen and Steve Sorrell;"Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Nuclear Energy and Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Social Sciences (miscellaneous) (Q1)";801.0;686.0;United Kingdom;2014-2020;10.1016/j.erss.2018.07.007;;63.0;;22146296;22146296;22146296;22146296;;Validity, Research methods, Research methodology, Interdisciplinary research, Research excellence;Elsevier Ltd.;;7949.0;Western Europe;2313.0;Q1;21100325067.0;Energy research and social science;Promoting novelty, rigor, and style in energy social science: towards codes of practice for appropriate methods and research design;;5449.0;419.0;801.0;33308.0;journal;article;2018
This paper analyses literature contributions in the search for safety stock problem under uncertainties and risks in the procurement process, focusing on the dimensioning problem (determination of the safety stock level). We perform a systematic literature review (SLR) from 1995 to 2019 in relevant journals, covering 193 selected articles. These selected articles were classified into three safety stock main issues: safety stock dimensioning, safety stock management, and safety stock positioning, allocation or placement. The SLR analysis allowed the identification of literature gaps and research opportunities, thus providing a road map to guide future research on this topic.;Júlio Barros and Paulo Cortez and M. Sameiro Carvalho;"Control and Optimization (Q1); Management Science and Operations Research (Q2); Statistics and Probability (Q2); Strategy and Management (Q2)";90.0;400.0;Netherlands;2014-2020;10.1016/j.orp.2021.100192;;16.0;;22147160;22147160;22147160;22147160;;Safety stocks, Inventory management, Procurement, Supply chain risk management, Uncertainty factors, Systematic literature review;Elsevier BV;;5300.0;Western Europe;697.0;Q1;21100379294.0;Operations research perspectives;A systematic literature review about dimensioning safety stock under uncertainties and risks in the procurement process;;357.0;33.0;91.0;1749.0;journal;article;2021
"ABSTRACT
Background
Most users of unsupported Internet interventions visit that site only once, therefore there is a need to create interventions that can be offered as a single brief interaction with the user.
Objective
The main goal of this study was to compare the effect of a one-session unsupported Internet intervention on participants' clinical symptoms (depressive and anxiety symptoms) and related variables (mood, confidence and motivation).
Method
A total of 765 adults residing in the United States took part in a randomized controlled trial. Participants were randomly assigned to one of five brief plain text interventions lasting 5–10min. The interventions designed to address depressive symptoms were: thoughts (increasing helpful thoughts), activities (increasing activity level), sleep hygiene, assertiveness (increasing assertiveness awareness), Own Methods (utilizing methods that were previously successful). They were followed-up one week after consenting.
Results
A main effect of time was observed for both depression (F(1, 563)=234.70, p<0.001) and anxiety (F(1, 551)=170.27, p<0.001). In all cases, regardless of assigned condition and Major Depressive Episode status, mean scores on both positive outcomes (mood, confidence and motivation) and negative outcome scores (depression and anxiety) improved over time.
Conclusions
Brief unsupported Internet interventions can improve depressive symptoms at one-week follow-up. Further outcome data and research implications will be discussed.";Eduardo L. Bunge and Rachel E. Williamson and Monique Cano and Yan Leykin and Ricardo F. Muñoz;Health Informatics (Q1);158.0;414.0;Netherlands;2014-2020;10.1016/j.invent.2016.06.001;;31.0;;22147829;22147829;22147829;22147829;;Depression, Anxiety, Mood, Confidence, Motivation, Internet intervention;Elsevier BV;;5259.0;Western Europe;1097.0;Q1;21100324367.0;Internet interventions;Mood management effects of brief unsupported internet interventions;;693.0;61.0;159.0;3208.0;journal;article;2016
The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.;Harika Vanam and Jeberson {Retna Raj R};Materials Science (miscellaneous);10409.0;124.0;United Kingdom;2005, 2014-2020;10.1016/j.matpr.2020.11.486;;47.0;;22147853;22147853;22147853;22147853;;Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm;Elsevier Ltd.;;2215.0;Western Europe;341.0;-;21100370037.0;Materials today: proceedings;Analysis of twitter data through big data based sentiment analysis approaches;;14096.0;3996.0;10585.0;88521.0;conference and proceedings;article;2021
"ABSTRACT
The world continues to experience a surge in data generation and digital transformation. Historic data is increasingly being replaced by modernized data, such as big data, which is regarded as data that exhibits the 5Vs: volume, variety, velocity, veracity and value. The capacity to optimally use and comprehend value from big data has become an indispensable aptitude for modern companies. In contrast to commercial and technology firms, usage, management and governance of data, including big data is a novel and evolving trend for mining and mineral industries. Although the mining industry can be unenthusiastic to change, embracing modernized data and big data is evolutionarily unavoidable, given many industry-wide challenges (i.e., fluctuation in commodity prices, geotechnical and harsh ground conditions, and ore grade), which corrode revenues and increase business risks, including the possibility of regulatory non-compliance. The minerals industry holds a genuine gold mine of data that were collected for scientific, engineering, operational and other purposes. Data and data-centric workspaces that are targeted towards innovation and experimentation, which if combined with in-discipline expertise are two harmonious ingredients that can provide many practical solutions for the mining and mineral industries. In this paper, the concept, the opportunity and the necessity for a move towards a technology- and innovation-based, data-centric ‘dry laboratories’ (common workspaces that facilitates data-centric experimentation and innovation) in the minerals industry are assessed. We contend that the dry laboratory environment maximizes the value of data for the minerals industry. Toward the establishment of dry laboratories, we propose several essential components of a framework that would enable the functionality of dry laboratories in the minerals industry, while concomitantly examining the components from both academia and industry perspectives.";Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau;"Development (Q1); Economic Geology (Q1); Geography, Planning and Development (Q1); Management, Monitoring, Policy and Law (Q1)";313.0;359.0;Netherlands;2014-2020;10.1016/j.exis.2022.101089;;29.0;;2214790X;2214790X;2214790X;2214790X;;Dry laboratory, Data analytics, Process simulation, Mining industry, Data-centric, Data-driven;Elsevier BV;;6198.0;Western Europe;999.0;Q1;21100305261.0;Extractive industries and society;Framework components for data-centric dry laboratories in the minerals industry: a path to science-and-technology-led innovation;;1173.0;206.0;337.0;12767.0;journal;article;2022
The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.;Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin;"Applied Microbiology and Biotechnology (Q1); Food Science (Q1)";286.0;516.0;Netherlands;2015-2021;10.1016/j.cofs.2020.11.006;0.00491;38.0;;22147993;22147993;22147993;22147993;6.031;;Elsevier BV;;5725.0;Western Europe;1297.0;Q1;21100370190.0;Current opinion in food science;Big data in food safety- a review;3298.0;1714.0;89.0;318.0;5095.0;journal;article;2020
The material extrusion additive manufacturing (AM) has been extensively used in fabricating structures with complex geometries. However, geometric defects often exist in an AM structure, which could compromise its final performance. In this paper, a real-time multiscale performance evaluation method is developed for material extrusion-based honeycomb structures. The representative cell boundary is extracted from three-dimensional (3D) point clouds obtained via an in-situ monitoring approach. The cell boundary is then used to generate the digital twin of the unit cell of the printed layer based on the finite element (FE) method. A physics-based multiscale modeling approach called mechanics of structure genome (MSG) is then employed to predict the effective material properties of the printed layer and plate stiffness matrix of the final structure. The proposed approach provides a highly efficient way to predict the real-time performance of the as-manufactured products. Moreover, the numerical example shows that the geometric defects could result in complex mechanical behaviors in the defected parts, which cannot be captured by the conventional approaches based on the shape deviations. The numerical results are validated by the three-point bending tests. The proposed method can be used in the closed-loop control of material extrusion-based manufacturing systems.;Xin Liu and Chen Kan and Zehao Ye;"Biomedical Engineering (Q1); Engineering (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Materials Science (miscellaneous) (Q1)";763.0;1184.0;Netherlands;2014-2020;10.1016/j.addma.2021.102503;0.01844;65.0;;22148604;22148604;22148604;22148604;10.998;Multiscale modeling, In-situ monitoring, Material extrusion, Honeycomb structure, Geometric defects;Elsevier BV;;5191.0;Western Europe;2710.0;Q1;21100349533.0;Additive manufacturing;Real-time multiscale prediction of structural performance in material extrusion additive manufacturing;12332.0;9606.0;644.0;763.0;33427.0;journal;article;2022
"Target maneuver trajectory prediction is an important prerequisite for air combat situation awareness and maneuver decision-making. However, how to use a large amount of trajectory data generated by air combat confrontation training to achieve real-time and accurate prediction of target maneuver trajectory is an urgent problem to be solved. To solve this problem, in this paper, a hybrid algorithm based on transfer learning, online learning, ensemble learning, regularization technology, target maneuvering segmentation point recognition algorithm, and Volterra series, abbreviated as AERTrOS-Volterra is proposed. Firstly, the model makes full use of a large number of trajectory sample data generated by air combat confrontation training, and constructs a Tr-Volterra algorithm framework suitable for air combat target maneuver trajectory prediction, which realizes the extraction of effective information from the historical trajectory data. Secondly, in order to improve the real-time online prediction accuracy and robustness of the prediction model in complex electromagnetic environments, on the basis of the Tr-Volterra algorithm framework, a robust regularized online Sequential Volterra prediction model is proposed by integrating online learning method, regularization technology and inverse weighting calculation method based on the priori error. Finally, inspired by the preferable performance of models ensemble, ensemble learning scheme is also incorporated into our proposed algorithm, which adaptively updates the ensemble prediction model according to the performance of the model on real-time samples and the recognition results of target maneuvering segmentation points, including the adaptation of model weights; adaptation of parameters; and dynamic inclusion and removal of models. Compared with many existing time series prediction methods, the newly proposed target maneuver trajectory prediction algorithm can fully mine the prior knowledge contained in the historical data to assist the current prediction. The rationality and effectiveness of the proposed algorithm are verified by simulation on three sets of chaotic time series data sets and a set of real target maneuver trajectory data sets.";Xi Zhi-fei and Kou Ying-xin and Li Zhan-wu and Lv Yue and Xu An and Li You and Li Shuang-qing;"Metals and Alloys (Q1); Ceramics and Composites (Q2); Computational Mechanics (Q2); Mechanical Engineering (Q2)";261.0;379.0;China;2013-2020;10.1016/j.dt.2022.06.006;0.0017800000000000001;28.0;;22149147;22149147;22149147;22149147;3.172;Maneuver trajectory prediction, Volterra series, Transfer learning, Online learning, Ensemble learning, Robust regularization;China Ordnance Society;;3664.0;Asiatic Region;560.0;Q1;21100823385.0;Defence technology;Air combat target maneuver trajectory prediction based on robust regularized volterra series and adaptive ensemble online transfer learning;1419.0;959.0;228.0;263.0;8353.0;journal;article;2022
Within CONNECTING Nature, we are dealing with developing innovative nature-based solutions (NBS) for climate change adaptation, health and well-being, social cohesion and sustainable economic development in European cities. In order to enable “learning by comparing” and “generating new knowledge” from multiple NBS related studies, a novel data and knowledge base is needed which requires a specified methodological approach for its development. This paper provides conceptual and methodological context and techniques for constructing such a data and knowledge base that will systematically support the process of NBS monitoring and assessment:•A methodology presents the comprehensive, multi-step approach to the NBS data and knowledge development that helps to guide work and influence the quality of an information included.•The paper describes the methodology and main steps/phases for developing a large data and knowledge base of NBS that will allow further systematic review.•The suggested methodology explains how to build NBS related databases from the conceptualization and requirements phases through to implementation and maintenance. In this regard, such a methodology is iterative, with extensive NBS stakeholders’ and end-user's involvement that are packaged with reusable templates or deliverables offering a good opportunity for success when used by practitioners and other end-users.•The NBS data and knowledge base gathers information about different NBS models and generations into one easy-to-find, easy-to-use place and provides detailed descriptions of each of the 1490 NBS cases from urban centers in Europe.•The data and knowledge base thus helps users identify the best and most appropriated NBS model/type for addressing the particular goals and, at the same time, considers the local context and potential.•The data obtained can be used for the further meta-analysis by applying statistics or searching for specific sample cases and thus enables to generate and expand the knowledge from multiple NBS related studies, in both qualitative and quantitative ways.;Diana Dushkova and Dagmar Haase;"Medical Laboratory Technology (Q2); Clinical Biochemistry (Q3)";548.0;184.0;Netherlands;2014-2020;10.1016/j.mex.2020.101096;;23.0;;22150161;22150161;22150161;22150161;;Nature-based solutions (NBS), Data- and knowledge base, Climate change, Societal challenges, Sustainability, Resilience, Urban Europe;Elsevier BV;;1809.0;Western Europe;356.0;Q2;21100317906.0;Methodsx;Methodology for development of a data and knowledge base for learning from existing nature-based solutions in europe: the connecting nature project;;1058.0;423.0;548.0;7652.0;journal;article;2020
"Summary
There is widespread agreement by health-care providers, medical associations, industry, and governments that automation using digital technology could improve the delivery and quality of care in psychiatry, and reduce costs. Many benefits from technology have already been realised, along with the identification of many challenges. In this Review, we discuss some of the challenges to developing effective automation for psychiatry to optimise physician treatment of individual patients. Using the perspective of automation experts in other industries, three examples of automation in the delivery of routine care are reviewed: (1) effects of electronic medical records on the patient interview; (2) effects of complex systems integration on e-prescribing; and (3) use of clinical decision support to assist with clinical decision making. An increased understanding of the experience of automation from other sectors might allow for more effective deployment of technology in psychiatry.";Michael Bauer and Scott Monteith and John Geddes and Michael J Gitlin and Paul Grof and Peter C Whybrow and Tasha Glenn;"Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)";224.0;488.0;United Kingdom;2014-2020;10.1016/S2215-0366(19)30041-0;;86.0;;22150366;22150374;22150366;22150374;;;Elsevier Ltd.;;1646.0;Western Europe;7447.0;Q1;21100356804.0;Lancet psychiatry,the;Automation to optimise physician treatment of individual patients: examples in psychiatry;;4496.0;347.0;937.0;5712.0;journal;article;2019
Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.;Chunquan Li and Yaqiong Chen and Yuling Shang;"Civil and Structural Engineering (Q1); Computer Networks and Communications (Q1); Electronic, Optical and Magnetic Materials (Q1); Fluid Flow and Transfer Processes (Q1); Hardware and Architecture (Q1); Mechanical Engineering (Q1); Metals and Alloys (Q1); Biomaterials (Q2)";407.0;509.0;Netherlands;2014-2020;10.1016/j.jestch.2021.06.001;;50.0;;22150986;22150986;22150986;22150986;;Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making;Elsevier BV;;4084.0;Western Europe;803.0;Q1;21100806003.0;Engineering science and technology, an international journal;A review of industrial big data for decision making in intelligent manufacturing;;2346.0;132.0;408.0;5391.0;journal;article;2022
Big data analytics and artificial intelligence, paired with blockchain technology, the Internet of Things, and other emerging technologies, are poised to revolutionise urban management. With massive amounts of data collected from citizens, devices, and traditional sources such as routine and well-established censuses, urban areas across the world have – for the first time in history – the opportunity to monitor and manage their urban infrastructure in real-time. This simultaneously provides previously unimaginable opportunities to shape the future of cities, but also gives rise to new ethical challenges. This paper provides a transdisciplinary synthesis of the developments, opportunities, and challenges for urban management and planning under this ongoing ‘digital revolution’ to provide a reference point for the largely fragmented research efforts and policy practice in this area. We consider both top-down systems engineering approaches and the bottom-up emergent approaches to coordination of different systems and functions, their implications for the existing physical and institutional constraints on the built environment and various planning practices, as well as the social and ethical considerations associated with this transformation from non-digital urban management to data-driven urban management.;Zeynep Engin and Justin {van Dijk} and Tian Lan and Paul A. Longley and Philip Treleaven and Michael Batty and Alan Penn;"Urban Studies (Q1); Geography, Planning and Development (Q2); Public Administration (Q2)";60.0;313.0;Netherlands;2012-2020;10.1016/j.jum.2019.12.001;;11.0;;22265856;25890360;22265856;25890360;;Data-driven society, Urban management and applications, Evidence-based decision making;Elsevier BV;;4028.0;Western Europe;587.0;Q1;21100907127.0;Journal of urban management;Data-driven urban management: mapping the landscape;;210.0;32.0;68.0;1289.0;journal;article;2020
The Asian Prostate Cancer (A-CaP) Study is an Asia-wide initiative that has been developed over the course of 2 years. The A-CaP Study is scheduled to begin in 2016, when each participating country or region will begin registration of newly diagnosed prostate cancer patients and conduct prognosis investigations. From the data gathered, common research themes will be identified, such as comparisons among Asian countries of background factors in newly diagnosed prostate cancer patients. This is the first Asia-wide study of prostate cancer and has developed from single country research efforts in this field, including in Japan and Korea. The inaugural Board Meeting of A-CaP was held on December 11, 2015 at the Research Center for Advanced Science and Technology, The University of Tokyo, attended by representatives of all participating countries and regions, who signed a memorandum of understanding concerning registration for A-CaP. Following the Board Meeting an A-CaP Launch Symposium was held. The symposium was attended by representatives of countries and regions participating in A-CaP, who gave presentations. Presentations and a keynote address were also delivered by representatives of the University of California San Francisco, USA, and the Peter MacCallum Cancer Centre, Australia, who provided insight and experience on similar databases compiled in their respective countries.;Hideyuki Akaza and Yoshihiko Hirao and Choung-Soo Kim and Mototsugu Oya and Seiichiro Ozono and Dingwei Ye and Matthew Cooperberg and Shiro Hinotsu and Ji Youl Lee and Gang Zhu and Mikio Namiki and Shigeo Horie and Byung Ha Chung and Chung-Hsin Chen and Ng Chi Fai and Lukman Hakim and Edmund Chiong and Jason Letran and Rainy Umbas and Kazuhiro Suzuki and Kazuo Nishimura and Teng Aik Ong and Bannakij Lojanapiwat and Tong-lin Wu and Wun-Jae Kim and Declan Murphy and Osamu Ogawa and Peter Carroll and Seiji Naito and Taiji Tsukamoto;Urology (Q2);84.0;214.0;United States;2015-2020;10.1016/j.prnil.2016.03.001;0.00081;16.0;;22878882;2287903X;22878882;2287903X;2.286;Asian Cancer, Database, Prospective Study, Prostate Cancer;Elsevier Inc.;;2931.0;Northern America;636.0;Q2;21100386506.0;Prostate international;Asia prostate cancer study (a-cap study) launch symposium;402.0;174.0;35.0;84.0;1026.0;journal;article;2016
In the case of a high-valuable asset, the Operation and Maintenance (O&M) phase requires heavy charges and more efforts than the installation (construction) phase, because it has long usage life and any accident of an asset during this period causes catastrophic damage to an industry. Recently, with the advent of emerging Information Communication Technologies (ICTs), we can get the visibility of asset status information during its usage period. It gives us new challenging issues for improving the efficiency of asset operations. One issue is to implement the Condition-Based Maintenance (CBM) approach that makes a diagnosis of the asset status based on wire or wireless monitored data, predicts the assets abnormality, and executes suitable maintenance actions such as repair and replacement before serious problems happen. In this study, we have addressed several aspects of CBM approach: definition, related international standards, procedure, and techniques with the introduction of some relevant case studies that we have carried out.;Jong-Ho Shin and Hong-Bae Jun;"Computational Mechanics (Q1); Computer Graphics and Computer-Aided Design (Q1); Engineering (miscellaneous) (Q1); Human-Computer Interaction (Q1); Modeling and Simulation (Q1); Computational Mathematics (Q2)";135.0;660.0;Netherlands;2014-2020;10.1016/j.jcde.2014.12.006;0.00146;24.0;;22884300;22885048;22884300;22885048;5.860;Condition-based maintenance, Predictive maintenance, Prognostic and health management;;;4116.0;Western Europe;764.0;Q1;21100820602.0;Journal of computational design and engineering;On condition based maintenance policy;1061.0;790.0;57.0;136.0;2346.0;journal;article;2015
Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.;Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan;"Computer Networks and Communications; Human-Computer Interaction; Information Systems";161.0;20.0;United States;2013, 2017;10.1109/ICRIIS.2017.8002473;;12.0;;23248157;23248149;23248157;23248149;;"Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text Analysis;Master Data Management";;;0.0;Northern America;178.0;-;21100298607.0;International conference on research and innovation in information systems, icriis;Descriptive analysis and text analysis in systematic literature review: a review of master data management;;123.0;0.0;163.0;0.0;conference and proceedings;inproceedings;2017
"ABSTRACT
Linear data projection is a commonly leveraged data scaling method for unbiased traffic data estimation. However, recent studies have shown that model estimations based on linearly projected data would certainly result in biased standard errors. Although methods have been developed to remove such biases for linear regression models, many transport models are nonlinear regression models. This study outlines the practical difficulties of the traditional approach to standard error estimation for generic nonlinear transport models, and proposes a bootstrapping mean value restoration method to accurately estimate the parameter standard errors of all nonlinear transport models based on linearly projected data. Comprehensive simulations with different settings using the most commonly adopted nonlinear functions in modeling traffic flow demonstrate that the proposed method outperforms the conventional method and accurately recovers the true standard errors. A case study of estimating a macroscopic fundamental diagram that illustrates situations necessitating the proposed method is presented.";Wai Wong and S.C. Wong and Henry X. Liu;"Engineering (miscellaneous) (Q1); Transportation (Q2)";167.0;342.0;United States;2013-2020;10.1080/23249935.2018.1519647;;39.0;;23249935;23249943;23249935;23249943;;Big data era, linear data projection, heteroscedasticity, bootstrap standard error, macroscopic fundamental diagram;Taylor and Francis Inc.;;4696.0;Northern America;873.0;Q1;21100237201.0;Transportmetrica a: transport science;Bootstrap standard error estimations of nonlinear transport models based on linearly projected data;;586.0;100.0;170.0;4696.0;journal;article;2019
The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.;Ezzine, Imane and Benhlima, Laila;"Computer Science Applications; Information Systems and Management; Management Science and Operations Research; Signal Processing";128.0;114.0;United States;2015;10.1109/CiSt49399.2021.9357200;;11.0;;23271884;2327185X;23271884;2327185X;;"COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance";;;0.0;Northern America;170.0;-;21100400809.0;Colloquium in information science and technology, cist;Technology against covid-19 a blockchain-based framework for data quality;;139.0;0.0;132.0;0.0;conference and proceedings;inproceedings;2020
Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.;Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)";1555.0;1237.0;United States;2014-2020;10.1109/JIOT.2020.2994556;0.032080000000000004;97.0;;23274662;23274662;23274662;23274662;9.471;"Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD)";Institute of Electrical and Electronics Engineers Inc.;;3472.0;Northern America;2075.0;Q1;21100338350.0;Ieee internet of things journal;Quality estimation for scarce scenarios within mobile crowdsensing systems;21151.0;20461.0;1163.0;1594.0;40380.0;journal;article;2020
"Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on <italic>TF masking</italic>, <italic>Filter&amp;Sum</italic> and <italic>mask-based MVDR</italic> neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.";Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong;"Acoustics and Ultrasonics (Q1); Computational Mathematics (Q1); Computer Science (miscellaneous) (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1); Media Technology (Q1); Signal Processing (Q1); Speech and Hearing (Q1)";560.0;757.0;United States;2014-2020;10.1109/TASLP.2021.3078883;;56.0;;23299290;23299304;23299290;23299304;;;IEEE Advancing Technology for Humanity;IEEE Press;5011.0;Northern America;916.0;Q1;21100368801.0;Ieee/acm transactions on audio speech and language processing;Audio-visual multi-channel integration and recognition of overlapped speech;;4218.0;245.0;563.0;12277.0;journal;article;2021
"Federated Learning (FL) has emerged as a privacy-preserving distributed machine learning paradigm. To motivate data owners to contribute towards FL, research on FL incentive mechanisms is gaining great interest. Existing monetary incentive mechanisms generally share the same FL model with all participants regardless of their contributions. Such an assumption can be unfair towards participants who contributed more and promote undesirable free-riding, especially when the final model is of great utility value to participants. In this paper, we propose a Fairness-Aware Incentive Mechanism for federated learning (FedFAIM) to address such problem. It satisfies two types of fairness notion: 1) aggregation fairness, which determines aggregation results according to data quality; 2) reward fairness, which assigns each participant a unique model with performance reflecting his contribution. Aggregation fairness is achieved through efficient gradient aggregation which examines local gradient quality and aggregates them based on data quality. Reward fairness is achieved through an efficient Shapley value-based contribution assessment method and a novel reward allocation method based on reputation and distribution of local and global gradients. We further prove reward fairness is theoretically guaranteed. Extensive experiments show that FedFAIM provides stronger incentives than similar non-monetary FL incentive mechanisms while achieving a high level of fairness.";Shi, Zhuan and Zhang, Lan and Yao, Zhenyu and Lyu, Lingjuan and Chen, Cen and Wang, Li and Wang, Junhao and Li, Xiang-Yang;"Information Systems (Q1); Information Systems and Management (Q1)";8.0;411.0;United States;2020;10.1109/TBDATA.2022.3183614;0.00134;6.0;;23327790;23327790;23327790;23327790;3.344;"Computational modeling;Resource management;Servers;Training;Collaborative work;Particle measurements;Atmospheric measurements;Federated Learning;Incentive Mechanism;Fairness";Institute of Electrical and Electronics Engineers Inc.;;1406.0;Northern America;959.0;Q1;21101019393.0;Ieee transactions on big data;Fedfaim: a model performance-based fair incentive mechanism for federated learning;636.0;37.0;71.0;9.0;998.0;journal;article;2022
Prominent urbanization forces governments to rethink their management processes, incorporate new technologies and ensure quality of life through practices aligned with the concepts of smart and sustainable cities. With a qualitative approach through semi-structured interviews with commanders of two local police departments, this study investigated information orientation in the strategic decision-making process in the area of public safety in a small Brazilian city. The police departments have a limited ICT infrastructure to support the strategic decision-making process because their information systems are not connected. The results show that although the city of Pato Branco (Brazil) is considered a smart city in the area of public security, there are limited resources in several aspects of the police departments for the effective management of their ICT infrastructures. The impact of resource constraints reflects throughout the entire information use lifecycle - identification, collection, organization, processing, etc. - which fuels the strategic decision-making process. The implantation of an operations center could significantly reduce the effects of the problems identified in this research and further research may reveal the operational, technical, economic and financial viability of this proposal.;M. Colla and G.D. Santos;"Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)";3572.0;179.0;Netherlands;2015-2020;10.1016/j.promfg.2020.01.238;;43.0;;23519789;23519789;23519789;23519789;;Smart cities, Sustainable cities, Information orientation, Information, Communication Technologies, Decision-making;Elsevier BV;;1997.0;Western Europe;504.0;Q2;21100792109.0;Procedia manufacturing;Public safety decision-making in the context of smart and sustainable cities;;8346.0;1390.0;3628.0;27760.0;journal;article;2019
"Climate change and anthropogenic disturbance are two main drivers for vegetation dynamics on the Qinghai-Tibetan Plateau (QTP). Alpine meadow and alpine steppe are the primary rangeland ecosystem types on the QTP. However, the vegetation trends of the two land cover types and the underlying mechanisms behind their variation remain under debate. In this study, we used Global Inventory Modeling and Mapping Studies (GIMMS) 3g Normalized Difference Vegetation Index (NDVI) (i.e., GIMMS NDVI3g) by coupling the Breaks for Additive Season and Trend (BFAST) model and the Boosted Regression Tree (BRT) model to analyze alpine meadow and alpine steppe vegetation trends on the QTP between 1982 and 2015. We also assessed vegetation variation response to climatic and anthropogenic indicators in conjunction with climatic and human footprint datasets. Results show that growing season NDVI (GSNDVI) values increased overall for both alpine meadow (0.0001 year−1, p = 0.33) and alpine steppe (0.0002 year−1, p < 0.05) throughout 1982–2015. Significant greening trends in both alpine meadow (0.0007 year−1; p < 0.05) and alpine steppe (0.0005 year−1; p < 0.05) ecosystems were obtained before 1998 and 2001, respectively. However, browning trends ascertained by GSNDVI (−0.0006 year−1; p = 0.12) in alpine meadows were observed throughout 1998–2015, while greening trends ascertained by GSNDVI (0.0002 year−1; p = 0.12) in alpine steppes were observed throughout during 2001–2015. Opposing trends in precipitation, solar radiation, and the Standardized Precipitation Evapotranspiration Index (SPEI) occurred before and after breakpoints in both ecosystems. For the alpine meadow ecosystem, adverse precipitation trends caused browning before 1998 followed by greening after 1998 in the Three-River-Source National Park (TNP). Conversely, opposing changes in precipitation, solar radiation, and SPEI resulted in greening before 1998 followed by browning after 1998 in southern Tibet and the southeastern QTP. Alpine meadow vegetation trends were generally dominated by solar radiation before 1998 and jointly by precipitation and solar radiation after 1998. Prior to 2001 variation in alpine steppe greenness was controlled by precipitation, while after 2001 solar radiation dominated. Along with an increase in human footprint pressure (HFP) gradients, greenness trends gradually increased before 1998 but reversed after 1998 in the alpine meadow ecosystem. Additionally, greenness trends gradually decreased before 2001 but remained unchanged after 2001 for the alpine steppe ecosystem. These results highlight the different effects that climate change and anthropogenic disturbances have had on alpine meadow and alpine steppe ecosystems on the QTP over different time frames.";Aihua Hao and Hanchen Duan and Xufeng Wang and Guohui Zhao and Quangang You and Fei Peng and Heqiang Du and Feiyao Liu and Chengyang Li and Chimin Lai and Xian Xue;"Ecology (Q1); Ecology, Evolution, Behavior and Systematics (Q1); Nature and Landscape Conservation (Q1)";508.0;330.0;Netherlands;2014-2020;10.1016/j.gecco.2021.e01512;0.00731;36.0;;23519894;23519894;23519894;23519894;3.380;NDVI, Vegetation variation, Climate change, Anthropogenic disturbance, Qinghai-Tibetan Plateau;Elsevier BV;;6471.0;Western Europe;1133.0;Q1;21100349522.0;Global ecology and conservation;Different response of alpine meadow and alpine steppe to climatic and anthropogenic disturbance on the qinghai-tibetan plateau;3723.0;1918.0;578.0;514.0;37400.0;journal;article;2021
"Buildings are the world's largest contributors to energy demand, greenhouse gases (GHG) emissions, resource consumption and waste generation. An unmissable opportunity exists to tackle climate change, global warming, and resource scarcity by rethinking how we approach building design. Structural materials often dominate the total mass of a building; therefore, a significant potential for material efficiency and GHG emissions mitigation is to be found in efficient structural design and use of structural materials. To this end, environmental impact assessment methods, such as life cycle assessment (LCA), are increasingly used. However, they risk failing to deliver the expected benefits due to the high number of parameters and uncertainty factors that characterise impacts of buildings along their lifespans. Additionally, effort and cost required for a reliable assessment seem to be major barriers to a more widespread adoption of LCA. More rapid progress towards reducing building impacts seems therefore possible by combining established environmental impact assessment methods with artificial intelligence approaches such as machine learning and neural networks. This short communication will briefly present previous attempts to employ such techniques in civil and structural engineering. It will present likely outcomes of machine learning and neural network applications in the field of structural engineering and – most importantly – it calls for data from professionals across the globe to form a fundamental basis which will enable quicker transition to a more sustainable built environment.";B. D'Amico and R.J. Myers and J. Sykes and E. Voss and B. Cousins-Jenvey and W. Fawcett and S. Richardson and A. Kermani and F. Pomponi;"Architecture (Q1); Building and Construction (Q1); Civil and Structural Engineering (Q1); Safety, Risk, Reliability and Quality (Q1)";377.0;326.0;United Kingdom;2015-2020;10.1016/j.istruc.2018.11.013;0.00303;22.0;;23520124;23520124;23520124;23520124;2.983;Sustainable, Structural, Materials, Embodied carbon, Life cycle assessment LCA, Machine learning, Neural networks;Elsevier Ltd.;;4144.0;Western Europe;834.0;Q1;21100372467.0;Structures;Machine learning for sustainable structures: a call for data;2176.0;1218.0;695.0;380.0;28798.0;journal;article;2019
Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.;Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan;Nursing (miscellaneous) (Q1);199.0;262.0;Singapore;2014-2020;10.1016/j.ijnss.2019.03.001;;16.0;;23520132;23520132;23520132;23520132;;Artificial intelligence, Data mining, Knowledge bases, Nursing;Elsevier (Singapore) Pte Ltd;;3280.0;Asiatic Region;703.0;Q1;21100469749.0;International journal of nursing sciences;The application of big data and the development of nursing science: a discussion paper;;526.0;87.0;229.0;2854.0;journal;article;2019
;David Duran-Rodas and Emmanouil Chaniotakis and Constantinos Antoniou;Transportation;1859.0;185.0;Netherlands;2014-2020;10.1016/j.trpro.2019.09.117;;40.0;;23521465;23521457;23521465;23521457;;bike-sharing, influencing factors, data mining, automated;Elsevier BV;;2433.0;Western Europe;657.0;-;21100448300.0;Transportation research procedia;Automated open-source data collection and processing: an example of openstreetmap and bike-sharing;;4133.0;751.0;1895.0;18272.0;conference and proceedings;article;2019
State of health (SOH) estimation of lithium-ion batteries is a challenging and crucial task for consumer electronics, electric vehicles, and micro-rids. This study presents a data-driven battery SOH estimation method based on a novel integrated Gaussian process regression (GPR) model. First, the aging characteristics of batteries are analyzed from multiple perspectives, and three health indicators (HIs) are extracted from battery charging and discharging curves. Then, the Pearson correlation analysis method is used to quantitatively analyze the correlation between the selected HIs and SOH. Next, a novel compound kernel function is proposed for battery SOH estimation, and different pairs of mean function and kernel function chosen from four mean functions and sixteen kernel functions are used to construct GPR models, and their estimation accuracy is compared subsequently. Finally, four different batteries with various initial health conditions from the NASA battery dataset are used to verify the performance of the proposed method. Experiments show that the method proposed in this paper has satisfactory estimation results in terms of accuracy, generalization ability, and robustness. Specifically, its estimated mean-absolute-error (MAE) and root-mean-square-error (RMSE) is only 1.7%, and 2.41%, respectively.;Jiwei Wang and Zhongwei Deng and Tao Yu and Akihiro Yoshida and Lijun Xu and Guoqing Guan and Abuliti Abudula;"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)";828.0;687.0;Netherlands;2015-2020;10.1016/j.est.2022.104512;0.00926;42.0;;2352152X;2352152X;2352152X;2352152X;6.583;Lithium-ion battery, State of health, Health indicator, Data-driven, Gaussian processes regression;Elsevier BV;;5191.0;Western Europe;1088.0;Q1;21100400826.0;Journal of energy storage;State of health estimation based on modified gaussian process regression for lithium-ion batteries;7765.0;5383.0;860.0;831.0;44642.0;journal;article;2022
The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.;Lihong Zhao;"Environmental Science (miscellaneous) (Q1); Plant Science (Q1); Soil Science (Q1)";310.0;551.0;Netherlands;2014-2020;10.1016/j.eti.2020.101196;;28.0;;23521864;23521864;23521864;23521864;;Big data analysis, Ecological environment, Water demand, Prediction;Elsevier BV;;5748.0;Western Europe;866.0;Q1;21100385961.0;Environmental technology and innovation;Prediction model of ecological environmental water demand based on big data analysis;;1661.0;409.0;310.0;23508.0;journal;article;2021
Sustainable development of modern society demands discovering new materials with superior properties in different applications such as aerospace, wind, civil, automotive, etc. Characterizing and predicting material properties using traditional methods are time consuming and expensive. Therefore, advanced methods have been developed to meet the need for quick and reliable design and characterizing of materials properties. ML methods have made it possible to optimize and automate design performance and discover new materials. This review paper gives an overview of the implementation of ML in i) discovery of new materials, and ii) characterization of materials ML. Various ML models for materials manufacturing as well as how ML is applied to model materials are discussed. Recent advances, ML applications, as well as upcoming challenges and perspectives are discussed.;Mohammad Asaduzzaman Chowdhury and Nayem Hossain and Md Bengir {Ahmed Shuvho} and Mohammad Fotouhi and Md Sakibul Islam and Md Ramjan Ali and Mohammod Abul Kashem;"Materials Science (miscellaneous) (Q2); Condensed Matter Physics (Q3); Electronic, Optical and Magnetic Materials (Q3); Materials Chemistry (Q3)";219.0;179.0;Netherlands;2014-2020;10.1016/j.cocom.2021.e00597;;14.0;;23522143;23522143;23522143;23522143;;ML, Material science, Design, Characterization, Advancements, Challenges;Elsevier BV;;4959.0;Western Europe;394.0;Q2;21100370079.0;Computational condensed matter;Recent machine learning guided material research - a review;;384.0;82.0;219.0;4066.0;journal;article;2021
In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.;Katalin Patonai and Ferenc Jordán;"Ecology (Q1); Ecology, Evolution, Behavior and Systematics (Q2)";91.0;262.0;Netherlands;2014-2020;10.1016/j.fooweb.2021.e00203;;15.0;;23522496;23522496;23522496;23522496;;Aggregation, Danube River, Food web, Incomplete data, Taxonomy;Elsevier BV;;6240.0;Western Europe;847.0;Q1;21100365104.0;Food webs;Integrating trophic data from the literature: the danube river food web;;224.0;43.0;93.0;2683.0;journal;article;2021
"Summary
Background
Evidence of whether people living with HIV are at elevated risk of adverse COVID-19 outcomes is inconclusive. We aimed to investigate this association using the population-based National COVID Cohort Collaborative (N3C) data in the USA.
Methods
We included all adult (aged ≥18 years) COVID-19 cases with any health-care encounter from 54 clinical sites in the USA, with data being deposited into the N3C. The outcomes were COVID-19 disease severity, hospitalisation, and mortality. Encounters in the same health-care system beginning on or after January 1, 2018, were also included to provide information about pre-existing health conditions (eg, comorbidities). Logistic regression models were employed to estimate the association of HIV infection and HIV markers (CD4 cell count, viral load) with hospitalisation, mortality, and clinical severity of COVID-19 (multinomial). The models were initially adjusted for demographic characteristics, then subsequently adjusted for smoking, obesity, and a broad range of comorbidities. Interaction terms were added to assess moderation effects by demographic characteristics.
Findings
In the harmonised N3C data release set from Jan 1, 2020, to May 8, 2021, there were 1 436 622 adult COVID-19 cases, of these, 13 170 individuals had HIV infection. A total of 26 130 COVID-19 related deaths occurred, with 445 among people with HIV. After adjusting for all the covariates, people with HIV had higher odds of COVID-19 death (adjusted odds ratio 1·29, 95% CI 1·16–1·44) and hospitalisation (1·20, 1·15–1·26), but lower odds of mild or moderate COVID-19 (0·61, 0·59–0·64) than people without HIV. Interaction terms revealed that the elevated odds were higher among older age groups, male, Black, African American, Hispanic, or Latinx adults. A lower CD4 cell count (<200 cells per μL) was associated with all the adverse COVID-19 outcomes, while viral suppression was only associated with reduced hospitalisation.
Interpretation
Given the COVID-19 pandemic's exacerbating effects on health inequities, public health and clinical communities must strengthen services and support to prevent aggravated COVID-19 outcomes among people with HIV, particularly for those with pronounced immunodeficiency.
Funding
National Center for Advancing Translational Sciences, National Institute of Allergy and Infectious Diseases, National Institutes of Health, USA.";Xueying Yang and Jing Sun and Rena C Patel and Jiajia Zhang and Siyuan Guo and Qulu Zheng and Amy L Olex and Bankole Olatosi and Sharon B Weissman and Jessica Y Islam and Christopher G Chute and Melissa Haendel and Gregory D Kirk and Xiaoming Li and Richard Moffitt and Hana Akelsrod and Keith A Crandall and Nora Francheschini and Evan French and Teresa {Po-Yu Chiang} and G Caleb-Alexander and Kathleen M Andersen and Amanda J Vinson and Todd T Brown and Roslyn B Mannon;"Epidemiology (Q1); Immunology (Q1); Infectious Diseases (Q1); Virology (Q1)";187.0;373.0;United Kingdom;2014-2020;10.1016/S2352-3018(21)00239-3;;54.0;;23523018;23523018;23523018;23523018;;;Elsevier Ltd.;;2001.0;Western Europe;5483.0;Q1;21100369870.0;Lancet hiv,the;Associations between hiv infection and clinical spectrum of covid-19: a population level analysis based on us national covid cohort collaborative (n3c) data;;2126.0;187.0;499.0;3742.0;journal;article;2021
"Summary
Tremendous progress in treatment and outcomes has been achieved across the whole range of haematological malignancies in the past two decades. Although cure rates for aggressive malignancies have increased, nowhere has progress been more impactful than in the management of typically incurable forms of haematological cancer. Population-based data have shown that 5-year survival for patients with chronic myelogenous and chronic lymphocytic leukaemia, indolent B-cell lymphomas, and multiple myeloma has improved markedly. This improvement is a result of substantial changes in disease management strategies in these malignancies. Several haematological malignancies are now chronic diseases that are treated with continuously administered therapies that have unique side-effects over time. In this Commission, an international panel of clinicians, clinical investigators, methodologists, regulators, and patient advocates representing a broad range of academic and clinical cancer expertise examine adverse events in haematological malignancies. The issues pertaining to assessment of adverse events examined here are relevant to a range of malignancies and have been, to date, underexplored in the context of haematology. The aim of this Commission is to improve toxicity assessment in clinical trials in haematological malignancies by critically examining the current process of adverse event assessment, highlighting the need to incorporate patient-reported outcomes, addressing issues unique to stem-cell transplantation and survivorship, appraising challenges in regulatory approval, and evaluating toxicity in real-world patients. We have identified a range of priority issues in these areas and defined potential solutions to challenges associated with adverse event assessment in the current treatment landscape of haematological malignancies.";Gita Thanarajasingam and Lori M Minasian and Frederic Baron and Franco Cavalli and R Angelo {De Claro} and Amylou C Dueck and Tarec C El-Galaly and Neil Everest and Jan Geissler and Christian Gisselbrecht and John Gribben and Mary Horowitz and S Percy Ivy and Caron A Jacobson and Armand Keating and Paul G Kluetz and Aviva Krauss and Yok Lam Kwong and Richard F Little and Francois-Xavier Mahon and Matthew J Matasar and María-Victoria Mateos and Kristen McCullough and Robert S Miller and Mohamad Mohty and Philippe Moreau and Lindsay M Morton and Sumimasa Nagai and Simon Rule and Jeff Sloan and Pieter Sonneveld and Carrie A Thompson and Kyriaki Tzogani and Flora E {van Leeuwen} and Galina Velikova and Diego Villa and John R Wingard and Sophie Wintrich and John F Seymour and Thomas M Habermann;Hematology (Q1);150.0;540.0;United Kingdom;2014-2020;10.1016/S2352-3026(18)30051-6;;54.0;;23523026;23523026;23523026;23523026;;;Lancet Publishing Group;;1784.0;Western Europe;5343.0;Q1;21100370017.0;Lancet haematology,the;Beyond maximum grade: modernising the assessment and reporting of adverse events in haematological malignancies;;1917.0;206.0;379.0;3676.0;journal;article;2018
Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter electronic health records. Cerner Corporation secured appropriate data use agreements and permissions from more than 100 health systems in the United States contributing to the database as of March 2022. A subset of the database was extracted to include data from only patients with SARS-CoV-2 infections and is referred to as the Cerner COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9 billion, 486 million, and 11.5 billion records in the condition, medication, procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19 Dataset consists of 130.1 million encounters from 3.8 million patients. The size and longitudinal nature of CRWD can be leveraged for advanced analytics and artificial intelligence in medical research across all specialties and is a rich source of novel discoveries on a wide range of conditions including but not limited to COVID-19.;Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster;"Education (Q4); Multidisciplinary (Q4)";8.0;113.0;Netherlands;2014-2020;10.1016/j.dib.2022.108120;;30.0;;23523409;23523409;23523409;23523409;;Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records (EHR), HealtheIntent, HealtheDataLab™, Cerner learning Health Network (LHN);Elsevier BV;;837.0;Western Europe;122.0;Q4;21100372856.0;Data in brief;Cerner real-world data (crwd) - a de-identified multicenter electronic health records database;;3980.0;1742.0;3537.0;14584.0;journal;article;2022
"Background
Real-time reverse transcription-PCR (rRT-PCR) has been the most effective and widely implemented diagnostic technology since the beginning of the COVID-19 pandemic. However, fuzzy rRT-PCR readouts with high Ct values are frequently encountered, resulting in uncertainty in diagnosis.
Methods
A Specific Enhancer for PCR-amplified Nucleic Acid (SENA) was developed based on the Cas12a trans-cleavage activity, which is specifically triggered by the rRT-PCR amplicons of the SARS-CoV-2 Orf1ab (O) and N fragments. SENA was first characterized to determine its sensitivity and specificity, using a systematic titration experiment with pure SARS-CoV-2 RNA standards, and was then verified in several hospitals, employing a couple of commercial rRT-PCR kits and testing various clinical specimens under different scenarios.
Findings
The ratio (10 min/5 min) of fluorescence change (FC) with mixed SENA reaction (mix-FCratio) was defined for quantitative analysis of target O and N genes, and the Limit of Detection (LoD) of mix-FCratio with 95% confidence interval was 1.2≤1.6≤2.1. Totally, 295 clinical specimens were analyzed, among which 21 uncertain rRT-PCR cases as well as 4 false negative and 2 false positive samples were characterized by SENA and further verified by next-generation sequencing (NGS). The cut-off values for mix-FCratio were determined as 1.145 for positive and 1.068 for negative.
Interpretation
SENA increases both the sensitivity and the specificity of rRT-PCR, solving the uncertainty problem in COVID-19 diagnosis and thus providing a simple and low-cost companion diagnosis for combating the pandemic.
Funding
Detailed funding information is available at the end of the manuscript.";Weiren Huang and Lei Yu and Donghua Wen and Dong Wei and Yangyang Sun and Huailong Zhao and Yu Ye and Wei Chen and Yongqiang Zhu and Lijun Wang and Li Wang and Wenjuan Wu and Qianqian Zhao and Yong Xu and Dayong Gu and Guohui Nie and Dongyi Zhu and Zhongliang Guo and Xiaoling Ma and Liman Niu and Yikun Huang and Yuchen Liu and Bo Peng and Renli Zhang and Xiuming Zhang and Dechang Li and Yang Liu and Guoliang Yang and Lanzheng Liu and Yunying Zhou and Yunshan Wang and Tieying Hou and Qiuping Gao and Wujiao Li and Shuo Chen and Xuejiao Hu and Mei Han and Huajun Zheng and Jianping Weng and Zhiming Cai and Xinxin Zhang and Fei Song and Guoping Zhao and Jin Wang;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Medicine (miscellaneous) (Q1)";1211.0;557.0;Netherlands;2014-2020;10.1016/j.ebiom.2020.103036;0.04073;63.0;;23523964;23523964;23523964;23523964;8.143;COVID-19, SARS-CoV-2, rRT-PCR, CRISPR diagnosis, Cas12a, SENA;Elsevier BV;;4015.0;Western Europe;2596.0;Q1;21100369815.0;Ebiomedicine;A crispr-cas12a-based specific enhancer for more sensitive detection of sars-cov-2 infection;15647.0;9220.0;559.0;1638.0;22442.0;journal;article;2020
A series of well-characterized specimens, known as the Peabody-Yale Reference Obsidians (PYRO) sets, has been designed to aid with calibrating and assessing X-ray fluorescence analysis (XRF) data, including portable XRF (pXRF) measurements, for obsidian sourcing. Each of these ten matched sets consists of 35 specimens: 20 specimens for calibration and 15 specimens for evaluation. These sets include not only obsidians with common geochemical compositions (i.e., alkaline rhyolites) but also rarer ones (i.e., peralkaline rhyolitic, trachytic, and andesitic specimens, including East African Rift obsidians). When used as described, the PYRO sets are suitable to calibrate and evaluate XRF data for obsidians worldwide. A set can be borrowed following loan policies of the Peabody Museum of Natural History, which will also accession a set. Publishing all source information – from their names and GPS coordinates to the datasets used for the recommended values – not only allows the sets to be replicated by others but also fulfills the demands of scientific transparency. Their main purpose is facilitating collaborations and “big data” projects, and the PYRO sets were designed to complement existing protocols for calibration and evaluation. In short, the sets are intended as a tool for almost anyone (e.g., a student who borrows an instrument to source artifacts) to meet – and even exceed – experts' practices involving transparency, accuracy, and reproducibility.;Ellery Frahm;"Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1)";1365.0;167.0;Netherlands;2015-2020;10.1016/j.jasrep.2019.101957;;26.0;;2352409X;2352409X;2352409X;2352409X;;Obsidian sourcing, EDXRF, Portable XRF, Accuracy, Reproducibility, Transparency, Collaboration;Elsevier BV;;6751.0;Western Europe;840.0;Q1;21100369721.0;Journal of archaeological science: reports;Introducing the peabody-yale reference obsidians (pyro) sets: open-source calibration and evaluation standards for quantitative x-ray fluorescence analysis;;2514.0;527.0;1389.0;35578.0;journal;article;2019
Architected materials consisting of periodic unit cells are desirable for many engineering applications. Characterizing the elastic isotropy is of great significance for the mechanical design of architected materials. However, prevailing experimental and numerical approaches are normally too costly and time-consuming to screen out isotropic architected materials in the large design space. Here, a deep learning-based approach is developed as a highly efficient and portable tool to identify the elastic isotropy of architected materials directly from images of their unit cells with arbitrary component distributions. The measure of elastic isotropy for heterogeneous architected materials is derived firstly in this paper to construct a database with associated images of unit cells. Then a convolutional neural network is fully trained with the database, performing well on the isotropy identification with about 90% accuracy and milliseconds processing time per sample. Meanwhile, it exhibits enough robustness to maintain its performance under the fluctuating material properties in test sets. Moreover, the transfer learning of the convolutional neural network is successfully implemented among architected materials with different numbers of material components, which further promotes the efficiency of the deep learning-based approach without scarifying its identification performance. This study gives new inspirations on the rapid mechanical characterization of architectured materials, which holds promising applications in the big-data driven topological design and nondestructive testing of architected materials.;Anran Wei and Jie Xiong and Weidong Yang and Fenglin Guo;"Bioengineering (Q1); Chemical Engineering (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1)";297.0;479.0;United Kingdom;2014-2020;10.1016/j.eml.2021.101173;0.00713;42.0;;23524316;23524316;23524316;23524316;4.567;Deep learning, Convolutional neural network, Architected material, Elastic isotropy, Rapid mechanical characterization, Transfer learning;Elsevier Ltd.;;4465.0;Western Europe;1524.0;Q1;21100376821.0;Extreme mechanics letters;Deep learning-assisted elastic isotropy identification for architected materials;2933.0;1693.0;232.0;301.0;10358.0;journal;article;2021
"Industrial development with the growth, strengthening, stability, technical advancement, reliability, selection, and dynamic response of the power system is essential. Governments and companies invest billions of dollars in technologies to convert, harvest, rising demand, changing demand and supply patterns, efficiency, lack of analytics required for optimal energy planning, and store energy. In this scenario, artificial intelligence (AI) is starting to play a major role in the energy market. Recognizing the importance of AI, this study was conducted on seven different energetics systems and their variety of applications, including: i) electricity production; ii) power delivery; iii) electric distribution networks; iv) energy storage; v) energy saving, new energy materials, and devices; vi) energy efficiency and nanotechnology; and vii) energy policy, and economics. The main drivers are the four key techniques used in current AI technologies, including: i) fuzzy logic systems; ii) artificial neural networks; iii) genetic algorithms; and iv) expert systems. In developed countries, the power industry has started using AI to connect with smart meters, smart grids, and the Internet of Things devices. These AI technologies will lead to the improvement of efficiency, energy management, transparency, and the usage of renewable energies. In recent decades/years, new AI technology has brought significant improvements to how power system devices monitor data, communicate with the system, analyze input–output, and display data in unprecedented ways. New applications in the energy system become feasible when these new AI developments are incorporated into the energy industry. But on the contrary, much more investment is needed in global research into AI and data-driven models. In terms of power supply, AI can help utilities provide customers with renewable and affordable electricity from complex sources in a secure manner, while at the same time providing these customers with the opportunity to use their own energy more efficiently. Moreover, policy recommendations, research opportunities, and how industry 4.0 will improve sustainability have been briefly described.";Tanveer Ahmad and Hongyu Zhu and Dongdong Zhang and Rasikh Tariq and A. Bassam and Fasee Ullah and Ahmed S AlGhamdi and Sultan S. Alshamrani;Energy (miscellaneous) (Q1);239.0;737.0;United Kingdom;2015-2020;10.1016/j.egyr.2021.11.256;0.00318;33.0;;23524847;23524847;23524847;23524847;6.870;Applications of industry 4.0, Power sector, Artificial intelligence, Energetics systems, Energy storage, Sustainability;Elsevier Ltd.;;2896.0;Western Europe;1199.0;Q1;21100389511.0;Energy reports;Energetics systems and artificial intelligence: applications of industry 4.0;2964.0;1788.0;983.0;239.0;28463.0;journal;article;2022
Environmental information disclosure (EID) is an important way for firms to communicate to the government and the public to fulfill their environmental protection responsibilities. Essentially, the dynamic impacts of firms’ activities on the ecological environment are evolving continuously. We aim to introduce functional data analysis (FDA) for exploring the dynamics in the relationship between environmental information disclosure (EID) and firms’ financial performance. Based on continuous curves smoothed from 75 Chinese listed firms of pollution-intensive industries, this study examined the dynamic effect and its structural break of EID on firms' financial performance. Furthermore, moderating effects of public attention, government subsidy and ratio of profits to total cost were tested within a functional framework. The results revealed that the positive effect of EID on firms' financial performance is constantly significant, whereas there existed a structural break in 2015 due to the implementation of new Environmental Protection Law. Moreover, the positive moderating effect of the ratio of profits to total cost is significant only before 2015, while both the main effect and moderating effect of government subsidy are not significant. Surprisingly, although the main effects of public attention are not significant, its positive moderating effect is statistically significant. We contributed in introducing FDA as a useful toolkit for quantifying the time-dynamics in ecological economics, and our findings could offer guidance for stakeholders seeking to improve EID.;Deqing Wang and Xuemei Li and Sihua Tian and Lingyun He and Yan Xu and Xu Wang;"Environmental Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Environmental Chemistry (Q2); Renewable Energy, Sustainability and the Environment (Q2)";194.0;534.0;Netherlands;2015-2021;10.1016/j.spc.2021.03.026;0.00196;26.0;;23525509;23525509;23525509;23525509;5.032;Environmental information disclosure, Firm financial performance, Data smoothing, Functional regression analysis, Moderating effect;Elsevier BV;;6701.0;Western Europe;1019.0;Q1;21100416081.0;Sustainable production and consumption;Quantifying the dynamics between environmental information disclosure and firms’ financial performance using functional data analysis;1403.0;1058.0;91.0;198.0;6098.0;journal;article;2021
;Patrice Forget and Karuna Dahlberg;"Anesthesiology and Pain Medicine (Q1); Critical Care and Intensive Care Medicine (Q1); Medicine (miscellaneous) (Q2)";179.0;148.0;Netherlands;2015-2020;10.1016/j.accpm.2021.100886;;38.0;;23525568;23525568;23525568;23525568;;;Elsevier BV;;2378.0;Western Europe;942.0;Q1;21100435142.0;"Anaesthesia, critical care &amp; pain medicine";Is multi-source feedback the future of perioperative medicine?;;590.0;192.0;396.0;4565.0;journal;article;2021
Smart Cities are city models that emerged to face the problems of current city configurations, promoting improvement in infrastructure and the provision of essential services through technological implementation. The provision of health services is necessary and a civil right, being one of the dimensions proposed in Smart Cities, represented by Smart Healthcare. This model of cities, as well as its dimensions, have great appeal in the technological application, as the main means of promoting smart services. However, it appears that the approach to the role of the citizen in the promotion of intelligence is neglected, leaving questions about the role of the citizen in Smart Cities. Thus, the present study aimed to identify the technological structures applied in Smart Healthcare, allowing us to identify the role of the citizen in the promotion of smarter health services. A systematic literature review was carried out using the Methodi Ordinatio, resulting in a portfolio composed of 26 articles with scientific relevance. From the content analysis, four main approaches were identified, as well as technologies and methods to address one of the biggest challenges of health technologies: data security and privacy. From this, the role of the citizen in the Smart Healthcare was evidenced, aiming to contribute to the planning of projects and the development of future research to consider with more attention the participation of citizens.;Alana Corsi and Fabiane {Florencio de Souza} and Regina Negri Pagani and João Luiz Kovaleski;"Information Systems (Q2); Computer Science Applications (Q3); Health Informatics (Q3); Health Information Management (Q3); Medicine (miscellaneous) (Q3)";69.0;271.0;Netherlands;2018-2020;10.1016/j.smhl.2022.100310;;9.0;;23526483;23526483;23526483;23526483;;Smart cities, Smart healthcare. smart citizens. Citizen's role. technology;Elsevier BV;;5200.0;Western Europe;410.0;Q2;21100894507.0;Smart health;Ultimate approach and technologies in smart healthcare: a broad systematic review focused on citizens;;210.0;28.0;74.0;1456.0;journal;article;2022
The traditional mechanism models of a chiller plant of HVAC are complicated with multiple variables and many constraints, so that it’s burdensome to optimize those operational parameters. Moreover, the optimization methods based on mechanism models are unpractical to be applied in the engineering projects. Therefore, an operational parameter optimization method based on the unsupervised data mining technology is proposed in this paper and verified with a large number of field operational data of the chiller plant of a shopping mall in the subtropical area. The unsupervised data mining procedure is illustrated in detail, including data preparation, data partitioning, strong association rules extraction by Apriori algorithm and so on. The definition, selection and discretization methods of external and operational parameters are described to determine the mining target and divide typical operating conditions. At last, 54 and 70 strong association rules, respectively, for the single larger chiller operating mode and the single smaller one under typical operating conditions are extracted. Simulation results shows that the energy consumption of the chiller plant is reduced by 11.60% and 13.33% after optimization, respectively, on the studied days in summer and that in winter. The analysis results mean that after optimization, the energy performance of a chiller plant was significantly improved. The strong association rules are easily utilized in the engineering projects, in terms of their simplicity and feasibility. And this method can also be used in other fields when there are enough effective operational data.;Xuan Zhou and Bingwen Wang and Liequan Liang and Junwei Yan and Dongmei Pan;"Architecture (Q1); Building and Construction (Q1); Civil and Structural Engineering (Q1); Mechanics of Materials (Q1); Safety, Risk, Reliability and Quality (Q1)";720.0;570.0;Netherlands;2015-2021;10.1016/j.jobe.2019.100870;0.007529999999999999;39.0;;23527102;23527102;23527102;23527102;5.318;Chiller plant, Operational parameters optimization, Unsupervised data mining, ARM;Elsevier BV;;4917.0;Western Europe;974.0;Q1;21100389518.0;Journal of building engineering;An operational parameter optimization method based on association rules mining for chiller plant;5990.0;4094.0;777.0;721.0;38204.0;journal;article;2019
Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation.;Marília Barandas and Duarte Folgado and Letícia Fernandes and Sara Santos and Mariana Abreu and Patrícia Bota and Hui Liu and Tanja Schultz and Hugo Gamboa;"Computer Science Applications (Q2); Software (Q2)";235.0;257.0;Netherlands;2015-2020;10.1016/j.softx.2020.100456;0.00813;21.0;;23527110;23527110;23527110;23527110;1.959;Time series, Machine learning, Feature extraction, Python;Elsevier BV;;2778.0;Western Europe;528.0;Q2;21100422153.0;Softwarex;Tsfel: time series feature extraction library;2553.0;654.0;162.0;235.0;4501.0;journal;article;2020
"Collaborative approaches in health, such as One Health (OH), are promising; nevertheless, several authors point at persistent challenges for designing and implementing OH initiatives. Among other challenges, OH practitioners struggle in their efforts to collaborate across disciplines and domains. This paper aims to provide insights into the existing challenges for designing and implementing OH initiatives, their causes and solutions, and points out strategic solutions with the potential to solve practical challenges. A systematic literature search was performed for emerging challenges and proposed solutions in the process of conducting OH initiatives. Next, a thematic and a causal analysis were performed to unravel challenges and their causes. Finally, solutions were discriminated on whether they were only recommended, or implemented as a proof-of-principle. The 56 included papers describe 21 challenges endured by OH initiatives that relate to different themes (policy and funding; education and training; surveillance; multi-actor, multi-domain, and multi-level collaborations; and evidence). These challenges occur in three different phases: the acquisition of sufficient conditions to start an initiative, its execution, and its monitoring and evaluation. The findings indicate that individual challenges share overlapping causes and crosscutting causal relations. Accordingly, solutions for the successful performance of OH initiatives should be implemented to tackle simultaneously different types of challenges occurring in different phases. Still, promoting collaboration between the wide diversity of stakeholders, as a fundamental aspect in the OH approach, is still by far the most challenging factor in performing OH initiatives. Causes for that are the difficulties in promoting meaningful and equal participation from diverse actors. Solutions proposed for this challenge focused on guiding stakeholders to think and collaborate beyond their professional and cultural silos to generate knowledge co-creation and innovative methodologies and frameworks. Finally, the biggest knowledge gap identified, in terms of proposed solutions, was for monitoring and evaluating OH initiatives. This highlights the need for future research on evaluation methods and tools specific for the OH approach, to provide credible evidence on its added value. When considering challenges endured by former OH initiatives and the proposed solutions for these challenges, practitioners should be able to plan and structure such initiatives in a more successful way, through the strategic pre-consideration of solutions or simply by avoiding known barriers.";Carolina {dos S. Ribeiro} and Linda H.M. {van de Burgwal} and Barbara J. Regeer;"Infectious Diseases (Q1); Public Health, Environmental and Occupational Health (Q1)";67.0;357.0;Netherlands;2015-2020;10.1016/j.onehlt.2019.100085;0.00156;22.0;;23527714;23527714;23527714;23527714;3.800;One health, Challenges, Design, Implementation, Interdisciplinary collaboration, Transdisciplinary research, strategic solutions;Elsevier BV;;3722.0;Western Europe;1439.0;Q1;21100404585.0;One health;Overcoming challenges for designing and implementing the one health approach: a systematic review of the literature;829.0;339.0;76.0;68.0;2829.0;journal;article;2019
During the initial construction of the Puguang Gas Field, information infrastructure was built. Due to the absence of unified planning and deployment, however, many “isolated information islands” were formed in data systems, and the data resources cannot meet the construction requirements of intelligent gas field. In this paper, the status quo and problems of data resources in the Puguang Gas Field were analyzed, and a data resource sharing center was constructed according to the overall architecture design of the Puguang Intelligent Gas Field. Based on the architecture design of data resource sharing center, the overall construction conception of data resource sharing center was put forward and the business data model was designed. Finally, the integrated data collection, storage, calculation and utilization was realized by establishing data standard, combing data sources and designing data services, and then it was applied on site. The following research results were obtained. First, the data resource sharing center is an important foundation for the construction of this project, and its overall architecture is divided into three layers from bottom to top, i.e., data specification and standard, data collection, storage, calculation and utilization, and data control. Second, the data resource sharing center achieves the one-time collection, centralized storage, shared use and unified management of exploration & development, gathering & purification, production & operation and safety & environmental protection data, and provides an important data base for the construction of business system of the intelligent gas field and a comprehensive, reliable and effective data support for the intelligent and mobile application in the Puguang Gas Field.;Yiwei Jiang and Jinxian Li and Hanwei Zhang and Qingyin Wang and Yanqiu Yu and Chunguang He and Meisheng Liang;"Energy Engineering and Power Technology (Q3); Geology (Q3); Geotechnical Engineering and Engineering Geology (Q3); Modeling and Simulation (Q3); Process Chemistry and Technology (Q3)";214.0;137.0;Netherlands;2014-2020;10.1016/j.ngib.2018.10.004;;16.0;;23528540;23528559;23528540;23528559;;Intelligent gas field, Puguang gas field, Standard and specification, Post data, Real time data, Video data, Distributed storage, Data sharing service;Elsevier BV;;2499.0;Western Europe;321.0;Q3;21100897123.0;Natural gas industry b;Construction of data resource sharing center of the puguang intelligent gas field;;299.0;77.0;215.0;1924.0;journal;article;2019
Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.;Siqi Liu and Tianyu Wang and Shaowei Wang;"Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1)";96.0;881.0;China;2015-2020;10.1016/j.dcan.2021.09.014;0.0013800000000000002;26.0;;23528648;23528648;24685925;23528648;6.797;Data-driven, Deep learning, Physical layer, Wireless communications;Chongqing University of Posts and Telecommunications;;4190.0;Asiatic Region;1082.0;Q1;21100823476.0;Digital communications and networks;Toward intelligent wireless communications: deep learning - based physical layer technologies;823.0;881.0;77.0;105.0;3226.0;journal;article;2021
A tumor is a group of cells with abnormal cell growth. Sequencing technology can help recognize genetic mutations that cause cancer. Next-generation sequencing (NGS) can supply genetic data and determine the number of mutant genes in different cancers by sequencing the whole genome, exome, and/or transcriptome. For a specific organism, ribonucleic acid (RNA) content is represented by the transcriptome containing information about different diseases, functional genome elements, and molecular components of tissues and cells. Whole transcriptome shotgun sequencing, known as RNA-Seq, is a technology that uses NGS to show a snapshot of RNA at a given time from millions of individual RNAs. There are some biases in RNA-Seq, which can be classified as nucleotide composition, guanine-cytosine (GC) content, insert size, cell type, and contaminations. In molecular biology, contaminations can lead to biases in the genetic analysis results and difficulties in observing viral infections. Here we reviewed RNA-Seq methodologies, different contaminations that may occur during RNA-Seq preparation, and some methods that can be applied to estimate the transcript abundances of contaminated samples.;Zahra Mortezaei;Health Informatics (Q3);217.0;337.0;United Kingdom;2015-2020;10.1016/j.imu.2022.101054;;21.0;;23529148;23529148;23529148;23529148;;Next-generation sequencing (NGS), RNA-Seq, Contaminations, Computational methods, Cancer;Elsevier Ltd.;;4581.0;Western Europe;440.0;Q3;21100780477.0;Informatics in medicine unlocked;Computational methods for analyzing rna-sequencing contaminated samples and its impact on cancer genome studies;;798.0;208.0;219.0;9529.0;journal;article;2022
Global lockdowns in response to the COVID-19 pandemic have led to changes in the anthropogenic activities resulting in perceivable air quality improvements. Although several recent studies have analyzed these changes over different regions of the globe, these analyses have been constrained due to the usage of station based data which is mostly limited up to the metropolitan cities. Also the quantifiable changes have been reported only for the developed and developing regions leaving the poor economies (e.g. Africa) due to the shortage of in-situ data. Using a comprehensive set of high spatiotemporal resolution satellites and merged products of air pollutants, we analyze the air quality across the globe and quantify the improvement resulting from the suppressed anthropogenic activity during the lockdowns. In particular, we focus on megacities, capitals and cities with high standards of living to make the quantitative assessment. Our results offer valuable insights into the spatial distribution of changes in the air pollutants due to COVID-19 enforced lockdowns. Statistically significant reductions are observed over megacities with mean reduction by 19.74%, 7.38% and 49.9% in nitrogen dioxide (NO2), aerosol optical depth (AOD) and PM2.5 concentrations. Google Earth Engine empowered cloud computing based remote sensing is used and the results provide a testbed for climate sensitivity experiments and validation of chemistry-climate models. Additionally, Google Earth Engine based apps have been developed to visualize the changes in a real-time fashion.;Manmeet Singh and Bhupendra Bahadur Singh and Raunaq Singh and Badimela Upendra and Rupinder Kaur and Sukhpal Singh Gill and Mriganka Sekhar Biswas;"Geography, Planning and Development (Q1); Computers in Earth Sciences (Q2)";220.0;337.0;Netherlands;2015-2020;10.1016/j.rsase.2021.100489;;19.0;;23529385;23529385;23529385;23529385;;COVID19, Google earth engine, PM, NO, AOD, Tropospheric ozone, Cloud computing;Elsevier BV;;5467.0;Western Europe;703.0;Q1;21100416071.0;Remote sensing applications: society and environment;Quantifying covid-19 enforced global changes in atmospheric pollutants using cloud computing based remote sensing;;780.0;143.0;220.0;7818.0;journal;article;2021
To identify potential aberrantly differentially methylated genes (DMGs) correlated with chemotherapy response (CR) and establish a polygenic methylation prediction model of CR in epithelial ovarian cancer (EOC), we accessed 177 (47 chemo-sensitive and 130 chemo-resistant) samples corresponding to three DNA-methylation microarray datasets from the Gene Expression Omnibus and 306 (290 chemo-sensitive and 16 chemo-resistant) samples from The Cancer Genome Atlas (TCGA) database. DMGs associated with chemotherapy sensitivity and chemotherapy resistance were identified by several packages of R software. Pathway enrichment and protein-protein interaction (PPI) network analyses were constructed by Metascape software. The key genes containing mRNA expressions associated with methylation levels were validated from the expression dataset by the GEO2R platform. The determination of the prognostic significance of key genes was performed by the Kaplan-Meier plotter database. The key genes-based polygenic methylation prediction model was established by binary logistic regression. Among accessed 483 samples, 457 (182 hypermethylated and 275 hypomethylated) DMGs correlated with chemo resistance. Twenty-nine hub genes were identified and further validated. Three genes, anterior gradient 2 (AGR2), heat shock-related 70-kDa protein 2 (HSPA2), and acetyltransferase 2 (ACAT2), showed a significantly negative correlation between their methylation levels and mRNA expressions, which also corresponded to prognostic significance. A polygenic methylation prediction model (0.5253 cutoff value) was established and validated with 0.659 sensitivity and 0.911 specificity.;Lanbo Zhao and Sijia Ma and Linconghua Wang and Yiran Wang and Xue Feng and Dongxin Liang and Lu Han and Min Li and Qiling Li;"Molecular Medicine (Q1); Oncology (Q1); Pharmacology (medical) (Q1); Cancer Research (Q2)";150.0;632.0;United Kingdom;2014-2020;10.1016/j.omto.2021.02.012;;23.0;;23727705;23727705;23727705;23727705;;ovarian cancer, bioinformatics, DNA methylation, chemotherapy response, prediction model, AGR2, HSPA2, ACAT2;Cell Press;;4869.0;Western Europe;1424.0;Q1;21100461974.0;Molecular therapy - oncolytics;A polygenic methylation prediction model associated with response to chemotherapy in epithelial ovarian cancer;;867.0;157.0;150.0;7644.0;journal;article;2021
Data quality issues have special implications in network data. Data glitches are propagated rapidly along pathways dictated by the hierarchy and topology of the network. In this paper, we use temporal data from a vast data network to study data glitches and their effect on network monitoring tasks such as anomaly detection. We demonstrate the consequences of cleaning the data, and develop targeted and customized cleaning strategies by exploiting the network hierarchy.;Loh, Ji Meng and Dasu, Tamraparni;"Computer Science Applications; Software";546.0;93.0;United States;2015;10.1109/ICDMW.2012.125;;21.0;;23759259;23759232;23759259;23759232;;"Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information management;Data glitches;Big Data;missing values;outliers;network analysis;Earth Mover Distance";;;2648.0;Northern America;212.0;-;21100398701.0;Ieee international conference on data mining workshops, icdmw;Effect of data repair on mining network streams;;789.0;131.0;558.0;3469.0;conference and proceedings;inproceedings;2012
With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.;Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping;"Artificial Intelligence (Q2); Computer Networks and Communications (Q2); Control and Optimization (Q2); Hardware and Architecture (Q2); Human-Computer Interaction (Q2)";115.0;320.0;United States;2017-2020;10.1145/3145623;;14.0;;2378962X;23789638;2378962X;23789638;;cyber-physical-systems, dependable time series analytics, sensor-network-regularization-based matrix factorization, IoT-based smart grid;Association for Computing Machinery (ACM);Association for Computing Machinery;4867.0;Northern America;542.0;Q2;21100935201.0;Acm transactions on cyber-physical systems;A dependable time series analytic framework for cyber-physical systems of iot-based smart grid;;429.0;27.0;122.0;1314.0;journal;article;2018
Phenotypic image analysis is the task of recognizing variations in cell properties using microscopic image data. These variations, produced through a complex web of interactions between genes and the environment, may hold the key to uncover important biological phenomena or to understand the response to a drug candidate. Today, phenotypic analysis is rarely performed completely by hand. The abundance of high-dimensional image data produced by modern high-throughput microscopes necessitates computational solutions. Over the past decade, a number of software tools have been developed to address this need. They use statistical learning methods to infer relationships between a cell's phenotype and data from the image. In this review, we examine the strengths and weaknesses of non-commercial phenotypic image analysis software, cover recent developments in the field, identify challenges, and give a perspective on future possibilities.;Kevin Smith and Filippo Piccinini and Tamas Balassa and Krisztian Koos and Tivadar Danka and Hossein Azizpour and Peter Horvath;"Cell Biology (Q1); Histology (Q1); Pathology and Forensic Medicine (Q1)";353.0;844.0;United States;2015-2020;10.1016/j.cels.2018.06.001;0.03533;58.0;;24054712;24054712;24054712;24054712;10.304;high-content screening, single-cell analysis, phenomics, oncology, drug screening, freely available tools, microscopy, machine learning, cell classification, phenotypic image analysis;Cell Press;;5030.0;Northern America;7638.0;Q1;21100394875.0;Cell systems;Phenotypic image analysis software tools for exploring and understanding big image data from cell-based assays;5813.0;3230.0;113.0;414.0;5684.0;journal;article;2018
Precision medicine requires appropriate application of genomics in clinical practice. In cancer, we have witnessed practice-changing examples of how genomic knowledge is translated into more tailored and effective therapies. The next opportunity is to embed cancer genomics in clinical context so that patient-centric longitudinal clinical, genomic, and molecular phenotypes can be compiled for adaptive learning between precision medicine research and clinical care with the goal of accelerating clinically-actionable discoveries. We describe here an adaptive learning platform, APOLLO™ (adaptive patient-oriented longitudinal learning and optimization) designed to integrate genomic research in the context of, but not in the path of, routine and investigational clinical care for purposes of enabling data-driven discovery across disciplines such that every patient can contribute to and potentially benefit from research discoveries.;Lynda Chin and Jennifer A. Wargo and Denise J. Spring and Hagop Kantarjian and P. Andrew Futreal;"Cancer Research (Q1); Oncology (Q1)";269.0;747.0;United States;2015-2020;10.1016/j.trecan.2015.07.010;0.012440000000000001;48.0;;24058033;24058033;24058033;24058033;14.226;N-of-ALL, adaptive learning, patient-oriented genomic research, longitudinal genomics–phenomics profiling, data-driven science and care;Cell Press;;6836.0;Northern America;4175.0;Q1;21100445640.0;Trends in cancer;Cancer genomics in clinical context;4237.0;2509.0;125.0;277.0;8545.0;journal;article;2015
The transition to energy systems with a high share of renewable energy depends on the availability of technologies that can connect the physical distances or bridge the time differences between the energy supply and demand points. This study focuses on energy storage technologies due to their expected role in liberating the energy sector from fossil fuels and facilitating the penetration of intermittent renewable sources. The performance of 27 energy storage alternatives is compared considering sustainability aspects by means of data envelopment analysis. To this end, storage alternatives are first classified into two clusters: fast-response and long-term. The levelized cost of energy, energy and water consumption, global warming potential, and employment are common indicators considered for both clusters, while energy density is used only for fast-response technologies, where it plays a key role in technology selection. Flywheel reveals the highest efficiency between all the fast-response technologies, while green ammonia powered with solar energy ranks first for long-term energy storage. An uncertainty analysis is incorporated to discuss the reliability of the results. Overall, results obtained, and guidelines provided can be helpful for both decision-making and research and development purposes. For the former, we identify the most appealing energy storage options to be promoted, while for the latter, we report quantitative improvement targets that would make inefficient technologies competitive if attained. This contribution paves the way for more comprehensive studies in the context of energy storage by presenting a powerful framework for comparing options according to multiple sustainability indicators.;Fatemeh Rostami and Zoltán Kis and Rembrandt Koppelaar and Laureano Jiménez and Carlos Pozo;"Energy Engineering and Power Technology (Q1); Materials Science (miscellaneous) (Q1); Renewable Energy, Sustainability and the Environment (Q1)";720.0;1642.0;Netherlands;2015-2020;10.1016/j.ensm.2022.03.026;0.028130000000000002;76.0;;24058297;24058297;24058297;24058297;17.789;Data envelopment analysis (DEA), Energy storage, Hydrogen, Power systems flexibility, Sustainable energy;Elsevier BV;;7738.0;Western Europe;5225.0;Q1;21100420314.0;Energy storage materials;Comparative sustainability study of energy storage technologies using data envelopment analysis;17077.0;11925.0;517.0;730.0;40004.0;journal;article;2022
Never in history have global supply-chain relationships in high-tech electronics firms been more sophisticated, complicated, and almost always tied in some major aspect to China. This research examines how interorganizational (IO) cooperation impacts performance and what role relationship learning and information technology (IT) integration play in the value-creation process for Chinese suppliers in business-to-business (B2B) supply chains. We examine this issue using data collected from face-to-face interviews with supply chain managers and executives from 1,004 Chinese high-tech electronic component suppliers. The results strongly support the hypothesis that IO cooperation improves a supplier's performance regarding both its major customer and overall marketplace. Relationship learning and IT integration are important mediating variables that drive performance. The strongest effect in our study was the influence of IO cooperation on relationship learning. A unique aspect of this study is that it focuses on a large sample of a specific supplier type—high-tech Chinese suppliers. This, combined with the fact that the sampled companies were involved in manufacturing 13 different product groups, greatly increases the generalizability of the results.;Neale O’ Connor and Paul Benjamin Lowry and Horst Treiblmaier;Multidisciplinary (Q1);2688.0;285.0;Netherlands;2015-2020;10.1016/j.heliyon.2020.e03434;;28.0;;24058440;24058440;24058440;24058440;;Supply chain performance, Buyer-supplier relationships, Relationship marketing theory, Social exchange theory, Collaboration, Electronics industry, Global supply chain, Business, Globalization, Operations management, Technology adoption, Technology management;Elsevier BV;;4878.0;Western Europe;455.0;Q1;21100411756.0;Heliyon;Interorganizational cooperation and supplier performance in high-technology supply chains;;7639.0;2721.0;2689.0;132717.0;journal;article;2020
Floods and stormwater events are the costliest natural catastrophes. Costs are expected to increase due to urbanization and climate change. Mitigation is needed. Different stakeholders with different motivations unfortunately often evaluate vulnerability by using fragmented and incomplete data sources. This paper intends to review the different approaches for collecting and analyzing data, and to evaluate their usefulness within the proposed framework for a “smart” use of data. The objectives of this work have been to review qualitatively and quantitatively a selection of Norwegian stormwater-related databases and to propose measures for improvement. The findings are seen according to the climate services literature and show that that data is spread around a heterogeneous community of stakeholders concerned with different motivations, different needs, and different levels of data processing. In general, the needs of the different stakeholders have not been surveyed and defined systematically enough and there is a substantial potential in upgrading from the delivery of passive raw data to the delivery of knowledge-driven decision-support tools.;Nathalie Labonnote and Åshild {Lappegard Hauge} and Edvard Sivertsen;"Atmospheric Science (Q1); Global and Planetary Change (Q2)";75.0;568.0;Netherlands;2016-2020;10.1016/j.cliser.2019.01.006;0.00131;21.0;;24058807;24058807;24058807;24058807;5.656;Climate adaptation, Climate services, Databases, Stormwater management, Flooding damage databases;Elsevier BV;;4635.0;Western Europe;1550.0;Q1;21100463842.0;Climate services;A climate services perspective on norwegian stormwater-related databases;664.0;474.0;37.0;87.0;1715.0;journal;article;2019
This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.;M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec;Control and Systems Engineering (Q3);8351.0;113.0;Austria;2002-2019;10.1016/j.ifacol.2020.12.299;;72.0;;24058963;24058963;24058963;24058963;;artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis;IFAC Secretariat;;1663.0;Western Europe;308.0;Q3;21100456158.0;Ifac-papersonline;Artificial intelligence platform proposal for paint structure quality prediction within the industry 4.0 concept;;9863.0;115.0;8407.0;1913.0;journal;article;2020
This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.;Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta;"Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Artificial Intelligence (Q2)";140.0;630.0;South Korea;2015-2020;10.1016/j.icte.2022.06.001;0.0014;22.0;;24059595;24059595;24059595;24059595;4.317;Data quality, Open data, IoT, Machine learning;Korean Institute of Communications Information Sciences;;1827.0;Asiatic Region;733.0;Q1;21100836194.0;Ict express;Intrinsic and extrinsic quality of data for open data repositories;789.0;813.0;85.0;141.0;1553.0;journal;article;2022
With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.;Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan;"Business and International Management (Q1); Economics and Econometrics (Q1); Management of Technology and Innovation (Q1); Marketing (Q1)";70.0;1083.0;Netherlands;2016-2020;10.1016/j.jik.2022.100197;;20.0;;2444569X;2444569X;25307614;2444569X;;Big data, Knowledge teaching evaluation, Performance management;Elsevier BV;;6751.0;Western Europe;1720.0;Q1;21100932830.0;Journal of innovation and knowledge;Review on a big data-based innovative knowledge teaching evaluation system in universities;;691.0;37.0;72.0;2498.0;journal;article;2022
Supply chain disruptions have serious consequences for society and this has made supply chain risk management (SCRM) an attractive area for researchers and managers. In this paper, we use an objective literature mapping approach to identify, classify, and analyze decision-making models and support systems for SCRM, providing an agenda for future research. Through bibliometric networks of articles published in the Scopus database, we analyze the most influential decision-making models and support systems for SCRM, evaluate the main areas of current research, and provide insights for future research in this field. The main results are the following: we found that the identity of the area is structured in three groups of risk decision support models: (i) quantitative multicriteria decision models, (ii) stochastic decision-making models, and (iii) computational simulation/optimization models. We mapped six current research clusters: (i) conceptual and qualitative risk models, (ii) upstream supply chain risk models, (iii) downstream supply chain risk models, (iv) supply chain sustainability risk models, (v) stochastic and multicriteria decision risk models, and (vi) emerging techniques risk models. We identified seven future research clusters, with insights from further studies for: (i) tools to operate SCRM data, (ii) validation of risk models, (iii) computational improvement for data analysis, (iv) multi-level and multi-period supply chains, (v) agrifood risks, (vi) energy risks and (vii) sustainability risks. Finally, the future research agenda should prioritize SCRM's holistic vision, the relationship between Big Data, Industry 4.0 and SCRM, as well as emerging social and environmental risks.;Marcus Vinicius Carvalho Fagundes and Eduardo Oliveira Teles and Silvio A.B. {Vieira de Melo} and Francisco Gaudêncio Mendonça Freires;"Business and International Management (Q1); Marketing (Q1); Strategy and Management (Q1); Economics and Econometrics (Q2)";63.0;633.0;Spain;2016-2020;10.1016/j.iedeen.2020.02.001;0.00054;18.0;;24448834;24448834;24448834;24448834;5.024;risk model, multicriteria decision, stochastic and computational model, bibliometrics;European Academy of Management and Business Economics;;5595.0;Western Europe;1024.0;Q1;21100465205.0;European research on management and business economics;Decision-making models and support systems for supply chain risk: literature mapping and future research agenda;395.0;427.0;22.0;63.0;1231.0;journal;article;2020
The nature of mental illness remains a conundrum. Traditional disease categories are increasingly suspected to misrepresent the causes underlying mental disturbance. Yet psychiatrists and investigators now have an unprecedented opportunity to benefit from complex patterns in brain, behavior, and genes using methods from machine learning (e.g., support vector machines, modern neural-network algorithms, cross-validation procedures). Combining these analysis techniques with a wealth of data from consortia and repositories has the potential to advance a biologically grounded redefinition of major psychiatric disorders. Increasing evidence suggests that data-derived subgroups of psychiatric patients can better predict treatment outcomes than DSM/ICD diagnoses can. In a new era of evidence-based psychiatry tailored to single patients, objectively measurable endophenotypes could allow for early disease detection, individualized treatment selection, and dosage adjustment to reduce the burden of disease. This primer aims to introduce clinicians and researchers to the opportunities and challenges in bringing machine intelligence into psychiatric practice.;Danilo Bzdok and Andreas Meyer-Lindenberg;"Biological Psychiatry (Q1); Cognitive Neuroscience (Q1); Neurology (clinical) (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";264.0;457.0;United States;2016-2020;10.1016/j.bpsc.2017.11.007;;30.0;;24519022;24519022;24519022;24519022;;Artificial intelligence, Endophenotypes, Machine learning, Null-hypothesis testing, Personalized medicine, Predictive analytics, Research Domain Criteria (RDoC), Single-subject prediction;Elsevier Inc.;;5670.0;Northern America;2510.0;Q1;21100445639.0;Biological psychiatry: cognitive neuroscience and neuroimaging;Machine learning for precision psychiatry: opportunities and challenges;;1614.0;170.0;356.0;9639.0;journal;article;2018
"Summary
Modern chemistry is the backbone of our society, but it is also a major contributor to global environmental pollution and the ongoing climate crisis. The transition toward a sustainable future requires a radical transformation of how chemistry is designed, developed, and used. This represents a “break it or make it” challenge for the chemical industry with significant technology lock-in and high entry barriers to radical innovations. We propose that urgently required systemic changes in chemical industry, research and development (R&D), chemicals assessment and management, and education to advance sustainable chemistry are attainable through increased and more rapid adoption of digitalization and new digital tools. This will enable flexible data exchange, increased transparency of information flows along cross-country chemical, material, and product life cycles, and chemistries that are safe and sustainable by design, addressing the complexity of chemicals-environment-health interactions and lowering the costs of entry into chemical R&D and manufacture, and new, more sustainable and collaborative business models.";Peter Fantke and Claudio Cinquemani and Polina Yaseneva and Jonathas {De Mello} and Henning Schwabe and Bjoern Ebeling and Alexei A. Lapkin;"Biochemistry (Q1); Biochemistry (medical) (Q1); Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1); Environmental Chemistry (Q1); Materials Chemistry (Q1)";596.0;1479.0;United States;2016-2020;10.1016/j.chempr.2021.09.012;0.03335;78.0;;24519294;24519294;24519294;24519294;22.804;safe and sustainable by design, artificial intelligence, big data, green transition, sustainable development, machine learning;Elsevier Inc.;;5060.0;Northern America;7057.0;Q1;21100788876.0;Chem;Transition to sustainable chemistry through digitalization;12491.0;9545.0;275.0;649.0;13915.0;journal;article;2021
The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.;Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh;Genetics (Q4);441.0;72.0;Netherlands;2015-2020;10.1016/j.genrep.2020.100869;;9.0;;24520144;24520144;24520144;24520144;;Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning;Elsevier BV;;4164.0;Western Europe;235.0;Q4;21100445644.0;Gene reports;Big data in biology: the hope and present-day challenges in it;;331.0;407.0;442.0;16946.0;journal;article;2020
"The empirical necessity for integrating informatics throughout the experimental process has become a focal point of the nano-community as we work in parallel to converge efforts for making nano-data reproducible and accessible. The NanoInformatics Knowledge Commons (NIKC) Database was designed to capture the complex relationship between nanomaterials and their environments over time in the concept of an ‘Instance’. Our Instance Organizational Structure (IOS) was built to record metadata on nanomaterial transformations in an organizational structure permitting readily accessible data for broader scientific inquiry. By transforming published and on-going data into the IOS we are able to tell the full transformational journey of a nanomaterial within its experimental life cycle. The IOS structure has prepared curated data to be fully analyzed to uncover relationships between observable phenomenon and medium or nanomaterial characteristics. Essential to building the NIKC database and associated applications was incorporating the researcher's needs into every level of development. We started by centering the research question, the query, and the necessary data needed to support the question and query. The process used to create nanoinformatic tools informs usability and analytical capability. In this paper we present the NIKC database, our developmental process, and its curated contents. We also present the Collaboration Tool which was built to foster building new collaboration teams. Through these efforts we aim to: 1) elucidate the general principles that determine nanomaterial behavior in the environment; 2) identify metadata necessary to predict exposure potential and bio-uptake; and 3) identify key characterization assays that predict outcomes of interest.";Jaleesia D. Amos and Yuan Tian and Zhao Zhang and Greg V. Lowry and Mark R. Wiesner and Christine Ogilvie Hendren;"Materials Science (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)";144.0;489.0;Netherlands;2016-2020;10.1016/j.impact.2021.100331;0.0017699999999999999;25.0;;24520748;24520748;24520748;24520748;5.316;Database, Nanoinformatics, Nanomaterials, Environmental nanotechnology, Transformations;Elsevier BV;;6378.0;Western Europe;1045.0;Q1;21100463052.0;Nanoimpact;The nanoinformatics knowledge commons: capturing spatial and temporal nanomaterial transformations in diverse systems;1189.0;843.0;79.0;150.0;5039.0;journal;article;2021
"Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.";Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo;"Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)";264.0;265.0;United States;2016-2020;10.1016/j.adro.2022.100925;;19.0;;24521094;24521094;24521094;24521094;;;Elsevier Inc.;;2474.0;Northern America;989.0;Q1;21100465104.0;Advances in radiation oncology;Head and neck radiotherapy patterns of practice variability identified as a challenge to real-world big data: results from the learning from analysis of multicentre big data aggregation (lambda) consortium;;746.0;211.0;271.0;5221.0;journal;article;2022
Systems biology involves network-oriented, computational approaches to modeling biological systems through analysis of big biological data. To contribute maximally to scientific progress, big biological data should be FAIR: findable, accessible, interoperable, and reusable. Here, we describe high-throughput sequencing data that characterize the vast diversity of B- and T-cell clones comprising the adaptive immune receptor repertoire (AIRR-seq data) and its contribution to our understanding of COVID-19 (coronavirus disease 19). We describe the accomplishments of the AIRR community, a grass-roots network of interdisciplinary laboratory scientists, bioinformaticians, and policy wonks, in creating and publishing standards, software and repositories for AIRR-seq data based on the FAIR principles.;Jamie K. Scott and Felix Breden;"Applied Mathematics (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Computer Science Applications (Q1); Drug Discovery (Q1); Modeling and Simulation (Q1)";239.0;290.0;United Kingdom;2017-2020;10.1016/j.coisb.2020.10.001;;21.0;;24523100;24523100;24523100;24523100;;Adaptive immunity, B-cell and T-cell receptor repertoires, FAIR Principles, Open source, Adaptive immune receptor repertoire (AIRR) community;Elsevier Ltd.;;6363.0;Western Europe;1576.0;Q1;21100857212.0;Current opinion in systems biology;The adaptive immune receptor repertoire community as a model for fair stewardship of big immunology data;;800.0;43.0;269.0;2736.0;journal;article;2020
Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.;Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett;"Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)";83.0;1226.0;Netherlands;2016-2020;10.1016/j.jii.2018.02.002;0.00148;24.0;;2452414X;2452414X;2452414X;2452414X;10.063;External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering;Elsevier BV;;7686.0;Western Europe;2042.0;Q1;21100787106.0;Journal of industrial information integration;Improving early osv design robustness by applying ‘multivariate big data analytics’ on a ship's life cycle;1149.0;1361.0;28.0;86.0;2152.0;journal;article;2018
The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.;Kok-Kwang Phoon and Jianye Ching and Zijun Cao;"Building and Construction (Q1); Civil and Structural Engineering (Q1); Geotechnical Engineering and Engineering Geology (Q2)";74.0;285.0;China;2016-2020;10.1016/j.undsp.2022.04.001;;12.0;;24679674;20962754;24679674;20962754;;Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization (DDSC), Project DeepGeo, Data-informed decision support index;Tongji University Press;;4642.0;Asiatic Region;774.0;Q1;21100939600.0;Underground space (china);Unpacking data-centric geotechnics;;234.0;64.0;76.0;2971.0;journal;article;2022
Phenomics is a new branch of science that provides high-throughput quantification of plant and animal traits at systems level. The last decade has witnessed great successes in high-throughput phenotyping of numerous morphological traits, yet major challenges still exist in precise phenotyping of physiological traits such as transpiration and photosynthesis. Due to the highly dynamic nature of physiological traits in responses to the environment, appropriate selection criteria and efficient screening systems at the physiological level for abiotic stress tolerance have been largely absent in plants. In this review, the current status of phenomics techniques was briefly summarized in horticultural plants. Specifically, the emerging field of high-throughput physiology-based phenotyping, which is referred to as “physiolomics”, for drought stress responses was highlighted. In addition to analyzing the advantages of physiology-based phenotyping over morphology-based approaches, recent examples that applied high-throughput physiological phenotyping to model and non-model horticultural plants were revisited and discussed. Based on the collective findings, we propose that high-throughput, non-destructive, and automatic physiological assays can and should be used as routine methods for phenotyping stress response traits in horticultural plants.;Yanwei Li and Xinyi Wu and Wenzhao Xu and Yudong Sun and Ying Wang and Guojing Li and Pei Xu;"Plant Science (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2); Renewable Energy, Sustainability and the Environment (Q2)";62.0;332.0;China;2018-2020;10.1016/j.hpj.2020.09.004;0.00078;9.0;;24680141;20959885;24680141;20959885;3.032;Phenomics, Physiolomics, Isohydric/anisohydric, Abiotic stress;Elsevier BV;;5934.0;Asiatic Region;807.0;Q1;21100900275.0;Horticultural plant journal;High-throughput physiology-based stress response phenotyping: advantages, applications and prospective in horticultural plants;418.0;209.0;44.0;63.0;2611.0;journal;article;2021
"Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.";Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang;"Nursing (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q2); Infectious Diseases (Q3)";73.0;227.0;Australia;2016-2020;10.1016/j.idh.2018.10.002;;14.0;;24680451;24680451;24680451;24680451;;Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning;Australasian College for Infection Prevention and Control (ACIPC);;2493.0;Pacific Region;724.0;Q1;21100466874.0;Infection, disease and health;Artificial intelligence for infectious disease big data analytics;;195.0;58.0;97.0;1446.0;journal;article;2019
The COSMOS Database (DB) was originally established to provide reliable data for cosmetics-related chemicals within the COSMOS Project funded as part of the SEURAT-1 Research Initiative. The database has subsequently been maintained and developed further into COSMOS Next Generation (NG), a combination of database and in silico tools, essential components of a knowledge base. COSMOS DB provided a cosmetics inventory as well as other regulatory inventories, accompanied by assessment results and in vitro and in vivo toxicity data. In addition to data content curation, much effort was dedicated to data governance – data authorisation, characterisation of quality, documentation of meta information, and control of data use. Through this effort, COSMOS DB was able to merge and fuse data of various types from different sources. Building on the previous effort, the COSMOS Minimum Inclusion (MINIS) criteria for a toxicity database were further expanded to quantify the reliability of studies. COSMOS NG features multiple fingerprints for analysing structure similarity, and new tools to calculate molecular properties and screen chemicals with endpoint-related public profilers, such as DNA and protein binders, liver alerts and genotoxic alerts. The publicly available COSMOS NG enables users to compile information and execute analyses such as category formation and read-across. This paper provides a step-by-step guided workflow for a simple read-across case, starting from a target structure and culminating in an estimation of a NOAEL confidence interval. Given its strong technical foundation, inclusion of quality-reviewed data, and provision of tools designed to facilitate communication between users, COSMOS NG is a first step towards building a toxicological knowledge hub leveraging many public data systems for chemical safety evaluation. We continue to monitor the feedback from the user community at support@mn-am.com.;C. Yang and M.T.D. Cronin and K.B. Arvidson and B. Bienfait and S.J. Enoch and B. Heldreth and B. Hobocienski and K. Muldoon-Jacobs and Y. Lan and J.C. Madden and T. Magdziarz and J. Marusczyk and A. Mostrag and M. Nelms and D. Neagu and K. Przybylak and J.F. Rathman and J. Park and A-N Richarz and A.M. Richard and J.V. Ribeiro and O. Sacher and C. Schwab and V. Vitcheva and P. Volarath and A.P. Worth;"Computer Science Applications (Q2); Health, Toxicology and Mutagenesis (Q2); Toxicology (Q2)";104.0;248.0;Netherlands;2017-2020;10.1016/j.comtox.2021.100175;;12.0;;24681113;24681113;24681113;24681113;;Toxicity, Database, Public database, Knowledge hub, Study reliability, Analogue selection, Guided workflow;Elsevier BV;;5004.0;Western Europe;754.0;Q2;21100824046.0;Computational toxicology;Cosmos next generation – a public knowledge base leveraging chemical and biological data to support the regulatory assessment of chemicals;;300.0;26.0;106.0;1301.0;journal;article;2021
"Summary
Background
There are concerns that the COVID-19 pandemic has had a negative effect on cancer care but there is little direct evidence to quantify any effect. This study aims to investigate the impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England.
Methods
Data were extracted from four population-based datasets spanning NHS England (the National Cancer Cancer Waiting Time Monitoring, Monthly Diagnostic, Secondary Uses Service Admitted Patient Care and the National Radiotherapy datasets) for all referrals, colonoscopies, surgical procedures, and courses of rectal radiotherapy from Jan 1, 2019, to Oct 31, 2020, related to colorectal cancer in England. Differences in patterns of care were investigated between 2019 and 2020. Percentage reductions in monthly numbers and proportions were calculated.
Findings
As compared to the monthly average in 2019, in April, 2020, there was a 63% (95% CI 53–71) reduction (from 36 274 to 13 440) in the monthly number of 2-week referrals for suspected cancer and a 92% (95% CI 89–95) reduction in the number of colonoscopies (from 46 441 to 3484). Numbers had just recovered by October, 2020. This resulted in a 22% (95% CI 8–34) relative reduction in the number of cases referred for treatment (from a monthly average of 2781 in 2019 to 2158 referrals in April, 2020). By October, 2020, the monthly rate had returned to 2019 levels but did not exceed it, suggesting that, from April to October, 2020, over 3500 fewer people had been diagnosed and treated for colorectal cancer in England than would have been expected. There was also a 31% (95% CI 19–42) relative reduction in the numbers receiving surgery in April, 2020, and a lower proportion of laparoscopic and a greater proportion of stoma-forming procedures, relative to the monthly average in 2019. By October, 2020, laparoscopic surgery and stoma rates were similar to 2019 levels. For rectal cancer, there was a 44% (95% CI 17–76) relative increase in the use of neoadjuvant radiotherapy in April, 2020, relative to the monthly average in 2019, due to greater use of short-course regimens. Although in June, 2020, there was a drop in the use of short-course regimens, rates remained above 2019 levels until October, 2020.
Interpretation
The COVID-19 pandemic has led to a sustained reduction in the number of people referred, diagnosed, and treated for colorectal cancer. By October, 2020, achievement of care pathway targets had returned to 2019 levels, albeit with smaller volumes of patients and with modifications to usual practice. As pressure grows in the NHS due to the second wave of COVID-19, urgent action is needed to address the growing burden of undetected and untreated colorectal cancer in England.
Funding
Cancer Research UK, the Medical Research Council, Public Health England, Health Data Research UK, NHS Digital, and the National Institute for Health Research Oxford Biomedical Research Centre.";Eva J A Morris and Raphael Goldacre and Enti Spata and Marion Mafham and Paul J Finan and Jon Shelton and Mike Richards and Katie Spencer and Jonathan Emberson and Sam Hollings and Paula Curnow and Dominic Gair and David Sebag-Montefiore and Chris Cunningham and Matthew D Rutter and Brian D Nicholson and Jem Rashbass and Martin Landray and Rory Collins and Barbara Casadei and Colin Baigent;"Gastroenterology (Q1); Hepatology (Q1)";240.0;486.0;United Kingdom;2016-2020;10.1016/S2468-1253(21)00005-4;;49.0;;24681253;24681253;24681253;24681253;;;Elsevier Ltd.;;1822.0;Western Europe;4897.0;Q1;21100786228.0;Lancet gastroenterology and hepatology, the;Impact of the covid-19 pandemic on the detection and management of colorectal cancer in england: a population-based study;;3209.0;234.0;617.0;4263.0;journal;article;2021
Environmental exposure to chemical toxins alters epigenetic modifications that culminate in altered cellular gene expression without changing the underlying DNA sequence. The complex interplay between the layers of epigenetic regulators ultimately results in observed cellular phenotype. This review highlights epigenetics annotations assayed in the Encyclopedia of DNA Elements (ENCODE) community resource project—a publically accessible database for understanding genomic function, development and disease etiologies. We outline the multiple levels of epigenetic control (DNA methylation, chromatin accessibility, histone modifications, genome topology) with their associated interrogation methodology. We explore the limitations and strengths of each methodology at every epigenetic checkpoint. This review points readers to epigenetic resources that have gathered focused scientific data and directs them toward data visualization tools that can help answer questions related to epigenetic controls. The purpose of this review is to highlight online resources available to toxicological epigenetic researchers that can help fast track novel insights using already curated reference epigenome datasets.;Paul J. Lee and Mayank NK Choudhary and Ting Wang;Toxicology (Q2);186.0;405.0;Netherlands;2016-2020;10.1016/j.cotox.2017.07.007;;21.0;;24682020;24682020;24682020;24682020;;Next generation sequencing, Online database, Epigenome, Methylome, Methylation, Chromatin accessibility, Histone modifications, 3D genome, TADs, ENCODE, ROADMAP;Elsevier BV;;4840.0;Western Europe;932.0;Q2;21100804457.0;Current opinion in toxicology;Online resources for studies of genome biology and epigenetics;;697.0;58.0;200.0;2807.0;journal;article;2017
"Summary
Background
A comprehensive evaluation of the burden of injury is an important foundation for selecting and formulating strategies of injury prevention. We present results from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017 of non-fatal and fatal outcomes of injury at the national and subnational level, and the changes in burden for key causes of injury over time in China.
Methods
Using the methods and results from GBD 2017, we describe the burden of total injury and the key causes of injury based on the rates of incidence, cause-specific mortality, and disability-adjusted life years (DALYs) in China estimated using DisMod-MR 2.1. We additionally evaluated these results at the provincial level for the 34 subnational locations of China in 2017, measured the change of injury burden from 1990 to 2017, and compared age-standardised DALYs due to injuries at the provincial level against the expected rates based on the Socio-demographic Index (SDI), a composite measure of development of income per capita, years of education, and total fertility rate.
Findings
In 2017, in China, there were 77·1 million (95% uncertainty interval [UI] 72·5–81·6) new cases of injury severe enough to warrant health care and 733 517 deaths (681 254–767 006) due to injuries. Injuries accounted for 7·0% (95% UI 6·6–7·2) of total deaths and 10·0% (9·5–10·5) of all-cause DALYs in China. In 2017, there was a three-times variation in age-standardised injury DALY rates between provinces of China, with the lowest value in Macao and the highest in Yunnan. Between 1990 and 2017, the age-standardised incidence rate of all injuries increased by 50·6% (95% UI 46·6–54·6) in China, whereas the age-standardised mortality and DALY rates decreased by 44·3% (41·1–48·9) and 48·1% (44·6–51·8), respectively. Between 1990 and 2017, all provinces of China experienced a substantial decline in DALY rates from all injuries ranging from 16·3% (3·1–28·6) in Shanghai and 60·4% (53·7–66·1) in Jiangxi. Age-standardised DALY rates for drowning; injuries from fire, heat and hot substances; adverse effects of medical treatments; animal contact; environmental heat and cold exposure; self-harm; and executions and police conflict each declined by more than 60% between 1990 and 2017.
Interpretation
Between 1990 and 2017, China experienced a decrease in the age-standardised DALY and mortality rates due to injury, despite an increase in the age-standardised incidence rate. These trends occurred in all provinces. The divergent trends in terms of incidence and mortality indicate that with rapid sociodemographic improvements, the case fatality of injuries has declined, which could be attributed to an improving health-care system but also to a decreasing severity of injuries over this time period.
Funding
Bill & Melinda Gates Foundation.";Duan Leilei and Ye Pengpeng and Juanita A Haagsma and Jin Ye and Wang Yuan and Er Yuliang and Deng Xiao and Gao Xin and Ji Cuirong and Wang Linhong and Marlena S Bannick and W Cliff Mountjoy-Venning and Caitlin N Hawley and Zichen Liu and Mari Smith and Spencer L James and Theo Vos and Christopher J L Murray;Public Health, Environmental and Occupational Health (Q1);134.0;533.0;United Kingdom;2016-2020;10.1016/S2468-2667(19)30125-2;;45.0;;24682667;24682667;24682667;24682667;;;Elsevier Ltd.;;1551.0;Western Europe;7226.0;Q1;21100804406.0;Lancet public health, the;The burden of injury in china, 1990–2017: findings from the global burden of disease study 2017;;2651.0;172.0;433.0;2668.0;journal;article;2019
The metaverse is a visual world that blends the physical world and digital world. At present, the development of the metaverse is still in the early stage, and there lacks a framework for the visual construction and exploration of the metaverse. In this paper, we propose a framework that summarizes how graphics, interaction, and visualization techniques support the visual construction of the metaverse and user-centric exploration. We introduce three kinds of visual elements that compose the metaverse and the two graphical construction methods in a pipeline. We propose a taxonomy of interaction technologies based on interaction tasks, user actions, feedback and various sensory channels, and a taxonomy of visualization techniques that assist user awareness. Current potential applications and future opportunities are discussed in the context of visual construction and exploration of the metaverse. We hope this paper can provide a stepping stone for further research in the area of graphics, interaction and visualization in the metaverse.;Yuheng Zhao and Jinjing Jiang and Yi Chen and Richen Liu and Yalong Yang and Xiangyang Xue and Siming Chen;"Computer Graphics and Computer-Aided Design (Q2); Human-Computer Interaction (Q2); Software (Q2)";51.0;364.0;Netherlands;2017-2020;10.1016/j.visinf.2022.03.002;;11.0;;2468502X;25432656;2468502X;25432656;;Metaverse, Virtual reality/Augmented reality, Computer graphics, User interaction, Immersive visualization;Elsevier BV;;4639.0;Western Europe;495.0;Q2;21100907392.0;Visual informatics;Metaverse: perspectives from graphics, interactions and visualization;;241.0;28.0;53.0;1299.0;journal;article;2022
Agricultural pesticides are a key component of the toolbox of most agricultural systems and are likely to continue to play a role in meeting the challenge of feeding a growing global population. However, pesticide use has well documented and often significant consequences for populations of native wildlife. Although rigorous, regulatory processes for the approval of new chemicals for agronomic use do have limitations which may fail to identify real-world negative effects of products. Here, we describe a possible approach to complement the existing regulatory process, which is to combine long-term and national-scale data sets on native wildlife with pesticide use data to understand long-term and large-scale impacts of agrochemicals on wildlife populations.;Francesca Mancini and Ben A. Woodcock and Nick J.B. Isaac;"Environmental Chemistry (Q1); Health, Toxicology and Mutagenesis (Q1); Public Health, Environmental and Occupational Health (Q1)";126.0;763.0;Netherlands;2018-2020;10.1016/j.coesh.2019.07.003;;20.0;;24685844;24685844;24685844;24685844;;Occupancy-detection models, Pollinators, Sustainable agriculture, Biological recording, Pesticide surveillance;Elsevier BV;;5559.0;Western Europe;1764.0;Q1;21100936579.0;Current opinion in environmental science and health;Agrochemicals in the wild: identifying links between pesticide use and declines of nontarget organisms;;1045.0;73.0;137.0;4058.0;journal;article;2019
"Purpose
Though the domain of big data and artificial intelligence in health care continues to evolve, there is a lack of systemic methods to improve data quality and streamline the preparation process. To address this, we aimed to develop an automated sorting system (RetiSort) that accurately labels the type and laterality of retinal photographs.
Design
Cross-sectional study.
Participants
RetiSort was developed with retinal photographs from the Singapore Epidemiology of Eye Diseases (SEED) study.
Methods
The development of RetiSort was composed of 3 steps: 2 deep-learning (DL) algorithms and 1 rule-based classifier. For step 1, a DL algorithm was developed to locate the optic disc, the “landmark feature.” For step 2, based on the location of the optic disc derived from step 1, a rule-based classifier was developed to sort retinal photographs into 3 types: macular-centered, optic disc–centered, or related to other fields. Step 2 concurrently distinguished laterality (i.e., the left or right eye) of macular-centered photographs. For step 3, an additional DL algorithm was developed to differentiate the laterality of disc-centered photographs. Via the 3 steps, RetiSort sorted and labeled retinal images into (1) right macular–centered, (2) left macular–centered, (3) right optic disc–centered, (4) left optic disc–centered, and (5) images relating to other fields. Subsequently, the accuracy of RetiSort was evaluated on 5000 randomly selected retinal images from SEED as well as on 3 publicly available image databases (DIARETDB0, HEI-MED, and Drishti-GS). The main outcome measure was the accuracy for sorting of retinal photographs.
Results
RetiSort mislabeled 48 out of 5000 retinal images from SEED, representing an overall accuracy of 99.0% (95% confidence interval [CI], 98.7–99.3). In external tests, RetiSort mislabeled 1, 0, and 2 images, respectively, from DIARETDB0, HEI-MED, and Drishti-GS, representing an accuracy of 99.2% (95% CI, 95.8–99.9), 100%, and 98.0% (95% CI, 93.1–99.8), respectively. Saliency maps consistently showed that the DL algorithm in step 3 required pixels in the central left lateral border and optic disc of optic disc–centered retinal photographs to differentiate the laterality.
Conclusions
RetiSort is a highly accurate automated sorting system. It can aid in data preparation and has practical applications in DL research that uses retinal photographs.";Tyler Hyungtaek Rim and Zhi Da Soh and Yih-Chung Tham and Henrik Hee Seung Yang and Geunyoung Lee and Youngnam Kim and Simon Nusinovici and Daniel Shu Wei Ting and Tien Yin Wong and Ching-Yu Cheng;Ophthalmology (Q1);394.0;234.0;United States;2017-2020;10.1016/j.oret.2020.03.007;;18.0;;24686530;24686530;24686530;24686530;;;Elsevier Inc.;;1897.0;Northern America;1517.0;Q1;21100925772.0;Ophthalmology retina;Deep learning for automated sorting of retinal photographs;;1327.0;254.0;586.0;4819.0;journal;article;2020
Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.;Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman;"Computer Science (miscellaneous) (Q1); Safety, Risk, Reliability and Quality (Q1)";59.0;480.0;United States;2016-2020;10.1145/3484945;0.00041;14.0;;24712566;24712566;24712574;24712566;1.909;data anonymization, resilient distributed dataset (RDD), multi-dimensional data, Mondrian, Spark;Association for Computing Machinery (ACM);Association for Computing Machinery;5739.0;Northern America;743.0;Q1;21100832567.0;Acm transactions on privacy and security;A novel hybrid approach for multi-dimensional data anonymization for apache spark;138.0;275.0;28.0;59.0;1607.0;journal;article;2021
"The inverse relationship between the cost of drug development and the successful integration of drugs into the market has resulted in the need for innovative solutions to overcome this burgeoning problem. This problem could be attributed to several factors, including the premature termination of clinical trials, regulatory factors, or decisions made in the earlier drug development processes. The introduction of artificial intelligence (AI) to accelerate and assist drug development has resulted in cheaper and more efficient processes, ultimately improving the success rates of clinical trials. This review aims to showcase and compare the different applications of AI technology that aid automation and improve success in drug development, particularly in novel drug target identification and design, drug repositioning, biomarker identification, and effective patient stratification, through exploration of different disease landscapes. In addition, it will also highlight how these technologies are translated into the clinic. This paradigm shift will lead to even greater advancements in the integration of AI in automating processes within drug development and discovery, enabling the probability and reality of attaining future precision and personalized medicine.
摘要
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。
초록
약물 개발 비용과 약물의 성공적인 시장 통합 간의 역상관관계는 이러한 급증하는 문제들을 극복하기 위한 혁신적인 해법이 필요하다는 인식을 제기했다. 이러한 문제는 임상시험의 조기 종료, 규제 요인 또는 초기 약물 개발 과정에서 이루어진 결정을 포함한 여러 요인들에서 기인할 수 있다. 약물 개발을 가속화하고 지원하기 위한 인공지능(artificial intelligence, AI)의 도입으로 약물 개발 과정이 더욱 저렴하고 더욱 효율적이 되었으며 궁극적으로 임상시험의 성공률이 향상되었다. 본 종설의 목적은 다양한 질병 환경의 탐색을 통해 자동화를 지원하고 특히 신약의 표적 확인 및 설계, 약물의 재포지셔닝, 생체표지자 확인 및 효과적인 환자 층화 부분에서 약물 개발의 성공을 향상시키는 AI 기술의 다양한 적용을 보여주고 비교하는 것이다. 또한 이러한 기술들이 임상으로 전환되는 방식에 대해서도 강조할 것이다. 이러한 패러다임 전환은 신약 개발 및 발견의 자동화 과정에서 AI의 통합을 더욱 크게 발전시켜 향후 정밀의학 및 맞춤 의학 달성 가능성을 높이고 그 실현을 가능하게 할 것이다.
抄録
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。";Masturah Bte Mohd Abdul Rashid;"Medical Laboratory Technology (Q1); Computer Science Applications (Q2)";183.0;240.0;United States;2017-2020;10.1177/2472630320956931;0.00118;16.0;;24726303;24726311;24726303;24726311;3.047;artificial intelligence, drug development, drug discovery, industry;SAGE Publications Inc.;;3578.0;Northern America;714.0;Q1;21100799933.0;Slas technology;Artificial intelligence effecting a paradigm shift in drug development;607.0;518.0;73.0;206.0;2612.0;journal;article;2021
"Summary
Hundreds of millions of people lack access to electricity. Decentralized solar-battery systems are key for addressing this while avoiding carbon emissions and air pollution but are hindered by relatively high costs and rural locations that inhibit timely preventive maintenance. Accurate diagnosis of battery health and prediction of end of life from operational data improves user experience and reduces costs. However, lack of controlled validation tests and variable data quality mean existing lab-based techniques fail to work. We apply a scalable probabilistic machine learning approach to diagnose health in 1,027 solar-connected lead-acid batteries, each running for 400–760 days, totaling 620 million data rows. We demonstrate 73% accurate prediction of end of life, 8 weeks in advance, rising to 82% at the point of failure. This work highlights the opportunity to estimate health from existing measurements using “big data” techniques, without additional equipment, extending lifetime and improving performance in real-world applications.";Antti Aitio and David A. Howey;Energy (miscellaneous) (Q1);512.0;2467.0;United States;2017-2020;10.1016/j.joule.2021.11.006;0.046239999999999996;84.0;;25424351;25424351;25424351;25424351;41.248;battery, health, machine learning, rural electrification, Gaussian process, classification, Kalman filter;Cell Press;;4470.0;Northern America;12532.0;Q1;21100834904.0;Joule;Predicting battery end of life from solar off-grid system field data using machine learning;17275.0;14739.0;237.0;598.0;10595.0;journal;article;2021
"Summary
Background
Antimicrobial resistance (AMR) is a serious threat to global public health. WHO emphasises the need for countries to monitor antibiotic consumption to combat AMR. Many low-income and middle-income countries (LMICs) lack surveillance capacity; we aimed to use multiple data sources and statistical models to estimate global antibiotic consumption.
Methods
In this spatial modelling study, we used individual-level data from household surveys to inform a Bayesian geostatistical model of antibiotic usage in children (aged <5 years) with lower respiratory tract infections in LMICs. Antibiotic consumption data were obtained from multiple sources, including IQVIA, WHO, and the European Surveillance of Antimicrobial Consumption Network (ESAC-Net). The estimates of the antibiotic usage model were used alongside sociodemographic and health covariates to inform a model of total antibiotic consumption in LMICs. This was combined with a single model of antibiotic consumption in high-income countries to produce estimates of antibiotic consumption covering 204 countries and 19 years.
Findings
We analysed 209 surveys done between 2000 and 2018, covering 284 045 children with lower respiratory tract infections. We identified large national and subnational variations of antibiotic usage in LMICs, with the lowest levels estimated in sub-Saharan Africa and the highest in eastern Europe and central Asia. We estimated a global antibiotic consumption rate of 14·3 (95% uncertainty interval 13·2–15·6) defined daily doses (DDD) per 1000 population per day in 2018 (40·2 [37·2–43·7] billion DDD), an increase of 46% from 9·8 (9·2–10·5) DDD per 1000 per day in 2000. We identified large spatial disparities, with antibiotic consumption rates varying from 5·0 (4·8–5·3) DDD per 1000 per day in the Philippines to 45·9 DDD per 1000 per day in Greece in 2018. Additionally, we present trends in consumption of different classes of antibiotics for selected Global Burden of Disease study regions using the IQVIA, WHO, and ESAC-net input data. We identified large increases in the consumption of fluoroquinolones and third-generation cephalosporins in North Africa and Middle East, and south Asia.
Interpretation
To our knowledge, this is the first study that incorporates antibiotic usage and consumption data and uses geostatistical modelling techniques to estimate antibiotic consumption for 204 countries from 2000 to 2018. Our analysis identifies both high rates of antibiotic consumption and a lack of access to antibiotics, providing a benchmark for future interventions.
Funding
Fleming Fund, UK Department of Health and Social Care; Wellcome Trust; and Bill & Melinda Gates Foundation.";Annie J Browne and Michael G Chipeta and Georgina Haines-Woodhouse and Emmanuelle P A Kumaran and Bahar H Kashef Hamadani and Sabra Zaraa and Nathaniel J Henry and Aniruddha Deshpande and Robert C Reiner and Nicholas P J Day and Alan D Lopez and Susanna Dunachie and Catrin E Moore and Andy Stergachis and Simon I Hay and Christiane Dolecek;"Health Policy (Q1); Health (social science) (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)";107.0;480.0;Netherlands;2017-2020;10.1016/S2542-5196(21)00280-1;;29.0;;25425196;25425196;25425196;25425196;;;Elsevier BV;;1795.0;Western Europe;3535.0;Q1;21100858657.0;Lancet planetary health, the;Global antibiotic consumption and usage in humans, 2000–18: a spatial modelling study;;1782.0;136.0;356.0;2441.0;journal;article;2021
"Summary
The accumulation of massive single-cell omics data provides growing resources for building biomolecular atlases of all cells of human organs or the whole body. The true assembly of a cell atlas should be cell-centric rather than file-centric. We developed a unified informatics framework for seamless cell-centric data assembly and built the human Ensemble Cell Atlas (hECA) from scattered data. hECA v1.0 assembled 1,093,299 labeled human cells from 116 published datasets, covering 38 organs and 11 systems. We invented three new methods of atlas applications based on the cell-centric assembly: “in data” cell sorting for targeted data retrieval with customizable logic expressions, “quantitative portraiture” for multi-view representations of biological entities, and customizable reference creation for generating references for automatic annotations. Case studies on agile construction of user-defined sub-atlases and “in data” investigation of CAR-T off-targets in multiple organs showed the great potential enabled by the cell-centric ensemble atlas.";Sijie Chen and Yanting Luo and Haoxiang Gao and Fanhong Li and Yixin Chen and Jiaqi Li and Renke You and Minsheng Hao and Haiyang Bian and Xi Xi and Wenrui Li and Weiyu Li and Mingli Ye and Qiuchen Meng and Ziheng Zou and Chen Li and Haochen Li and Yangyuan Zhang and Yanfei Cui and Lei Wei and Fufeng Chen and Xiaowo Wang and Hairong Lv and Kui Hua and Rui Jiang and Xuegong Zhang;Multidisciplinary (Q1);717.0;508.0;United States;2018-2020;10.1016/j.isci.2022.104318;0.0123;27.0;;25890042;25890042;25890042;25890042;5.458;Cell biology, Stem cells research, Bioinformatics;Elsevier Inc.;;6059.0;Northern America;1805.0;Q1;21100907125.0;Iscience;Heca: the cell-centric assembly of a cell atlas;5235.0;3676.0;1073.0;724.0;65016.0;journal;article;2022
"Background
Depression is the leading cause of disability worldwide with > 50% of cases emerging before the age of 25 years. Large-scale neuroimaging studies in depression implicate robust structural brain differences in the disorder. However, most studies have been conducted in adults and therefore, the temporal origins of depression-related imaging features remain largely unknown. This has important implications for understanding aetiology and informing timings of potential intervention.
Methods
Here, we examine associations between brain structure (cortical metrics and white matter microstructural integrity) and depression ratings (from caregiver and child), in a large sample (N = 8634) of early adolescents (9 to 11 years old) from the US-based, Adolescent Brain and Cognitive Development (ABCD) Study®. Data was collected from 2016 to 2018.
Findings
We report significantly decreased global cortical and white matter metrics, and regionally in frontal, limbic and temporal areas in adolescent depression (Cohen's d = -0⋅018 to -0⋅041, β = -0·019 to -0⋅057). Further, we report consistently stronger imaging associations for caregiver-reported compared to child-reported depression ratings. Divergences between reports (caregiver vs child) were found to significantly relate to negative socio-environmental factors (e.g., family conflict, absolute β = 0⋅048 to 0⋅169).
Interpretation
Depression ratings in early adolescence were associated with similar imaging findings to those seen in adult depression samples, suggesting neuroanatomical abnormalities may be present early in the disease course, arguing for the importance of early intervention. Associations between socio-environmental factors and reporter discrepancy warrant further consideration, both in the wider context of the assessment of adolescent psychopathology, and in relation to their role in aetiology.
Funding
Wellcome Trust (References: 104036/Z/14/Z and 220857/Z/20/Z) and the Medical Research Council (MRC, Reference: MC_PC_17209).";Xueyi Shen and Niamh MacSweeney and Stella W.Y. Chan and Miruna C. Barbu and Mark J. Adams and Stephen M. Lawrie and Liana Romaniuk and Andrew M. McIntosh and Heather C. Whalley;Medicine (miscellaneous) (Q1);127.0;349.0;United Kingdom;2018-2020;10.1016/j.eclinm.2021.101204;;20.0;;25895370;25895370;25895370;25895370;;Big data, Adolescent depression, Adolescent Brain and Cognitive Development Study, Brain structure;Lancet Publishing Group;;2698.0;Western Europe;1915.0;Q1;21100903225.0;Eclinicalmedicine;Brain structural associations with depression in a large early adolescent sample (the abcd study®);;754.0;415.0;216.0;11195.0;journal;article;2021
Recent literature suggests that the fields of machine learning (ML) and high-throughput experimentation (HTE) have separately received considerable attention from chemists and engineers, leading to the development of powerful reactivity models and platforms capable of rapidly performing thousands of reactions. The merger of ML with HTE presents a wealth of opportunities for the exploration of chemical space, but the integration of the two has yet to be fully realized. We highlight examples of recent developments in ML and HTE that collectively suggest the utility of their integration. Our analysis highlights the complementarity of the two fields, while exposing a number of obstacles that can and should be overcome to take full advantage of this merger and thereby accelerate chemical research.;Natalie S. Eyke and Brent A. Koscher and Klavs F. Jensen;Chemistry (miscellaneous) (Q1);83.0;1537.0;United States;2019-2020;10.1016/j.trechm.2020.12.001;0.00469;26.0;;25895974;25895974;25895974;25895974;24.081;high-throughput experimentation, machine learning, active learning;Cell Press;;6541.0;Northern America;8037.0;Q1;21100904988.0;Trends in chemistry;Toward machine learning-enhanced high-throughput experimentation;1919.0;1399.0;115.0;91.0;7522.0;journal;article;2021
"Summary
Big data is important to new developments in global clinical science that aim to improve the lives of patients. Technological advances have led to the regular use of structured electronic health-care records with the potential to address key deficits in clinical evidence that could improve patient care. The COVID-19 pandemic has shown this potential in big data and related analytics but has also revealed important limitations. Data verification, data validation, data privacy, and a mandate from the public to conduct research are important challenges to effective use of routine health-care data. The European Society of Cardiology and the BigData@Heart consortium have brought together a range of international stakeholders, including representation from patients, clinicians, scientists, regulators, journal editors, and industry members. In this Review, we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.";Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert Kazmierski and Susanne Løgstrup and R Thomas Lumbers and Thomas F Lüscher and Paul McGreavy and Ileana L Piña and Lothar Roessig and Carl Steinbeisser and Mats Sundgren and Benoît Tyl and Ghislaine van Thiel and Kees van Bochove and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee;"Decision Sciences (miscellaneous) (Q1); Health Informatics (Q1); Health Information Management (Q1); Medicine (miscellaneous) (Q1)";26.0;394.0;United Kingdom;2019-2020;10.1016/S2589-7500(22)00151-0;;14.0;;25897500;25897500;25897500;25897500;;;Elsevier Ltd.;;1670.0;Western Europe;2346.0;Q1;21100922606.0;Lancet digital health, the;Code-ehr best-practice framework for the use of structured electronic health-care records in clinical research;;319.0;138.0;81.0;2305.0;journal;article;2022
"Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.";Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis;"Cardiology and Cardiovascular Medicine (Q4); Internal Medicine (Q4)";16.0;71.0;Canada;2019-2020;10.1016/j.ijchy.2020.100027;;2.0;;25900862;25900862;25900862;25900862;;Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network;Canadian Medical Association;;2998.0;Northern America;194.0;Q4;21100922613.0;International journal of cardiology: hypertension;Uses and opportunities for machine learning in hypertension research;;12.0;43.0;17.0;1289.0;journal;article;2020
We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.;Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He;"Computer Networks and Communications (Q3); Human-Computer Interaction (Q3); Software (Q3)";47.0;212.0;United Kingdom;2019-2020;10.1016/j.cola.2020.100964;0.0001;6.0;;25901184;25901184;26659182;25901184;1.271;Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools;Elsevier Ltd.;;5247.0;Western Europe;254.0;Q3;21100904890.0;Journal of computer languages;An end-to-end model-based approach to support big data analytics development;78.0;104.0;36.0;49.0;1889.0;journal;article;2020
Southern California is prone to wildfire events that spark major evacuations of communities in the Wildland-Urban Interface. Highly developed regions such as Southern California have a number of transportation data sources to draw from that can support emergency managers’ decision-making processes. Up to date traffic sensors such as those found on the majority of California’s highways can inform emergency managers on current traffic densities, flows and speeds. Yet, in many wildfire prone regions of the United States, this is not the case. Despite this data shortfall, many regions do have robust cellular networks that inherently produce substantial amounts of location data. The location data produced by cellphone users can be used to predict vehicular densities on evacuation routes. This study examines how cellular data can be used to predict vehicular densities on evacuation routes. A mathematical model was developed to aid in the prediction of vehicular densities on evacuation networks. Correction factors were produced to adjust for the overestimation of users on roadways by cellular networks. Extrapolation factors were also developed for estimation of the number of cellular users based on a single cellphone counts data point. The Lilac Wildfire data in Dec 2017, was used to test and validate the developed model. This methodology may prove useful to transportation planners and emergency managers in planning evacuations in areas not served by a network of traffic sensors.;Benjamin Melendez and Sahar {Ghanipoor Machiani} and Atsushi Nara;"Automotive Engineering (Q2); Civil and Structural Engineering (Q3); Management Science and Operations Research (Q3); Transportation (Q3)";62.0;178.0;United Kingdom;2019-2020;10.1016/j.trip.2021.100335;;10.0;;25901982;25901982;25901982;25901982;;Wildfire, Evacuation, Traffic, Cellular data;Elsevier Ltd.;;4895.0;Western Europe;383.0;Q2;21100943534.0;Transportation research interdisciplinary perspectives;Modelling traffic during lilac wildfire evacuation using cellular data;;114.0;171.0;64.0;8370.0;journal;article;2021
"Summary
With the ever-growing need from the electric transportation industry, lithium-ion batteries are the systems of choice, offering high energy density, flexible and lightweight design, and longer lifespan than comparable battery technologies. Here, an effective management strategy, namely CHAIN, is presented for a multi-scale design and manufacturing process regarding material synthesis, characterization, electrochemical performance, and safety. The physical and electrochemical parameters of the materials are uploaded and shared in the cloud to perform real-time model calculation, achieving traceability from raw materials to products. Based on the cloud platform, a closed-loop design-and-optimization system is established, which can predict battery performance and provide an optimized management scheme by adjusting parameters simultaneously. As a multi-disciplinary system, the framework of CHAIN has profound theoretical and applicable value in full-lifespan management for battery systems, electric vehicles, and other emerging engineering systems yet to be addressed.";Shichun Yang and Rong He and Zhengjie Zhang and Yaoguang Cao and Xinlei Gao and Xinhua Liu;Materials Science (miscellaneous) (Q1);133.0;906.0;United States;2019-2020;10.1016/j.matt.2020.04.015;0.00396;22.0;;25902385;25902393;25902385;25902393;15.589;battery, cloud management, multi-scale, modeling, manufacturing;Cell Press;;4711.0;Northern America;4138.0;Q1;21100943502.0;Matter;Chain: cyber hierarchy and interactional network enabling digital solution for battery full-lifespan management;2357.0;1414.0;322.0;156.0;15168.0;journal;article;2020
Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.;Ann Hill Duin and Jason Tham;"Language and Linguistics (Q1); Linguistics and Language (Q1); Computer Science (miscellaneous) (Q2); Education (Q2)";102.0;150.0;United Kingdom;1983-2020;10.1016/j.compcom.2020.102544;;35.0;;87554615;87554615;87554615;87554615;;Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access;Elsevier Ltd.;;5244.0;Western Europe;521.0;Q1;26168.0;Computers and composition;The current state of analytics: implications for learning management system (lms) use in writing pedagogy;;159.0;43.0;112.0;2255.0;journal;article;2020
;Betty Rambur and Therese Fitzpatrick;"Medicine (miscellaneous) (Q1); Nursing (miscellaneous) (Q1)";205.0;174.0;United Kingdom;1985-2020;10.1016/j.profnurs.2017.10.005;0.00228;53.0;;87557223;15328481;87557223;15328481;2.104;;W.B. Saunders Ltd;;3011.0;Western Europe;960.0;Q1;28249.0;Journal of professional nursing;A plea to nurse educators: incorporate big data use as a foundational skill for undergraduate and graduate nurses;2171.0;508.0;137.0;229.0;4125.0;journal;article;2018
"Background
The present study aimed to investigate the association between type 2 diabetes mellitus (T2DM) and hip fractures using a large-scale nationwide population-based cohort that is representative of the Republic of Korea. We determined the risks of hip fractures in individuals with prediabetes and T2DM with different diabetes durations, and compared them with the risks of hip fractures in individuals without T2DM.
Methods
A total of 5,761,785 subjects over 50 years old who underwent the National Health Insurance Service medical checkup in 2009–2010 were included. Subjects were classified into 5 groups based on the diabetes status; Normal, Prediabetes, Newly-diagnosed T2DM, T2DM less than 5 years, and T2DM more than 5 years. They were followed from the date of the medical checkup to the end of 2016. The endpoint was a new development of hip fracture during follow-up. The hazard ratios (HRs) and 95% confidence intervals (CIs) of hip fractures for each group were analyzed using Cox proportional hazard regression models after adjusting for age, sex, smoking, alcohol drinking, regular exercise, body mass index, hypertension, dyslipidemia, and chronic kidney disease.
Results
The HRs of hip fractures were 1 in the Normal group, 1.032 (95% CI: 1.009, 1.056) in the Prediabetes group, 1.168 (95% CI: 1.113, 1.225) in the Newly-diagnosed T2DM2, 1.543 (95% CI: 1.495, 1.592) in the T2DM less than 5 years and 2.105 (95% CI: 2.054, 2.157) in the T2DM more than 5 years. The secular trend of the HRs of hip fractures according to the duration of T2DM was statistically significant (P < .001). Subgroup analyses also showed the same increasing pattern of the HRs of hip fractures according to the duration of T2DM in both sexes and all age groups (50–64 years, 65–74 years, over 75 years).
Conclusions
In summary, this large-scale, retrospective, longitudinal, nationwide population-based cohort study of 5,761,785 subjects demonstrated that the risks of hip fractures started to increase in prediabetes and was associated linearly with the duration of T2DM. The secular trend of risks of hip fractures according to the duration of T2DM was consistent in both sexes and all age groups.";Ho Youn Park and Kyoungdo Han and Youngwoo Kim and Yoon Hwan Kim and Yoo Joon Sur;"Endocrinology, Diabetes and Metabolism (Q1); Histology (Q1); Physiology (Q1)";910.0;405.0;United States;1985-2020;10.1016/j.bone.2020.115691;0.01989;200.0;;87563282;87563282;87563282;87563282;4.398;Diabetes mellitus, Prediabetic state, Diabetic complications, Hip fractures, Risk assessment, Cohort studies;Elsevier Inc.;;5715.0;Northern America;1346.0;Q1;29551.0;Bone;The risk of hip fractures in individuals over 50 years old with prediabetes and type 2 diabetes – a longitudinal nationwide population-based study;26396.0;4238.0;420.0;942.0;24005.0;journal;article;2021
