title;abstract;publication_year;authors;doi;content_type;issn;isbn
Data system for the monitoring of power quality in the transmission substations supplying big consumers;During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.;2007;Fanica Vatra;10.1109/EPQU.2007.4424094;Conferences;2150-6655;978-84-690-9441-9
Gas Emergence Big Data and neural network filter;The gas supervision is a safety core of the coal mine production, which widespread existent a trouble, gas emergence big data, namely the pulse interference, the cause of the gas signal mistake alarm, is hardly resolved very often. In this paper, the reason and characteristics of gas emergence big data are analyzed to establish a kind of filter based on BP Neural Network. Through great quantities monitor data as training sample to train the network model, and tested by test samples, we get the needed network model. Results show that the model can guarantee alarm occurrence while the gas density is beyond the limit, meanwhile preventing supervision system from mistake alarm caused by pulse interference. Applying this research can promote the robustness of existing coal mine safety supervision system without update any device, which can improve the safety production in both social meaning and economic value.;2008;Li Kun;10.1109/CHICC.2008.4605537;Conferences;2161-2927;978-7-900719-70-6
Big Data;This introduction to the special issue on big data discusses the significant scientific opportunities offered by massive amounts of data, along with some directions for future research.;2011;Francis J. Alexander;10.1109/MCSE.2011.99;Magazines;1558-366X;
Entertainment in the Age of Big Data;Everybody loves a good story. People all over the world seek out and crave entertainment. This will not change in the coming age of big data and cloud computing. But stories are more than just a distraction from the big problems of the world we are all supposed to be solving. Stories are (computationally) one of the most challenging problems facing our computers. Humans are hardcoded for story. Our computers are not. Or not yet. Through our research we examine a future of entertainment where increasing computational power allows us to be genuinely connected to a truly digital world. We argue that a genuinely digital and connected future opens the door for a new kind of storytelling, and ultimately, a new way of managing and interacting with the massive data sets collected and shared by humans.;2012;Tawny Schlieski;10.1109/JPROC.2012.2189918;Journals;1558-2256;
Big data challenges for large radio arrays;"Future large radio astronomy arrays, particularly the Square Kilometre Array (SKA), will be able to generate data at rates far higher than can be analyzed or stored affordably with current practices. This is, by definition, a ""big data"" problem, and requires an end-to-end solution if future radio arrays are to reach their full scientific potential. Similar data processing, transport, storage, and management challenges face next-generation facilities in many other fields. The Jet Propulsion Laboratory is developing technologies to address big data issues, with an emphasis in three areas: 1) Lower-power digital processing architectures to make highvolume data generation operationally affordable, 2) Date-adaptive machine learning algorithms for real-time analysis (or ""data triage"") of large data volumes, and 3) Scalable data archive systems that allow efficient data mining and remote user code to run locally where the data are stored.1";2012;Dayton L. Jones;10.1109/AERO.2012.6187090;Conferences;1095-323X;978-1-4577-0555-7
Considerations for big data: Architecture and approach;The amount of data in our industry and the world is exploding. Data is being collected and stored at unprecedented rates. The challenge is not only to store and manage the vast volume of data (“big data”), but also to analyze and extract meaningful value from it. There are several approaches to collecting, storing, processing, and analyzing big data. The main focus of the paper is on unstructured data analysis. Unstructured data refers to information that either does not have a pre-defined data model or does not fit well into relational tables. Unstructured data is the fastest growing type of data, some example could be imagery, sensors, telemetry, video, documents, log files, and email data files. There are several techniques to address this problem space of unstructured analytics. The techniques share a common characteristics of scale-out, elasticity and high availability. MapReduce, in conjunction with the Hadoop Distributed File System (HDFS) and HBase database, as part of the Apache Hadoop project is a modern approach to analyze unstructured data. Hadoop clusters are an effective means of processing massive volumes of data, and can be improved with the right architectural approach.;2012;Kapil Bakshi;10.1109/AERO.2012.6187357;Conferences;1095-323X;978-1-4577-0555-7
From Databases to Big Data;"There is a tremendous amount of buzz around the concept of ""big data."" In this article, the author discusses the origins of this trend, the relationship between big data and traditional databases and data processing platforms, and some of the new challenges that big data presents.";2012;Sam Madden;10.1109/MIC.2012.50;Magazines;1941-0131;
Big data privacy issues in public social media;Big Data is a new label given to a diverse field of data intensive informatics in which the datasets are so large that they become hard to work with effectively. The term has been mainly used in two contexts, firstly as a technological challenge when dealing with dataintensive domains such as high energy physics, astronomy or internet search, and secondly as a sociological problem when data about us is collected and mined by companies such as Facebook, Google, mobile phone companies, retail chains and governments. In this paper we look at this second issue from a new perspective, namely how can the user gain awareness of the personally relevant part Big Data that is publicly available in the social web. The amount of user-generated media uploaded to the web is expanding rapidly and it is beyond the capabilities of any human to sift through it all to see which media impacts our privacy. Based on an analysis of social media in Flickr, Locr, Facebook and Google+, we discuss privacy implications and potential of the emerging trend of geo-tagged social media. We then present a concept with which users can stay informed about which parts of the social Big Data deluge is relevant to them.;2012;Matthew Smith;10.1109/DEST.2012.6227909;Conferences;2150-4938;978-1-4673-1701-6
How Different is Big Data?;One buzzword that has been popular in the last couple of years is Big Data. In simplest terms, Big Data symbolizes the aspiration to build platforms and tools to ingest, store and analyze data that can be voluminous, diverse, and possibly fast changing. In this talk, I will try to reflect on a few of the technical problems presented by the exploration of Big Data. Some of these challenges in data analytics have been addressed by our community in the past in a more traditional relational database context but only with mixed results. I will review these quests and study some of the key lessons learned. At the same time, significant developments such as the emergence of cloud infrastructure and availability of data rich web services hold the potential for transforming our industry. I will discuss the unique opportunities they present for Big Data Analytics.;2012;Surajit Chaudhuri;10.1109/ICDE.2012.153;Conferences;1063-6382;978-1-4673-0042-1
Temporal Analytics on Big Data for Web Advertising;"""Big Data"" in map-reduce (M-R) clusters is often fundamentally temporal in nature, as are many analytics tasks over such data. For instance, display advertising uses Behavioral Targeting (BT) to select ads for users based on prior searches, page views, etc. Previous work on BT has focused on techniques that scale well for offline data using M-R. However, this approach has limitations for BT-style applications that deal with temporal data: (1) many queries are temporal and not easily expressible in M-R, and moreover, the set-oriented nature of M-R front-ends such as SCOPE is not suitable for temporal processing, (2) as commercial systems mature, they may need to also directly analyze and react to real-time data feeds since a high turnaround time can result in missed opportunities, but it is difficult for current solutions to naturally also operate over real-time streams. Our contributions are twofold. First, we propose a novel framework called TiMR (pronounced timer), that combines a time-oriented data processing system with a M-R framework. Users write and submit analysis algorithms as temporal queries - these queries are succinct, scale-out-agnostic, and easy to write. They scale well on large-scale offline data using TiMR, and can work unmodified over real-time streams. We also propose new cost-based query fragmentation and temporal partitioning schemes for improving efficiency with TiMR. Second, we show the feasibility of this approach for BT, with new temporal algorithms that exploit new targeting opportunities. Experiments using real data from a commercial ad platform show that TiMR is very efficient and incurs orders-of-magnitude lower development effort. Our BT solution is easy and succinct, and performs up to several times better than current schemes in terms of memory, learning time, and click-through-rate/coverage.";2012;Badrish Chandramouli;10.1109/ICDE.2012.55;Conferences;1063-6382;978-1-4673-0042-1
Efficient Skyline Computation on Big Data;Skyline is an important operation in many applications to return a set of interesting points from a potentially huge data space. Given a table, the operation finds all tuples that are not dominated by any other tuples. It is found that the existing algorithms cannot process skyline on big data efficiently. This paper presents a novel skyline algorithm SSPL on big data. SSPL utilizes sorted positional index lists which require low space overhead to reduce I/O cost significantly. The sorted positional index list Lj is constructed for each attribute Aj and is arranged in ascending order of Aj. SSPL consists of two phases. In phase 1, SSPL computes scan depth of the involved sorted positional index lists. During retrieving the lists in a round-robin fashion, SSPL performs pruning on any candidate positional index to discard the candidate whose corresponding tuple is not skyline result. Phase 1 ends when there is a candidate positional index seen in all of the involved lists. In phase 2, SSPL exploits the obtained candidate positional indexes to get skyline results by a selective and sequential scan on the table. The experimental results on synthetic and real data sets show that SSPL has a significant advantage over the existing skyline algorithms.;2013;Xixian Han;10.1109/TKDE.2012.203;Journals;2326-3865;
Design Principles for Effective Knowledge Discovery from Big Data;Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.;2012;Edmon Begoli;10.1109/WICSA-ECSA.212.32;Conferences;;978-0-7695-4827-2
Mastiff: A MapReduce-based System for Time-Based Big Data Analytics;Existing MapReduce-based warehousing systems are not specially optimized for time-based big data analysis applications. Such applications have two characteristics: 1) data are continuously generated and are required to be stored persistently for a long period of time, 2) applications usually process data in some time period so that typical queries use time-related predicates. Time-based big data analytics requires both high data loading speed and high query execution performance. However, existing systems including current MapReduce-based solutions do not solve this problem well because the two requirements are contradictory. We have implemented a MapReduce-based system, called Mastiff, which provides a solution to achieve both high data loading speed and high query performance. Mastiff exploits a systematic combination of a column group store structure and a lightweight helper structure. Furthermore, Mastiff uses an optimized table scan method and a column-based query execution engine to boost query performance. Based on extensive experiments results with diverse workloads, we will show that Mastiff can significantly outperform existing systems including Hive, HadoopDB, and GridSQL.;2012;Sijie Guo;10.1109/CLUSTER.2012.10;Conferences;2168-9253;978-1-4673-2422-9
Asian Information HUB Project: NICT's R&D Vsion and Strategies for Universal Communication Technology in the Big Data Era;The Universal Communications Research Institute (UCRI), NICT conducts research and development on universal communication technologies: multi-lingual machine translation, spoken dialogue, information analysis and ultra-realistic interaction technologies, through which people can truly interconnect, anytime, anywhere, about any topic, and by any method, transcending the boundaries of language, culture, ability and distance. To realizing universal communication, UCRI collects diverse information including huge volumes of web pages focusing on information from Asia. This paper introduces NICT's vision and strategies for Asian information hub as a platform for collecting, storing, analyzing large-scale information and providing advanced communication services in Big Data Era.;2012;Michiaki Iwazume;10.1109/COMPSACW.2012.11;Conferences;;978-0-7695-4758-9
A Big-Data Perspective on AI: Newton, Merton, and Analytics Intelligence;"The flood of big data in cyberspace will require immediate actions from the AI and intelligent systems community to address how we manage knowledge. Besides new methods and systems, we need a total knowledge-management approach that willl require a new perspective on AI. We need ""Merton's systems"" in which machine intelligence and human intelligence work in tandem. This should become a normal mode of operation for the next generation of AI and intelligent systems.";2012;Fei-Yue Wang;10.1109/MIS.2012.91;Magazines;1941-1294;
The larging-up of big data;The term 'Big Data' has been getting big much exposure in IT circles over the last year or two, on a scale that is bound to cause seasoned industry-watchers to sniff the air for the familiar aroma of industry hyperbole. There is the customary amount of hype, of course, but there is more to it than the covert repackaging and repurposing of existing products.;2012;Martin Courtney;10.1049/et.2012.0814;Magazines;1750-9637;
A three-dimensional display for big data sets;Facing with high dimensional information in fields of Science, Technology and Commerce, users need effective visualization tools to find more useful information. For big data sets, it is very difficult to get useful information because the dimension is too large for a practical solution. This paper proposes a 3-D visualization method for big data sets. First of all, we employed the K-means clustering method to get the basic vectors. Then, we use these vectors to construct the reduction mapping. Finally, we get the three dimensional display for a sample point. To verify the feasibility of this method, we perform experiment on some well-known databases such as iris, wine and a large data set: Pendigits. The results are favorable. According to the 3-D display results, we can also get messages like classification, outliers, and classification level when given the level standards.;2012;Cheng-Long Ma;10.1109/ICMLC.2012.6359594;Conferences;2160-133X;978-1-4673-1486-2
Bias Correction in a Small Sample from Big Data;"This paper discusses the bias problem when estimating the population size of big data such as online social networks (OSN) using uniform random sampling and simple random walk. Unlike the traditional estimation problem where the sample size is not very small relative to the data size, in big data, a small sample relative to the data size is already very large and costly to obtain. We point out that when small samples are used, there is a bias that is no longer negligible. This paper shows analytically that the relative bias can be approximated by the reciprocal of the number of collisions; thereby, a bias correction estimator is introduced. The result is further supported by both simulation studies and the real Twitter network that contains 41.7 million nodes.";2013;Jianguo Lu;10.1109/TKDE.2012.220;Journals;2326-3865;
Big Data Challenges: A Program Optimization Perspective;Big Data is characterized by the increasing volume (of the order of zeta bytes) and velocity of data generation. It is projected that the market size of Big Data shall climb up to $53.7 billion by 2017 from the current market size of $5.1 billion. Big Data in conjunction with emerging applications such as RMS applications and others has sown the seeds of exascale computing. In a similar vein, In [12], Sexton argued that applications from domains such as materials science, energy, environment and life sciences will require exascale computing. Recent studies directed towards challenges in building exascale systems and charting the roadmap of exascale computing conjecture that exascale systems would support 10-to 100-way concurrency per core and hundreds of cores per die. In [15], HPC Advisory Council predicts that the first exaflop system will be built between 2018 -- 2020. In this paper present a program optimization perspective to the challenges posed by Big Data.;2012;Arun Kejariwal;10.1109/CGC.2012.17;Conferences;;978-0-7695-4864-7
Ontology-Based Temporal Relation Modeling with MapReduce Latent Dirichlet Allocations for Big EHR Data;In this paper, we propose a model called Temporal & Co reference Topic Modeling (TCTM) to do automatic annotation with respect to the Time Event Ontology (TEO) for the big-size Electronic Health Record (EHR). TCTM, based on Latent Dirichlet Allocations (LDA) and integrated into MapReduce framework, inherently addresses the twin problem of data sparseness and high dimensionality. As a non-parametric Bayesian model, it can flexibly add new attributes or features. Side information associated with corpora, such as section header, timestamp, sentence distance, event distance or disease category in clinical notes makes latent topics more interpretable and more biased toward co referring events. Furthermore, TCTM integrates Hidden Markov Model LDA (HMM-LDA) to obtain the power of both sequential modeling and exchangeability. A MapReduce based variational method is employed to do parameter estimation and inferences, thus enabling TCTM to overcome the bottleneck brought by big data.;2012;Dingcheng Li;10.1109/CGC.2012.112;Conferences;;978-0-7695-4864-7
Beyond Simple Integration of RDBMS and MapReduce -- Paving the Way toward a Unified System for Big Data Analytics: Vision and Progress;MapReduce has shown vigorous vitality and penetrated both academia and industry in recent years. MapReduce not only can be used as an ETL tool, it can do even much more. The technique has been applied to SQL summation, OLAP, data mining, machine learning, information retrieval, multimedia data processing, science data processing etc. Basically MapReduce is a general purpose parallel computing framework for large dataset processing. A big data analytics ecosystem built around MapReduce is emerging alongside the traditional one built around RDBMS. The objectives of RDBMS and MapReduce, as well as the ecosystems built around them, overlap much really, in some sense they do the same thing and MapReduce can accomplish more works, such as graph processing, which RDBMS can not handle well. RBDMS enjoys high performance of relational data processing, which MapReduce needs to catch up. The authors envision that the two techniques are fusing into a unified system for big data analytics. With the ongoing endeavor to build up the system, much of the groundwork has been laid while some critical issues are still unresolved, we try to identify some of them. Two of our works as well as experiment results are presented, one is applying a hierarchical encoding to star schema data in Hadoop for high performance of OLAP processing, another is leveraging the natural three copies of HDFS blocks to exploit different data layouts to speed up queries in a OLAP workload, a cost model is used to route user queries to different data layouts.;2012;Xiongpai Qin;10.1109/CGC.2012.39;Conferences;;978-0-7695-4864-7
A Big Data Model Supporting Information Recommendation in Social Networks;As information systems are becoming sophisticated and mobile, cloud computing, social networking services are now very popular to people, the amount of data is rapidly increasing every year. Big data is data which should be analyzed by a company or an organization, but has not been tried to be analyzed or could not have been processed by current technology. In this paper, we introduce a big data model for recommender systems using social network data. The model incorporates factors related to social networks and can be applied to information recommendation with respect to various social behaviors that can increase the reliability of the recommended information. The big data model has the flexibility to be expanded to incorporate more sophisticated additional factors if needed. The experimental results using it in information recommendation and using map-reduce to process it show that it is a feasible model to be used for information recommendation.;2012;Xiaoyue Han;10.1109/CGC.2012.125;Conferences;;978-0-7695-4864-7
Big Data analytics;In this paper, we explain the concept, characteristics & need of Big Data & different offerings available in the market to explore unstructured large data. This paper covers Big Data adoption trends, entry & exit criteria for the vendor and product selection, best practices, customer success story, benefits of Big Data analytics, summary and conclusion. Our analysis illustrates that the Big Data analytics is a fast-growing, influential practice and a key enabler for the social business. The insights gained from the user generated online contents and collaboration with customers is critical for success in the age of social media.;2012;Sachchidanand Singh;10.1109/ICCICT.2012.6398180;Conferences;;978-1-4577-2076-5
Agile visual analytics for banking cyber “big data”;This paper describes the rapid development of a tailored cyber situational awareness and analysis application for the 2012 IEEE VAST Mini-Challenge 1 (MC1) — Cyber Situation Awareness. The novel aspect of this project was in the process of developing the tailored solution for a “big data” application. Aperture is an open, adaptable, and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer-based approach to visualization assembly, and a data mapping API that simplifies the process of transformation of data or analytic results into visual forms and properties.;2012;David Jonker;10.1109/VAST.2012.6400507;Conferences;;978-1-4673-4752-5
Big data exploration through visual analytics;SAS® Visual Analytics Explorer is an advanced data visualization and exploratory data analysis application that is a component of the SAS Visual Analytics solution. It excels at handling big data problems like the VAST challenge. With a wide range of visual analytics features and the ability to scale to massive datasets, SAS Visual Analytics Explorer enables analysts to find patt er n s and relationships quickly and easily, no matter the size of their data. In this summary paper, we explain how we used SAS Visual Analytics Explorer to solve the VAST Challenge 2012 minichallenge 1.;2012;Nascif A. Abousalh-Neto;10.1109/VAST.2012.6400514;Conferences;;978-1-4673-4752-5
Towards HPC for the digital Humanities, Arts, and Social Sciences: Needs and challenges of adapting academic HPC for big data;This paper examines the needs of emerging applications of High Performance Computing by the Humanities, Arts, and Social Sciences (HASS) disciplines and presents a vision for how the current academic HPC environment could be adapted to better serve this new class of “big data” research.;2012;Kalev H. Leetaru;10.1109/eScience.2012.6404439;Conferences;;978-1-4673-4465-4
Distributed Big Advertiser Data Mining;"Advertisers and big data mining experts alike are today are dealing with complex datasets of increasing variety (first and third party data), volume (events, impressions, clicks), and velocity (real time bidding). Creating predictive models to customize advertiser requirements and campaign analytics to show targeted ads to users who are most likely to convert has become increasingly challenging. Advertisers often group customers into a segment defined by a given set of demographic or behavioral attributes. Such segments are often very sparse. ""Look-Alike Modeling"" enables advertisers to enhance the target segment by using predictive models to expand the segment membership by assigning a probability score to users that did not explicitly belong to that segment based on the original segment definition. In this paper accompanied by the demo of a distributed platform, we describe a Look-Alike Modeling framework to expand segment membership using a novel high-dimensional distributed algorithm based on frequent pattern mining. We describe how the distributed algorithm is more efficient than traditional classification techniques that (a) require multiple passes over the dataset and (b) require both positive and negative class labels for training. Our solution is capable of concurrently and continuously processing thousands of segments and includes an efficient grouping operator and a distributed scoring algorithm for predicting multiple segment membership for a given (very large) set of users. This leverages the power of in-database analytics as compared to using standard data mining libraries and is currently deployed on a real-world highly scalable distributed columnar database that powers several hundred campaigns and processes look-alike models for large online display advertisers. The results from the study demonstrate that the proposed algorithm outperforms other comparable techniques for predicting and expanding segments.";2012;Ashish Bindra;10.1109/ICDMW.2012.73;Conferences;2375-9259;978-0-7695-4925-5
Driving big data with big compute;Big Data (as embodied by Hadoop clusters) and Big Compute (as embodied by MPI clusters) provide unique capabilities for storing and processing large volumes of data. Hadoop clusters make distributed computing readily accessible to the Java community and MPI clusters provide high parallel efficiency for compute intensive workloads. Bringing the big data and big compute communities together is an active area of research. The LLGrid team has developed and deployed a number of technologies that aim to provide the best of both worlds. LLGrid MapReduce allows the map/reduce parallel programming model to be used quickly and efficiently in any language on any compute cluster. D4M (Dynamic Distributed Dimensional Data Model) provided a high level distributed arrays interface to the Apache Accumulo database. The accessibility of these technologies is assessed by measuring the effort to use these tools and is typically a few lines of code. The performance is assessed by measuring the insert rate into the Accumulo database. Using these tools a database insert rate of 4M inserts/second has been achieved on an 8 node cluster.;2012;Chansup Byun;10.1109/HPEC.2012.6408678;Conferences;;978-1-4673-1575-3
ZIP-IO: Architecture for application-specific compression of Big Data;We have entered the “Big Data” age: scaling of networks and sensors has led to exponentially increasing amounts of data. Compression is an effective way to deal with many of these large data sets, and application-specific compression algorithms have become popular in problems with large working sets. Unfortunately, these compression algorithms are often computationally difficult and can result in application-level slow-down when implemented in software. To address this issue, we investigate ZIP-IO, a framework for FPGA-accelerated compression. Using this system we demonstrate that an unmodified industrial software workload can be accelerated 3x while simultaneously achieving more than 1000x compression in its data set.;2012;Sang Woo Jun;10.1109/FPT.2012.6412159;Conferences;;978-1-4673-2844-9
An instances placement algorithm based on disk I/O load for big data in private cloud;In generally, large companies or organizations have the demand of big data processing and they do not want to entrust their business processes and data to third parties (Amazon, Google, etc.). The private cloud could meet their needs. In private cloud, tasks run at multiple instances (also known as virtual machines), which could be paced in different physical nodes. Obviously, the instances which be used to process big data need higher CPU and disk performance than other kinds of instances. If the instances of disk resource consuming are placed in the same physical node, clearly, the disk I/O bandwidth would be used up quickly that would affect the performance of the entire node seriously. This paper proposes an instances placement algorithm FFDL that based on disk I/O for private cloud environment to deal with big data that would adopt the disk I/O load balancing strategy and reduce competition for the disk I/O load between instances. We have validated our approach by conducting a performance evaluation study on the open source private cloud platform—Openstack. The results demonstrate that our algorithm has immense potential as it offers significant computation time savings than the Greedy algorithm and demonstrates high potential for the improvement of disk I/O load balancing in the entire private cloud system for the big data.;2012;Jian Guo;10.1109/ICWAMTIP.2012.6413495;Conferences;;978-1-4673-4683-2
Finding the Needle in the Big Data Systems Haystack;"With the increasing importance of big data, many new systems have been developed to ""solve"" the big data challenge. At the same time, famous database researchers argue that there is nothing new about these systems and that they're actually a step backward. This article sheds some light on this discussion.";2013;Tim Kraska;10.1109/MIC.2013.10;Magazines;1941-0131;
Asynchronous Index Strategy for high performance real-time big data stream storage;Big data insert-intensive applications challenge traditional RDBMS. Key-Value databases achieve the same throughput with much more price/performance ratio, which makes them popular recent years. However, Key-Value databases are not suitable for high performance real-time applications. In this paper we introduce Asynchronous Index Strategy as a high performance solution for insert-intensive time series big data storage. It takes advantage of partial replication and asynchronous indexes, which results in zero overhead for index updates. Furthermore, a general middle-ware for clustering databases based on Asynchronous Index Strategy is implemented. Finally, indexing and inserting performance experiments highlight the efficiency of Asynchronous Index Strategy. As for AIS based on MongoDB, it achieves a throughput that is 17 times of MongoDB sharding cluster.;2012;Xiao Mo;10.1109/ICNIDC.2012.6418750;Conferences;2374-0272;978-1-4673-2203-4
Grand Challenge: Applying Regulatory Science and Big Data to Improve Medical Device Innovation;Understanding how proposed medical devices will interface with humans is a major challenge that impacts both the design of innovative new devices and approval and regulation of existing devices. Today, designing and manufacturing medical devices requires extensive and expensive product cycles. Bench tests and other preliminary analyses are used to understand the range of anatomical conditions, and animal and clinical trials are used to understand the impact of design decisions upon actual device success. Unfortunately, some scenarios are impossible to replicate on the bench, and competitive pressures often accelerate initiation of animal trials without sufficient understanding of parameter selections. We believe that these limitations can be overcome through advancements in data-driven and simulation-based medical device design and manufacturing, a research topic that draws upon and combines emerging work in the areas of Regulatory Science and Big Data. We propose a cross-disciplinary grand challenge to develop and holistically apply new thinking and techniques in these areas to medical devices in order to improve and accelerate medical device innovation.;2013;Arthur G. Erdman;10.1109/TBME.2013.2244600;Journals;1558-2531;
Puzzling out big data [Information Technology Analytics];"Big data comes in many forms. It comes as customer information and transactions contained in customer-relationship management and enterprise resourceplanning systems and HTML-based web stores. It comes as information generated by machine-to-machine applications collecting data from smart meters, manufacturing sensors, equipment logs, trading systems data and call detail records compiled by fixed and mobile telecommunications companies. Big data can come with big differences. Some say that the `three Vs' of big data should more properly be tagged as the `three HVs': high-volume, high-variety, high-velocity, and high-veracity. Apply those tags to the mountains of information posted on social network and blogging sites, including Facebook, Twitter and VouTube; the deluge of text contained in email and instant messages; not to mention audio and video files. It is evident then that it's not necessarily the 'big-ness' of information that presents big-data applications and services with their greatest challenge, but the variety and the speed at which all that constantly changing information must be ingested, processed, aggregated, filtered, organised and fed back in a meaningful way for businesses to get some value out of it.";2013;Martin Courtney;;Magazines;1750-9637;
Addressing Big Data challenges for Scientific Data Infrastructure;This paper discusses the challenges that are imposed by Big Data Science on the modern and future Scientific Data Infrastructure (SDI). The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model.;2012;Yuri Demchenko;10.1109/CloudCom.2012.6427494;Conferences;;978-1-4673-4509-5
Big Data Processing in Cloud Computing Environments;With the rapid growth of emerging applications like social network analysis, semantic Web analysis and bioinformatics network analysis, a variety of data to be processed continues to witness a quick increase. Effective management and analysis of large-scale data poses an interesting but critical challenge. Recently, big data has attracted a lot of attention from academia, industry as well as government. This paper introduces several big data processing technics from system and application aspects. First, from the view of cloud data management and big data processing mechanisms, we present the key issues of big data processing, including cloud computing platform, cloud architecture, cloud database and data storage scheme. Following the Map Reduce parallel processing framework, we then introduce Map Reduce optimization strategies and applications reported in the literature. Finally, we discuss the open issues and challenges, and deeply explore the research directions in the future on big data processing in cloud computing environments.;2012;Changqing Ji;10.1109/I-SPAN.2012.9;Conferences;2375-527X;978-0-7695-4930-9
Robust Decision Engineering: Collaborative Big Data and its application to international development/aid;"Much of the research that goes into Big Data, and specifically on Collaborative Big Data, is focused upon questions, such as: how to get more of it? (e.g., · participatory mechanisms, social media, geo-coded data from personal electronic devices) and · how to handle it? (e.g., how to ingest, sort, store, and link up disparate data sets). A question that receives far less attention is that of Collaborative analysis of Big Data; how can a multi-disciplinary layered analysis of Big Data be used to support robust decisions, especially in a collaborative setting, and especially under time pressure? The robust Decision Engineering required can be achieved by employing an approach related to Network Science, that we call Relationship Science. In Relationship Science, our methodological framework, karassian netchain analysis (KNA), is utilized to ascertain islands of stability or positive influence dominating sets (PIDS), so that a form of annealed resiliency or latent stability is achieved, thereby mitigating against unintended consequences, elements of instability, and “perfect storm” crises lurking within the network.";2012;Steve Chan;10.4108/icst.collaboratecom.2012.250715;Conferences;;978-1-4673-2740-4
T*: A data-centric cooling energy costs reduction approach for Big Data analytics cloud;Explosion in Big Data has led to a surge in extremely large-scale Big Data analytics platforms, resulting in burgeoning energy costs. Big Data compute model mandates strong data-locality for computational performance, and moves computations to data. State-of-the-art cooling energy management techniques rely on thermal-aware computational job placement/migration and are inherently data-placement-agnostic in nature. T* takes a novel, data-centric approach to reduce cooling energy costs and to ensure thermal-reliability of the servers. T* is cognizant of the uneven thermal-profile and differences in thermal-reliability-driven load thresholds of the servers, and the differences in the computational jobs arrival rate, size, and evolution life spans of the Big Data placed in the cluster. Based on this knowledge, and coupled with its predictive file models and insights, T* does proactive, thermal-aware file placement, which implicitly results in thermal-aware job placement in the Big Data analytics compute model. Evaluation results with one-month long real-world Big Data analytics production traces from Yahoo! show up to 42% reduction in the cooling energy costs with T* courtesy of its lower and more uniform thermal-profile and 9x better performance than the state-of-the-art data-agnostic cooling techniques.;2012;Rini T. Kaushik;10.1109/SC.2012.103;Conferences;2167-4329;978-1-4673-0804-5
Introduction to Big Data: Scalable Representation and Analytics for Data Science Minitrack;Big data is an emerging phenomenon characterized by the three Vs: volume, velocity, and variety. The volume of data has increased from terabytes to petabytes and is encroaching on exabytes. Some pundits are suggesting that zettabytes (1021) are reachable within the next several years. Velocity is concerned with not only how fast we accumulate data, but also how fast some of the data that we already have is changing. Some systems accumulate data at the rate of multiple petabytes per year, some systems have stored data that changes at the rate of terabytes per year. Changing data usually lags accumulating data by several orders of magnitude. Data accumulating at a multiple petabyte rate requires terabits to petabits of transport capacity. Finally, the variety and modality of data is continually evolving, it may be both structured and unstructured.;2013;Stephen Kaisler;10.1109/HICSS.2013.292;Conferences;1530-1605;978-0-7695-4892-0
An Agent Model for Incremental Rough Set-Based Rule Induction: A Big Data Analysis in Sales Promotion;Rough set-based rule induction is able to generate decision rules from a database and has mechanisms to handle noise and uncertainty in data. This technique facilitates managerial decision-making and strategy formulation. However, the process for RS-based rule induction is complex and computationally intensive. Moreover, operational databases that are used to run the day-to-day operations, thus large volumes of data are continually updated within a short period of time. The infrastructure required to analyze such large amounts of data must be able to handle extreme data volumes, to allow fast response times, and to automate decisions based on analytical models. This study proposes an Incremental Rough Set-based Rule Induction Agent (IRSRIA). Rule induction is based on creating agents for the main modeling processes. In addition, an incremental architecture is designed, to address large-scale dynamic database problems. A case study of a Home shopping company is used to show the validity and efficiency of this method. The results of experiments show that the IRSRIA can considerably reduce the computation time for inducing decision rules, while maintaining the same quality of rules.;2013;Yu-Neng Fan;10.1109/HICSS.2013.79;Conferences;1530-1605;978-0-7695-4892-0
Big Data: Issues and Challenges Moving Forward;"Big data refers to data volumes in the range of exabytes (1018) and beyond. Such volumes exceed the capacity of current on-line storage systems and processing systems. Data, information, and knowledge are being created and collected at a rate that is rapidly approaching the exabyte/year range. But, its creation and aggregation are accelerating and will approach the zettabyte/year range within a few years. Volume is only one aspect of big data; other attributes are variety, velocity, value, and complexity. Storage and data transport are technology issues, which seem to be solvable in the near-term, but represent longterm challenges that require research and new paradigms. We analyze the issues and challenges as we begin a collaborative research program into methodologies for big data analysis and design.";2013;Stephen Kaisler;10.1109/HICSS.2013.645;Conferences;1530-1605;978-0-7695-4892-0
Introduction to Predictive Analytics and Big Data Minitrack;Introduction to Predictive Analytics and Big Data Minitrack.;2013;Dursun Delen;10.1109/HICSS.2013.322;Conferences;1530-1605;978-0-7695-4892-0
Introduction to Business Analytics, Business Intelligence, and Big Data Minitrack;Introduction to the Business Analytics, Business Intelligence and Big Data Minitrack;2013;Robert Winter;10.1109/HICSS.2013.334;Conferences;1530-1605;978-0-7695-4892-0
Danger Theory: A new approach in big data analysis;Danger Theory is a novel computing model inspired by biological immune systems which is some different with the traditional Artificial Immune Systems, especially the self non-self model. The Danger Theory concerns the potential dangers (which presents like the danger signals) rather than non-self pathogens, so the new computing model introduced into information analysis can take a new approach for big data processing on key features and properties choosing. The authors introduce some works on Danger Theory about the definition of danger, and the capture of danger signals, which could be helpful to increase the abilities of self-learning and intelligence in different applications.;2012;Lin Lu;10.1049/cp.2012.1083;Conferences;;978-1-84919-537-9
Addressing big data problem using Hadoop and Map Reduce;The size of the databases used in today's enterprises has been growing at exponential rates day by day. Simultaneously, the need to process and analyze the large volumes of data for business decision making has also increased. In several business and scientific applications, there is a need to process terabytes of data in efficient manner on daily bases. This has contributed to the big data problem faced by the industry due to the inability of conventional database systems and software tools to manage or process the big data sets within tolerable time limits. Processing of data can include various operations depending on usage like culling, tagging, highlighting, indexing, searching, faceting, etc operations. It is not possible for single or few machines to store or process this huge amount of data in a finite time period. This paper reports the experimental work on big data problem and its optimal solution using Hadoop cluster, Hadoop Distributed File System (HDFS) for storage and using parallel processing to process large data sets using Map Reduce programming framework. We have done prototype implementation of Hadoop cluster, HDFS storage and Map Reduce framework for processing large data sets by considering prototype of big data application scenarios. The results obtained from various experiments indicate favorable results of above approach to address big data problem.;2012;Aditya B. Patel;10.1109/NUICONE.2012.6493198;Conferences;2375-1282;978-1-4673-1718-4
Ethics of Big Data;This column discusses the book Ethics of Big Data by Kord Davis with Doug Patterson.;2013;Richard Mateosian;10.1109/MM.2013.35;Magazines;1937-4143;
Ubiquitous Analytics: Interacting with Big Data Anywhere, Anytime;Ubilytics amplifies human cognition by embedding analytical processes into the physical environment to make sense of big data anywhere, anytime.;2013;Niklas Elmqvist;10.1109/MC.2013.147;Magazines;1558-0814;
Journey to the centre of big data;In a general context big data is an aggregation of data sets that are so large and complex that it becomes difficult to process using readily available database management tools or traditional data processing applications. This challenge also contains an opportunity for commercial organisations that are equipped to find ways to use it - or elements of it - to inform and enhance revenue drivers (see 'Puzzling out big data', E&T Vol 7 Issue 12). The term 'big data' is partly a recognition of the growing relative weight and importance of unstructured data not amenable to conventional database analytics and reporting tools and techniques. Above all, though, it embodies an ambition to extract value from data, particularly for sales, marketing, and customer relations.;2013;Philip Hunter;10.1049/et.2013.0307;Magazines;1750-9637;
Abstract: Networking Research Activities at Fermilab for Big Data Analysis;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center. It must deal with both scaling and wide-area distribution challenges in processing its CMS data. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;P. DeMar;10.1109/SC.Companion.2012.214;Conferences;;978-1-4673-6218-4
Poster: Big Data Networking at Fermilab;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R&D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;Phillip J. Demar;10.1109/SC.Companion.2012.215;Conferences;;978-1-4673-6218-4
Abstract: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.229;Conferences;;978-1-4673-6218-4
Poster: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.230;Conferences;;978-1-4673-6218-4
Abstract: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;A. Klimentov;10.1109/SC.Companion.2012.301;Conferences;;978-1-4673-6218-4
Poster: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;K. De;10.1109/SC.Companion.2012.302;Conferences;;978-1-4673-6218-4
Feasibility considerations of multipath TCP in dealing with big data application;In this paper we present an idea to handle big data in a better way. We consider Multipath TCP (MPTCP) to use all available paths simultaneously. MPTCP is a tailored form of TCP which aspire to improve throughput by sharing available resources smartly and fairly. The key concentration of this paper is to analyze the benefit of MPTCP over single path TCP for bandwidth and time sensitive applications as well as big data application. Our simulation shows that MPTCP can higher goodput by bandwidth aggregation, Couple Congestion Control(CCC) provide better throughput without being unfair to other legacy TCP flows and also portray that large receive buffer causes performance enhancement for relatively large RTT link. As a result, MPTCP can be a enormous addition in contrast with single path TCP in dealing with big data application.;2013;Zia Ush Shamszaman;10.1109/ICOIN.2013.6496714;Conferences;2332-5658;978-1-4673-5741-8
Customizing Computational Methods for Visual Analytics with Big Data;The volume of available data has been growing exponentially, increasing data problem's complexity and obscurity. In response, visual analytics (VA) has gained attention, yet its solutions haven't scaled well for big data. Computational methods can improve VA's scalability by giving users compact, meaningful information about the input data. However, the significant computation time these methods require hinders real-time interactive visualization of big data. By addressing crucial discrepancies between these methods and VA regarding precision and convergence, researchers have proposed ways to customize them for VA. These approaches, which include low-precision computation and iteration-level interactive visualization, ensure real-time interactive VA for big data.;2013;Jaegul Choo;10.1109/MCG.2013.39;Magazines;1558-1756;
Shared disk big data analytics with Apache Hadoop;Big Data is a term applied to data sets whose size is beyond the ability of traditional software technologies to capture, store, manage and process within a tolerable elapsed time. The popular assumption around Big Data analytics is that it requires internet scale scalability: over hundreds of compute nodes with attached storage. In this paper., we debate on the need of a massively scalable distributed computing platform for Big Data analytics in traditional businesses. For organizations which don't need a horizontal., internet order scalability in their analytics platform., Big Data analytics can be built on top of a traditional POSIX Cluster File Systems employing a shared storage model. In this study., we compared a widely used clustered file system: VERITAS Cluster File System (SF-CFS) with Hadoop Distributed File System (HDFS) using popular Map-reduce benchmarks like Terasort., DFS-IO and Gridmix on top of Apache Hadoop. In our experiments VxCFS could not only match the performance of HDFS., but also outperformed in many cases. This way., enterprises can fulfill their Big Data analytics need with a traditional and existing shared storage model without migrating to a different storage model in their data centers. This also includes other benefits like stability & robustness., a rich set of features and compatibility with traditional analytics applications.;2012;Anirban Mukherjee;10.1109/HiPC.2012.6507520;Conferences;;978-1-4673-2370-3
Big Data Conferences, Here We Come!;Conferences on big data are starting to appear this year. What can we expect from a big Data conference? As researchers, should we plan to attend, and if yes, what should we submit?;2013;Peter Mika;10.1109/MIC.2013.45;Magazines;1941-0131;
Transforming Big Data into Collective Awareness;"Integrating social and sensor networks can transform big data, if treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems. The Web extra at http://youtu.be/xSoAvMNZSL8 is a video in which author Jeremy Pitt expands on his article ""Transforming Big Data into Collective Awareness"" and discusses how integrating social and sensor networks can transform big data, if it's treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems.";2013;Jeremy Pitt;10.1109/MC.2013.153;Magazines;1558-0814;
A big data implementation based on Grid computing;Big Data is a term defining data that has three main characteristics. First, it involves a great volume of data. Second, the data cannot be structured into regular database tables and third, the data is produced with great velocity and must be captured and processed rapidly. Oracle adds a fourth characteristic for this kind of data and that is low value density, meaning that sometimes there is a very big volume of data to process before finding valuable needed information. Big Data is a relatively new term that came from the need of big companies like Yahoo, Google, Facebook to analyze big amounts of unstructured data, but this need could be identified in a number of other big enterprises as well in the research and development field. The framework for processing Big Data consists of a number of software tools that will be presented in the paper, and briefly listed here. There is Hadoop, an open source platform that consists of the Hadoop kernel, Hadoop Distributed File System (HDFS), MapReduce and several related instruments. Two of the main problems that occur when studying Big Data are the storage capacity and the processing power. That is the area where using Grid Technologies can provide help. Grid Computing refers to a special kind of distributed computing. A Grid computing system must contain a Computing Element (CE), and a number of Storage Elements (SE) and Worker Nodes (WN). The CE provides the connection with other GRID networks and uses a Workload Management System to dispatch jobs on the Worker Nodes. The Storage Element is in charge with the storage of the input and the output of the data needed for the job execution. The main purpose of this article is to present a way of processing Big Data using Grid Technologies. For that, the framework for managing Big Data will be presented along with the way to implement it around a grid architecture.;2013;Dan Garlasu;10.1109/RoEduNet.2013.6511732;Conferences;2068-1038;978-1-4673-6114-9
Big Data in Neonatal Intensive Care;"The effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions. The Web extra at http://youtu.be/OIQBCboQs0g is a video in which author Carolyn McGregor expands on her article ""Big Data in Neonatal Intensive Care"" and discusses how the effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions.";2013;Carolyn McGregor;10.1109/MC.2013.157;Magazines;1558-0814;
Governing Big Data: Principles and practices;As data-intensive decision making is being increasingly adopted by businesses, governments, and other agencies around the world, most organizations encountering a very large amount and variety of data are still contemplating and assessing their readiness to embrace “Big Data.” While these organizations devise various ways to deal with the challenges such data brings, the impact and importance of Big Data to information quality and governance programs should not be underestimated. Drawing upon implementation experiences of early adopters of Big Data technologies across multiple industries, this paper explores the issues and challenges involved in the management of Big Data, highlighting the principles and best practices for effective Big Data governance.;2013;P. Malik;10.1147/JRD.2013.2241359;Journals;0018-8646;
IBM Streams Processing Language: Analyzing Big Data in motion;The IBM Streams Processing Language (SPL) is the programming language for IBM InfoSphere® Streams, a platform for analyzing Big Data in motion. By “Big Data in motion,” we mean continuous data streams at high data-transfer rates. InfoSphere Streams processes such data with both high throughput and short response times. To meet these performance demands, it deploys each application on a cluster of commodity servers. SPL abstracts away the complexity of the distributed system, instead exposing a simple graph-of-operators view to the user. SPL has several innovations relative to prior streaming languages. For performance and code reuse, SPL provides a code-generation interface to C++ and Java®. To facilitate writing well-structured and concise applications, SPL provides higher-order composite operators that modularize stream sub-graphs. Finally, to enable static checking while exposing optimization opportunities, SPL provides a strong type system and user-defined operator models. This paper provides a language overview, describes the implementation including optimizations such as fusion, and explains the rationale behind the language design.;2013;M. Hirzel;10.1147/JRD.2013.2243535;Journals;0018-8646;
Big Data text-oriented benchmark creation for Hadoop;Massive-scale Big Data analytics is representative of a new class of workloads that justifies a rethinking of how computing systems should be optimized. This paper addresses the need for a set of benchmarks that system designers can use to measure the quality of their designs and that customers can use to evaluate competing systems offerings with respect to commonly performed text-oriented workflows in Hadoop™. Additions are needed to existing benchmarks such as HiBench in terms of both scale and relevance. We describe a methodology for creating a petascale data-size text-oriented benchmark that includes representative Big Data workflows and can be used to test total system performance, with demands balanced across storage, network, and computation. Creating such a benchmark requires meeting unique challenges associated with the data size and its often unstructured nature. To be useful, the benchmark also needs to be sufficiently generic to be accepted by the community at large. Here, we focus on a text-oriented Hadoop workflow that consists of three common tasks: categorizing text documents, identifying significant documents within each category, and analyzing significant documents for new topic creation.;2013;A. Gattiker;10.1147/JRD.2013.2240732;Journals;0018-8646;
GPFS-SNC: An enterprise cluster file system for Big Data;A new class of data-intensive applications commonly referred to as Big Data applications (e.g., customer sentiment analysis based on click-stream logs) involves processing massive amounts of data with a focus on semantically transforming the data. This class of applications is massively parallel and well suited for the MapReduce programming framework that allows users to perform large-scale data analyses such that the application execution layer handles the system architecture, data partitioning, and task scheduling. In this paper, we introduce GPFS-SNC (General Parallel File System for Shared Nothing Clusters), a scalable file system that operates over a cluster of commodity machines and direct-attached storage and meets the requirements of analytics and traditional applications that are typically used together in analytics solutions. The architecture extends an existing enterprise cluster file system to support these emerging classes of workloads by applying five innovative optimizations: 1) locality awareness to allow compute jobs to be scheduled on nodes where the data resides, 2) metablocks that allow large and small block sizes to co-exist in the same file system to meet the needs of different types of applications, 3) write affinity that allows applications to dictate the layout of files on different nodes in order to maximize both write and read bandwidth, 4) pipelined replication to maximize use of network bandwidth for data replication, and 5) distributed recovery to minimize the effect of failures on ongoing computation.;2013;R. Jain;10.1147/JRD.2013.2243531;Journals;0018-8646;
Understanding system design for Big Data workloads;This paper explores the design and optimization implications for systems targeted at Big Data workloads. We confirm that these workloads differ from workloads typically run on more traditional transactional and data-warehousing systems in fundamental ways, and, therefore, a system optimized for Big Data can be expected to differ from these other systems. Rather than only studying the performance of representative computational kernels, and focusing on central-processing-unit performance, this paper studies the system as a whole. We identify three major phases in a typical Big Data workload, and we propose that each of these phases should be represented in a Big Data systems benchmark. We implemented our ideas on two distinct IBM POWER7® processor-based systems that target different market sectors, and we analyze their performance on a sort benchmark. In particular, this paper includes an evaluation of POWER7 processor-based systems using MapReduce TeraSort, which is a workload that can be a “stress test” for multiple dimensions of system performance. We combine this work with a broader perspective on Big Data workloads and suggest a direction for a future benchmark definition effort. A number of methods to further improve system performance are proposed.;2013;H. P. Hofstee;10.1147/JRD.2013.2242674;Journals;0018-8646;
Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost;"Finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The first Web extra at http://youtu.be/B2RlkoNjrzA is a video in which author Paul Tallon expands on his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and discusses how finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The second Web extra at http://youtu.be/g0RFa4swaf4 is a video in which author Paul Tallon discusses the supplementary material to his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and how projection models can help individuals responsible for data handling plan for and understand big data storage issues.";2013;Paul P. Tallon;10.1109/MC.2013.155;Magazines;1558-0814;
Bootstrapping smart cities through a self-sustainable model based on big data flows;"We have a clear idea today about the necessity and usefulness of making cities smarter, the potential market size, and trials and tests. However, it seems that business around Smart Cities is having difficulties taking off and is thus running short of projected potentials. This article looks into why this is the case and proposes a procedure to make smart cities happen based on big data exploitation through the API stores concept. To this end, we first review involved stakeholders and the ecosystem at large. We then propose a viable approach to scale business within that ecosystem. We also describe the available ICT technologies and finally exemplify all findings by means of a sustainable smart city application. Over the course of the article, we draw two major observations, which are seen to facilitate sustainable smart city development. First, independent smart city departments (or the equivalent) need to emerge, much like today's well accepted IT departments, which clearly decouple the political element of the improved city servicing from the underlying technologies. Second, a coherent three-phase smart city rollout is vital, where in phase 1 utility and revenues are generated; in phase 2 only-utility service is also supported; and in phase 3, in addition, a fun/leisure dimension is permitted.";2013;Ignasi Vilajosana;10.1109/MCOM.2013.6525605;Magazines;1558-1896;
Big Data's Big Unintended Consequences;"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article ""Big Data's Big Unintended Consequences"" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.";2013;Marcus R. Wigan;10.1109/MC.2013.195;Magazines;1558-0814;
Big Data: New Opportunities and New Challenges [Guest editors' introduction];"We can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldn't blind ourselves to the possible irreversibility of changes-whether good or bad-to society. The first Web extra at http://youtu.be/24czULRCI9c is an audio recording in which Katina Michael at the University of Wollongong discusses the June 2013 Computer magazine special issue on ""Big Data: New Opportunities and New Challenges,"" introducing the special issue, the guest editors, the authors, the articles, and the IEEE Society on Social Implications of Technology (SSIT). The second Web extra at http://youtu.be/9zpFqEDydDA is an audio recording in which Katina Michael at the University of Wollongong talks about the IEEE Society on the Social Implications of Technology (SSIT), IEEE Technology and Society (T&S) magazine, and the International Symposium on Technology and Society (ISTAS). The third Web extra at http://youtu.be/mn_9YHV2RGQis an audio recording in which Katina Michael at the University of Wollongong discusses how we can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldnít blind ourselves to the possible irreversibility of changes-whether good or bad-to society.";2013;Katina Michael;10.1109/MC.2013.196;Magazines;1558-0814;
Efficient and secure Cloud storage for handling big data;The term “Cloud” has been used historically as a metaphor for the internet. It is one of the most active application for enterprise. It has been more and more accepted by enterprises which can take advantage of low cost, fast deployment and elastic scaling. Due to demand of large volume of data processing in enterprises, huge amount of data are generated and dispersed on internet around the globe. Currently, storing the data safely and efficiently on Cloud is one of the biggest challenge in Cloud computing. There is no guarantee that data stored on Cloud is securely protected. We propose a method to build a trusted computing environment by providing a secure platform in a Cloud computing system. The proposed method allows users to store data safely and efficiently in the Cloud. It solves the problem of handling big data and security issues using encryption and compression technique while uploading data to the Cloud storage.;2012;Arjun Kumar;;Conferences;;978-1-4673-0876-2
Scalable single linkage hierarchical clustering for big data;"Personal computing technologies are everywhere; hence, there are an abundance of staggeringly large data sets-the Library of Congress has stored over 160 terabytes of web data and it is estimated that Facebook alone logs nearly a petabyte of data per day. Thus, there is a pertinent need for systems by which one can elucidate the similarity and dissimilarity among and between groups in these big data sets. Clustering is one way to find these groups. In this paper, we extend the scalable Visual Assessment of Tendency (sVAT) algorithm to return single-linkage partitions of big data sets. The sVAT algorithm is designed to provide visual evidence of the number of clusters in unloadable (big) data sets. The extension we describe for sVAT enables it to also then efficiently return the data partition as indicated by the visual evidence. The computational complexity and storage requirements of sVAT are (usually) significantly less than the O(n2) requirement of the classic single-linkage hierarchical algorithm. We show that sVAT is a scalable instantiation of single-linkage clustering for data sets that contain c compact-separated clusters, where c ≪ n; n is the number of objects. For data sets that do not contain compact-separated clusters, we show that sVAT produces a good approximation of single-linkage partitions. Experimental results are presented for both synthetic and real data sets.";2013;Timothy C. Havens;10.1109/ISSNIP.2013.6529823;Conferences;;978-1-4673-5500-1
Data stream mining to address big data problems;Today, the IT world is trying to cope with “big data” problems (data volume, velocity, variety, veracity) on the path to obtaining useful information. In this paper, we present implementation details and performance results of realizing “online” Association Rule Mining (ARM) over big data streams for the first time in the literature. Specifically, we added Apriori and FP-Growth algorithms for stream mining inside an event processing engine, called Esper. Using the system, these two algorithms were compared over LastFM social music site data and by using tumbling windows. The better-performing FP-Growth was selected and used in creation of a real-time rule-based recommendation engine. Our most important findings show that online association rule mining can generate (1) more rules, (2) much faster and more efficiently, and (3) much sooner than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as “George Harrison⇒Beatles”. We hope that our findings can shed light on the design and implementation of other big data analytics systems in the future.;2013;Erdi Ölmezoğulları;10.1109/SIU.2013.6531483;Conferences;;978-1-4673-5561-2
Panel: Big data for the public;"Summary form only given. While data are now being produced and collected on unprecedented scales, most of the ""big data"" remain inaccessible or difficult to use by the public.";2013;;10.1109/ICDE.2013.6544803;Conferences;1063-6382;978-1-4673-4908-6
Machine learning on Big Data;Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.;2013;Tyson Condie;10.1109/ICDE.2013.6544913;Conferences;1063-6382;978-1-4673-4908-6
Big data integration;The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.;2013;Xin Luna Dong;10.1109/ICDE.2013.6544914;Conferences;1063-6382;978-1-4673-4908-6
Workload management for Big Data analytics;Parallel database systems and MapReduce systems are essential components of today's infrastructure for Big Data analytics. These systems process multiple concurrent workloads consisting of complex user requests, where each request is associated with an (explicit or implicit) service level objective.;2013;Ashraf Aboulnaga;10.1109/ICDE.2013.6544915;Conferences;1063-6382;978-1-4673-4908-6
Very fast estimation for result and accuracy of big data analytics: The EARL system;Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets (a.k.a. `big data') can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in big data systems (e.g., Hadoop). Therefore, we propose a nonparametric accuracy estimation method and system to speedup big data analytics. Our framework is called EARL (Early Accurate Result Library) and it works by predicting the learning curve and choosing the appropriate sample size for achieving the desired error bound specified by the user. The error estimates are based on a technique called bootstrapping that has been widely used and validated by statisticians, and can be applied to arbitrary functions and data distributions. Therefore, this demo will elucidate (a) the functionality of EARL and its intuitive GUI interface whereby first-time users can appreciate the accuracy obtainable from increasing sample sizes by simply viewing the learning curve displayed by EARL, (b) the usability of EARL, whereby conference participants can interact with the system to quickly estimate the sample sizes needed to obtain the desired accuracies or response times, and then compare them against the accuracies and response times obtained in the actual computations.;2013;Nikolay Laptev;10.1109/ICDE.2013.6544928;Conferences;1063-6382;978-1-4673-4908-6
Towards an Optimized Big Data Processing System;Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.;2013;Bogdan Ghit;10.1109/CCGrid.2013.53;Conferences;;978-1-4673-6465-2
A Holistic Architecture for the Internet of Things, Sensing Services and Big Data;Wireless Sensor Networks (WSNs) increasingly enable the interaction of the physical world with services, which may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). This potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which provides a set of abstractions for the different types of sensors and services, enabling them to take advantage of Big Data and cloud technologies and which is termed holistic as it caters for the data flow from sensors through to services. The architecture has been designed for implementation on a resource constrained node and to be extensible to server environments, shown in this paper where we present a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase data store.;2013;David Tracey;10.1109/CCGrid.2013.100;Conferences;;978-1-4673-6465-2
Getting an Intuition for Big Data;"IEEE Software Editor-in-Chief Forrest Shull discusses the importance of building reliable systems to interpret big data. In addition, he discusses the IBM Impact 2013 Unconference; the Software Engineering Institute's SATURN 2013 conference in which the IEEE Software Architecture in Practice Award went to Simon Brown of Coding the Architecture, for his presentation titled ""The Conflict between Agile and Architecture: Myth or Reality"" and the IEEE Software New Directions Award went to Darryl Nelson of Raytheon for his presentation titled, ""Next-Gen Web Architecture for the Cloud Era."" He also welcomes Professor Rafael Prikladnicki of the Computer Science School at PUCRS, Brazil, and Chief Software Economist Walker Royce of IBM's Software Group to the IEEE Software Advisory Board. The first Web extra at http://youtu.be/JrQorWS5m6w is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Paul Zikopoulos, Director--IBM Information Management Technical Professionals, Competitive Database, and Big Data at IBM, about the potentials of mining big data. Zikopoulos will deliver a keynote at Software Experts Summit 2013 on 17 July in Redmond, Washington. The second Web extra at http://youtu.be/NHHThAeONv8 is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Catherine Plaisant and Megan Monroe of the University of Maryland Human-Computer Interaction Laboratory about big data information visualization and its applications to software development. The third Web extra at http://youtu.be/NqXE0ewoTKA is a video overview of the IBM Impact 2013 Unconference, sponsored by IEEE Software magazine, an event specifically designed for developers that featured Grady Booch and Tim O'Reilly as keynote speakers.";2013;Forrest Shull;10.1109/MS.2013.76;Magazines;1937-4194;
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.84;Magazines;1937-4194;
Data mining with big data;Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.;2014;Xindong Wu;10.1109/TKDE.2013.109;Journals;2326-3865;
Big Data Analytics: Perspective Shifting from Transactions to Ecosystems;Understanding the flow and interrelated nature of institutions and business entities' processes and exchanges helps researchers develop and apply big data analytics techniques more effectively from an ecosystem-based perspective (rather than individual transactions and components).;2013;Daniel Zeng;10.1109/MIS.2013.40;Magazines;1941-1294;
Artificial Intelligence and Big Data;AI Innovation in Industry is a new department for IEEE Intelligent Systems, and this paper examines some of the basic concerns and uses of AI for big data (AI has been used in several different ways to facilitate capturing and structuring big data, and it has been used to analyze big data for key insights).;2013;;10.1109/MIS.2013.39;Magazines;1941-1294;
Big data analysis of irregular operations: Aborted approaches and their underlying factors;Procedures such as Missed Approaches and Holding Patterns are designed into Air Traffic Control procedures to provide a safe manner for flights to temporarily exit the airspace or the traffic flow when irregular operations occur. These procedures serve as “pressure release valves” and in this way are symptoms of the occurrence of infrequent phenomena that impact efficiency and safety margins. The occurrence of these procedures is not currently tracked by airlines or Air Navigation Service Providers (ANSP) due to the inability to identify these situations using the existing time-stamped event data (i.e. OOOI data) that is the basis for NAS performance analysis today. This paper describes a Big Data analysis of surveillance track data to establish the frequency of occurrence of Aborted Approaches, and an analysis of voluntary pilot/air traffic controller reports to establish factors leading to Aborted Approaches. Aborted Approaches include a Go Around for a Missed Approach as well as a turn off the final approach segment prior to the Missed Approach Point (MAP). Analysis of 21 days of surveillance track data for approaches at ORD identified a 7.4 in 1000 frequency of approaches resulting in an Aborted Approach. Daily Aborted Approaches ranged from 0 per day to 21 per 1000 approaches per day. Eighty percent of the Aborted Approaches involved a turn off the final approach segment prior to the MAP. An analysis of 467 voluntary pilot/air traffic controller reports from all U.S. airports identified factors leading to aborted approaches: (1) 48% airplane issues (e.g. onboard failure, unstable approach), (2) 27% traffic separation issues, (3) 16% weather (e.g. ceiling, visibility, crosswind), (4) 5% runway issues, and (5) 4% flightcrew-ATC interaction issues. These results suggest mitigation strategies to reduce the high variance in daily occurrences through procedure modification, training and equipment design.;2013;Lance Sherry;10.1109/ICNSurv.2013.6548548;Conferences;2155-4943;978-1-4673-6252-8
Cross-platform aviation analytics using big-data methods;This paper identifies key aviation data sets for operational analytics, presents a methodology for application of big-data analysis methods to operational problems, and offers examples of analytical solutions using an integrated aviation data warehouse. Big-data analysis methods have revolutionized how both government and commercial researchers can analyze massive aviation databases that were previously too cumbersome, inconsistent or irregular to drive high-quality output. Traditional data-mining methods are effective on uniform data sets such as flight tracking data or weather. Integrating heterogeneous data sets introduces complexity in data standardization, normalization, and scalability. The variability of underlying data warehouse can be leveraged using virtualized cloud infrastructure for scalability to identify trends and create actionable information. The applications for big-data analysis in airspace system performance and safety optimization have high potential because of the availability and diversity of airspace related data. Analytical applications to quantitatively review airspace performance, operational efficiency and aviation safety require a broad data set. Individual information sets such as radar tracking data or weather reports provide slices of relevant data, but do not provide the required context, perspective and detail on their own to create actionable knowledge. These data sets are published by diverse sources and do not have the standardization, uniformity or defect controls required for simple integration and analysis. At a minimum, aviation big-data research requires the fusion of airline, aircraft, flight, radar, crew, and weather data in a uniform taxonomy, organized so that queries can be automated by flight, by fleet, or across the airspace system.;2013;Tulinda Larsen;10.1109/ICNSurv.2013.6548579;Conferences;2155-4943;978-1-4673-6252-8
Predictive analytics with aviation big data;"Current archive is 50 billion records and growing - Approximately 34 million elements per day - \textasciitilde 1GB/day; Sheer volume of raw surveillance data makes analytics process very difficult; The raw data runs through a series of processes before it can be used for analytics; Next Steps - Continue application of predictive and prescriptive analytics - Big data visualization.";2013;Paul Comitz;10.1109/ICNSurv.2013.6548645;Conferences;2155-4943;978-1-4673-6252-8
On interference-aware provisioning for cloud-based big data processing;Recent advances in cloud-based big data analysis offers a convenient mean for providing an elastic and cost-efficient exploration of voluminous data sets. Following such a trend, industry leaders as Amazon, Google and IBM deploy various of big data systems on their cloud platforms, aiming to occupy the huge market around the globe. While these cloud systems greatly facilitate the implementation of big data analysis, their real-world applicability remains largely unclear. In this paper, we take the first steps towards a better understanding of the big data system on the cloud platforms. Using the typical MapReduce framework as a case study, we find that its pipeline-based design intergrades the computational-intensive operations (such as mapping/reducing) together with the I/O-intensive operations (such as shuffling). Such computational-intensive and I/O-intensive operations will seriously affect the performance of each other and largely reduces the system efficiency especially on the low-end virtual machines (VMs). To make the matter worse, our measurement also indicates that more than 90 % of the task-lifetime is in the shadow of such interference. This unavoidably reduces the applicability of cloud-based big data processing and makes the overall performance hard to predict. To address this problem, we re-model the resource provisioning problem in the cloud-based big data systems and present an interference-aware solution that smartly allocates the MapReduce jobs to different VMs. Our evaluation result shows that our new model can accurately predict the job completion time across different configurations and significantly improve the user experience for this new generation of data processing service.;2013;Yi Yuan;10.1109/IWQoS.2013.6550282;Conferences;1548-615X;978-1-4799-0588-1
Case of small-data analysis for ion implanters in the era of big-data FDC;This paper presents a case study of constructing process models based on physical mechanisms of semiconductor manufacturing tools in attempts to predict behaviours of process conditions. Actual measurements from the processing tools are always corrupted with noises and crunching huge volumes of temporal traces of status variables very often fail to pinpoint the accurate fault conditions, not to mention any of their efficient classifications, should abnormal conditions really exist. The current fashion of moving into massive big data computing is yet to distill concrete correlations among tool conditions and impacts on process results of semiconductor devices. As an alternative before the foolproof maturity of big data cracking, and in contrast to the conventional black-box approach of statistical regressions, we take a fundamental view in constructing physical model of the ion implantation process for a flywheel implanter, first to calculate the motion trajectories and subsequently, the implantation dosage on the wafer. We summarize the underlying solution techniques in principles and leave the specific details of parameter calibrations to individual field practitioners.;2013;Keung Hui;10.1109/ASMC.2013.6552752;Conferences;2376-6697;978-1-4673-5006-8
Heading towards big data building a better data warehouse for more data, more speed, and more users;As a new company, GLOBALFOUNDRIES is aggressively agile and looking at ways to not just mimic existing semiconductor manufacturing data management but to leverage new technologies and advances in data management without sacrificing performance or scalability. Being a global technology company that relies on the understanding of data, it is important to centralize the visibility and control of this information, bringing it to the engineers and customers as they need it. Currently, the factories are employing the best practices and data architectures combined with business intelligence analysis and reporting tools. However, the expected growth in data over the next several years and the need to deliver more complex data integration for analysis will easily stress the traditional tools beyond the limits of the traditional data infrastructure. The manufacturing systems vendors need to offer new solutions based on Big Data concepts to reach the new level of information processing that work well with other vendor offerings. In this paper, we will show where we are and where we are heading to manage the increasing needs for handling larger amounts of data with faster as well as secure access for more users.;2013;Raymond Gardiner Goss;10.1109/ASMC.2013.6552808;Conferences;2376-6697;978-1-4673-5006-8
A comparative study of enterprise and open source big data analytical tools;In this paper, we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same. The Transaction Processing Council (TPC) has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment, amount of data that can be processed, decision making capabilities, ease of use, energy and time consumed, and the pricing.;2013;Udaigiri Chandrasekhar;10.1109/CICT.2013.6558123;Conferences;;978-1-4673-5757-9
Leveraging Big Data Analytics to Reduce Healthcare Costs;The healthcare sector deals with large volumes of electronic data related to patient services. This article describes two novel applications that leverage big data to detect fraud, abuse, waste, and errors in health insurance claims, thus reducing recurrent losses and facilitating enhanced patient care. The results indicate that claim anomalies detected using these applications help private health insurance funds recover hidden cost overruns that aren't detectable using transaction processing systems. This article is part of a special issue on leveraging big data and business analytics.;2013;Uma Srinivasan;10.1109/MITP.2013.55;Magazines;1941-045X;
Big Data and Transformational Government;The big data phenomenon is growing throughout private and public sector domains. Profit motives make it urgent for companies in the private sector to learn how to leverage big data. However, in the public sector, government services could also be greatly improved through the use of big data. Here, the authors describe some drivers, barriers, and best practices affecting the use of big data and associated analytics in the government domain. They present a model that illustrates how big data can result in transformational government through increased efficiency and effectiveness in the delivery of services. Their empirical basis for this model uses a case vignette from the US Department of Veterans Affairs, while the theoretical basis is a balanced view of big data that takes into account the continuous growth and use of such data. This article is part of a special issue on big data and business analytics.;2013;Rhoda C. Joseph;10.1109/MITP.2013.61;Magazines;1941-045X;
Business Process Analytics Using a Big Data Approach;"Continuous improvement of business processes is a challenging task that requires complex and robust supporting systems. Using advanced analytics methods and emerging technologies--such as business intelligence systems, business activity monitoring, predictive analytics, behavioral pattern recognition, and ""type simulations""--can help business users continuously improve their processes. However, the high volumes of event data produced by the execution of processes during the business lifetime prevent business users from efficiently accessing timely analytics data. This article presents a technological solution using a big data approach to provide business analysts with visibility on distributed process and business performance. The proposed architecture lets users analyze business performance in highly distributed environments with a short time response. This article is part of a special issue on leveraging big data and business analytics.";2013;Alejandro Vera-Baquero;10.1109/MITP.2013.60;Magazines;1941-045X;
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MCG.2013.62;Magazines;1558-1756;
Moving big data to the cloud;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geodispersed data into the cloud, for processing using a MapReducelike framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1/l+λ κ/λ with a lookahead window of l, where κ and λ are system parameters of similar magnitude.;2013;Linquan Zhang;10.1109/INFCOM.2013.6566804;Conferences;0743-166X;978-1-4673-5945-0
Comparative performance analysis of a Big Data NORA problem on a variety of architectures;Non Obvious Relationship Analysis (NORA) is one of the most stressing classes of Big Data Analytics problems. This paper proposes a reference NORA problem that is representative of real problems, and can rationally scale to very large sizes. It then develops a highly concurrent implementation that can run on large systems. Each step of this implementation is sized in terms of how much of four different resources (CPU, memory, disk, and network) might be used. From this, a parameterized model projecting both execution time and utilizations is used to identify the “tall poles” in performance. The parameters are then modified to represent several different target systems, from a large cluster typical of today to variations in an advanced architecture where processing has been moved into memory. A “thought experiment” then uses this model to discover the parameters of a system that would provide both a near 100X speedup, but with a balanced design where no resource is badly over or under utilized.;2013;Peter M. Kogge;10.1109/CTS.2013.6567199;Conferences;;978-1-4673-6402-7
Big data, deep data, and the effect of system architectures on performance;"Summary form only given. “Big Data” traditionally refers to some combination of high volume of data, high velocity of change, and/or wide variety and complexity of the underlying data. Solving such problems has evolved into using paradigms like MapReduce on large clusters of compute nodes. More recently, a growing number of “Deep Data” problems have arisen where it is the relationships between objects, and not necessarily the collections of objects, that are important, and for which the traditional implementation techniques are unsatisfactory. This talk addresses a study of a class of such “challenge problems” first formulated by David Bayliss of LexisNexis, and what are their execution characteristics on both current and future architectures. The goal is to discover, to at least a first order approximation, what are the tall poles preventing a speedup of their solution. A variety or architectures are considered, ranging from standard server blades in large scale configurations, to emerging variations that leverage simpler and more energy efficient chip sets, through systems built on 3D chip stacks, and on to new architectures that were designed from the ground up to “follow the links.” Such architectures are considered for two variants of such problems: a traditional partitioned data approach where data is “pre-boiled” to provide fast response, and one that uses very large graphs in very large shared memories. The results are not necessarily intuitive; the bottlenecks in such problems are not where current systems have the bulk of their capabilities or costs, nor where obvious near term upgrades will have major effects. Instead, it appears that only highly scalable memory-intensive architectures offer the potential for truly major gains in application performance.";2013;Peter M. Kogge;10.1109/CTS.2013.6567201;Conferences;;978-1-4673-6402-7
Big data: A review;Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.;2013;Seref Sagiroglu;10.1109/CTS.2013.6567202;Conferences;;978-1-4673-6402-7
Addressing big data issues in Scientific Data Infrastructure;Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.;2013;Yuri Demchenko;10.1109/CTS.2013.6567203;Conferences;;978-1-4673-6402-7
A disk based stream oriented approach for storing big data;This paper proposes an extension to the generally accepted definition of Big Data and from this extended definition proposes a specialized database design for storing high throughput data from low-latency sources. It discusses the challenges a financial company faces with regards to processing and storing data and how existing database technologies are unsuitable for this niche task. A prototype database called CakeDB is built using a stream oriented, disk based storage design and insert throughput tests are conducted to demonstrate how effectively such a design would handle high throughput data as per the use case.;2013;Peter Membrey;10.1109/CTS.2013.6567204;Conferences;;978-1-4673-6402-7
An application-aware approach to systems support for big data [Keynote address];Summary form only given. Everyday 2.5 quintillion (2.5×1018, or 2.5 million trillion) bytes of data are created by people. This data comes from everywhere: from traditional scientific computing and on-line transactions, to popular social network and mobile applications. Data produced in the last two years alone amounts to 90% of the data in the world today! This phenomenal growth and ubiquity of data has ushered in an era of “Big Data”, which brings with it new challenges as well as opportunities. In this talk, I will first discuss big data challenges facing computer and storage systems research, brought on by the huge volume, high velocity, great variety and veracity with which digital data are being produced in the world. I will first introduce some new and ongoing programs at NSF that are relevant to Big Data and to ASAP. I will then present research being conducted in my research group that seeks a scalable systems and application-aware approach to addressing some of the challenges, from the many core and storage architectures to the systems and up to the applications.;2013;Hong Jiang;10.1109/ASAP.2013.6567537;Conferences;1063-6862;978-1-4799-0494-5
From green computing to big-data learning: A kernel learning perspective [Keynote address];Summary form only given. The SVM learning model has been successfully applied to an enormously broad spectrum of application domains and has become a main stream of the modern machine learning technologies. Unfortunately, along with its success and popularity, there also raises a grave concern on it suitability for big data learning applications. For example, in some biomedical applications, the sizes may be hundreds of thousands. In social media application, the sizes could be easily in the order of millions. This curse of dimensionality represents a new challenge calling for new learning paradigm as well as application-specific parallel and distributed hardware and software. This talk will explore cost-effective design on kernel-based machine learning and classification for big data learning applications. It will present a recursive tensor based classification algorithm, especially amenable to systolic/wavefront array processors, which may potentially expedite realtime prediction speed by orders of magnitude. For time-series analysis, with nonstationary environment, it is vital to develop time-adaptive learning algorithms so as to allow incremental and active learning. The talk will tackle the active learning problems from two kernel-induced perspectives, one in intrinsic space and another in empirical space. The talk will show, if time permits, an algorithmic example highlighting the application of Map-Reduce technologies to supervised kernel (Slackmin) learning under a parallel and distributed processing framework.;2013;Sun-Yuan Kung;10.1109/ASAP.2013.6567539;Conferences;1063-6862;978-1-4799-0494-5
Big Data in 10 Years;Summary form only given, as follows. The complete panel presentation was not made available for publication as part of the conference proceedings. There is a lot of excitement about “Big Data” which is at the intersection of the ongoing explosion in data (volumes, variety, and velocity at which it arrives and must be acted upon), the dramatic increase in cost-effective memory capacities, and the maturation of scale-out processing technologies. Huge investments are being made, and there are great expectations for the gains to be had and the range of applications that will be transformed by new data-driven approaches. What does the future hold? Will the changes indeed be transformative, and if so, what will some of the main changes be? What domains are likely to benefit the most? Or is this just a case of unrealistic expectations waiting to be debunked by reality? We will ask panelists drawn from diverse backgrounds to offer their opinions and, hopefully, to get into violent arguments!;2013;Raghu Ramakrishnan;10.1109/IPDPS.2013.124;Conferences;1530-2075;978-0-7695-4971-2
Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that address the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more. This Special Issue therefore aims to publish high quality, original, unpublished research papers in the broad area of Cloud Computing and Big Data, and thus presents a platform for scientists and scholars to share their observations and research results in the field.;2013;;10.1109/TST.2013.6574681;Journals;1007-0214;
Authorized Public Auditing of Dynamic Big Data Storage on Cloud with Efficient Verifiable Fine-Grained Updates;"Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the verification is done by a trusted third party, this verification process is also called data auditing, and this third party is called an auditor. However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof of integrity of certain file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads. In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of frequent small updates, such as applications in social media and business transactions.";2014;Chang Liu;10.1109/TPDS.2013.191;Journals;2161-9883;
Modeling of system of systems via data analytics — Case for “Big Data” in SoS;Large data has been accumulating in all aspects of our lives for quite some time. Advances in sensor technology, the Internet, wireless communication, and inexpensive memory have all contributed to an explosion of “Big Data”. System of Systems (SoS) integrate independently operating, non-homogeneous systems to achieve a higher goal than the sum of the parts. Today's SoS are also contributing to the existence of unmanageable “Big Data”. Recent efforts have developed a promising approach, called “Data Analytics”, which uses statistical and computational intelligence (CI) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro-computing, evolutionary computation, Bayesian networks, etc. to reduce the size of “Big Data” to a manageable size and apply these tools to a) extract information, b) build a knowledge base using the derived data, and c) eventually develop a non-parametric model for the “Big Data”. This paper attempts to construct a bridge between SoS and Data Analytics to develop reliable models for such systems. A photovoltaic energy forecasting problem of a micro grid SoS will be offered here for a case study of this modeling relation.;2013;Barnabas K. Tannahill;10.1109/SYSoSE.2013.6575263;Conferences;;978-1-4673-5595-7
Scaling challenges of packaging in the Era of Big Data;The exascale computing is required in the Era of Big Data. In order to achieve this demand, new technology innovation must be required and packaging scaling including 3D-IC with TSV (Through Silicon Vias) is one of most promising technology. To increase the total bandwidth, the fine pitch die to die interconnection is necessary. Micro-bumping, thermally enhanced underfill and advanced interposer technologies are one of the key technologies. Material selection for reliable fine-pitch interconnection has become a critical challenge in 3D chip stacking. Underfill material between die to die is also very important to reduce the total packaging stress and to enhance the vertical thermal conductivity. Low CTE high density organic substrate is emerging technology for 2.5D structure.;2013;Yasumitsu Orii;;Conferences;0743-1562;978-1-4673-5226-0
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.245;Magazines;1558-0814;
Detecting unfolding crises with Visual Analytics and Conceptual Maps Emerging phenomena and big data;Detecting the emergence of a political crisis is a key goal of security informatics. Big data provides us with valuable information on the many socio-economic indicators of crisis dynamics, ranging from unemployment to the trustworthiness of political institutions. However, it is currently challenging to link information on these factors in order for analysts to assess the possible directions of a conflict. At present, while some solutions offer theoretical frameworks for understanding those indicators in the abstract, these frameworks cannot easily be operationalised to the level needed for automatic processing of big data streams. Alternative solutions do automatically code political events, but only offer a high level picture that cannot support the analysis of deeper conflict processes. In this paper, we combine Visual Analytics with Concept Maps to support analysts in monitoring conflicts. Visual Analytics allows the interactive visual exploration of data, while Concept Maps keep this exploration focused by linking data patterns (e.g., occurrence and frequency of keywords) to underlying dynamics (e.g., coordination of activism, salience of violence). We illustrate the potential of our approach through a discussion of how it could be used to study the on-going Syrian crisis. While this approach still requires validation with analysts, we fully specify the technical structure of our approach and exemplify its use to detect shifts in political stability.;2013;Simon F. Pratt;10.1109/ISI.2013.6578819;Conferences;;978-1-4673-6212-2
Big Data Security Hardening Methodology Using Attributes Relationship;"Recently developments in network, mining and data store technology have heightened the need for big data and big data security. In this paper, we focus on the big data's characteristic which takes seriously the analysis of value than the data itself. We express the relationship between attributes using nodes and edges. Through this, we propose a big data security hardening methodology by selecting ""protect attributes"" from attributes relationship graph.";2013;Sung-Hwan Kim;10.1109/ICISA.2013.6579427;Conferences;2162-9048;978-1-4799-0602-4
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.287;Magazines;1558-0814;
Storage Challenge: Where Will All That Big Data Go?;Big data creates numerous exciting possibilities for organizations, but first they must figure out where they're going to store all that information.;2013;Neal Leavitt;10.1109/MC.2013.326;Magazines;1558-0814;
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.324;Magazines;1558-0814;
High productivity processing - Engaging in big data around distributed computing;The steadily increasing amounts of scientific data and the analysis of `big data' is a fundamental characteristic in the context of computational simulations that are based on numerical methods or known physical laws. This represents both an opportunity and challenge on different levels for traditional distributed computing approaches, architectures, and infrastructures. On the lowest level data-intensive computing is a challenge since CPU speed has surpassed IO capabilities of HPC resources and on the higher levels complex cross-disciplinary data sharing is envisioned via data infrastructures in order to engage in the fragmented answers to societal challenges. This paper highlights how these levels share the demand for `high productivity processing' of `big data' including the sharing and analysis of `large-scale science data-sets'. The paper will describe approaches such as the high-level European data infrastructure EUDAT as well as low-level requirements arising from HPC simulations used in distributed computing. The paper aims to address the fact that big data analysis methods such as computational steering and visualization, map-reduce, R, and others are around, but a lot of research and evaluations still need to be done to achieve scientific insights with them in the context of traditional distributed computing infrastructures.;2013;Morris Riedel;;Conferences;;978-953-233-076-2
Social-Network-Sourced Big Data Analytics;Very large datasets, also known as big data, originate from many domains. Deriving knowledge is more difficult than ever when we must do it by intricately processing this big data. Leveraging the social network paradigm could enable a level of collaboration to help solve big data processing challenges. Here, the authors explore using personal ad hoc clouds comprising individuals in social networks to address such challenges.;2013;Wei Tan;10.1109/MIC.2013.100;Magazines;1941-0131;
A Discussion of Privacy Challenges in User Profiling with Big Data Techniques: The EEXCESS Use Case;User profiling is the process of collecting information about a user in order to construct their profile. The information in a user profile may include various attributes of a user such as geographical location, academic and professional background, membership in groups, interests, preferences, opinions, etc. Big data techniques enable collecting accurate and rich information for user profiles, in particular due to their ability to process unstructured as well as structured information in high volumes from multiple sources. Accurate and rich user profiles are important for applications such as recommender systems, which try to predict elements that a user has not yet considered but may find useful. The information contained in user profiles is personal and thus there are privacy issues related to user profiling. In this position paper, we discuss user profiling with big data techniques and the associated privacy challenges. We also discuss the ongoing EU-funded EEXCESS project as a concrete example of constructing user profiles with big data techniques and the approaches being considered for preserving user privacy.;2013;Omar Hasan;10.1109/BigData.Congress.2013.13;Conferences;2379-7703;978-0-7695-5006-0
Engineering Privacy for Big Data Apps with the Unified Modeling Language;This paper describes proposed privacy extensions to UML to help software engineers to quickly visualize privacy requirements, and design privacy into big data applications. To adhere to legal requirements and/or best practices, big data applications will need to apply Privacy by Design principles and use privacy services, such as, and not limited to, anonymization, pseudonymization, security, notice on usage, and consent for usage. We extend UML with ribbon icons representing needed big data privacy services. We further illustrate how privacy services can be usefully embedded in use case diagrams using containers. These extensions to UML help software engineers to visually and quickly model privacy requirements in the analysis phase, this phase is the longest in any software development effort. As proof of concept, a prototype based on our privacy extensions to Microsoft Visio's UML is created and the utility of our UML privacy extensions to the Use Case Diagram artifact is illustrated employing an IBM Watson-like commercial use case on big data in a health sector application.;2013;Dawn N. Jutla;10.1109/BigData.Congress.2013.15;Conferences;2379-7703;978-0-7695-5006-0
Consistent Process Mining over Big Data Triple Stores;'Big Data' techniques are often adopted in cross-organization scenarios for integrating multiple data sources to extract statistics or other latent information. Even if these techniques do not require the support of a schema for processing data, a common conceptual model is typically defined to address name resolution. This implies that each local source is tasked of applying a semantic lifting procedure for expressing the local data in term of the common model. Semantic heterogeneity is then potentially introduced in data. In this paper we illustrate a methodology designed to the implementation of consistent process mining algorithms in a `Big Data' context. In particular, we exploit two different procedures. The first one is aimed at computing the mismatch among the data sources to be integrated. The second uses mismatch values to extend data to be processed with a traditional map reduce algorithm.;2013;Antonia Azzini;10.1109/BigData.Congress.2013.17;Conferences;2379-7703;978-0-7695-5006-0
Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud;Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.;2013;Farhana Zulkernine;10.1109/BigData.Congress.2013.18;Conferences;2379-7703;978-0-7695-5006-0
Towards a Quality-centric Big Data Architecture for Federated Sensor Services;As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.;2013;Lakshmish Ramaswamy;10.1109/BigData.Congress.2013.21;Conferences;2379-7703;978-0-7695-5006-0
Approximate Incremental Big-Data Harmonization;The needs of `big data analytics' increasingly require IT organizations to ingest, process, and extract business insights from ever larger volumes of data that arrive far more rapidly than before, as well as from new sources such as social media, mobile devices, and sensors. However, in order to extract insights from diverse information feeds from multiple, often unrelated sources, these first need to be correlated or harmonized to a common level of granularity. We formally define this commonly arising data harmonization problem. We show how to correlate disparate data sources using map-reduce, but in an approximate and/or incremental manner as often required in practice. We motivate our techniques through a real-life enterprise data-harmonization case study for which we describe our performance results on big-data technologies, namely, Map Reduce, Hadoop and PIG.;2013;Puneet Agarwal;10.1109/BigData.Congress.2013.24;Conferences;2379-7703;978-0-7695-5006-0
Countering the Concept-Drift Problem in Big Data Using iOVFDT;How to efficiently uncover the knowledge hidden within massive and big data remains an open problem. One of the challenges is the issue of 'concept drift' in streaming data flows. Concept drift is a well-known problem in data analytics, in which the statistical properties of the attributes and their target classes shift over time, making the trained model less accurate. Many methods have been proposed for data mining in batch mode. Stream mining represents a new generation of data mining techniques, in which the model is updated in one pass whenever new data arrive. This one-pass mechanism is inherently adaptive and hence potentially more robust than its predecessors in handling concept drift in data streams. In this paper, we evaluate the performance of a family of decision-tree-based data stream mining algorithms. The advantage of incremental decision tree learning is the set of rules that can be extracted from the induced model. The extracted rules, in the form of predicate logics, can be used subsequently in many decision-support applications. However, the induced decision tree must be both accurate and compact, even in the presence of concept drift. We compare the performance of three typical incremental decision tree algorithms (VFDT [2], ADWIN [3], iOVFDT [4]) in dealing with concept-drift data. Both synthetic and real-world drift data are used in the experiment. iOVFDT is found to produce superior results.;2013;Hang Yang;10.1109/BigData.Congress.2013.25;Conferences;2379-7703;978-0-7695-5006-0
Challenges of Privacy Protection in Big Data Analytics;The big data paradigm implies that almost every type of information eventually can be derived from sufficiently large datasets. However, in such terms, linkage of personal data of individuals poses a severe threat to privacy and civil rights. In this position paper, we propose a set of challenges that have to be addressed in order to perform big data analytics in a privacy-compliant way.;2013;Meiko Jensen;10.1109/BigData.Congress.2013.39;Conferences;2379-7703;978-0-7695-5006-0
Techniques for Graph Analytics on Big Data;Graphs enjoy profound importance because of their versatility and expressivity. They can be effectively used to represent social networks, web search engines and genome sequencing. The field of graph pattern matching has been of significant importance and has wide-spread applications. Conceptually, we want to find subgraphs that match a pattern in a given graph. Much work has been done in this field with solutions like Subgraph Isomorphism and Regular Expression matching. With Big Data, scientists are frequently running into massive graphs that have amplified the challenge that this area poses. We study the speedup and communication behavior of three distributed algorithms for inexact graph pattern matching. We also study the impact of different graph partitionings on runtime and network I/O. Our extensive results show that the algorithms exhibit excellent scalable behavior and min-cut partitioning can lead to improved performance under some circumstances, and can drastically reduce the network traffic as well.;2013;M. Usman Nisar;10.1109/BigData.Congress.2013.78;Conferences;2379-7703;978-0-7695-5006-0
Service-Generated Big Data and Big Data-as-a-Service: An Overview;With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.;2013;Zibin Zheng;10.1109/BigData.Congress.2013.60;Conferences;2379-7703;978-0-7695-5006-0
Big Data Infrastructure for Active Situation Awareness on Social Network Services;Awareness computing aims at our final goal in computer science to simulate human's awareness and cognition. Awareness of social network knowledge in everyday life is actively enabled by big data society. In this paper, we investigate infrastructure for big data analytics for social network services, and propose TF-IDF calculation on big data infrastructure to be aware of social relations on social networks.;2013;Incheon Paik;10.1109/BigData.Congress.2013.61;Conferences;2379-7703;978-0-7695-5006-0
Storage Mining: Where IT Management Meets Big Data Analytics;The emerging paradigm shift to cloud based data center infrastructures imposes remarkable challenges to IT management operations, e.g., due to virtualization techniques and more stringent requirements for cost and efficiency. On one hand, the voluminous data generated by daily IT operations such as logs and performance measurements contain abundant information and insights which can be leveraged to assist the IT management. On the other hand, traditional IT management solutions cannot consume and exploit the rich information contained in the data due to the daunting volume, velocity, variety, as well as the lack of scalable data mining and machine learning frameworks to extract insights from such raw data. In this paper, we present our on-going research thrust of designing novel IT management solutions by leveraging big data analytics frameworks. As an example, we introduce our project of Storage Mining, which exploits big data analytics techniques to facilitate storage cloud management. The challenges are discussed and our proof-of-concept big data analytics framework is presented.;2013;Yang Song;10.1109/BigData.Congress.2013.66;Conferences;2379-7703;978-0-7695-5006-0
Distributed Stochastic Aware Random Forests -- Efficient Data Mining for Big Data;Some top data mining algorithms, as ensemble classifiers, may be inefficient to very large data set. This paper makes an initial proposal of a distributed ensemble classifier algorithm based on the popular Random Forests for Big Data. The proposed algorithm aims to improve the efficiency of the algorithm by a distributed processing model called MapReduce. At the same time, our proposed algorithm aims to reduce the randomness impact by following an algorithm called Stochastic Aware Random Forests - SARF.;2013;Joaquim Assunção;10.1109/BigData.Congress.2013.68;Conferences;2379-7703;978-0-7695-5006-0
The Knowledge Service Project in the Era of Big Data;The integration of industrialization and IT application is going to be one of the Chinese new economy development strategies in the future. Therefore, how to make Big Data useful in generating significant productivity improvement in industries has already become one of the most important issues. This paper outlines the platform of knowledge service based on big data processing techniques, which have guided the implementation of the integration of industrialization and IT application in Shenyang. And some challenges we met during implementation of the project were also discussed.;2013;Dongfeng Cai;10.1109/BigData.Congress.2013.70;Conferences;2379-7703;978-0-7695-5006-0
A Novel Use of Big Data Analytics for Service Innovation Harvesting;Service innovation has assumed considerable significance with the growth of the services sectors of economies globally, yet progress has been slow in devising carefully formulated, systematic techniques to under pin service innovation. This paper argues that a novel approach to big data analytics offers interesting solutions in this space. The paper argues that the use of big data analytics for generating enterprise service insights is often ignored (while the extraction of insights about customers, the market and the enterprise context has received considerable attention). The paper offers a set of techniques (collectively referred to as innovation harvesting) which leverage big data in various forms, including object state sensor data, behaviour logs as well large-scale sources of open data such as the web to mine service innovation insights. The paper also outlines how systematic search might help overcome the limitations of big data analytics in this space.;2013;Aditya K. Ghose;10.1109/ICSSI.2013.45;Conferences;;978-0-7695-4985-9
Innovation as the strategic driver of sustainability: big data knowledge for profit and survival;Innovation has long been a central strategic focus of firms, and sustainability has recently become such a focus. We posit that innovation-across the value chain, in strategy, and in business models-is the central element of any truly sustainable business. Linking the theoretical models of Market Orientation (MO) and the Resource Based View of the Firm (RBV), purposive search directed through a Knowledge Based View (KBV) offers a schematic outline for how and where applications of big data analytics can facilitate innovation for long-term sustainability of the firm-for survival, profit, and dynamic fit with the changing environment.;2013;Mariann Jelinek;10.1109/EMR.2013.2259978;Journals;1937-4178;
Efficient global portfolios: Big data and investment universes;In this analysis of the risk and return of stocks in the United States and global markets, we apply several portfolio construction and optimization techniques to U.S. and global stock universes. We find that (1) mean-variance techniques continue to produce portfolios capable of generating excess returns above transaction costs and statistically significant asset selection, (2) optimization techniques minimizing expected tail loss are statistically significant in portfolio construction, and (3) global markets offer the potential for greater returns relative to risk than domestic markets. In this experiment, mean-variance, enhanced-index-tracking techniques, and mean-expected tail-loss methodologies are examined. Global equity data and the vast quantity (and quality) of the data relative to U.S. equity modeling have been discussed in the literature. We estimate expected return models in the U.S. and global equity markets using a given stock-selection model and generate statistically significant active returns from various portfolio construction techniques.;2013;J. B. Guerard;10.1147/JRD.2013.2272483;Journals;0018-8646;
Storm System Database: A Big Data Approach to Moving Object Databases;Rainfall data is often collected by measuring the amount of precipitation collected in a physical container at a site. Such methods provide precise data for those sites, but are limited in granularity to the number and placement of collection devices. We use radar images of storm systems that are publicly available and provide rainfall estimates for large regions of the globe, but at the cost of loss of precision. We present a moving object database called Storm DB that stores decibel measurements of rain clouds as moving regions, i.e., we store a single rain cloud as a region that changes shape and position over time. Storm DB is a prototype system that answers rain amount queries over a user defined time duration for any point in the continental United States. In other words, a user can ask the database for the amount of rainfall that fell at any point in the US over a specified time window. Although this single query seems straightforward, it is complicated due to the expected size of the dataset: storm clouds are numerous, radar images are available in high resolution, and our system will collect data over a large timeframe, thus, we expect the number and size of moving regions representing storm clouds to be large. To implement our proposed query, we bring together the following concepts: (i) image processing to retrieve storm clouds from radar images, (ii) interpolation mechanisms to construct moving regions with infinite temporal resolution from region snapshots, (iii) transformations to compute exact point in moving polygon queries using 2-dimensional rather than 3-dimensional algorithms, (iv) GPU algorithms for massively parallel computation of the duration that a point lies inside a moving polygon, and (v) map/reduce algorithms to provide scalability. The resulting prototype lays the groundwork for building big data solutions for moving object databases.;2013;Brian Olsen;10.1109/COMGEO.2013.30;Conferences;;978-0-7695-5012-1
Efficient Online Sharing of Geospatial Big Data Using NoSQL XML Databases;Summary form only given: Today a huge amount of geospatial data is being created, collected and used more than ever before. The ever increasing observations and measurements of geo-sensor networks, satellite imageries, point clouds from laser scanning, geospatial data of Location Based Services (LBS) and location-based social networks has become a serious challenge for data management and analysis systems. Traditionally, Relational Database Management Systems (RDBMS) were used to manage and to some extent analyze the geospatial data. Nowadays these systems can be used in many scenarios but there are some situations when using these systems may not provide the required efficiency and effectiveness. More specifically when the geospatial data has high volume, high frequency of change (in both data content and data structure) and variety of structures, the conventional data storage systems cannot provide needed efficiency in online systems in terms of performance and scalability. In these situations, NoSQL solutions can provide the efficiency necessary for applications using geospatial data. This paper provides an overview of the characteristics of geospatial big data, possible solutions for managing and processing them. Then the paper provides an overview of the major types of NoSQL solutions, their advantages and disadvantages and the challenges they present in managing geospatial big data. Then the paper elaborates on serving geospatial data using standard geospatial web services with a NoSQL XML database as a backend.;2013;Pouria Amirian;10.1109/COMGEO.2013.34;Conferences;;978-0-7695-5012-1
Big Data: Unleashing information;Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.;2013;James M. Tien;10.1109/ICSSSM.2013.6602615;Conferences;2161-1904;978-1-4673-4434-0
Store, schedule and switch - A new data delivery model in the big data era;The big data era is posing unprecedented challenges on the existing network infrastructure. In today's networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service level requirements. The so called elephant data, which may be less sensitive to transfer delay, compete precious network resources with mice data, in most cases from interactive and delay sensitive applications. Consequently, the Quality of Service (QoS) of interactive applications is hard to provision, and the utility of network is low. We propose a new data transfer model to complement the existing per-packet forwarding paradigm. In the new data transfer model, a service level requirement is assigned (by the data source) to each big data transfer request. Instead of transferring these data on per-packet bases immediately upon entering the network, the network stores the data until it find necessary, or enough network resource is available for that transfer. The scheduled data delivery is realized through the use of dynamic circuit switching. We also present some preliminary simulation results of SSS networks.;2013;Weiqiang Sun;10.1109/ICTON.2013.6602860;Conferences;2162-7339;978-1-4799-0683-3
Study on Big Data Center Traffic Management Based on the Separation of Large-Scale Data Stream;The network of traditional data center has been usually designed and constructed for the provision of user's equal access of data centre's resource or data. Therefore, network administrators have a strong tendency to manage user traffic from the viewpoint that the traffic has a similar size and characteristics. But, the emersion of big data begins to make data centers have to deal with 1015 byte-data transfer at once. Such a big data transfer can cause problems in network traffic management in the existed data center. And, the tiered network architecture of the legacy data center magnifies the magnitude of the problems. One of the well-known big data in science is from large hadron collider such as LHC in Swiss CERN. CERN LHC generates multi-peta byte data per year. From our experience of CERN data service, this paper showed the impact of network traffic affected by large-scale data stream using NS2 simulation, and then, suggested the evolution direction based on separating of large-scale data stream for the big data center's network architecture.;2013;Hyoung Woo Park;10.1109/IMIS.2013.104;Conferences;;978-0-7695-4974-3
Split File Model for Big Data in Low Throughput Storage;The demand for low-cost, large-scale storage is increasing. Recently, several low-throughput storage services such as the Pogo plug Cloud have been developed. These services are based on Amazon Glacier. They have low throughput, but low cost and large capacity. Therefore, these services are suitable for backups or archiving big data and can be used instead of offline storage tiers. To utilize such low throughput storage efficiently, we need tools for effective deduplication and resumable transfers, amongst others. We propose a split file model that can represent big data efficiently in low throughput storage. In the split file model, a large file is divided into many small parts, which are stored in a directory. We have developed tool commands to support the use of split files in a transparent way. Using these commands, replicated data is naturally excluded and effective shallow copying is supported. In this paper, we describe the split file model in detail and evaluate an implementation thereof.;2013;Minoru Uehara;10.1109/CISIS.2013.48;Conferences;;978-0-7695-4992-7
Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds;Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.;2013;Weiyi Shang;10.1109/ICSE.2013.6606586;Conferences;1558-1225;978-1-4673-3073-2
How deep data becomes big data;We present some problems and solutions for situations when compound and semantically rich nature of data records, such as scientific articles, creates challenges typical for big data processing. Using a case study of named entity matching in SONCA system we show how big data problems emerge and how they are solved by bringing together methods from database management and computational intelligence.;2013;Marcin Szczuka;10.1109/IFSA-NAFIPS.2013.6608465;Conferences;;978-1-4799-0348-1
Big-data integration methodologies for effective management and data mining of petroleum digital ecosystems;"Petroleum industries' big data characterize heterogeneity and they are often multidimensional in nature. In the recent past, explorers narrate petroleum system, as an ecosystem, in which elements and processes are constantly interacted and communicated each other. Exploration is one of the key super-type data dimensions of petroleum ecosystem, (including seismic dimension), exhibiting high degree of heterogeneity, sequence identity and structural similarity; this is especially the case for, elements and processes that are unique to petroleum systems of South East Asia. Existing approaches of petroleum data organizations have limitations in capturing and integrating petroleum systems data. An alternative method uses ontologies and does not rely on keywords or similarity metrics. The conceptual framework of petroleum ontology (PO) is to promote reuse of concepts and a set of algebraic operators for querying petroleum ontology instances. This ontology-based fine-grained multidimensional data structuring adapts to warehouse metadata modeling. The data integration process facilitates to metadata models, which are deduced for Indonesian sedimentary basins, and is useful for data mining and subsequent data interpretation including geological knowledge mapping.";2013;Shastri L Nimmagadda;10.1109/DEST.2013.6611345;Conferences;2150-4946;978-1-4799-0784-7
Data migration ecosystem for big data invited paper;Data Migration is the process of moving data from a system or systems to a new environment. Often, it is a sub-activity of a business application deployment. Big data is defined as data that is huge, has heterogeneous data dictionaries and involves complex manipulation. Due to nature of the process complexity and its resources hungry approach in migrating Big Data, special attention is required to have a proven methodology and ecosystem to govern the process. The Data Migration Ecosystem for Big Data is the productive set of interacting processes, practices and environments, to collect data from one location, storage medium, or hardware/software system, to cleanse, transform and transfer it to another. The processes and practices are governed by rules and disciplines, with the goal of ensuring information is complete, of high accuracy and consistent. This paper is based on our experience in migrating data for a Malaysia government agency, which involves approximately 1 billion rows of data from 31 heterogeneous sources / systems. Some of the data migrated was created in the seventies (1970), for which the business logic has since been enhanced or changed. The challenge is further complicated by available data being from proprietary databases that are non-RDMS compliance and includes data that is manually maintained in Microsoft Excel spreadsheets.;2013;Koong Wah Yan;10.1109/DEST.2013.6611352;Conferences;2150-4946;978-1-4799-0784-7
Big data: Issues, challenges, tools and Good practices;Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.;2013;Avita Katal;10.1109/IC3.2013.6612229;Conferences;;978-1-4799-0190-6
Wearable monitors on babies: Big data saving little people;"Today 8% of Canadian babies are born premature and internationally the average is 10% These early births, are responsible for three quarters of all infant deaths in Canada. Premature infants together with ill term infants are cared for in Neonatal Intensive Care U nits (NICUs) internationally contain state of th e art medical equipment to monitor and provide life support, resulting in a significant Big Data environment. In addition, graduates of neonatal intensive care may be discharged with medical devices to support continued monitoring as ambulatory patients in and outside the ho me setting. In both NICU and ambulatory contexts wearable patient monitoring has many social implications. This research presents an assessment of the social implications of Big Data solutions for criti cal care within the context of the Artemis project that is enabling Big Data solutions for: 1) Real-ti me processing of complex intensive care physiological signals for new and earlier condition onset detection; 2) new approaches to physiological data analysis to support clinical research; and 3) cloud computing/services computing to provide rural and remote communities with greater options for a dvanced critical care within their own community healthcare facilities.";2013;Carolyn McGregor;10.1109/ISTAS.2013.6613120;Conferences;2158-3404;978-1-4799-1242-1
HireSome-II: Towards Privacy-Aware Cross-Cloud Service Composition for Big Data Applications;Cloud computing promises a scalable infrastructure for processing big data applications such as medical data analysis. Cross-cloud service composition provides a concrete approach capable for large-scale big data processing. However, the complexity of potential compositions of cloud services calls for new composition and aggregation methods, especially when some private clouds refuse to disclose all details of their service transaction records due to business privacy concerns in cross-cloud scenarios. Moreover, the credibility of cross-clouds and on-line service compositions will become suspicional, if a cloud fails to deliver its services according to its “promised” quality. In view of these challenges, we propose a privacy-aware cross-cloud service composition method, named HireSome-II (History record-based Service optimization method) based on its previous basic version HireSome-I. In our method, to enhance the credibility of a composition plan, the evaluation of a service is promoted by some of its QoS history records, rather than its advertised QoS values. Besides, the k-means algorithm is introduced into our method as a data filtering tool to select representative history records. As a result, HireSome-II can protect cloud privacy, as a cloud is not required to unveil all its transaction records. Furthermore, it significantly reduces the time complexity of developing a cross-cloud service composition plan as only representative ones are recruited, which is demanded for big data processing. Simulation and analytical results demonstrate the validity of our method compared to a benchmark.;2015;Wanchun Dou;10.1109/TPDS.2013.246;Journals;2161-9883;
Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that addresses the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more.;2013;;10.1109/TST.2013.6616527;Journals;1007-0214;
Inconsistencies in big data;We are faced with a torrent of data generated and captured in digital form as a result of the advancement of sciences, engineering and technologies, and various social, economical and human activities. This big data phenomenon ushers in a new era where human endeavors and scientific pursuits will be aided by not only human capital, and physical and financial assets, but also data assets. Research issues in big data and big data analysis are embedded in multi-dimensional scientific and technological spaces. In this paper, we first take a close look at the dimensions in big data and big data analysis, and then focus our attention on the issue of inconsistencies in big data and the impact of inconsistencies in big data analysis. We offer classifications of four types of inconsistencies in big data and point out the utility of inconsistency-induced learning as a tool for big data analysis.;2013;Du Zhang;10.1109/ICCI-CC.2013.6622226;Conferences;;978-1-4799-0781-6
Is privacy still an issue in the era of big data? — Location disclosure in spatial footprints;Geospatial data that were once difficult to obtain are now readily available to the public with the development of geospatial technologies. The ubiquitous use of social networking and location-based services enables easy sharing of personal stories among many people. With or without awareness of location disclosure, some users reveal a considerable amount of geopersonal information to the general public. Privacy issues have been raised again in the GIScience community. This study shows that home and work places may be inferred from georeferenced tweets of heavy Twitter users. Is privacy still an issue in the era of big data when people freely share blogs, photos, videos, and spatial footprints over the Internet? Or do people willingly decide to give out location information in order to gain benefits from the disclosure? After a discussion on possible reasons for people to share spatial footprints, this paper invites more research on perception of geoprivacy.;2013;Linna Li;10.1109/Geoinformatics.2013.6626191;Conferences;2161-0258;978-1-4673-6227-6
BigLS - The 1st International Workshop on Big Data in Life Sciences;With the ever-increasing volume, velocity and variety biological and biomedical data collections continue to pose new challenges and increasing demands on computing and data management. The inherent complexity of this big data has forced us to rethink how we collect, store, combine and analyze it.;2013;Ananth Kalyanaraman;10.1109/ICCABS.2013.6629242;Conferences;;978-1-4799-0716-8
Workflow-driven programming paradigms for distributed analysis of biological big data;Scientific workflows have been used as a programming model to automate scientific tasks ranging from short pipelines to complex workflows that span across heterogeneous data and computing resources. While utilization of scientific workflow technologies varies slightly across different scientific disciplines, all informatics and computational science disciplines provide a common set of attributes to facilitate and accelerate workflow-driven research. Scientific workflows provide assembly of complex processing easily in local or distributed environments via rich and expressive programming models. Scientific workflows enable transparent access to diverse resources ranging from local clusters and traditional supercomputers to elastic and heterogeneous Cloud resources. Scientific workflows support incorporation of multiple software tools including domain specific tools for standard processing to custom generalized workflows and middleware tools that can be reused in various contexts. Scientific workflows often collect provenance information on workflow entities, e.g., workflow definitions, their executions and run time parameters, and, in turn, assure a level of reproducibility while enabling referencing and replicating results. While doing all these, scientific workflows often foster an open-source, open-access and standards-driven community development model based on sharing and collaborations. Cyberinfrastructure platforms and gateways commonly employ scientific workflows to bridge the gap between the infrastructure and users needs. While capturing and communicating the scientific process formally, workflows ensure flexibility, synergy between users, provide optimized usage of resources, increase reuse and ensure compliance with system specific data models and community-driven standards. Currently, scientific workflows are used widely in life sciences at different stages of end-to-end data lifecycle from generation to analysis and publication of biological data. The data handled by such workflows can be produced by sequencers, sensor networks, medical imaging instruments and other heterogeneous resources at significant rates at decreasing costs making the analysis and archival of such data a 'big data' challenge. Additionally, these new biological data resources are making new and exciting research in areas including metagenomics and personalized medicine possible. However, the analysis of big biological data is still very costly requiring new scalable computational models and programming paradigms to be applied to biological analysis. Although, some new paradigms exists for analysis of big data, application of these best practices to life sciences is still in its infancy. Scientific workflows can act as a scaffold and help speed this process up via combination of existing programming models and computational models with the challenges of biological problems as reusable blocks. In this talk, I will talk about such an approach that builds upon distributed data parallel patterns, e.g., MapReduce, and underlying execution engines, e.g., Hadoop, and matches the computational requirements of bioinformatics tools with such patterns and engines. The results of the presented approach is developed as a part of the bioKepler (bioKepler.org) module and can be downloaded to work within the release 2.4 of the Kepler scientific workflow system (kepler-project.org).;2013;Ilkay Altintas;10.1109/ICCABS.2013.6629243;Conferences;;978-1-4799-0716-8
Breaking the boundary for whole-system performance optimization of big data;MapReduce plays an critical role in finding insights in Big Data. The performance optimization of MapReduce programs is challenging because it requires a comprehensive understanding of the whole system including both hardware layers (processors, storages, networks and etc), and software stacks (operating systems, JVM, runtime, applications and etc). However, most of the existing performance tuning and optimization are based on empirical and heuristic attempts. It remains a blank on how to build a systematical framework which breaks the boundary of multiple layers for performance optimization. In this paper, we propose a performance evaluation framework by correlating performance metrics from different layers, which provides insights to efficiently pinpoint the performance issue. This framework is composed of a series of predefined patterns. Each pattern indicates one or more potential issues. The behavior of a MapReduce program is mapped to the corresponding resource utilization. The framework provides a holistic approach which allows users at different levels of experience to conduct MapReduce program performance optimization. We use Terasort benchmark running on a 10-node Power7R2 cluster as a real case to show how this framework improves the performance. By this framework, we finally get the Terasort result improved from 47 mins to less than 8 mins. In addition to the best practice on performance tuning, several key findings are summarized as valuable workload analysis for JVM, MapReduce runtime and application design.;2013;Yan Li;10.1109/ISLPED.2013.6629278;Conferences;;978-1-4799-1234-6
The role of big data in improving power system operation and protection;This paper focuses on the use of extremely large data sets in power system operation, control, and protection, which are difficult to process with traditional database tools and often termed big data. We will discuss three aspects of using such data sets: feature extraction, systematic integration for power system applications, and examples of typical applications in the utility industry. The following analytics tasks based on big data methodology are elaborated upon: corrective, predictive, distributed and adaptive. The paper also outlines several research topics related to asset management, operation planning, realtime monitoring and fault detection/protection that present new opportunities but require further investigation.;2013;Mladen Kezunovic;10.1109/IREP.2013.6629368;Conferences;;978-1-4799-0199-9
Big Data's Risks and Opportunities for ICT Agriculture;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address these issues and to create a common background to be applied in the new NEDO project in which Sojo University is an active research member.";2013;Dennis A. Ludena R;10.1109/IIAI-AAI.2013.60;Conferences;;978-1-4799-2134-8
Virtual Dataspace -- A Service Oriented Model for Scientific Big Data;The massive, distributed, heterogeneous and diverse features of big data have raised challenges to traditional data management systems. As the development and innovation of Data Space, virtual data space (VDS) model is proposed for big data management. Local ontologies are created from data sources. Then the local ontologies are mapped and formed a global ontology. Based on this, access log and user feedback are considered for data evolution. At last, a material scientists-oriented service (materials scholar assistant) is introduced as the application case of VDS.;2013;Wei Lin;10.1109/EIDWT.2013.5;Conferences;;978-1-4799-2140-9
On use of big data for enhancing network coverage analysis;Proliferation of data services has made it mandatory for operators to be able identify geographical regions with 3G connectivity discontinuity in a scalable and cost-efficient manner. The currently used methods for such analysis are either costly — such as in drive tests, partly unreliable — such as in network simulation approaches, or are not precise enough — such as in base station key performance indicators (KPI) based approaches. In this paper, towards addressing these inadequacies, we propose a 3G coverage analysis method that makes use of “big data” processing schemes and the vast amounts of network data logged in mobile operators. In the proposed scheme, the BSSAP mobility and radio resource management messages between the BSS and MSC nodes of the operator network are processed to identify inter-technology handovers from 3G (WCDMA) access to 2G (EDGE, GPRS, GSM). Demonstrative examples show that the proposed mechanism produces accurate and precise results, outperforming the base station KPI-based approach.;2013;Ömer Faruk Çelebi;10.1109/ICTEL.2013.6632155;Conferences;;978-1-4673-6425-6
Survey of Research on Big Data Storage;With the development of cloud computing and mobile Internet, the issues related to big data have been concerned by both academe and industry. Based on the analysis of the existed work, the research progress of how to use distributed file system to meet the challenge in storing big data is expatiated, including four key techniques: storage of small files, load balancing, copy consistency, and de-duplication. We also indicate some key issues need to concern in the future work.;2013;Xiaoxue Zhang;10.1109/DCABES.2013.21;Conferences;;978-0-7695-5060-2
Big data on small screens;Data is one of the integral assets of the organization. There are many companies which have the huge amount of data but the challenge lies in how to make it more usable. In this 21st century the world around us is rapidly changing and smartphones are tightly coupled to the users. Customers mainly concentrate on using smartphone devices to access telephonic services. Whereas now it's the time for companies to make use of this available big data from the customers. This big data can be used in one or the other form in understanding the behavior of the customers. In this paper author would present the usage of the big data on smart phones to provide sophisticated services to the customers in real time. Also author would explore how this big data on small screens can make a difference in major events like US Presidential Elections.;2013;Aditya R Desai;10.1109/ICACCI.2013.6637287;Conferences;;978-1-4799-2659-6
HPCS 2013 Keynotes: Tuesday keynote: Big process for big data;"These keynotes discuss the following: Big Process for Big Data; Killer-Mobiles: The Way Towards Energy Efficient High Performance Computers?; The UberCloud HPC Experiment - Paving the Way to HPC as a Service; and High Performance Fault Tolerance / Resilience at Extreme Scale.";2013;Ian Foster;10.1109/HPCSim.2013.6641378;Conferences;;978-1-4799-0837-0
Big data analytics as a service: Exploring reuse opportunities;As data scientists, we live in interesting times. Data has been the No. 1 fast growing phenomenon on the Internet for the last decade. Big data analytics have the potential to reveal deep insights hidden by big data that exceeds the processing capacity of existing systems, such as peer influence among customers, revealed by analyzing shoppers' transactions, social and geographical data. In the past 40 years, data was primarily used to record and report business activities and scientific events, and in the next 40 years data will be used also to derive new insights, to influence business decisions and to accelerate scientific discovery. The key challenge is to provide the right platforms and tools to make reasoning of big data easy and simple. In this keynote talk, I will explore reuse opportunities and challenges from multiple dimensions towards delivering big data analytics as a service. I will illustrate by example the importance and challenges of utilizing programmable algorithm abstractions for many seemingly domain-dependent data analytics tasks. Another reuse opportunity is to exploit unconventional data structures and big data processing constructs to simplify and speed up the big data processing.;2013;Ling Liu;10.1109/IRI.2013.6642438;Conferences;;978-1-4799-1050-2
Implementation of the Big Data concept in organizations - possibilities, impediments and challenges;This paper is devoted to the analysis of the Big Data phenomenon. It is composed of seven parts. In the first, the growing role of data and information and their rapid increase in the new socio-economical reality, are discussed. Next, the notion of Big Data is defined and the main sources of growth of data are characterized. In the following part of the paper the most significant possibilities linked with Big Data are presented and discussed. The next part is devoted to the characterization of tools, techniques and the most useful data in the context of Big Data initiatives. In the following part of the paper the success factors of Big Data initiatives are analyzed, followed by an analysis of the most important problems and challenges connected with Big Data. In the final part of the paper, the most significant conclusions and suggestions are offered.;2013;Janusz Wielki;;Conferences;;978-83-60810-52-1
Applying big data and linked data concepts in supply chains management;"One of the contemporary problems, and at the same time a big opportunity, in business networks of supply chains are the issues associated with the vast amounts of data arising there. The data may be utilized by the decision support systems in logistics; nevertheless, often there is an information integration problem. The problems with information interchange are related to issues with exchange between independently designed data systems. The networked supply chains will need appropriate IT architectures to support the cooperating business units utilizing structured and unstructured big data and the mechanisms to integrate data in heterogeneous supply chains. In this paper we analyze the capabilities of the big data technology architectures with cloud computing under usage of Linked Data in business process management in supply chains to cope with unstructured near-time data and data silos problems. We present our approach on a 4PL (Fourth-party Logistics) integrator business process example.";2013;Silva Robak;;Conferences;;978-83-60810-52-1
Using Big Data and predictive machine learning in aerospace test environments;"It is estimated that in 2012 most mid-size companies in the USA generate the equivalent data of the US Library of Congress in 1 year. As a company, Wal-Mart creates the equivalent of 50 million filing cabinets worth of data every hour. While these numbers seem incredible, the trend for most companies is an increasing volume of data generation and storage. Test Data generated by Automatic Test Equipment (ATE) in R&D, manufacturing and Repair environments is no exception to this increased volume of data. The challenge of this enormous amount of Test Data is how to provide people with effective ways to make decisions from it. Data visualization through charts, graphs and reports has been, historically, one of the more effective ways to provide actionable intelligence because humans can readily make decisions based on patterns and comparisons. But as data volume goes up, even this method is reaching its limits. When one starts to combine large datasets like Manufacturing Test Data and Repair Data together, data visualization becomes problematic. More sophisticated algorithmic, machine learning and predictive approaches become critical. In this paper, we will explore the experiences of using predictive algorithms on ""Big Data"" from both Manufacturing Test and Repair Test environments in the complex mission critical aerospace industry. By effectively using datasets from different functional areas, we will be looking at applying SPC techniques to answer new questions about the correlation of Repair test data and manufacturing data with the end goal to predict number of returns in the future and minimize product escapes.";2013;Tom Armes;10.1109/AUTEST.2013.6645085;Conferences;1558-4550;978-1-4673-5681-7
Embedded Analytics and Statistics for Big Data;Embedded analytics and statistics for big data have emerged as an important topic across industries. As the volumes of data have increased, software engineers are called to support data analysis and applying some kind of statistics to them. This article provides an overview of tools and libraries for embedded data analytics and statistics, both stand-alone software packages and programming languages with statistical capabilities.;2013;Panos Louridas;10.1109/MS.2013.125;Magazines;1937-4194;
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.130;Magazines;1937-4194;
Location: A Feature for Service Selection in the Era of Big Data;This paper introduces a service selection model with the service location considered. The location of a service represents its position in the network, which determines the transmission cost of calling this service in the composite service. The more concentrated the invoking services are, the less transmission time the composite service costs. On the other hand, the more and more popular big data processing services, which need to transfer mass data as input, make the effect much more obvious than ever before. Therefore, it is necessary to introduce service location as a basic feature in service selection. The definition and membership functions of service location are presented in this paper. After that, the optimal service selection problem is represented as an optimization problem under some reasonable assumptions. A shortest-path based algorithm is proposed to solve this optimization problem. At last, the case of railway detection is studied for better understanding of our model.;2013;Luo Zhiling;10.1109/ICWS.2013.75;Conferences;;978-0-7695-5025-1
pLSM: A Highly Efficient LSM-Tree Index Supporting Real-Time Big Data Analysis;Big Data boosts the development of data management and analysis in database systems but it also poses a challenge to traditional database. NoSQL databases are provided to deal with the new challenges brought by Big Data because of its high performance, storage, scalability and availability. In NoSQL databases, it is an essential requirement to provide scalable and efficient index services for real-time data analysis. Most existing index solutions focus on improving write throughput, but at the cost of poor read performance. We designed a new plug-in system PuntStore with pLSM (Punt Log Structured Merge Tree) index engine. To improve read performance, Cache Oblivious Look-ahead Array (COLA) is adopted in our design. We also presented a novel compact algorithm in bulk deletion to support migration of data from temporary storage to data warehouse for further analysis.;2013;Jin Wang;10.1109/COMPSAC.2013.40;Conferences;0730-3157;978-0-7695-4986-6
Big Data -- Opportunities and Challenges Panel Position Paper;This paper summarizes opportunities and challenges of big data. It identifies important research directions and includes a number of questions that have been debated by the panel.;2013;Elisa Bertino;10.1109/COMPSAC.2013.143;Conferences;0730-3157;978-0-7695-4986-6
Analytics over Big Data: Exploring the Convergence of DataWarehousing, OLAP and Data-Intensive Cloud Infrastructures;This paper explores the convergence of Data Warehousing, OLAP and data-intensive Cloud Infrastructures in the context of so-called analytics over Big Data. The paper briefly reviews some state-of-the-art proposals, highlights open research issues and, finally, it draws possible research directions in this scientific field.;2013;Alfredo Cuzzocrea;10.1109/COMPSAC.2013.152;Conferences;0730-3157;978-0-7695-4986-6
Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.370;Magazines;1558-0814;
Data Evolution Analysis of Virtual DataSpace for Managing the Big Data Lifecycle;New challenge about the constantly changing of associated data in big data management has arisen, which leads to the issue of data evolution. In this paper, a data evolution model of Virtual Data Space (VDS) is proposed for managing the big data lifecycle. Firstly, the concept of data evolution cycle is defined, and the lifecycle process of big data management is described. Based on these, the data evolution lifecycle is analyzed from the data relationship, the user requirements, and the operation behavior. Secondly, the classification and key concepts about the data evolution process are described in detail. According to this, the data evolution model is constructed by defining the related concepts and analyzing the data association in VDS, for the capture and tracking of dynamic data in the data evolution cycle. Then we discuss the cost problem about data dissemination and change. Finally, as the application case, the service process of dynamic data in the field of materials science is described and analyzed. We verify the validity of data evolution modeling in VDS by the comparison of traditional database, data space, and VDS. It shows that this analysis method is efficient for the data evolution processing, and very suitable for the data-intensive application and the real-time dynamic service.;2013;Xin Cheng;10.1109/IPDPSW.2013.57;Conferences;;978-0-7695-4979-8
A Workflow Framework for Big Data Analytics: Event Recognition in a Building;This paper studies event recognition in a building based on the patterns of power consumption. It is a big challenge to identify what kinds of events happened in a building without additional devices such as camera and motion sensors, etc. Instead, we learn when and how the events happened from the historical record of power consumption and apply the lesson into the design of an event recognition system (ERS). The ERS will find out abnormal power usage to avoid wasting power, which leads to the energy savings in a building. The ERS involves big data analytics with a large size of dataset collected in a real time. Such a data intensive system is usually viewed as a workflow. A workflow management is a significant task of the system requiring data analysis in terms of the system scalability to maintain high throughput or fast speed analysis. We propose a workflow framework that allows users to perform remote and parallel workflow execution, whose tasks are efficiently scheduled and distributed in cloud computing environment. We run the ERS as a target system for the proposed framework with power consumption data (whose size is approximately 20GB or more) collected from each of over 240 rooms in a building at Dept. of Engineering, Tokyo University in 2011. We show that the proposed framework accelerates the speed of data analysis by providing scaling infrastructure and parallel processing feature utilizing cloud computing technologies. We also share our experience and results on the big data analytics and discuss how the studies contribute to achieve Green Campus.;2013;Chonho Lee;10.1109/SERVICES.2013.29;Conferences;2378-3818;978-0-7695-5024-4
Social Issues of Big Data and Cloud: Privacy, Confidentiality, and Public Utility;"Business people and academia are now excited about Big Data and Cloud Computing as the new and most innovative means for enhancing productivity and customer satisfaction. Simultaneously, there are strong concerns about privacy not only among privacy advocates but among consumers in general, and how to strike a right balance is the main theme in every field of science. However, it is quite strange that very little attention has been paid to the concept of confidentiality, which must be the core element of privacy. This paper first tries to analyze the following two dichotomies as a basis for possible policy considerations: (1) privacy approach in the United States versus confidentiality approach in the United Kingdom, though they share the same common law tradition, and (2) clear demarcation between Information Service and Telecommunications in the United States, dating back to the Computer Inquiry in the 1970s. This paper also analyzes the features of the Cloud and discusses the possibility of treating it as a new type of Public Utility, namely Information Utility. This hypothesis should be rejected, because there are crucial differences in market structures, regardless of clear similarities in service features. Instead, this paper emphasizes the necessity of protecting confidentiality as an industrial norm. Taking into account the long tradition of free market for computing industries, self-regulation is basically preferable to government regulation. But from a different viewpoint of ""nudge"", a hybrid combination of libertarianism and paternalism, this paper concludes by proposing five short recommendations including fair contract terms as well as unbundling confidentiality from privacy.";2013;Koichiro Hayashi;10.1109/ARES.2013.66;Conferences;;978-0-7695-5008-4
Towards an efficient routing web processing service through capturing real-time road conditions from big data;The rapidly growing number of crowdsourcing platforms generates huge volumes of volunteered geographic information (VGI), which requires analysis to reveal their potential. The huge volumes of data appear as an opportunity to improve various applications, including routing and navigation services. How existing techniques for dealing with Big Data could be useful for the analysis of VGI remains an open question, since VGI differs from traditional data. In this paper, we focus on examining the latest developments and issues associated with big data from the perspective of the analysis of VGI. This paper notably presents our new architecture for exploiting Big VGI in event service processing in support to optimization of routing service. In addition, our study highlights the opportunities that are created by the emergence of Big VGI and crowdsourced data on improving routing and navigation services, as well as the challenges that remain to be addressed to make this a reality. Finally, avenues for future research on the next generation of collaborative routing and navigation services are presented.;2013;Mohamed Bakillah;10.1109/CEEC.2013.6659463;Conferences;;978-1-4799-0383-2
A big data file transfer tool for tablet-class machines;A big data file transport (BDFT) protocol is presented that minimize overheads associated with packet streaming in Java. The BDFT protocol relies on block 1-data transfers, and the elimination of unnecessary data copying between the application layer and the send socket in the standard Java IO model. The implementation uses the Java New IO (NIO) and the Zerocopy libraries. Several experiments are described and results compared against the standard Java IO - a stream-based file transport protocol. The motivation for this study is the development of a client/server big data file transport protocol for tablet-class client machines that rely on the Java Remote Method Invocation (RMI) package for distributed computing.;2013;Tevaganthan Veluppillai;10.1109/IDAACS.2013.6663011;Conferences;;978-1-4799-1427-2
A Big Data application framework for consumer behavior analysis;More than ever before, the amount of data about consumers, suppliers and products has been exploding in today consumer world referred as “Big Data”. In addition, more data is available to the consumer world from multiple sources including social network platforms. In order to deal with such amount of data, a new emerging technology “Big Data Analytics” is explored and employed for analyzing consumer behaviors and searching their information needs. Specifically, this paper proposes a Big Data application framework for analyzing consumer behaviors by using topological data structure, co-occurrence methodology and Markov chain theory. First, the consumer related data is translated into a topological data structure. Second, using topological relationships, a co-occurrence matrix is formed to deduce Markov chain model for consumer behavior analysis. Finally, some simulation results are shown to confirm the effectiveness of the proposed framework.;2013;Thi Thi Zin;10.1109/GCCE.2013.6664813;Conferences;2378-8143;978-1-4799-0890-5
A successful application of big data storage techniques implemented to criminal investigation for telecom;With the emerging of digital convergence, lots of communication services are generated, and the quantity of data grows rapidly. We face the scalability issue to deal with call data records (CDRs) and so are the other telecom companies. This research uses police CDR query as an example with an intension to increase system execution efficiency and scalability and to reduce total cost by applying cloud service. The implementation applying distributed parallel database (Hive), distributed computing (Hadoop MapReduce), and distributed file system (Hadoop HDFS) will be introduced by a simulation to evaluate the execution efficiency of a query from CDRs. The factors influencing query efficiency, such as the settings of data block size and partition size in HDFS, will also be explored. The experimental results show that applying the big data processing technologies to execute queries from a huge amount of CDRs could improve the system execution efficiency significantly and reduce cost.;2013;Ju-Chi Tseng;;Conferences;;978-4-8855-2279-6
Beyond Big Data?;What can we expect to find beyond Big Data? And how can we exploit Big Data to get there?;2013;Judith Bayard Cushing;10.1109/MCSE.2013.102;Magazines;1558-366X;
Leveraging Big Data and Business Analytics [Guest editors' introduction];"Big data projects and programs provide tremendous opportunity for organizations looking to transform their operations, innovate in their markets, and better serve their customers; however, these initiatives must be based on sound approaches and principles and not fads or empty vendor claims. This special issue aims to promote a better understanding of big data to foster wider deployment of big data approaches and a new era of business analytics capabilities.";2013;Sunil Mithas;10.1109/MITP.2013.95;Magazines;1941-045X;
Proper orthogonal decomposition based parallel compression for visualizing big data on the K computer;The development of supercomputers has greatly help us to carry on large-scale computing for dealing with various problems through simulating and analyzing them. Visualization is an indispensable tool to understand the properties of the data from supercomputers. Especially, interactive visualization can help us to analyze data from various viewpoints and even to find out some local small but important features. However, it is still difficult to interactively visualize such kind of big data directly due to the slow file I/O problem and the limitation of memory size. For resolving these problems, we proposed a parallel compression method to reduce the data size with low computational cost. Furthermore, the fast linear decompression process is another merit for interactive visualization. Our method uses proper orthogonal decomposition (POD) to compress data because it can effectively extract important features from the data and the resulting compressed data can also be linearly decompressed. Our implementation achieves high parallel efficiency with a binary load-distributed approach, which is similar to the binary-swap image composition used in parallel volume rendering [2]. This approach allows us to effectively utilize all the processing nodes and reduce the interprocessor communication cost throughout the parallel compression calculations. Our test results on the K computer demonstrate superior performance of our design and implementation.;2013;Chongke Bi;10.1109/LDAV.2013.6675169;Conferences;;978-1-4799-1659-7
RUBA: Real-time unstructured big data analysis framework;We are greeting “Big Data Generation”. As ICT technology is developing, the volume of data is incredibly growing and many works to deal with a big data are underway. In this paper, we proposed a novel framework for real-time unstructured big data analysis, such as a movie, sound, text and image data. Our proposed framework provides functions of a real-time analysis and dynamic modification for unstructured big data analysis. We have implemented the object monitoring system as a test system which is applied our framework, and we have confirmed each functions and the availability of our framework.;2013;Jaein Kim;10.1109/ICTC.2013.6675410;Conferences;2162-1241;978-1-4799-0698-7
Efficient and Customizable Data Partitioning Framework for Distributed Big RDF Data Processing in the Cloud;Big data business can leverage and benefit from the Clouds, the most optimized, shared, automated, and virtualized computing infrastructures. One of the important challenges in processing big data in the Clouds is how to effectively partition the big data to ensure efficient distributed processing of the data. In this paper we present a Scalable and yet customizable data PArtitioning framework, called SPA, for distributed processing of big RDF graph data. We choose big RDF datasets as our focus of the investigation for two reasons. First, the Linking Open Data cloud has put forwards a good number of big RDF datasets with tens of billions of triples and hundreds of millions of links. Second, such huge RDF graphs can easily overwhelm any single server due to the limited memory and CPU capacity and exceed the processing capacity of many conventional data processing software systems. Our data partitioning framework has two unique features. First, we introduce a suite of vertexcentric data partitioning building blocks to allow efficient and yet customizable partitioning of large heterogeneous RDF graph data. By efficient, we mean that the SPA data partitions can support fast processing of big data of different sizes and complexity. By customizable, we mean that the SPA partitions are adaptive to different query types. Second, we propose a selection of scalable techniques to distribute the building block partitions across a cluster of compute nodes in a manner that minimizes inter-node communication cost by localizing most of the queries on distributed partitions. We evaluate our data partitioning framework and algorithms through extensive experiments using both benchmark and real datasets. Our experimental results show that the SPA data partitioning framework is not only efficient for partitioning and distributing big RDF datasets of diverse sizes and structures but also effective for processing big data queries of different types and complexity.;2013;Kisung Lee;10.1109/CLOUD.2013.63;Conferences;2159-6182;978-0-7695-5028-2
Toward an Ecosystem for Precision Sharing of Segmented Big Data;As the amount of data created and stored by organizations continues to increase, attention is turning to extracting knowledge from that raw data, including making some data available outside of the organization to enable crowd analytics. The adoption of the MapReduce paradigm has made processing Big Data more accessible, but is still limited to data that is currently available, often only within an organization. Fine-grained control over what information is shared outside an organization is difficult to achieve with Big Data, particularly in the MapReduce model. We introduce a novel approach to sharing that enables fine-grained control over what data is shared. Users submit analytics tasks that run on infrastructure near the actual data, reducing network bottlenecks. Organizations allow access to a logical version of their data created at runtime by filtering and transforming the actual data without creating storage-intensive stale copies, and resellers can further segment or augment this data to provide added value to analytics tasks. A loosely-coupled ecosystem driven by web services allows for discovery and sharing with a flexible, secure environment that limits the knowledge those running analytics need to have about the actual provider of the data. We describe a proof-of-concept implementation of the various components required to realize this ecosystem, and present a set of experiments to demonstrate feasibility, showing advantageous performance versus storage trade-offs.;2013;Mark Shtern;10.1109/CLOUD.2013.131;Conferences;2159-6182;978-0-7695-5028-2
Tape Cloud: Scalable and Cost Efficient Big Data Infrastructure for Cloud Computing;Magnetic tapes have been a primary medium of backup storage for a long time in many organizations. In this paper, the possibility of establishing an inter-network accessible, centralized, tape based data backup facility is evaluated. Our motive is to develop a cloud storage service that organizations can use for long term storage of big data which is typically Write-Once-Read-Many. This Infrastructure-as-a-Service (IaaS) cloud can provide the much needed cost effectiveness in storing huge amounts of data exempting client organizations from high infrastructure investments. We make an attempt to understand some of the limitations induced by the usage of tapes by studying the latency of tape libraries in scenarios most likely faced in the backing up process in comparison to its hard disk counterpart. The result of this study is an outline of methods to overcome these limitations by adopting novel tape storage architectures, filesystem, schedulers to manage data transaction requests from various clients and develop faster ways to retrieve requested data to extend the applications beyond backup. We use commercially available tapes and a tape library to perform latency tests and understand the basic operations of tape. With the optimistic backing of statistics that suggests the extensive usage of tapes to this day and in future, we propose an architecture to provide data backup to a large and diverse client base.;2013;Varun S. Prakash;10.1109/CLOUD.2013.129;Conferences;2159-6182;978-0-7695-5028-2
Moving Big Data to The Cloud: An Online Cost-Minimizing Approach;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible or secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geo-dispersed data into the cloud, for processing using a MapReduce-like framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms: an online lazy migration (OLM) algorithm and a randomized fixed horizon control (RFHC) algorithm , for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. Careful comparisons among these online and offline algorithms in realistic settings are conducted through extensive experiments, which demonstrate close-to-offline-optimum performance of the online algorithms.;2013;Linquan Zhang;10.1109/JSAC.2013.131211;Journals;1558-0008;
Social Genome: Putting Big Data to Work for Population Informatics;Data-intensive research using distributed, federated, person-level datasets in near real time has the potential to transform social, behavioral, economic, and health sciences--but issues around privacy, confidentiality, access, and data integration have slowed progress in this area. When technology is properly used to manage both privacy concerns and uncertainty, big data will help move the growing field of population informatics forward.;2014;Hye-Chung Kum;10.1109/MC.2013.405;Magazines;1558-0814;
Influence maximization for Big Data through entropy ranking and min-cut;As Big Data becomes prevalent, the traditional models from Data Mining or Data Analysis, although very efficient, lack the speed necessary to process problems with data sets in the range of million samples. Therefore, the need for designing more efficient and faster algorithms for these new types of problems. Specifically, from the field of social network analysis, we have the influence maximization problem. This is a problem with many possible applications in advertising, marketing, social studies, etc, where we have representations of influences by large scale graphs. Even though, the optimal solution of this problem, the minimum set of graph nodes which can influence a maximum set of nodes, is a NP-Hard problem, it is possible to devise an approximated solution to the problem. In this paper, we have proposed a novel algorithm for influence maximization analysis. This algorithm consist in two phases: the first one is an entropy based node ranking where entropy ranking is used to determine node importance in a directed weighted influence graph. The second phase computes the minimum cut using a novel metric. To test the propose algorithm, experiments were performed in several popular data sets to evaluate performance and the seed quality over the influences.;2013;Agustin Sancen-Plaza;10.4108/icst.collaboratecom.2013.254119;Conferences;;978-1-936968-92-3
Real-time collaborative planning with big data: Technical challenges and in-place computing (invited paper);There is increasing collaboration in new generation supply chain planning applications, where participants across a supply chain analyze and plan on a big volume of sales data over the internet together. To achieve real-time collaborative planning over big data, we have developed an unconventional technology, BigObject, based on an in-place computing approach in two ways. First, instead of moving (big) data around, move (small) code to where data resides for execution. Second, organize the complexity by determining the basic functional units (objects) for computing in the same sense that macromolecules are determined for living cells. The term ”in-place” indicates that data is in residence in memory space and ready for computing. BigObject is an in-place computing system, designed for storing and computing multidimensional data. Our experiment shows that in-place computing approach outperforms traditional computing approach in two orders of magnitude.;2013;Wenwey Hseush;10.4108/icst.collaboratecom.2013.254100;Conferences;;978-1-936968-92-3
A Sketch of Big Data Technologies;This paper outlines the recent developed information technologies in big data. The basic principles and theories, concepts and terminologies, methods and implementations, and the status of research and development in big data are depicted. The paper also highlights the technical challenges and major difficulties. The number of key technologies required to handle big data are deliberated. They include big data acquisition, pre/post-processing, data storage and distribution, networks, and analysis and mining, etc. At last, the development trend in big data technologies is addressed for discussion.;2013;Zaiying Liu;10.1109/ICICSE.2013.13;Conferences;2330-9857;978-0-7695-5118-0
An Iterative Hierarchical Key Exchange Scheme for Secure Scheduling of Big Data Applications in Cloud Computing;As the new-generation distributed computing platform, cloud computing environments offer high efficiency and low cost for data-intensive computation in big data applications. Cloud resources and services are available in pay-as-you-go mode, which brings extraordinary flexibility and cost-effectiveness as well as zero investment in their own computing infrastructure. However, these advantages come at a price-people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated Key Exchange (AKE) is essential to a security system that is based on high efficiency symmetric-key encryption. With virtualization technology being applied, existing key exchange schemes such as Internet Key Exchange (IKE) becomes time-consuming when directly deployed into cloud computing environment. In this paper we propose a novel hierarchical key exchange scheme, namely Cloud Background Hierarchical Key Exchange (CBHKE). Based on our previous work, CBHKE aims at providing secure and efficient scheduling for cloud computing environment. In our new scheme, we design a two-phase layer-by-layer iterative key exchange strategy to achieve more efficient AKE without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud computing environment, efficiency of the proposed scheme is dramatically superior to its predecessors CCBKE and IKE schemes.;2013;Chang Liu;10.1109/TrustCom.2013.65;Conferences;2324-9013;978-0-7695-5022-0
Combining Top-Down and Bottom-Up: Scalable Sub-tree Anonymization over Big Data Using MapReduce on Cloud;In big data applications, data privacy is one of the most concerned issues because processing large-scale privacy-sensitive data sets often requires computation power provided by public cloud services. Sub-tree data anonymization, achieving a good trade-off between data utility and distortion, is a widely adopted scheme to anonymize data sets for privacy preservation. Top-Down Specialization (TDS) and Bottom-Up Generalization (BUG) are two ways to fulfill sub-tree anonymization. However, existing approaches for sub-tree anonymization fall short of parallelization capability, thereby lacking scalability in handling big data on cloud. Still, both TDS and BUG suffer from poor performance for certain value of k-anonymity parameter if they are utilized individually. In this paper, we propose a hybrid approach that combines TDS and BUG together for efficient sub-tree anonymization over big data. Further, we design MapReduce based algorithms for two components (TDS and BUG) to gain high scalability by exploiting powerful computation capability of cloud. Experiment evaluations demonstrate that the hybrid approach significantly improves the scalability and efficiency of sub-tree anonymization scheme over existing approaches.;2013;Xuyun Zhang;10.1109/TrustCom.2013.235;Conferences;2324-9013;978-0-7695-5022-0
Big Data Real-Time Processing Based on Storm;As the growth of Internet, Cloud Computing, Mobile Network and Internet of Things is increasing rapidly, Big Data is becoming a hot-spot in recent years. Big Data Processing is involved in our daily life such as mobile devices, RFID and wireless sensors, which aims at dealing with billions of users' interactive data. At the same time, real-time processing is eagerly needed in integrated system. In this paper, several technologies associated with real-time big data processing are introduced, among which the core technology called Storm is emphasized. An entire system is built based on Storm, associated with RabbitMQ, NoSQL and JSP. To ensure the practical applicability and high efficiency, a simulation system is established and shows acceptable performance in various expressions using data sheet and Ganglia. It is proved that the big data real-time processing based on Storm can be widely used in various computing environment.;2013;Wenjie Yang;10.1109/TrustCom.2013.247;Conferences;2324-9013;978-0-7695-5022-0
A Universal Storage Architecture for Big Data in Cloud Environment;With the rapid development of the Internet of Things and Electronic Commerce, we have entered the era of big data. The characteristics, such as great amount and heterogeneousity, of big data bring the challenge to the storage and analytics. The paper presented a universal storage architecture for big data in cloud environment. We use clustering analysis to divide the cloud nodes into multiple clusters according to the communication cost between different nodes. The cluster with the strongest computing power is selected to provide the universal storage and query interface for users. Each of other clusters is responsible for storing the data of a particular model, such as relational data, key-value data, and document data and so on. Experiments show that our architecture can store all kinds of heterogeneous big data and provide users with unified storage and query interface for big data easily and quickly.;2013;Qingchen Zhang;10.1109/GreenCom-iThings-CPSCom.2013.96;Conferences;;978-0-7695-5046-6
IOT-StatisticDB: A General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things;In large scale Internet of Things (IoT) systems, statistical analysis is a crucial technique for transforming data into knowledge and for obtaining overall information about the physical world. However, most existing statistical analysis methods for sensor sampling data are implemented outside the database kernel and focus on specialized analytics, making them unsuited for the IoT environment where both the data types and the statistical queries are diverse. To solve this problem, we propose a General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things (IOT-StatisticDB) in this paper. In IOT-StatisticDB, statistical functions are performed through statistical operators inside the DBMS kernel, so that complicated statistical queries can be expressed in the standard SQL format. Besides, statistical analysis is executed in a distributed and parallel manner over multiple servers so that the performance can be greatly improved, which is confirmed by the experiments.;2013;Zhiming Ding;10.1109/GreenCom-iThings-CPSCom.2013.104;Conferences;;978-0-7695-5046-6
Big Data Analytics for Security;"Big data is changing the landscape of security tools for network monitoring, security information and event management, and forensics; however, in the eternal arms race of attack and defense, security researchers must keep exploring novel ways to mitigate and contain sophisticated attackers.";2013;Alvaro A. Cárdenas;10.1109/MSP.2013.138;Magazines;1558-4046;
Rapid Scanning of Spectrograms for Efficient Identification of Bioacoustic Events in Big Data;Acoustic sensing is a promising approach to scaling faunal biodiversity monitoring. Scaling the analysis of audio collected by acoustic sensors is a big data problem. Standard approaches for dealing with big acoustic data include automated recognition and crowd based analysis. Automatic methods are fast at processing but hard to rigorously design, whilst manual methods are accurate but slow at processing. In particular, manual methods of acoustic data analysis are constrained by a 1:1 time relationship between the data and its analysts. This constraint is the inherent need to listen to the audio data. This paper demonstrates how the efficiency of crowd sourced sound analysis can be increased by an order of magnitude through the visual inspection of audio visualized as spectrograms. Experimental data suggests that an analysis speedup of 12× is obtainable for suitable types of acoustic analysis, given that only spectrograms are shown.;2013;Anthony Truskinger;10.1109/eScience.2013.25;Conferences;;978-0-7695-5083-1
Catching the wave: Big data in the classroom;Many diverse domains-in the sciences, engineering, healthcare, and homeland security-have been grappling with the analysis of “Big Data,” which has become shorthand to represent extremely large amounts of diverse types of data. A recent Gartner report predicts that around 4.4 million IT jobs globally will be created by 2015 to support Big Data, with 1.9 million of those jobs in the United States. Therefore, understanding approaches and techniques for handling and analyzing Big Data from diverse domains has become crucial for not only in computing but also engineering students. The mini-workshop will make use of active and collaborative learning exercises to introduce faculty in computer science, software engineering, and other disciplines to concepts and techniques involved in managing and analyzing Big Data. Approaches for incorporating Big Data into the engineering and computing curricula will also be presented.;2013;Carol J. Romanowski;10.1109/FIE.2013.6684855;Conferences;2377-634X;978-1-4673-5261-1
The Convergence of Big Data and Mobile Computing;In recent years, we have witnessed the influx of data driven by the exponential use of digital video, music, and mobile applications. The combination of the internet, mobile technologies and social media applications has been the most influential factor in the emergence of Big Data paradigm. Given that online activities of mobile users constitute the major contributors to the Big Data analytics, which significantly benefit various business enterprises, it is also interesting to look at how the information derived may also be useful for mobile users, and how it can be delivered in the most effective, and efficient manner. These will be the key points of discussion in this paper.;2013;Agustinus Borgy Waluyo;10.1109/NBiS.2013.15;Conferences;2157-0426;978-1-4799-2509-4
A Big Data Approach for a New ICT Agriculture Application Development;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address the impact of these issues in the novel nutrition-based vegetable production and distribution system project in which Sojo University is an active member.";2013;R. Dennis A. Ludena;10.1109/CyberC.2013.30;Conferences;;978-0-7695-5106-7
Big Data a Sure Thing for Telecommunications: Telecom's Future in Big Data;Big Data is made for telecommunications. No other industry has access to the wealth of information about their customers the way communications service providers (CSPs) do. Big data can be seen as the CSPs' most valuable asset. It puts them in a key position to win the battle for customers and generate new revenue streams - provided they can get their acts together.;2013;Rob Van Den Dam;10.1109/CyberC.2013.32;Conferences;;978-0-7695-5106-7
Big Data Analytics in the Public Sector: Improving the Strategic Planning in World Class Universities;"This paper presents an application in Information Fusion related to Analytics and fusion of Big Data. The approach involves the analysis of specific aspects of the management of a Brazilian university and the proposition of a new framework that may be useful mainly for universities focused on improving their strategic planning while having excellence in operational execution. The framework intends to be used for the development of new decision support systems that integrate ""past data"" with ""real time data"". The research methodology emphasizes induction with the collection of qualitative data in order to move from observed facts to theory. The main result is the framework itself while future work may involve the development and the validation of the new system.";2013;Joni A. Amorim;10.1109/CyberC.2013.33;Conferences;;978-0-7695-5106-7
A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud;The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.;2013;Xuyun Zhang;10.1109/CGC.2013.24;Conferences;;978-0-7695-5114-2
Visual Analytics for Big Data Using R;The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.;2013;Aziz Nasridinov;10.1109/CGC.2013.96;Conferences;;978-0-7695-5114-2
A distribute parallel approach for big data scale optimal power flow with security constraints;This paper presents a mathematical optimization framework for security-constrained optimal power flow (SCOPF) computations. The SCOPF problem determines the optimal control of power systems under constraints arising from a set of postulated contingencies. This problem is challenging due to the significantly large problem size, the stringent real-time requirement and the variety of numerous post-contingency states. In order to solve the resultant big data scale optimization problem with manageable complexity, the alternating direction method of multipliers (ADMM) is utilized. The SCOPF is decomposed into independent subproblems correspond to each individual pre-contingency and post-contingency case. Those subproblems are solved in parallel on distributed nodes and coordinated through dual (prices) variables. As a result, the algorithm is implemented in a distributive and parallel fashion. Numerical tests validate the effectiveness of the proposed algorithm.;2013;Lanchao Liu;10.1109/SmartGridComm.2013.6688053;Conferences;;978-1-4799-1526-2
Towards Service-Oriented Enterprise Architectures for Big Data Applications in the Cloud;Applications with Service-oriented Enterprise Architectures in the Cloud are emerging and will shape future trends in technology and communication. The development of such applications integrates Enterprise Architecture and Management with Architectures for Services & Cloud Computing, Web Services, Semantics and Knowledge-based Systems, Big Data Management, among other Architecture Frameworks and Software Engineering Methods. In the present work in progress research, we explore Service-oriented Enterprise Architectures and application systems in the context of Big Data applications in cloud settings. Using a Big Data scenario, we investigate the integration of Services and Cloud Computing architectures with new capabilities of Enterprise Architectures and Management. The underlying architecture reference model can be used to support semantic analysis and program comprehension of service-oriented Big Data Applications. Enterprise Services Computing is the current trend for powerful large-scale information systems, which increasingly converge with Cloud Computing environments. In this paper we combine architectures for services with cloud computing. We propose a new integration model for service-oriented Enterprise Architectures on basis of ESARC - Enterprise Services Architecture Reference Cube, which is our previous developed service-oriented enterprise architecture classification framework, with MFESA - Method Framework for Engineering System Architectures - for the design of service-oriented enterprise architectures, and the systematic development, diagnostics and optimization of architecture artifacts of service-oriented cloud-based enterprise systems for Big Data applications.;2013;Alfred Zimmermann;10.1109/EDOCW.2013.21;Conferences;2325-6605;978-1-4799-3048-7
Strategic Alignment of Cloud-Based Architectures for Big Data;Big Data is an increasingly significant topic for management and IT departments. In the beginning, Big Data applications were large on premise installations. Today, cloud services are used increasingly to implement Big Data applications. This can be done on different ways supporting different strategic enterprise goals. Therefore, we develop a framework that enumerates the alternatives for implementing Big Data applications using cloud-services and identify the strategic goals supported by these Alternatives. The created framework clarifies the options for Big Data initiatives using cloud-computing and thus improves the strategic alignment of Big Data applications.;2013;Rainer Schmidt;10.1109/EDOCW.2013.22;Conferences;2325-6605;978-1-4799-3048-7
Security — A big question for big data;Summary form only given. Big data implies performing computation and database operations for massive amounts of data, remotely from the data owner's enterprise. Since a key value proposition of big data is access to data from multiple and diverse domains, security and privacy will play a very important role in big data research and technology. The limitations of standard IT security practices are well-known, making the ability of attackers to use software subversion to insert malicious software into applications and operating systems a serious and growing threat whose adverse impact is intensified by big data. So, a big question is what security and privacy technology is adequate for controlled assured sharing for efficient direct access to big data. Making effective use of big data requires access from any domain to data in that domain, or any other domain it is authorized to access. Several decades of trusted systems developments have produced a rich set of proven concepts for verifiable protection to substantially cope with determined adversaries, but this technology has largely been marginalized as “overkill” and vendors do not widely offer it. This talk will discuss pivotal choices for big data to leverage this mature security and privacy technology, while identifying remaining research challenges.;2013;Roger Schell;10.1109/BigData.2013.6691547;Conferences;;978-1-4799-1293-3
Communication efficient algorithms for fundamental big data problems;Big Data applications often store or obtain their data distributed over many computers connected by a network. Since the network is usually slower than the local memory of the machines, it is crucial to process the data in such a way that not too much communication takes place. Indeed, only communication volume sublinear in the input size may be affordable. We believe that this direction of research deserves more intensive study. We give examples for several fundamental algorithmic problems where nontrivial algorithms with sublinear communication volume are possible. Our main technical contribution are several related results on distributed Bloom filter replacements, duplicate detection, and data base join. As an example of a very different family of techniques, we discuss linear programming in low dimensions.;2013;Peter Sanders;10.1109/BigData.2013.6691549;Conferences;;978-1-4799-1293-3
P-DOT: A model of computation for big data;"In response to the high demand of big data analytics, several programming models on large and distributed cluster systems have been proposed and implemented, such as MapRe-duce, Dryad and Pregel. However, compared with high performance computing areas, the basis and principles of computation and communication behavior of big data analytics is not well studied. In this paper, we review the current big data computational model DOT and DOTA, and propose a more general and practical model p-DOT (p-phases DOT). p-DOT is not a simple extension, but with profound significance: for general aspects, any big data analytics job execution expressed in DOT model or BSP model can be represented by it; for practical aspects, it considers I/O behavior to evaluate performance overhead. Moreover, we provide a cost function implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, and demonstrate the effectiveness of the function through several experiments.";2013;Tao Luo;10.1109/BigData.2013.6691551;Conferences;;978-1-4799-1293-3
Elastic algorithms for guaranteeing quality monotonicity in big data mining;When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.;2013;Rui Han;10.1109/BigData.2013.6691553;Conferences;;978-1-4799-1293-3
Building a generic platform for big sensor data application;The drive toward smart cities alongside the rising adoption of personal sensors is leading to a torrent of sensor data. While systems exist for storing and managing sensor data, the real value of such data is the insight which can be generated from it. However there is currently no platform which enables sensor data to be taken from collection, through use in models to produce useful data products. The architecture of such a platform is a current research question in the field of Big Data and Smart Cities. In this paper we explore five key challenges in this field and provide a response through a sensor data platform “Concinnity” which can take sensor data from collection to final product via a data repository and workflow system. This will enable rapid development of applications built on sensor data using data fusion and the integration and composition of models to form novel workflows. We summarize the key features of our approach, exploring how it enables value to be derived from sensor data efficiently.;2013;Chun-Hsiang Lee;10.1109/BigData.2013.6691559;Conferences;;978-1-4799-1293-3
clusiVAT: A mixed visual/numerical clustering algorithm for big data;Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.;2013;Dheeraj Kumar;10.1109/BigData.2013.6691561;Conferences;;978-1-4799-1293-3
Algebraic dataflows for big data analysis;Analyzing big data requires the support of dataflows with many activities to extract and explore relevant information from the data. Recent approaches such as Pig Latin propose a high-level language to model such dataflows. However, the dataflow execution is typically delegated to a MapRe-duce implementation such as Hadoop, which does not follow an algebraic approach, thus it cannot take advantage of the optimization opportunities of PigLatin algebra. In this paper, we propose an approach for big data analysis based on algebraic workflows, which yields optimization and parallel execution of activities and supports user steering using provenance queries. We illustrate how a big data processing dataflow can be modeled using the algebra. Through an experimental evaluation using real datasets and the execution of the dataflow with Chiron, an engine that supports our algebra, we show that our approach yields performance gains of up to 19.6% using algebraic optimizations in the dataflow and up to 39.1% of time saved on a user steering scenario.;2013;Jonas Dias;10.1109/BigData.2013.6691567;Conferences;;978-1-4799-1293-3
Robot: An efficient model for big data storage systems based on erasure coding;It is well-known that with the explosive growth of data, the age of big data has arrived. How to save huge amounts of data is of great importance to both industry and academia. This paper puts forward a solution based on coding technologies in big data system that store a lot of cold data. By studying existing coding technologies and big data systems, we can not only maintain the system's reliability, but also improve the security and the utilization of storage systems. Due to the remarkable reliability and space saving rate of coding technologies, importing coding schema in to big data systems becomes prerequisite. In our presented schema, the storage node is divided into several virtual nodes to keep load balancing. By setting up different virtual node storage groups for different codec server, we can ensure system availability. And by utilizing the parallel decoding computing of the node and the block of data, we can also reduce the system recovery time when data is corrupted. Additionally, different users set different coding parameters can improve the robustness of big data storage systems. We configure various data block m and calibration block k to improve the utilization rate in the quantitative experiments. The results shows that parallel decoding speed can rise up two times than the past serial decoding speed. The encoding efficiency with ICRS coding is 34.2% higher than using CRS and 56.5% more than using RS coding equally. The decoding rate by using ICRS is 18.1% higher than using CRS and 31.1% higher than using RS averagely.;2013;Chao Yin;10.1109/BigData.2013.6691569;Conferences;;978-1-4799-1293-3
Multilevel Active Storage for big data applications in high performance computing;Given the growing importance of supporting dataintensive sciences and big data applications, an effective HPC I/O solution has become a key issue and has attracted intensive attention in recent years. Active storage has been shown effective in reducing data movement and network traffic as a potential new I/O solution. Existing prototypes and systems, however, are primarily designed for read-intensive applications. In addition, they generally assume that offloaded processing kernels have small computational demands, which makes this solution a poor fit for data-intensive operations that have significant computational demands, including write-intensive operations. In this research, we propose a new Multilevel Active Storage (MAS) solution. The new MAS design can support and handle both read- and write-intensive operations, as well as complex operations that have considerable computational demands. Experimental tests have been carried out and confirmed that the MAS approach is feasible and outperformed existing approaches. The new multilevel active storage design has a potential to deliver a high performance I/O solution for big data applications in HPC.;2013;Chao Chen;10.1109/BigData.2013.6691570;Conferences;;978-1-4799-1293-3
GPU accelerated item-based collaborative filtering for big-data applications;"Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.";2013;Chandima Hewa Nadungodage;10.1109/BigData.2013.6691571;Conferences;;978-1-4799-1293-3
Using pattern-models to guide SSD deployment for Big Data applications in HPC systems;"Flash-memory based Solid State Drives (SSDs) embrace higher performance and lower power consumption compared to traditional storage devices (HDDs). These benefits are needed in HPC systems, especially with the growing demand of supporting Big Data applications. In this paper, we study placement and deployment strategies of SSDs in HPC systems to maximize the performance improvement, given a practical fixed hardware budget constraint. We propose a pattern-model approach to guide SSD deployment for HPC systems through two steps; characterizing workload and mapping deployment strategy. The first step is responsible for characterizing the access patterns of the workload and the second step contributes the actual deployment recommendation for Parallel File System (PFS) configuration combining with an analytical model. We have carried out initial experimental tests and the results confirmed that the proposed approach can guide placement of SSDs in HPC systems for accelerating data accesses. Our research will be helpful in guiding designs and developments for Big Data applications in current and projected HPC systems including exascale systems.";2013;Junjie Chen;10.1109/BigData.2013.6691592;Conferences;;978-1-4799-1293-3
Efficient large graph pattern mining for big data in the cloud;Mining big graph data is an important problem in the graph mining research area. Although cloud computing is effective at solving traditional algorithm problems, mining frequent patterns of a massive graph with cloud computing still faces the three challenges: 1) the graph partition problem, 2) asymmetry of information, and 3) pattern-preservation merging. Therefore, this paper presents a new approach, the cloud-based SpiderMine (c-SpiderMine), which exploits cloud computing to process the mining of large patterns on big graph data. The proposed method addresses the above issues for implementing a big graph data mining algorithm in the cloud. We conduct the experiments with three real data sets, and the experimental results demonstrate that c-SpiderMine can significantly reduce execution time with high scalability in dealing with big data in the cloud.;2013;Chun-Chieh Chen;10.1109/BigData.2013.6691618;Conferences;;978-1-4799-1293-3
A Higher-order data flow model for heterogeneous Big Data;We introduce a data flow model that supports highly parallelisable design patterns and also has useful properties for analysing data serially over extended time periods without requiring traditional Big Data computing facilities. The model ranges over a class of higher-order relations which are sufficiently expressive to represent a wide variety of unstructured, semi-structured and structured data. Using JSONMatch, our web service implementation of the model, we show that the combination of this model and higher-order representation provides a powerful and extensible framework that is particularly well suited to analysing Big Variety data in a web application context.;2013;Simon Price;10.1109/BigData.2013.6691624;Conferences;;978-1-4799-1293-3
Breaking the Arc: Risk control for Big Data;"The use of Big Data technologies and analytics have the potential to revolutionise the world. The mass instrumentation of the planet and society is providing intelligence that is not only enhancing our personal lives, but also opening up new opportunities for addressing some of key environmental, social and economic challenges of the 21st century. Unfortunately, as with all technology, there is the potential for misuse; in the case of personal data the ability to gather, enrich and mine at extreme pace and volume could result in societal-scale privacy intrusions. We apply a model for identity across cyber and physical spaces to the question of risk control for personal-data in the context of big data analytics. Using a graphical model for identity we reflect on the response options we have and how such risk controls may or may not be effective.";2013;Duncan Hodges;10.1109/BigData.2013.6691630;Conferences;;978-1-4799-1293-3
The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation;"The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10-6) and very poor (102) selectivity, and short (seconds) to long (hours) job duration.";2013;Tim Hegeman;10.1109/BigData.2013.6691631;Conferences;;978-1-4799-1293-3
Big data analytics on high Velocity streams: A case study;"Big data management is often characterized by three Vs: Volume, Velocity and Variety. While traditional batch-oriented systems such as MapReduce are able to scale-out and process very large volumes of data in parallel, they also introduce some significant latency. In this paper, we focus on the second V (Velocity) of the Big Data triad; We present a case-study where we use a popular open-source stream processing engine (Storm) to perform real-time integration and trend detection on Twitter and Bitly streams. We describe our trend detection solution below and experimentally demonstrate that our architecture can effectively process data in real-time - even for high-velocity streams.";2013;Thibaud Chardonnens;10.1109/BigData.2013.6691653;Conferences;;978-1-4799-1293-3
Visualization and rhetoric: Key concerns for utilizing big data in humanities research: A case study of vaccination discourses: 1918-1919;Visualization of data mining results is the linchpin of successful research in the humanities that uses computational techniques. This paper describes efforts to utilize “big data” in a case study of news reporting on vaccination before, during, and after the 1918 influenza pandemic, focusing primarily on the conventions underlying methods of data extraction, data visualization practices, and the rhetorical impact of visualization design choices on researchers' observations and interpretive decisions. Purposeful attention to visualization and the methodological conventions that are embedded in particular visualization practices will allow humanists to have more confidence in their interpretations of big data, a key element in the acceptance of data mining as a valuable method for humanities research.;2013;Kathleen Kerr;10.1109/BigData.2013.6691666;Conferences;;978-1-4799-1293-3
Humanities ‘big data’: Myths, challenges, and lessons;This paper argues that there have always been `big data' in the humanities, and challenges commonly held myths in this regard. It does so by discussing the case of transnational research on dispersed communities. Concluding, it examines the lessons humanities and sciences can learn from each other.;2013;Amalia S. Levi;10.1109/BigData.2013.6691667;Conferences;;978-1-4799-1293-3
Bibliographic records as humanities big data;Most discussion hitherto of big data in the humanities has assumed that it is characterized by its heterogeneous nature. This paper examines the extent to which bibliographic records generated by libraries represent a more homogenous form of humanities big data, more closely related to the observational big data generated by scientific data. It is suggested from an examination of the British Library catalogue that, while superficially bibliographic records appear to be created according to consistent standards and form a more homogenous dataset, close examination reveals that bibliographical records often go through a marked process of historical development. However, the critical methods require to disaggregate such data are perhaps analogous to those used in some scientific disciplines.;2013;Andrew Prescott;10.1109/BigData.2013.6691670;Conferences;;978-1-4799-1293-3
A concept of Generic Workspace for Big Data Processing in Humanities;Big Data challenges often require application of new data processing paradigms (like MapReduce), and corresponding software solutions (e. g. Hadoop). This trend causes a pressure on both cyber-infrastructure providers (to quickly integrate new services) and infrastructure users (to quickly learn to use new tools). In this paper we present the concept of DARIAH Generic Workspace for Big Data Processing in eHumanities which alleviates the aforementioned problems. It establishes a common integration layer, thus enables a quick integration of new services, and by providing unified interfaces, allows the users to start using new tools without learning their internal details. We describe the overall architecture and implementation details of the working prototype. The presented concept is generic enough to be applied in other emerging cyber-infrastructures for humanities.;2013;Jedrzej Rybicki;10.1109/BigData.2013.6691672;Conferences;;978-1-4799-1293-3
A case study on entity Resolution for Distant Processing of big Humanities data;At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.;2013;Weijia Xu;10.1109/BigData.2013.6691678;Conferences;;978-1-4799-1293-3
Business model canvas perspective on big data applications;Large and complex data that becomes difficult to be handled by traditional data processing applications triggers the development of big data applications which have become more pervasive than ever before. In the era of big data, data exploration and analysis turned into a difficult problem in many sectors such as the smart routing and health care sectors. Companies which can adapt their businesses well to leverage big data have significant advantages over those that lag this capability. The need for exploring new approaches to address the challenges of big data forces companies to shape their business models accordingly. In this paper, we summarize and share our findings regarding the business models deployed in big data applications in different sectors. We analyze existing big data applications by taking into consideration the core elements of a business (via business model canvas) and present how these applications provide value to their customers by making profit out of using big data.;2013;F. Canan Pembe Muhtaroğlu;10.1109/BigData.2013.6691684;Conferences;;978-1-4799-1293-3
Understanding the value of (big) data;This paper acts as a primer on an economic outlook at the value and pricing of big data. We introduce a simple taxonomy, discuss rights to access and analyze the case of big data as a common pool resource.;2013;Koutroumpis Pantelis;10.1109/BigData.2013.6691691;Conferences;;978-1-4799-1293-3
Memory system characterization of big data workloads;Two recent trends that have emerged include (1) Rapid growth in big data technologies with new types of computing models to handle unstructured data, such as map-reduce and noSQL (2) A growing focus on the memory subsystem for performance and power optimizations, particularly with emerging memory technologies offering different characteristics from conventional DRAM (bandwidths, read/write asymmetries). This paper examines how these trends may intersect by characterizing the memory access patterns of various Hadoop and noSQL big data workloads. Using memory DIMM traces collected using special hardware, we analyze the spatial and temporal reference patterns to bring out several insights related to memory and platform usages, such as memory footprints, read-write ratios, bandwidths, latencies, etc. We develop an analysis methodology to understand how conventional optimizations such as caching, prediction, and prefetching may apply to these workloads, and discuss the implications on software and system design.;2013;Martin Dimitrov;10.1109/BigData.2013.6691693;Conferences;;978-1-4799-1293-3
An ensemble MIC-based approach for performance diagnosis in big data platform;The era of big data has began. Although applications based on big data bring considerable benefit to IT industries, governments and social organizations, they bring more challenges to the management of big data platforms which are the fundamental infrastructures due to the complexity, variety, velocity and volume of big data. To offer a healthy platform for big data applications, we propose a novel signature-based performance diagnosis approach employing MIC invariants between performance metrics. We formalize the performance diagnosis as a pattern recognition problem. The normal state of a big data application is used to train a set of MIC (Maximum Information Criterion) invariants. One performance problem occurred in the big data application is identified by a unique binary tuple consisted by a set violations of MIC invariants. All the signatures of performance problems form a diagnosis knowledge database. If the KPI (Key Performance Indicator) of the big data application deviates its normal region, our approach can identify the real culprits through looking for similar signatures in the signature database. To detect the deviation of the KPI, we propose a new metric named unpredictability based on ARIMA model. And considering the variety of big data applications, we build an ensemble performance diagnosis approach which means a unique ARIMA model and a unique set of MIC invariants are built for a specific kind of application. Through experiment evaluation in a controlled environment running a state of the art big data benchmark, we find our approach can pinpoint the real culprits of performance problems in an average 83% precision and 87% recall which is better than a correlation based and single model based performance diagnosis.;2013;Pengfei Chen;10.1109/BigData.2013.6691701;Conferences;;978-1-4799-1293-3
The implications from benchmarking three big data systems;Along with today's data explosion and application diversification, a variety of hardware platforms for data centers are emerging and are attracting interests from both industry and academia. The existing hardware platforms represent a wide range of implementation approaches, and different hardware have different strengths. In this paper, we conduct comprehensive evaluations on three representative data center systems based on BigDataBench, which is a benchmark suite for benchmarking and ranking systems running big data applications. Then we explore the relative performance of the three implementation approaches with different big data applications, and provide strong guidance for the data center system construction. Through our experiments, we has inferred that a data center system based on specific hardware has different performance in the context of different applications and data volumes. When we construct a system, we can take into account not only the performance or energy consumption of the pure hardwares, but also the application-level characteristics. Data scale, application type and complexity should be considered comprehensively when researchers or architects plan to choose fundamental components for their data center system.;2013;Jing Quan;10.1109/BigData.2013.6691706;Conferences;;978-1-4799-1293-3
A characterization of big data benchmarks;Recently, big data has been evolved into a buzzword from academia to industry all over the world. Benchmarks are important tools for evaluating an IT system. However, benchmarking big data systems is much more challenging than ever before. First, big data systems are still in their infant stage and consequently they are not well understood. Second, big data systems are more complicated compared to previous systems such as a single node computing platform. While some researchers started to design benchmarks for big data systems, they do not consider the redundancy between their benchmarks. Moreover, they use artificial input data sets rather than real world data for their benchmarks. It is therefore unclear whether these benchmarks can be used to precisely evaluate the performance of big data systems. In this paper, we first analyze the redundancy among benchmarks from ICTBench, HiBench and typical workloads from real world applications: spatio-temporal data analysis for Shenzhen transportation system. Subsequently, we present an initial idea of a big data benchmark suite for spatio-temporal data. There are three findings in this work: (1) redundancy exists in these pioneering benchmark suites and some of them can be removed safely. (2) The workload behavior of trajectory data analysis applications is dramatically affected by their input data sets. (3) The benchmarks created for academic research cannot represent the cases of real world applications.;2013;Wen Xiong;10.1109/BigData.2013.6691707;Conferences;;978-1-4799-1293-3
A big data analytics framework for scientific data management;The Ophidia project is a research effort addressing big data analytics requirements, issues, and challenges for eScience. We present here the Ophidia analytics framework, which is responsible for atomically processing, transforming and manipulating array-based data. This framework provides a common way to run on large clusters analytics tasks applied to big datasets. The paper highlights the design principles, algorithm, and most relevant implementation aspects of the Ophidia analytics framework. Some experimental results, related to a couple of data analytics operators in a real cluster environment, are also presented.;2013;Sandro Fiore;10.1109/BigData.2013.6691720;Conferences;;978-1-4799-1293-3
Scalable sentiment classification for Big Data analysis using Naïve Bayes Classifier;A typical method to obtain valuable information is to extract the sentiment or opinion from a message. Machine learning technologies are widely used in sentiment classification because of their ability to “learn” from the training dataset to predict or support decision making with relatively high accuracy. However, when the dataset is large, some algorithms might not scale up well. In this paper, we aim to evaluate the scalability of Naïve Bayes classifier (NBC) in large datasets. Instead of using a standard library (e.g., Mahout), we implemented NBC to achieve fine-grain control of the analysis procedure. A Big Data analyzing system is also design for this study. The result is encouraging in that the accuracy of NBC is improved and approaches 82% when the dataset size increases. We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput.;2013;Bingwei Liu;10.1109/BigData.2013.6691740;Conferences;;978-1-4799-1293-3
Frequent Itemset Mining for Big Data;Frequent Itemset Mining (FIM) is one of the most well known techniques to extract knowledge from data. The combinatorial explosion of FIM methods become even more problematic when they are applied to Big Data. Fortunately, recent improvements in the field of parallel programming already provide good tools to tackle this problem. However, these tools come with their own technical challenges, e.g. balanced data distribution and inter-communication costs. In this paper, we investigate the applicability of FIM techniques on the MapReduce platform. We introduce two new methods for mining large datasets: Dist-Eclat focuses on speed while BigFIM is optimized to run on really large datasets. In our experiments we show the scalability of our methods.;2013;Sandy Moens;10.1109/BigData.2013.6691742;Conferences;;978-1-4799-1293-3
A look at challenges and opportunities of Big Data analytics in healthcare;Big Data analytics can revolutionize the healthcare industry. It can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. This paper provides an overview of Big Data, applicability of it in healthcare, some of the work in progress and a future outlook on how Big Data analytics can improve overall quality in healthcare systems.;2013;Raghunath Nambiar;10.1109/BigData.2013.6691753;Conferences;;978-1-4799-1293-3
BIG DATA infrastructures for pharmaceutical research;Big Data is an emerging paradigm covering production, collection, processing, analysis, access and presentation of huge data-sets: Big Data infrastructures represent an opportunity to approach this paradigm on an organizational level. We describe challenges and opportunities of big data infrastructures for the pharmaceutical industry. Pharmaceutical research and product development are huge investments and require intense exploration and analysis of data. Future trends show that pharmaceutical companies need to develop new methods of data and information processing to quicker and more precisely respond to changing markets and the need of patients and providers. The individual case and patients will become more influential in healthcare delivery and in consequence for the rationale behind these decisions. Our approach of a platform for semantic exploitation of BIG DATA supports a knowledge based infrastructure for deep analysis of clinical information from structured sources and clinical narratives. This infrastructure is able to pave the way towards the necessary big data-management possibilities. Example applications for cancer research will be given.;2013;Christian Seebode;10.1109/BigData.2013.6691759;Conferences;;978-1-4799-1293-3
Big data solutions for predicting risk-of-readmission for congestive heart failure patients;Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.;2013;Kiyana Zolfaghar;10.1109/BigData.2013.6691760;Conferences;;978-1-4799-1293-3
Big spatial data mining;In this paper, spatial data mining is discussed in the context of big data. Firstly, we elaborate the fact that spatial data plays a primary role in big data, attracting academic community, business industry and governments. Secondly, the adverse of spatial data mining is discussed, such as much garbage, heavy pollution and its difficulties in utilization. Finally, we dissect the value in spatial big data, expound the techniques to discover knowledge from spatial big data, and investigate the transformation from knowledge into data intelligences.;2013;Wang Shuliang;10.1109/BigData.2013.6691764;Conferences;;978-1-4799-1293-3
IntegrityMR: Integrity assurance framework for big data analytics and management applications;Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).;2013;Yongzhi Wang;10.1109/BigData.2013.6691780;Conferences;;978-1-4799-1293-3
Exploring big data in small forms: A multi-layered knowledge extraction of social networks;Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.;2013;Yun Wei Zhao;10.1109/BigData.2013.6691784;Conferences;;978-1-4799-1293-3
Tile based visual analytics for Twitter big data exploratory analysis;New tools for raw data exploration and characterization of “big data” sets are required to suggest initial hypotheses for testing. The widespread use and adoption of web-based geo maps have provided a familiar set of interactions for exploring extremely large geo data spaces and can be applied to similarly large abstract data spaces. Building on these techniques, a tile based visual analytics system (TBVA) was developed that demonstrates interactive visualization for a one billion point Twitter dataset. TBVA enables John Tukey-inspired exploratory data analysis to be performed on massive data sets of effectively unlimited size.;2013;Daniel Cheng;10.1109/BigData.2013.6691787;Conferences;;978-1-4799-1293-3
Risk adjustment of patient expenditures: A big data analytics approach;For healthcare applications, voluminous patient data contain rich and meaningful insights that can be revealed using advanced machine learning algorithms. However, the volume and velocity of such high dimensional data requires new big data analytics framework where traditional machine learning tools cannot be applied directly. In this paper, we introduce our proof-of-concept big data analytics framework for developing risk adjustment model of patient expenditures, which uses the “divide and conquer” strategy to exploit the big-yet-rich data to improve the model accuracy. We leverage the distributed computing platform, e.g., MapReduce, to implement advanced machine learning algorithms on our data set. In specific, random forest regression algorithm, which is suitable for high dimensional healthcare data, is applied to improve the accuracy of our predictive model. Our proof-of-concept framework demonstrates the effectiveness of predictive analytics using random forest algorithm as well as the efficiency of the distributed computing platform.;2013;Lin Li;10.1109/BigData.2013.6691790;Conferences;;978-1-4799-1293-3
Big data for business managers — Bridging the gap between potential and value;Given the surge of interest in research, publication and application on Big Data over the last few years, the potential of Big Data seems to be well-established now across businesses. However, in most of the business implementations Big Data still seem to be struggling to deliver the promised value (ROI). Such results despite using the market leading Big Data solutions and talented deployment team are forcing the business managers to think what needs to be done differently. This paper lays down the framework for business managers to understand Big Data processes. Besides providing a business overview of Big Data core components, the paper presents several questions that the managers must ask to assess the effectiveness of their Big Data processes. This paper is based on the analysis of several Big Data projects that never delivered and comparison against successful ones. The hypothesis is developed based on public information and is proposed as the first step for business managers keen on effectively leveraging Big Data.;2013;Anmol Rajpurohit;10.1109/BigData.2013.6691794;Conferences;;978-1-4799-1293-3
Access control for big data using data content;Conventional database access control models have difficulties in dealing with big data, especially for the features of volume, variety and velocity. To address the problem, we introduce the Content-based Access Control (CBAC) model for content-centric information sharing. As a complement to conventional models, CBAC makes access control decisions based on the content similarity between user credentials and data content dynamically. We present an enforcement mechanism for CBAC exploiting Oracle's Virtual Private Database (VPD). Experimental results show that CBAC makes reasonable access control decision with a small overhead.;2013;Wenrong Zeng;10.1109/BigData.2013.6691798;Conferences;;978-1-4799-1293-3
Knowledge cubes — A proposal for scalable and semantically-guided management of Big Data;A Knowledge Cube, or cube for short, is an intelligent and adaptive database instance capable of storing, analyzing, and searching data. Each cube is established based on semantic aspects, e.g., (1) Topical, (2) Contextual, (3) Spatial, or (4) Temporal. A cube specializes in handling data that is only relevant to the cube's semantics. Knowledge cubes are inspired by two prime architectures: (1) Dataspaces that provides an abstraction for data management where heterogeneous data sources can co-exist and it requires no prespecified unifying schema, and (2) Linked Data that provides best practices for publishing and interlinking structured data on the web. A knowledge cube uses Linked Data as its main building block for its data layer and encompasses some of the data integration abstractions defined by Dataspaces. In this paper, knowledge cubes are proposed as a semantically-guided data management architecture, where data management is influenced by the data semantics rather than by a predefined scheme. Knowledge cubes support the five pillars of Big Data also known as the five V's, namely Volume, Velocity, Veracity, Variety, and Value. Interesting opportunities can be leveraged when learning the semantics of the data. This paper highlights these opportunities and proposes a strawman design for knowledge cubes along with the research challenges that arise when realizing them.;2013;Amgad Madkour;10.1109/BigData.2013.6691800;Conferences;;978-1-4799-1293-3
Towards Network Reduction on Big Data;"The increasing ease of data collection experience and the increasing availability of large data storage space lead to the existence of very large datasets that are commonly referred as ""Big Data"". Such data not only take over large amount of database storage, but also increase the difficulties for data analysis due to data diversity, which, also makes the datasets seemingly isolated with each other. In this paper, we present a solution to the problem that is to build up connections among the diverse datasets, based upon their similarities. Particularly, a concept of similarity graph along with a similarity graph generation algorithm were introduced. We then proposed a similarity graph reduction algorithm that reduces vertices of the graph for the purpose of graph simplification.";2013;Xing Fang;10.1109/SocialCom.2013.103;Conferences;;978-0-7695-5137-1
Big Data and Policy Design for Data Sovereignty: A Case Study on Copyright and CCL in South Korea;The purpose of this paper is as follows. First, I am trying to conceptualize big data as a social problem. Second, I would like to explain the difference between big data and conventional mega information. Third, I would like to recommend the role of the government for utilization of big data as a policy tools. Fourth, while referring to copyright and CCL(Creative Commons License) cases, I would like to explain the regulation for big data on data sovereignty. Finally, I would like to suggest a direction of policy design for big data. As for the result of this study, policy design for big data should be distinguished from policy design for mega information to solve data sovereignty issues. From a law system perspective, big data is generated autonomously. It has been accessed openly and shared without any intention. In market perspective, big data is created without any intention. Big data can be changed automatically in case of openness with reference feature such as Linked of Data. Some policy issues such as responsibility and authenticity should be raised. Big data is generated in a distributed and diverse way without any concrete form in technology perspective. So, we need a different approach.;2013;Hyejung Moon;10.1109/SocialCom.2013.165;Conferences;;978-0-7695-5137-1
Next Big Thing in Big Data: The Security of the ICT Supply Chain;In contemporary society, with supply chains becoming more and more complex, the data in supply chains increases by means of volume, variety and velocity. Big data rise in response to the proper time and conditions to offer advantages for the nodes in supply chains to solve prewiously difficult problems. For any big data project to succeed, it must first depend on high-quality data but not merely on quantity. Further, it will become increasingly important in many big data projects to add external data to the mix and companies will eventually turn from only looking inward to also looking outward into the market, which means the use of big data must be broadened considerably. Hence the data supply chains, both internally and externally, become of prime importance. ICT (Information and Telecommunication) supply chain management is especially important as supply chain link the world closely and ICT supply chain is the base of all supply chains in today's world. Though many initiatives to supply chain security have been developed and taken into practice, most of them are emphasized in physical supply chain which is addressed in transporting cargos. The research on ICT supply chain security is still in preliminary stage. The use of big data can promote the normal operation of ICT supply chain as it greatly improve the data collecting and processing capacity and in turn, ICT supply chain is a necessary carrier of big data as it produces all the software, hardware and infrastructures for big data's collection, storage and application. The close relationship between big data and ICT supply chain make it an effective way to do research on big data security through analysis on ICT supply chain security. This paper first analyzes the security problems that the ICT supply chain is facing in information management, system integrity and cyberspace, and then introduces several famous international models both on physical supply chain and ICT supply chain. After that the authors describe a case of communication equipment with big data in ICT supply chain and propose a series of recommendations conducive to developing secure big data supply chain from five dimensions.;2013;Tianbo Lu;10.1109/SocialCom.2013.172;Conferences;;978-0-7695-5137-1
Analysis of Big Data Technologies and Method - Query Large Web Public RDF Datasets on Amazon Cloud Using Hadoop and Open Source Parsers;Extremely large datasets found in Big Data projects are difficult to work with using conventional databases, statistical software, and visualization tools. Massively parallel software, such as Hadoop, running on tens, hundreds, or even thousands of servers is more suitable for Big Data challenges. Additionally, in order to achieve the highest performance when querying large datasets, it is necessary to work these datasets at rest without preprocessing or moving them into a repository. Therefore, this work will analyze tools and techniques to overcome working with large datasets at rest. Parsing and querying will be done on the raw dataset - the untouched Web Data Commons RDF files. Web Data Commons comprises five billion pages of web pages crawled from the Internet. This work will analyze available tools and appropriate methods to assist the Big Data developer in working with these extremely large, semantic RDF datasets. Hadoop, open source parsers, and Amazon Cloud services will be used to data mine these files. In order to assist in further discovery, recommendations for future research will be included.;2013;Ted Garcia;10.1109/ICSC.2013.49;Conferences;;978-0-7695-5119-7
An improved BP algorithm over out-of-order streams for big data;Due to the difficulty of getting the association rules over out-of-order streams for big data, a new improved BP algorithm based on dynamic adjustment is proposed. We firstly use a dynamic adaptive structural adjustment mechanism to change the network training structure according to the environmental requirements, which can automatically remove invalid training node, and optimize the iterative training process. Secondly, we adjust three factors (i.e. learning index, momentum factor and scaling factor) during the learning process to speed up the learning response, and to enhance the stability of the network. Simulation results show that compared with traditional BP algorithm, this algorithm can get more convergence times,the convergence rate can be improved effectively, and finally obtain the association rules over out-of-order data streams.;2013;Kun Wang;10.1109/ChinaCom.2013.6694712;Conferences;;978-1-4799-1406-7
Advanced analytics for harnessing the power of smart meter big data;Smart meters or advanced metering infrastructure (AMI) are being deployed in many countries around the world. Smart meters are the basic building block of the smart grid and governments have invested vast amounts in smart meter deployment targeting wide economic, social and environmental benefits. The key functionality of the smart meter is the capture and transfer of data relating to the consumption (electricity, gas) and events such as power quality and meter status. Such capability has also resulted in the generation of an unprecedented data volume, speed of collection and complexity, which has resulted in the so called big data challenge. To realize the hidden value and power in such data, it is important to use the appropriate tools and technology which are currently being called advanced analytics. In this paper we define a smart metering landscape and discuss different technologies available for harnessing the smart meter captured data. Main limitations and challenges with existing techniques with big data are also highlighted and several future directions in smart metering are presented.;2013;Damminda Alahakoon;10.1109/IWIES.2013.6698559;Conferences;;978-1-4799-1135-6
The Oklahoma PetaStore: Big data on a small budget;In the era of Big Data, research productivity can be highly sensitive to the availability of large scale, long term storage. Unfortunately, most mass storage systems are prohibitively expensive at scales appropriate for individual institutions rather than national centers. Furthermore, many researchers won't adopt any centralized technology that they perceive as more expensive than what their research teams could do on their own. This poster examines a combination of business model and technology that addresses these concerns in a comprehensive way, distributing the costs among a funding agency, the institution and the research teams, thereby reducing the challenges faced by each.;2013;Patrick Calhoun;10.1109/CLUSTER.2013.6702629;Conferences;2168-9253;978-1-4799-0898-1
HcBench: Methodology, development, and characterization of a customer usage representative big data/Hadoop benchmark;"Big Data analytics using Map-Reduce over Hadoop has become a leading edge paradigm for distributed programming over large server clusters. The Hadoop platform is used extensively for interactive and batch analytics in ecommerce, telecom, media, retail, social networking, and being actively evaluated for use in other areas. However, to date no industry standard or customer representative benchmarks exist to measure and evaluate the true performance of a Hadoop cluster. Current Hadoop micro-benchmarks such as HiBench-2, GridMix-3, Terasort, etc. are narrow functional slices of applications that customers run to evaluate their Hadoop clusters. However, these benchmarks fail to capture the real usages and performance in a datacenter environment. Given that typical datacenter deployments of Hadoop process a wide variety of analytic interactive and query jobs in addition to batch transform jobs under strict Service Level Agreement (SLA) requirements, performance benchmarks used to evaluate clusters must capture the effects of concurrently running such diverse job types in production environments. In this paper, we present the methodology and the development of a customer datacenter usage representative Hadoop benchmark ""HcBench"" which includes a mix of large number of customer representative interactive, query, machine learning, and transform jobs, a variety of data sizes, and includes compute, storage 110, and network intensive jobs, with inter-job arrival times as in a typical datacenter environment. We present the details of this benchmark and discuss application level, server and cluster level performance characterization collected on an Intel Sandy Bridge Xeon Processor Hadoop cluster.";2013;Vikram A. Saletore;10.1109/IISWC.2013.6704672;Conferences;;978-1-4799-0553-9
Characterizing the efficiency of data deduplication for big data storage management;The demand for data storage and processing is increasing at a rapid speed in the big data era. Such a tremendous amount of data pushes the limit on storage capacity and on the storage network. A significant portion of the dataset in big data workloads is redundant. As a result, deduplication technology, which removes replicas, becomes an attractive solution to save disk space and traffic in a big data environment. However, the overhead of extra CPU computation (hash indexing) and IO latency introduced by deduplication should be considered. Therefore, the net effect of using deduplication for big data workloads needs to be examined. To this end, we characterize the redundancy of typical big data workloads to justify the need for deduplication. We analyze and characterize the performance and energy impact brought by deduplication under various big data environments. In our experiments, we identify three sources of redundancy in big data workloads: 1) deploying more nodes, 2) expanding the dataset, and 3) using replication mechanisms. We elaborate on the advantages and disadvantages of different deduplication layers, locations, and granularities. In addition, we uncover the relation between energy overhead and the degree of redundancy. Furthermore, we investigate the deduplication efficiency in an SSD environment for big data workloads.;2013;Ruijin Zhou;10.1109/IISWC.2013.6704674;Conferences;;978-1-4799-0553-9
Big data and humanitarian supply networks: Can Big Data give voice to the voiceless?;"Billions of US dollars are spent each year in emergency aid to save lives and alleviate the suffering of those affected by disaster. This aid flows through a humanitarian system that consists of governments, different United Nations agencies, the Red Cross movement and myriad non-governmental organizations (NGOs). As scarcer resources, financial crisis and economic inter-dependencies continue to constrain humanitarian relief there is an increasing focus from donors and governments to assess the impact of humanitarian supply networks. Using commercial (`for-profit') supply networks as a benchmark; this paper exposes the counter-intuitive competition dynamic of humanitarian supply networks, which results in an open-loop system unable to calibrate supply with actual need and impact. In that light, the phenomenon of Big Data in the humanitarian field is discussed and an agenda for the `datafication' of the supply network set out as a means of closing the loop between supply, need and impact.";2013;Asmat Monaghan;10.1109/GHTC.2013.6713725;Conferences;;978-1-4799-2401-1
A predictive tool for nonattendance at a specialty clinic: An application of multivariate probabilistic big data analytics;The field of data analytics has recently garnered significant attention as a means of improving service, outcome and cost in healthcare. Health informatics tools to date are largely rules-based, static in knowledge-gathering capability, and are not well incorporated within standard clinical processes. We introduce a multivariate analysis tool, built at the user-client interface, directly from data extracted from thousands of EMRs, for automated decision making. The predictive algorithms created to date have been implemented in a variety of clinical scenarios, including ER triage of chest pain, selection of optimal diagnostic testing, and management of chronic illnesses. This technology has now been applied to prediction of patient compliance with scheduled appointments in a medical specialty clinic. Approximately 16% of scheduled VA patients do not show for their scheduled appointments for Gastroenterology clinic. This results in misallocation of provider resources and inefficiencies in health care delivery. The improvement in quality of care, patient outcomes and cost is potentially immense.;2013;Victor Levy;10.1109/CEWIT.2013.6713760;Conferences;;978-1-4799-2546-9
KASR: A Keyword-Aware Service Recommendation Method on MapReduce for Big Data Applications;Service recommender systems have been shown as valuable tools for providing appropriate recommendations to users. In the last decade, the amount of customers, services and online information has grown rapidly, yielding the big data analysis problem for service recommender systems. Consequently, traditional service recommender systems often suffer from scalability and inefficiency problems when processing or analysing such large-scale data. Moreover, most of existing service recommender systems present the same ratings and rankings of services to different users without considering diverse users' preferences, and therefore fails to meet users' personalized requirements. In this paper, we propose a Keyword-Aware Service Recommendation method, named KASR, to address the above challenges. It aims at presenting a personalized service recommendation list and recommending the most appropriate services to the users effectively. Specifically, keywords are used to indicate users' preferences, and a user-based Collaborative Filtering algorithm is adopted to generate appropriate recommendations. To improve its scalability and efficiency in big data environment, KASR is implemented on Hadoop, a widely-adopted distributed computing platform using the MapReduce parallel processing paradigm. Finally, extensive experiments are conducted on real-world data sets, and results demonstrate that KASR significantly improves the accuracy and scalability of service recommender systems over existing approaches.;2014;Shunmei Meng;10.1109/TPDS.2013.2297117;Journals;2161-9883;
A Time Efficient Approach for Detecting Errors in Big Sensor Data on Cloud;Big sensor data is prevalent in both industry and scientific research applications where the data is generated with high volume and velocity it is difficult to process using on-hand database management tools or traditional data processing applications. Cloud computing provides a promising platform to support the addressing of this challenge as it provides a flexible stack of massive computing, storage, and software services in a scalable manner at low cost. Some techniques have been developed in recent years for processing sensor data on cloud, such as sensor-cloud. However, these techniques do not provide efficient support on fast detection and locating of errors in big sensor data sets. For fast data error detection in big sensor data sets, in this paper, we develop a novel data error detection approach which exploits the full computation potential of cloud platform and the network feature of WSN. Firstly, a set of sensor data error types are classified and defined. Based on that classification, the network feature of a clustered WSN is introduced and analyzed to support fast error detection and location. Specifically, in our proposed approach, the error detection is based on the scale-free network topology and most of detection operations can be conducted in limited temporal or spatial data blocks instead of a whole big data set. Hence the detection and location process can be dramatically accelerated. Furthermore, the detection and location tasks can be distributed to cloud platform to fully exploit the computation power and massive storage. Through the experiment on our cloud computing platform of U-Cloud, it is demonstrated that our proposed approach can significantly reduce the time for error detection and location in big data sets generated by large scale sensor network systems with acceptable error detecting accuracy.;2015;Chi Yang;10.1109/TPDS.2013.2295810;Journals;2161-9883;
Attribute Relationship Evaluation Methodology for Big Data Security;There has been an increasing interest in big data and big data security with the development of network technology and cloud computing. However, big data is not an entirely new technology but an extension of data mining. In this paper, we describe the background of big data, data mining and big data features, and propose attribute selection methodology for protecting the value of big data. Extracting valuable information is the main goal of analyzing big data which need to be protected. Therefore, relevance between attributes of a dataset is a very important element for big data analysis. We focus on two things. Firstly, attribute relevance in big data is a key element for extracting information. In this perspective, we studied on how to secure a big data through protecting valuable information inside. Secondly, it is impossible to protect all big data and its attributes. We consider big data as a single object which has its own attributes. We assume that a attribute which have a higher relevance is more important than other attributes.;2013;Sung-Hwan Kim;10.1109/ICITCS.2013.6717808;Conferences;;978-1-4799-2845-3
Secured e-health data retrieval in DaaS and Big Data;Big Data is one of rising IT trends such as cloud computing, social networking or ubiquitous computing. Big Data can offer beneficial scenarios in the e-health arena. However, one of the scenarios can be that Big Data needs to be kept secured for a long time in order to gain its benefits such as finding cures for infectious diseases and keeping patients' privacy. From this connection, it is beneficial to analyze Big Data to make meaningful information while the data are stored in a secure manner. Thus, the analysis of various database encryption techniques is essential. In this study, we simulated 3 types of technical environments such as Plain-text, Microsoft Built-in Encryption, and custom Advanced Encryption Standard using Bucket Index in Data-as-a-Service. The results showed that custom AES-DaaS has faster range query response time than MS built-in encryption. In addition, while carrying out the scalability test, we acknowledged there are performance thresholds according to physical IT resources. Therefore, for the purpose of efficient Big Data management in e-health, it is noteworthy to examine its scalability limits as well even if it is under cloud computing environment. Furthermore, when designing an e-health database, both patients' privacy and system performance needs to be dealt as top priorities.;2013;David Shin;10.1109/HealthCom.2013.6720677;Conferences;;978-1-4673-5800-2
Applying Sensor Web strategies to Big Data earth observations;Although the focus of the Sensor Web has been somewhat limited to a single architectural view in the form of web services and service oriented architectures our experience has shown in a number of projects that this is not always the most effective solution, especially when deal with Big Data. The correct approach is then to hold to the overall vision of the Sensor Web as a way of gaining access to and organising sensors and sensor data and to use the appropriate architectural patterns and strategies in overcoming the challenges presented. Using these other approaches is not necessarily conflict with an open systems view nor is it non web centric.;2013;Terence L. van Zyl;10.1109/IGARSS.2013.6721278;Conferences;2153-7003;978-1-4799-1114-1
Big Data and the bright future of simulation — The case of agent-based modeling;Big Data may be an ugly word describing a diverse reality, but it also points to a bright future for simulation in general, and Agent-Based Modeling (ABM) in particular. As companies are struggling to make sense of the staggering amounts of data they have been amassing, data-driven simulation will be the backbone of how value is discovered and captured from data. Drawing from successful applications of ABM, I will make it explicit that what used to be an afterthought of simulation modeling (calibration) is now the cornerstone of the Big Data edifice.;2013;Eric Bonabeau;10.1109/WSC.2013.6721399;Conferences;1558-4305;978-1-4799-2077-8
Big Data Framework;We are constantly being told that we live in the Information Era - the Age of BIG data. It is clearly apparent that organizations need to employ data-driven decision making to gain competitive advantage. Processing, integrating and interacting with more data should make it better data, providing both more panoramic and more granular views to aid strategic decision making. This is made possible via Big Data exploiting affordable and usable Computational and Storage Resources. Many offerings are based on the Map-Reduce and Hadoop paradigms and most focus solely on the analytical side. Nonetheless, in many respects it remains unclear what Big Data actually is, current offerings appear as isolated silos that are difficult to integrate and/or make it difficult to better utilize existing data and systems. Paper addresses this lacunae by characterising the facets of Big Data and proposing a framework in which Big Data applications can be developed. The framework consists of three Stages and seven Layers to divide Big Data application into modular blocks. The aim is to enable organizations to better manage and architect a very large Big Data application to gain competitive advantage by allowing management to have a better handle on data processing.;2013;Firat Tekiner;10.1109/SMC.2013.258;Conferences;1062-922X;978-1-4799-0652-9
MapReduce Based Method for Big Data Semantic Clustering;Big data analysis is very hot in cloud computing environments. How to automatically map heterogeneous data with the same semantics is one of the key problems in big data analysis. A big data clustering method based on the MapReduce framework is proposed in this paper. Big data are decomposed into many data chunks for parallel clustering, which is implemented by Ant Colony. Data elements are moved and clustered by ants according to the presented criterion. The proposed method is compared with the MapReduce framework based k-means clustering algorithm on a great amount of practical data. Experimental results show that the proposal is much effective for big data clustering.;2013;Jie Yang;10.1109/SMC.2013.480;Conferences;1062-922X;978-1-4799-0652-9
A Reduction Algorithm for the Big Data in 3D Surface Reconstruction;As big data acquisition and storage becomes increasingly affordable, especially in the modern range sensing technology for the scans of complex objects, it is a challenge to reconstruct the surface of 3D geometric model effectively and precisely. In this paper, we describe a reduction method for the big data with noises in the 3D surface reconstruction based on partition of unity, Hermite radial basis functions, and sparse regularization. The proposed method not only provides an approach for pruning some redundant data according to the sparsity, but also contains a good robustness to the noises. This approach can be regarded as one of effective methods for processing big data. Experimental results are also provided.;2013;Jianwei Zhao;10.1109/SMC.2013.824;Conferences;1062-922X;978-1-4799-0652-9
Road traffic big data collision analysis processing framework;With the advancement of sensor technologies, big data processing becomes a new paradigm for large scale information processing. Big data comes from different sources, such as from traffic information, social sites, mobile phone GPS signals and so on. In this paper, we propose a new architecture for distributed processing that enables big data processing on the road traffic data and its related information analysis. We applied Hadoop and HBase that can store and analyze real-time collision data in a distributed processing framework. This framework is designed as flexible and scalable framework using distributed CEP that process massive real-time traffic data and ESB that integrates other services. We tested the proposed framework on road traffic data on a 45-mile section of I-880N freeway CA, USA. By integrating freeway traffic big data and collision data over a ten-year period (1TB Size), we obtained the collision probability.;2013;Duckwon Chung;10.1109/ICAICT.2013.6722733;Conferences;;978-1-4673-6419-5
VegaIndexer: A Distributed composite index scheme for big spatio-temporal sensor data on cloud;With the prevalence of data-intensive geospatial applications, massive spatio-temporal sensor data are obtained and the big data have posed grand challenges on existing index methods based on spatial databases due to their intrinsic poor scalability and retrieval efficiency. Motivated by the deficiencies, in this paper, we propose a distributed composite spatio-temporal index scheme called VegaIndexer for efficiently answering queries from large collections of space-time sensor data. Firstly, we present a distributed spatio-temporal indexing architecture based on cloud platform which consists of global index and local index. Moreover, we propose Multi-version Distributed enhanced R+ (MDR+) tree algorithm for accelerating data retrieval and spatio-temporal query efficiency. Furthermore, we design a MapReduce-based parallel processing approach of batch constructing indices for big spatiotemporal sensor data. In addition, we implement VegaIndexer middleware on top of the leading cloud platform, i.e., Hadoop and associated NoSQL database. The experimental experiments show that VegaIndexer outperforms the index methods of typical spatial databases.;2013;Yunqin Zhong;10.1109/IGARSS.2013.6723126;Conferences;2153-7003;978-1-4799-1114-1
Game theory applied to big data analytics in geosciences and remote sensing;This paper introduces the basic concepts of game theory and outlines the mechanisms for applying game theory models to big data analytics and decision making in the field of geosciences and remote sensing. The author proposes the use of strategic, competitive game theory models for the purpose of spectral band grouping when exploiting hyperspectral imagery. The proposed system uses conflict data filtering based on mutual entropy and a strategy interaction process of multiple band groups in a conflict environment, the goal of which is to maximize the payoff benefit of multiple groups of the whole system. The proposed system uses the Nash equilibrium as the means to find a steady state solution to the band grouping problem, and implements the model under the assumption that all players are rational. The author uses the proposed band grouping as a component in a multi-classifier decision fusion (MCDF) system for automated ground cover classification with hyperspectral imagery. The paper provides experimental results demonstrating that the proposed game theoretic approach significantly outperforms the comparison methods.;2013;Lori Mann Bruce;10.1109/IGARSS.2013.6723733;Conferences;2153-7003;978-1-4799-1114-1
A cloud-based architecture for Big-Data analytics in smart grid: A proposal;A Smart Grid is an enhanced version of electric grid in which the demand and supply are balanced to meet the customers need. The paper deals with the formation of a cloud-based Smart Grid for analyzing the Bid-Data and taking decisions to balance the demand of customer needs. The proposed formation of smart grid will deal with Big Data set which will contain the data regarding the power usage patterns of customers, historic weather data of the location, the current demand and supply details. The grid will operate on the data being fetched from the cloud storage. The paper also focuses on smart grid being framed with the renewable energy sources.;2013;M. Mayilvaganan;10.1109/ICCIC.2013.6724168;Conferences;;978-1-4799-1595-8
Forecasting consumer behavior with innovative value proposition for organizations using big data analytics;The term `Big Data' is used to represent collection of such a huge amount of data that it becomes impossible to manage and process data using conventional database management tools. Big Data is defined by three important parameters `Volume' - Size of Data, `Velocity' - Speed of increase of data and `Variety' - Type of Data. Big data analytics is the process of analyzing this ever growing Big Data. The goal of every organization is to maximize its value for its stake holders. The paper aims to demonstrate that Big data analytics can be used as a catalyst for generating and increasing value for organizations by improving various business parameters. Furthermore, by utilizing case studies the paper also aims to establish that big data analytics supports creation, enhancement and improvement of various business services to significantly improve customer experience as well as value creation for organizations.;2013;Ankur Balar;10.1109/ICCIC.2013.6724280;Conferences;;978-1-4799-1595-8
Security Analytics: Big Data Analytics for cybersecurity: A review of trends, techniques and tools;The rapid growth of the Internet has brought with it an exponential increase in the type and frequency of cyber attacks. Many well-known cybersecurity solutions are in place to counteract these attacks. However, the generation of Big Data over computer networks is rapidly rendering these traditional solutions obsolete. To cater for this problem, corporate research is now focusing on Security Analytics, i.e., the application of Big Data Analytics techniques to cybersecurity. Analytics can assist network managers particularly in the monitoring and surveillance of real-time network streams and real-time detection of both malicious and suspicious (outlying) patterns. Such a behavior is envisioned to encompass and enhance all traditional security techniques. This paper presents a comprehensive survey on the state of the art of Security Analytics, i.e., its description, technology, trends, and tools. It hence aims to convince the reader of the imminent application of analytics as an unparalleled cybersecurity solution in the near future.;2013;Tariq Mahmood;10.1109/NCIA.2013.6725337;Conferences;;978-1-4799-1287-2
Big data platform development with a domain specific language for telecom industries;This paper introduces a system that offer a special big data analysis platform with Domain Specific Language for telecom industries. This platform has three main parts that suggests a new kind of domain specific system for processing and visualization of large data files for telecom organizations. These parts are Domain Specific Language (DSL), Parallel Processing/Analyzing Platform for Big Data and an Integrated Result Viewer. In addition to these main parts, Distributed File Descriptor (DFD) is designed for passing information between these modules and organizing communication. To find out benefits of this domain specific solution, standard framework of big data concept is examined carefully. Big data concept has special infrastructure and tools to perform for data storing, processing, analyzing operations. This infrastructure can be grouped as four different parts, these are infrastructure, programming models, high performance schema free databases, and processing-analyzing. Although there are lots of advantages of Big Data concept, it is still very difficult to manage these systems for many enterprises. Therefore, this study suggest a new higher level language, called as DSL which helps enterprises to process big data without writing any complex low level traditional parallel processing codes, a new kind of result viewer and this paper also presents a Big Data solution system that is called Petaminer.;2013;Cüneyt Şenbalcı;10.1109/HONET.2013.6729768;Conferences;1949-4106;978-1-4799-2568-1
