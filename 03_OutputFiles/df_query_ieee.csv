search_id;title;abstract;publication_year;authors;doi;content_type;issn;isbn
ieee_20221205_08_11_42;Data system for the monitoring of power quality in the transmission substations supplying big consumers;During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.;2007;Fanica Vatra;10.1109/EPQU.2007.4424094;Conferences;2150-6655;978-84-690-9441-9
ieee_20221205_08_11_42;Gas Emergence Big Data and neural network filter;The gas supervision is a safety core of the coal mine production, which widespread existent a trouble, gas emergence big data, namely the pulse interference, the cause of the gas signal mistake alarm, is hardly resolved very often. In this paper, the reason and characteristics of gas emergence big data are analyzed to establish a kind of filter based on BP Neural Network. Through great quantities monitor data as training sample to train the network model, and tested by test samples, we get the needed network model. Results show that the model can guarantee alarm occurrence while the gas density is beyond the limit, meanwhile preventing supervision system from mistake alarm caused by pulse interference. Applying this research can promote the robustness of existing coal mine safety supervision system without update any device, which can improve the safety production in both social meaning and economic value.;2008;Li Kun;10.1109/CHICC.2008.4605537;Conferences;2161-2927;978-7-900719-70-6
ieee_20221205_08_11_42;Big Data;This introduction to the special issue on big data discusses the significant scientific opportunities offered by massive amounts of data, along with some directions for future research.;2011;Francis J. Alexander;10.1109/MCSE.2011.99;Magazines;1558-366X;
ieee_20221205_08_11_42;Entertainment in the Age of Big Data;Everybody loves a good story. People all over the world seek out and crave entertainment. This will not change in the coming age of big data and cloud computing. But stories are more than just a distraction from the big problems of the world we are all supposed to be solving. Stories are (computationally) one of the most challenging problems facing our computers. Humans are hardcoded for story. Our computers are not. Or not yet. Through our research we examine a future of entertainment where increasing computational power allows us to be genuinely connected to a truly digital world. We argue that a genuinely digital and connected future opens the door for a new kind of storytelling, and ultimately, a new way of managing and interacting with the massive data sets collected and shared by humans.;2012;Tawny Schlieski;10.1109/JPROC.2012.2189918;Journals;1558-2256;
ieee_20221205_08_11_42;Big data challenges for large radio arrays;"Future large radio astronomy arrays, particularly the Square Kilometre Array (SKA), will be able to generate data at rates far higher than can be analyzed or stored affordably with current practices. This is, by definition, a ""big data"" problem, and requires an end-to-end solution if future radio arrays are to reach their full scientific potential. Similar data processing, transport, storage, and management challenges face next-generation facilities in many other fields. The Jet Propulsion Laboratory is developing technologies to address big data issues, with an emphasis in three areas: 1) Lower-power digital processing architectures to make highvolume data generation operationally affordable, 2) Date-adaptive machine learning algorithms for real-time analysis (or ""data triage"") of large data volumes, and 3) Scalable data archive systems that allow efficient data mining and remote user code to run locally where the data are stored.1";2012;Dayton L. Jones;10.1109/AERO.2012.6187090;Conferences;1095-323X;978-1-4577-0555-7
ieee_20221205_08_11_42;Considerations for big data: Architecture and approach;The amount of data in our industry and the world is exploding. Data is being collected and stored at unprecedented rates. The challenge is not only to store and manage the vast volume of data (“big data”), but also to analyze and extract meaningful value from it. There are several approaches to collecting, storing, processing, and analyzing big data. The main focus of the paper is on unstructured data analysis. Unstructured data refers to information that either does not have a pre-defined data model or does not fit well into relational tables. Unstructured data is the fastest growing type of data, some example could be imagery, sensors, telemetry, video, documents, log files, and email data files. There are several techniques to address this problem space of unstructured analytics. The techniques share a common characteristics of scale-out, elasticity and high availability. MapReduce, in conjunction with the Hadoop Distributed File System (HDFS) and HBase database, as part of the Apache Hadoop project is a modern approach to analyze unstructured data. Hadoop clusters are an effective means of processing massive volumes of data, and can be improved with the right architectural approach.;2012;Kapil Bakshi;10.1109/AERO.2012.6187357;Conferences;1095-323X;978-1-4577-0555-7
ieee_20221205_08_11_42;From Databases to Big Data;"There is a tremendous amount of buzz around the concept of ""big data."" In this article, the author discusses the origins of this trend, the relationship between big data and traditional databases and data processing platforms, and some of the new challenges that big data presents.";2012;Sam Madden;10.1109/MIC.2012.50;Magazines;1941-0131;
ieee_20221205_08_11_42;Big data privacy issues in public social media;Big Data is a new label given to a diverse field of data intensive informatics in which the datasets are so large that they become hard to work with effectively. The term has been mainly used in two contexts, firstly as a technological challenge when dealing with dataintensive domains such as high energy physics, astronomy or internet search, and secondly as a sociological problem when data about us is collected and mined by companies such as Facebook, Google, mobile phone companies, retail chains and governments. In this paper we look at this second issue from a new perspective, namely how can the user gain awareness of the personally relevant part Big Data that is publicly available in the social web. The amount of user-generated media uploaded to the web is expanding rapidly and it is beyond the capabilities of any human to sift through it all to see which media impacts our privacy. Based on an analysis of social media in Flickr, Locr, Facebook and Google+, we discuss privacy implications and potential of the emerging trend of geo-tagged social media. We then present a concept with which users can stay informed about which parts of the social Big Data deluge is relevant to them.;2012;Matthew Smith;10.1109/DEST.2012.6227909;Conferences;2150-4938;978-1-4673-1701-6
ieee_20221205_08_11_42;How Different is Big Data?;One buzzword that has been popular in the last couple of years is Big Data. In simplest terms, Big Data symbolizes the aspiration to build platforms and tools to ingest, store and analyze data that can be voluminous, diverse, and possibly fast changing. In this talk, I will try to reflect on a few of the technical problems presented by the exploration of Big Data. Some of these challenges in data analytics have been addressed by our community in the past in a more traditional relational database context but only with mixed results. I will review these quests and study some of the key lessons learned. At the same time, significant developments such as the emergence of cloud infrastructure and availability of data rich web services hold the potential for transforming our industry. I will discuss the unique opportunities they present for Big Data Analytics.;2012;Surajit Chaudhuri;10.1109/ICDE.2012.153;Conferences;1063-6382;978-1-4673-0042-1
ieee_20221205_08_11_42;Temporal Analytics on Big Data for Web Advertising;"""Big Data"" in map-reduce (M-R) clusters is often fundamentally temporal in nature, as are many analytics tasks over such data. For instance, display advertising uses Behavioral Targeting (BT) to select ads for users based on prior searches, page views, etc. Previous work on BT has focused on techniques that scale well for offline data using M-R. However, this approach has limitations for BT-style applications that deal with temporal data: (1) many queries are temporal and not easily expressible in M-R, and moreover, the set-oriented nature of M-R front-ends such as SCOPE is not suitable for temporal processing, (2) as commercial systems mature, they may need to also directly analyze and react to real-time data feeds since a high turnaround time can result in missed opportunities, but it is difficult for current solutions to naturally also operate over real-time streams. Our contributions are twofold. First, we propose a novel framework called TiMR (pronounced timer), that combines a time-oriented data processing system with a M-R framework. Users write and submit analysis algorithms as temporal queries - these queries are succinct, scale-out-agnostic, and easy to write. They scale well on large-scale offline data using TiMR, and can work unmodified over real-time streams. We also propose new cost-based query fragmentation and temporal partitioning schemes for improving efficiency with TiMR. Second, we show the feasibility of this approach for BT, with new temporal algorithms that exploit new targeting opportunities. Experiments using real data from a commercial ad platform show that TiMR is very efficient and incurs orders-of-magnitude lower development effort. Our BT solution is easy and succinct, and performs up to several times better than current schemes in terms of memory, learning time, and click-through-rate/coverage.";2012;Badrish Chandramouli;10.1109/ICDE.2012.55;Conferences;1063-6382;978-1-4673-0042-1
ieee_20221205_08_11_42;Efficient Skyline Computation on Big Data;Skyline is an important operation in many applications to return a set of interesting points from a potentially huge data space. Given a table, the operation finds all tuples that are not dominated by any other tuples. It is found that the existing algorithms cannot process skyline on big data efficiently. This paper presents a novel skyline algorithm SSPL on big data. SSPL utilizes sorted positional index lists which require low space overhead to reduce I/O cost significantly. The sorted positional index list Lj is constructed for each attribute Aj and is arranged in ascending order of Aj. SSPL consists of two phases. In phase 1, SSPL computes scan depth of the involved sorted positional index lists. During retrieving the lists in a round-robin fashion, SSPL performs pruning on any candidate positional index to discard the candidate whose corresponding tuple is not skyline result. Phase 1 ends when there is a candidate positional index seen in all of the involved lists. In phase 2, SSPL exploits the obtained candidate positional indexes to get skyline results by a selective and sequential scan on the table. The experimental results on synthetic and real data sets show that SSPL has a significant advantage over the existing skyline algorithms.;2013;Xixian Han;10.1109/TKDE.2012.203;Journals;2326-3865;
ieee_20221205_08_11_42;Design Principles for Effective Knowledge Discovery from Big Data;Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.;2012;Edmon Begoli;10.1109/WICSA-ECSA.212.32;Conferences;;978-0-7695-4827-2
ieee_20221205_08_11_42;Mastiff: A MapReduce-based System for Time-Based Big Data Analytics;Existing MapReduce-based warehousing systems are not specially optimized for time-based big data analysis applications. Such applications have two characteristics: 1) data are continuously generated and are required to be stored persistently for a long period of time, 2) applications usually process data in some time period so that typical queries use time-related predicates. Time-based big data analytics requires both high data loading speed and high query execution performance. However, existing systems including current MapReduce-based solutions do not solve this problem well because the two requirements are contradictory. We have implemented a MapReduce-based system, called Mastiff, which provides a solution to achieve both high data loading speed and high query performance. Mastiff exploits a systematic combination of a column group store structure and a lightweight helper structure. Furthermore, Mastiff uses an optimized table scan method and a column-based query execution engine to boost query performance. Based on extensive experiments results with diverse workloads, we will show that Mastiff can significantly outperform existing systems including Hive, HadoopDB, and GridSQL.;2012;Sijie Guo;10.1109/CLUSTER.2012.10;Conferences;2168-9253;978-1-4673-2422-9
ieee_20221205_08_11_42;Asian Information HUB Project: NICT's R&D Vsion and Strategies for Universal Communication Technology in the Big Data Era;The Universal Communications Research Institute (UCRI), NICT conducts research and development on universal communication technologies: multi-lingual machine translation, spoken dialogue, information analysis and ultra-realistic interaction technologies, through which people can truly interconnect, anytime, anywhere, about any topic, and by any method, transcending the boundaries of language, culture, ability and distance. To realizing universal communication, UCRI collects diverse information including huge volumes of web pages focusing on information from Asia. This paper introduces NICT's vision and strategies for Asian information hub as a platform for collecting, storing, analyzing large-scale information and providing advanced communication services in Big Data Era.;2012;Michiaki Iwazume;10.1109/COMPSACW.2012.11;Conferences;;978-0-7695-4758-9
ieee_20221205_08_11_42;A Big-Data Perspective on AI: Newton, Merton, and Analytics Intelligence;"The flood of big data in cyberspace will require immediate actions from the AI and intelligent systems community to address how we manage knowledge. Besides new methods and systems, we need a total knowledge-management approach that willl require a new perspective on AI. We need ""Merton's systems"" in which machine intelligence and human intelligence work in tandem. This should become a normal mode of operation for the next generation of AI and intelligent systems.";2012;Fei-Yue Wang;10.1109/MIS.2012.91;Magazines;1941-1294;
ieee_20221205_08_11_42;The larging-up of big data;The term 'Big Data' has been getting big much exposure in IT circles over the last year or two, on a scale that is bound to cause seasoned industry-watchers to sniff the air for the familiar aroma of industry hyperbole. There is the customary amount of hype, of course, but there is more to it than the covert repackaging and repurposing of existing products.;2012;Martin Courtney;10.1049/et.2012.0814;Magazines;1750-9637;
ieee_20221205_08_11_42;A three-dimensional display for big data sets;Facing with high dimensional information in fields of Science, Technology and Commerce, users need effective visualization tools to find more useful information. For big data sets, it is very difficult to get useful information because the dimension is too large for a practical solution. This paper proposes a 3-D visualization method for big data sets. First of all, we employed the K-means clustering method to get the basic vectors. Then, we use these vectors to construct the reduction mapping. Finally, we get the three dimensional display for a sample point. To verify the feasibility of this method, we perform experiment on some well-known databases such as iris, wine and a large data set: Pendigits. The results are favorable. According to the 3-D display results, we can also get messages like classification, outliers, and classification level when given the level standards.;2012;Cheng-Long Ma;10.1109/ICMLC.2012.6359594;Conferences;2160-133X;978-1-4673-1486-2
ieee_20221205_08_11_42;Bias Correction in a Small Sample from Big Data;"This paper discusses the bias problem when estimating the population size of big data such as online social networks (OSN) using uniform random sampling and simple random walk. Unlike the traditional estimation problem where the sample size is not very small relative to the data size, in big data, a small sample relative to the data size is already very large and costly to obtain. We point out that when small samples are used, there is a bias that is no longer negligible. This paper shows analytically that the relative bias can be approximated by the reciprocal of the number of collisions; thereby, a bias correction estimator is introduced. The result is further supported by both simulation studies and the real Twitter network that contains 41.7 million nodes.";2013;Jianguo Lu;10.1109/TKDE.2012.220;Journals;2326-3865;
ieee_20221205_08_11_42;Big Data Challenges: A Program Optimization Perspective;Big Data is characterized by the increasing volume (of the order of zeta bytes) and velocity of data generation. It is projected that the market size of Big Data shall climb up to $53.7 billion by 2017 from the current market size of $5.1 billion. Big Data in conjunction with emerging applications such as RMS applications and others has sown the seeds of exascale computing. In a similar vein, In [12], Sexton argued that applications from domains such as materials science, energy, environment and life sciences will require exascale computing. Recent studies directed towards challenges in building exascale systems and charting the roadmap of exascale computing conjecture that exascale systems would support 10-to 100-way concurrency per core and hundreds of cores per die. In [15], HPC Advisory Council predicts that the first exaflop system will be built between 2018 -- 2020. In this paper present a program optimization perspective to the challenges posed by Big Data.;2012;Arun Kejariwal;10.1109/CGC.2012.17;Conferences;;978-0-7695-4864-7
ieee_20221205_08_11_42;Ontology-Based Temporal Relation Modeling with MapReduce Latent Dirichlet Allocations for Big EHR Data;In this paper, we propose a model called Temporal & Co reference Topic Modeling (TCTM) to do automatic annotation with respect to the Time Event Ontology (TEO) for the big-size Electronic Health Record (EHR). TCTM, based on Latent Dirichlet Allocations (LDA) and integrated into MapReduce framework, inherently addresses the twin problem of data sparseness and high dimensionality. As a non-parametric Bayesian model, it can flexibly add new attributes or features. Side information associated with corpora, such as section header, timestamp, sentence distance, event distance or disease category in clinical notes makes latent topics more interpretable and more biased toward co referring events. Furthermore, TCTM integrates Hidden Markov Model LDA (HMM-LDA) to obtain the power of both sequential modeling and exchangeability. A MapReduce based variational method is employed to do parameter estimation and inferences, thus enabling TCTM to overcome the bottleneck brought by big data.;2012;Dingcheng Li;10.1109/CGC.2012.112;Conferences;;978-0-7695-4864-7
ieee_20221205_08_11_42;Beyond Simple Integration of RDBMS and MapReduce -- Paving the Way toward a Unified System for Big Data Analytics: Vision and Progress;MapReduce has shown vigorous vitality and penetrated both academia and industry in recent years. MapReduce not only can be used as an ETL tool, it can do even much more. The technique has been applied to SQL summation, OLAP, data mining, machine learning, information retrieval, multimedia data processing, science data processing etc. Basically MapReduce is a general purpose parallel computing framework for large dataset processing. A big data analytics ecosystem built around MapReduce is emerging alongside the traditional one built around RDBMS. The objectives of RDBMS and MapReduce, as well as the ecosystems built around them, overlap much really, in some sense they do the same thing and MapReduce can accomplish more works, such as graph processing, which RDBMS can not handle well. RBDMS enjoys high performance of relational data processing, which MapReduce needs to catch up. The authors envision that the two techniques are fusing into a unified system for big data analytics. With the ongoing endeavor to build up the system, much of the groundwork has been laid while some critical issues are still unresolved, we try to identify some of them. Two of our works as well as experiment results are presented, one is applying a hierarchical encoding to star schema data in Hadoop for high performance of OLAP processing, another is leveraging the natural three copies of HDFS blocks to exploit different data layouts to speed up queries in a OLAP workload, a cost model is used to route user queries to different data layouts.;2012;Xiongpai Qin;10.1109/CGC.2012.39;Conferences;;978-0-7695-4864-7
ieee_20221205_08_11_42;A Big Data Model Supporting Information Recommendation in Social Networks;As information systems are becoming sophisticated and mobile, cloud computing, social networking services are now very popular to people, the amount of data is rapidly increasing every year. Big data is data which should be analyzed by a company or an organization, but has not been tried to be analyzed or could not have been processed by current technology. In this paper, we introduce a big data model for recommender systems using social network data. The model incorporates factors related to social networks and can be applied to information recommendation with respect to various social behaviors that can increase the reliability of the recommended information. The big data model has the flexibility to be expanded to incorporate more sophisticated additional factors if needed. The experimental results using it in information recommendation and using map-reduce to process it show that it is a feasible model to be used for information recommendation.;2012;Xiaoyue Han;10.1109/CGC.2012.125;Conferences;;978-0-7695-4864-7
ieee_20221205_08_11_42;Big Data analytics;In this paper, we explain the concept, characteristics & need of Big Data & different offerings available in the market to explore unstructured large data. This paper covers Big Data adoption trends, entry & exit criteria for the vendor and product selection, best practices, customer success story, benefits of Big Data analytics, summary and conclusion. Our analysis illustrates that the Big Data analytics is a fast-growing, influential practice and a key enabler for the social business. The insights gained from the user generated online contents and collaboration with customers is critical for success in the age of social media.;2012;Sachchidanand Singh;10.1109/ICCICT.2012.6398180;Conferences;;978-1-4577-2076-5
ieee_20221205_08_11_42;Agile visual analytics for banking cyber “big data”;This paper describes the rapid development of a tailored cyber situational awareness and analysis application for the 2012 IEEE VAST Mini-Challenge 1 (MC1) — Cyber Situation Awareness. The novel aspect of this project was in the process of developing the tailored solution for a “big data” application. Aperture is an open, adaptable, and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer-based approach to visualization assembly, and a data mapping API that simplifies the process of transformation of data or analytic results into visual forms and properties.;2012;David Jonker;10.1109/VAST.2012.6400507;Conferences;;978-1-4673-4752-5
ieee_20221205_08_11_42;Big data exploration through visual analytics;SAS® Visual Analytics Explorer is an advanced data visualization and exploratory data analysis application that is a component of the SAS Visual Analytics solution. It excels at handling big data problems like the VAST challenge. With a wide range of visual analytics features and the ability to scale to massive datasets, SAS Visual Analytics Explorer enables analysts to find patt er n s and relationships quickly and easily, no matter the size of their data. In this summary paper, we explain how we used SAS Visual Analytics Explorer to solve the VAST Challenge 2012 minichallenge 1.;2012;Nascif A. Abousalh-Neto;10.1109/VAST.2012.6400514;Conferences;;978-1-4673-4752-5
ieee_20221205_08_11_42;Towards HPC for the digital Humanities, Arts, and Social Sciences: Needs and challenges of adapting academic HPC for big data;This paper examines the needs of emerging applications of High Performance Computing by the Humanities, Arts, and Social Sciences (HASS) disciplines and presents a vision for how the current academic HPC environment could be adapted to better serve this new class of “big data” research.;2012;Kalev H. Leetaru;10.1109/eScience.2012.6404439;Conferences;;978-1-4673-4465-4
ieee_20221205_08_11_42;Distributed Big Advertiser Data Mining;"Advertisers and big data mining experts alike are today are dealing with complex datasets of increasing variety (first and third party data), volume (events, impressions, clicks), and velocity (real time bidding). Creating predictive models to customize advertiser requirements and campaign analytics to show targeted ads to users who are most likely to convert has become increasingly challenging. Advertisers often group customers into a segment defined by a given set of demographic or behavioral attributes. Such segments are often very sparse. ""Look-Alike Modeling"" enables advertisers to enhance the target segment by using predictive models to expand the segment membership by assigning a probability score to users that did not explicitly belong to that segment based on the original segment definition. In this paper accompanied by the demo of a distributed platform, we describe a Look-Alike Modeling framework to expand segment membership using a novel high-dimensional distributed algorithm based on frequent pattern mining. We describe how the distributed algorithm is more efficient than traditional classification techniques that (a) require multiple passes over the dataset and (b) require both positive and negative class labels for training. Our solution is capable of concurrently and continuously processing thousands of segments and includes an efficient grouping operator and a distributed scoring algorithm for predicting multiple segment membership for a given (very large) set of users. This leverages the power of in-database analytics as compared to using standard data mining libraries and is currently deployed on a real-world highly scalable distributed columnar database that powers several hundred campaigns and processes look-alike models for large online display advertisers. The results from the study demonstrate that the proposed algorithm outperforms other comparable techniques for predicting and expanding segments.";2012;Ashish Bindra;10.1109/ICDMW.2012.73;Conferences;2375-9259;978-0-7695-4925-5
ieee_20221205_08_11_42;Driving big data with big compute;Big Data (as embodied by Hadoop clusters) and Big Compute (as embodied by MPI clusters) provide unique capabilities for storing and processing large volumes of data. Hadoop clusters make distributed computing readily accessible to the Java community and MPI clusters provide high parallel efficiency for compute intensive workloads. Bringing the big data and big compute communities together is an active area of research. The LLGrid team has developed and deployed a number of technologies that aim to provide the best of both worlds. LLGrid MapReduce allows the map/reduce parallel programming model to be used quickly and efficiently in any language on any compute cluster. D4M (Dynamic Distributed Dimensional Data Model) provided a high level distributed arrays interface to the Apache Accumulo database. The accessibility of these technologies is assessed by measuring the effort to use these tools and is typically a few lines of code. The performance is assessed by measuring the insert rate into the Accumulo database. Using these tools a database insert rate of 4M inserts/second has been achieved on an 8 node cluster.;2012;Chansup Byun;10.1109/HPEC.2012.6408678;Conferences;;978-1-4673-1575-3
ieee_20221205_08_11_42;ZIP-IO: Architecture for application-specific compression of Big Data;We have entered the “Big Data” age: scaling of networks and sensors has led to exponentially increasing amounts of data. Compression is an effective way to deal with many of these large data sets, and application-specific compression algorithms have become popular in problems with large working sets. Unfortunately, these compression algorithms are often computationally difficult and can result in application-level slow-down when implemented in software. To address this issue, we investigate ZIP-IO, a framework for FPGA-accelerated compression. Using this system we demonstrate that an unmodified industrial software workload can be accelerated 3x while simultaneously achieving more than 1000x compression in its data set.;2012;Sang Woo Jun;10.1109/FPT.2012.6412159;Conferences;;978-1-4673-2844-9
ieee_20221205_08_11_42;An instances placement algorithm based on disk I/O load for big data in private cloud;In generally, large companies or organizations have the demand of big data processing and they do not want to entrust their business processes and data to third parties (Amazon, Google, etc.). The private cloud could meet their needs. In private cloud, tasks run at multiple instances (also known as virtual machines), which could be paced in different physical nodes. Obviously, the instances which be used to process big data need higher CPU and disk performance than other kinds of instances. If the instances of disk resource consuming are placed in the same physical node, clearly, the disk I/O bandwidth would be used up quickly that would affect the performance of the entire node seriously. This paper proposes an instances placement algorithm FFDL that based on disk I/O for private cloud environment to deal with big data that would adopt the disk I/O load balancing strategy and reduce competition for the disk I/O load between instances. We have validated our approach by conducting a performance evaluation study on the open source private cloud platform—Openstack. The results demonstrate that our algorithm has immense potential as it offers significant computation time savings than the Greedy algorithm and demonstrates high potential for the improvement of disk I/O load balancing in the entire private cloud system for the big data.;2012;Jian Guo;10.1109/ICWAMTIP.2012.6413495;Conferences;;978-1-4673-4683-2
ieee_20221205_08_11_42;Finding the Needle in the Big Data Systems Haystack;"With the increasing importance of big data, many new systems have been developed to ""solve"" the big data challenge. At the same time, famous database researchers argue that there is nothing new about these systems and that they're actually a step backward. This article sheds some light on this discussion.";2013;Tim Kraska;10.1109/MIC.2013.10;Magazines;1941-0131;
ieee_20221205_08_11_42;Asynchronous Index Strategy for high performance real-time big data stream storage;Big data insert-intensive applications challenge traditional RDBMS. Key-Value databases achieve the same throughput with much more price/performance ratio, which makes them popular recent years. However, Key-Value databases are not suitable for high performance real-time applications. In this paper we introduce Asynchronous Index Strategy as a high performance solution for insert-intensive time series big data storage. It takes advantage of partial replication and asynchronous indexes, which results in zero overhead for index updates. Furthermore, a general middle-ware for clustering databases based on Asynchronous Index Strategy is implemented. Finally, indexing and inserting performance experiments highlight the efficiency of Asynchronous Index Strategy. As for AIS based on MongoDB, it achieves a throughput that is 17 times of MongoDB sharding cluster.;2012;Xiao Mo;10.1109/ICNIDC.2012.6418750;Conferences;2374-0272;978-1-4673-2203-4
ieee_20221205_08_11_42;Grand Challenge: Applying Regulatory Science and Big Data to Improve Medical Device Innovation;Understanding how proposed medical devices will interface with humans is a major challenge that impacts both the design of innovative new devices and approval and regulation of existing devices. Today, designing and manufacturing medical devices requires extensive and expensive product cycles. Bench tests and other preliminary analyses are used to understand the range of anatomical conditions, and animal and clinical trials are used to understand the impact of design decisions upon actual device success. Unfortunately, some scenarios are impossible to replicate on the bench, and competitive pressures often accelerate initiation of animal trials without sufficient understanding of parameter selections. We believe that these limitations can be overcome through advancements in data-driven and simulation-based medical device design and manufacturing, a research topic that draws upon and combines emerging work in the areas of Regulatory Science and Big Data. We propose a cross-disciplinary grand challenge to develop and holistically apply new thinking and techniques in these areas to medical devices in order to improve and accelerate medical device innovation.;2013;Arthur G. Erdman;10.1109/TBME.2013.2244600;Journals;1558-2531;
ieee_20221205_08_11_42;Puzzling out big data [Information Technology Analytics];"Big data comes in many forms. It comes as customer information and transactions contained in customer-relationship management and enterprise resourceplanning systems and HTML-based web stores. It comes as information generated by machine-to-machine applications collecting data from smart meters, manufacturing sensors, equipment logs, trading systems data and call detail records compiled by fixed and mobile telecommunications companies. Big data can come with big differences. Some say that the `three Vs' of big data should more properly be tagged as the `three HVs': high-volume, high-variety, high-velocity, and high-veracity. Apply those tags to the mountains of information posted on social network and blogging sites, including Facebook, Twitter and VouTube; the deluge of text contained in email and instant messages; not to mention audio and video files. It is evident then that it's not necessarily the 'big-ness' of information that presents big-data applications and services with their greatest challenge, but the variety and the speed at which all that constantly changing information must be ingested, processed, aggregated, filtered, organised and fed back in a meaningful way for businesses to get some value out of it.";2013;Martin Courtney;;Magazines;1750-9637;
ieee_20221205_08_11_42;Addressing Big Data challenges for Scientific Data Infrastructure;This paper discusses the challenges that are imposed by Big Data Science on the modern and future Scientific Data Infrastructure (SDI). The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model.;2012;Yuri Demchenko;10.1109/CloudCom.2012.6427494;Conferences;;978-1-4673-4509-5
ieee_20221205_08_11_42;Big Data Processing in Cloud Computing Environments;With the rapid growth of emerging applications like social network analysis, semantic Web analysis and bioinformatics network analysis, a variety of data to be processed continues to witness a quick increase. Effective management and analysis of large-scale data poses an interesting but critical challenge. Recently, big data has attracted a lot of attention from academia, industry as well as government. This paper introduces several big data processing technics from system and application aspects. First, from the view of cloud data management and big data processing mechanisms, we present the key issues of big data processing, including cloud computing platform, cloud architecture, cloud database and data storage scheme. Following the Map Reduce parallel processing framework, we then introduce Map Reduce optimization strategies and applications reported in the literature. Finally, we discuss the open issues and challenges, and deeply explore the research directions in the future on big data processing in cloud computing environments.;2012;Changqing Ji;10.1109/I-SPAN.2012.9;Conferences;2375-527X;978-0-7695-4930-9
ieee_20221205_08_11_42;Robust Decision Engineering: Collaborative Big Data and its application to international development/aid;"Much of the research that goes into Big Data, and specifically on Collaborative Big Data, is focused upon questions, such as: how to get more of it? (e.g., · participatory mechanisms, social media, geo-coded data from personal electronic devices) and · how to handle it? (e.g., how to ingest, sort, store, and link up disparate data sets). A question that receives far less attention is that of Collaborative analysis of Big Data; how can a multi-disciplinary layered analysis of Big Data be used to support robust decisions, especially in a collaborative setting, and especially under time pressure? The robust Decision Engineering required can be achieved by employing an approach related to Network Science, that we call Relationship Science. In Relationship Science, our methodological framework, karassian netchain analysis (KNA), is utilized to ascertain islands of stability or positive influence dominating sets (PIDS), so that a form of annealed resiliency or latent stability is achieved, thereby mitigating against unintended consequences, elements of instability, and “perfect storm” crises lurking within the network.";2012;Steve Chan;10.4108/icst.collaboratecom.2012.250715;Conferences;;978-1-4673-2740-4
ieee_20221205_08_11_42;T*: A data-centric cooling energy costs reduction approach for Big Data analytics cloud;Explosion in Big Data has led to a surge in extremely large-scale Big Data analytics platforms, resulting in burgeoning energy costs. Big Data compute model mandates strong data-locality for computational performance, and moves computations to data. State-of-the-art cooling energy management techniques rely on thermal-aware computational job placement/migration and are inherently data-placement-agnostic in nature. T* takes a novel, data-centric approach to reduce cooling energy costs and to ensure thermal-reliability of the servers. T* is cognizant of the uneven thermal-profile and differences in thermal-reliability-driven load thresholds of the servers, and the differences in the computational jobs arrival rate, size, and evolution life spans of the Big Data placed in the cluster. Based on this knowledge, and coupled with its predictive file models and insights, T* does proactive, thermal-aware file placement, which implicitly results in thermal-aware job placement in the Big Data analytics compute model. Evaluation results with one-month long real-world Big Data analytics production traces from Yahoo! show up to 42% reduction in the cooling energy costs with T* courtesy of its lower and more uniform thermal-profile and 9x better performance than the state-of-the-art data-agnostic cooling techniques.;2012;Rini T. Kaushik;10.1109/SC.2012.103;Conferences;2167-4329;978-1-4673-0804-5
ieee_20221205_08_11_42;Introduction to Big Data: Scalable Representation and Analytics for Data Science Minitrack;Big data is an emerging phenomenon characterized by the three Vs: volume, velocity, and variety. The volume of data has increased from terabytes to petabytes and is encroaching on exabytes. Some pundits are suggesting that zettabytes (1021) are reachable within the next several years. Velocity is concerned with not only how fast we accumulate data, but also how fast some of the data that we already have is changing. Some systems accumulate data at the rate of multiple petabytes per year, some systems have stored data that changes at the rate of terabytes per year. Changing data usually lags accumulating data by several orders of magnitude. Data accumulating at a multiple petabyte rate requires terabits to petabits of transport capacity. Finally, the variety and modality of data is continually evolving, it may be both structured and unstructured.;2013;Stephen Kaisler;10.1109/HICSS.2013.292;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_11_42;An Agent Model for Incremental Rough Set-Based Rule Induction: A Big Data Analysis in Sales Promotion;Rough set-based rule induction is able to generate decision rules from a database and has mechanisms to handle noise and uncertainty in data. This technique facilitates managerial decision-making and strategy formulation. However, the process for RS-based rule induction is complex and computationally intensive. Moreover, operational databases that are used to run the day-to-day operations, thus large volumes of data are continually updated within a short period of time. The infrastructure required to analyze such large amounts of data must be able to handle extreme data volumes, to allow fast response times, and to automate decisions based on analytical models. This study proposes an Incremental Rough Set-based Rule Induction Agent (IRSRIA). Rule induction is based on creating agents for the main modeling processes. In addition, an incremental architecture is designed, to address large-scale dynamic database problems. A case study of a Home shopping company is used to show the validity and efficiency of this method. The results of experiments show that the IRSRIA can considerably reduce the computation time for inducing decision rules, while maintaining the same quality of rules.;2013;Yu-Neng Fan;10.1109/HICSS.2013.79;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_11_42;Big Data: Issues and Challenges Moving Forward;"Big data refers to data volumes in the range of exabytes (1018) and beyond. Such volumes exceed the capacity of current on-line storage systems and processing systems. Data, information, and knowledge are being created and collected at a rate that is rapidly approaching the exabyte/year range. But, its creation and aggregation are accelerating and will approach the zettabyte/year range within a few years. Volume is only one aspect of big data; other attributes are variety, velocity, value, and complexity. Storage and data transport are technology issues, which seem to be solvable in the near-term, but represent longterm challenges that require research and new paradigms. We analyze the issues and challenges as we begin a collaborative research program into methodologies for big data analysis and design.";2013;Stephen Kaisler;10.1109/HICSS.2013.645;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_11_42;Introduction to Predictive Analytics and Big Data Minitrack;Introduction to Predictive Analytics and Big Data Minitrack.;2013;Dursun Delen;10.1109/HICSS.2013.322;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_11_42;Introduction to Business Analytics, Business Intelligence, and Big Data Minitrack;Introduction to the Business Analytics, Business Intelligence and Big Data Minitrack;2013;Robert Winter;10.1109/HICSS.2013.334;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_11_42;Danger Theory: A new approach in big data analysis;Danger Theory is a novel computing model inspired by biological immune systems which is some different with the traditional Artificial Immune Systems, especially the self non-self model. The Danger Theory concerns the potential dangers (which presents like the danger signals) rather than non-self pathogens, so the new computing model introduced into information analysis can take a new approach for big data processing on key features and properties choosing. The authors introduce some works on Danger Theory about the definition of danger, and the capture of danger signals, which could be helpful to increase the abilities of self-learning and intelligence in different applications.;2012;Lin Lu;10.1049/cp.2012.1083;Conferences;;978-1-84919-537-9
ieee_20221205_08_11_42;Addressing big data problem using Hadoop and Map Reduce;The size of the databases used in today's enterprises has been growing at exponential rates day by day. Simultaneously, the need to process and analyze the large volumes of data for business decision making has also increased. In several business and scientific applications, there is a need to process terabytes of data in efficient manner on daily bases. This has contributed to the big data problem faced by the industry due to the inability of conventional database systems and software tools to manage or process the big data sets within tolerable time limits. Processing of data can include various operations depending on usage like culling, tagging, highlighting, indexing, searching, faceting, etc operations. It is not possible for single or few machines to store or process this huge amount of data in a finite time period. This paper reports the experimental work on big data problem and its optimal solution using Hadoop cluster, Hadoop Distributed File System (HDFS) for storage and using parallel processing to process large data sets using Map Reduce programming framework. We have done prototype implementation of Hadoop cluster, HDFS storage and Map Reduce framework for processing large data sets by considering prototype of big data application scenarios. The results obtained from various experiments indicate favorable results of above approach to address big data problem.;2012;Aditya B. Patel;10.1109/NUICONE.2012.6493198;Conferences;2375-1282;978-1-4673-1718-4
ieee_20221205_08_11_42;Ethics of Big Data;This column discusses the book Ethics of Big Data by Kord Davis with Doug Patterson.;2013;Richard Mateosian;10.1109/MM.2013.35;Magazines;1937-4143;
ieee_20221205_08_11_42;Ubiquitous Analytics: Interacting with Big Data Anywhere, Anytime;Ubilytics amplifies human cognition by embedding analytical processes into the physical environment to make sense of big data anywhere, anytime.;2013;Niklas Elmqvist;10.1109/MC.2013.147;Magazines;1558-0814;
ieee_20221205_08_11_42;Journey to the centre of big data;In a general context big data is an aggregation of data sets that are so large and complex that it becomes difficult to process using readily available database management tools or traditional data processing applications. This challenge also contains an opportunity for commercial organisations that are equipped to find ways to use it - or elements of it - to inform and enhance revenue drivers (see 'Puzzling out big data', E&T Vol 7 Issue 12). The term 'big data' is partly a recognition of the growing relative weight and importance of unstructured data not amenable to conventional database analytics and reporting tools and techniques. Above all, though, it embodies an ambition to extract value from data, particularly for sales, marketing, and customer relations.;2013;Philip Hunter;10.1049/et.2013.0307;Magazines;1750-9637;
ieee_20221205_08_11_42;Abstract: Networking Research Activities at Fermilab for Big Data Analysis;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center. It must deal with both scaling and wide-area distribution challenges in processing its CMS data. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;P. DeMar;10.1109/SC.Companion.2012.214;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Poster: Big Data Networking at Fermilab;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R&D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;Phillip J. Demar;10.1109/SC.Companion.2012.215;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Abstract: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.229;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Poster: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.230;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Abstract: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;A. Klimentov;10.1109/SC.Companion.2012.301;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Poster: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;K. De;10.1109/SC.Companion.2012.302;Conferences;;978-1-4673-6218-4
ieee_20221205_08_11_42;Feasibility considerations of multipath TCP in dealing with big data application;In this paper we present an idea to handle big data in a better way. We consider Multipath TCP (MPTCP) to use all available paths simultaneously. MPTCP is a tailored form of TCP which aspire to improve throughput by sharing available resources smartly and fairly. The key concentration of this paper is to analyze the benefit of MPTCP over single path TCP for bandwidth and time sensitive applications as well as big data application. Our simulation shows that MPTCP can higher goodput by bandwidth aggregation, Couple Congestion Control(CCC) provide better throughput without being unfair to other legacy TCP flows and also portray that large receive buffer causes performance enhancement for relatively large RTT link. As a result, MPTCP can be a enormous addition in contrast with single path TCP in dealing with big data application.;2013;Zia Ush Shamszaman;10.1109/ICOIN.2013.6496714;Conferences;2332-5658;978-1-4673-5741-8
ieee_20221205_08_11_42;Customizing Computational Methods for Visual Analytics with Big Data;The volume of available data has been growing exponentially, increasing data problem's complexity and obscurity. In response, visual analytics (VA) has gained attention, yet its solutions haven't scaled well for big data. Computational methods can improve VA's scalability by giving users compact, meaningful information about the input data. However, the significant computation time these methods require hinders real-time interactive visualization of big data. By addressing crucial discrepancies between these methods and VA regarding precision and convergence, researchers have proposed ways to customize them for VA. These approaches, which include low-precision computation and iteration-level interactive visualization, ensure real-time interactive VA for big data.;2013;Jaegul Choo;10.1109/MCG.2013.39;Magazines;1558-1756;
ieee_20221205_08_11_42;Shared disk big data analytics with Apache Hadoop;Big Data is a term applied to data sets whose size is beyond the ability of traditional software technologies to capture, store, manage and process within a tolerable elapsed time. The popular assumption around Big Data analytics is that it requires internet scale scalability: over hundreds of compute nodes with attached storage. In this paper., we debate on the need of a massively scalable distributed computing platform for Big Data analytics in traditional businesses. For organizations which don't need a horizontal., internet order scalability in their analytics platform., Big Data analytics can be built on top of a traditional POSIX Cluster File Systems employing a shared storage model. In this study., we compared a widely used clustered file system: VERITAS Cluster File System (SF-CFS) with Hadoop Distributed File System (HDFS) using popular Map-reduce benchmarks like Terasort., DFS-IO and Gridmix on top of Apache Hadoop. In our experiments VxCFS could not only match the performance of HDFS., but also outperformed in many cases. This way., enterprises can fulfill their Big Data analytics need with a traditional and existing shared storage model without migrating to a different storage model in their data centers. This also includes other benefits like stability & robustness., a rich set of features and compatibility with traditional analytics applications.;2012;Anirban Mukherjee;10.1109/HiPC.2012.6507520;Conferences;;978-1-4673-2370-3
ieee_20221205_08_11_42;Big Data Conferences, Here We Come!;Conferences on big data are starting to appear this year. What can we expect from a big Data conference? As researchers, should we plan to attend, and if yes, what should we submit?;2013;Peter Mika;10.1109/MIC.2013.45;Magazines;1941-0131;
ieee_20221205_08_11_42;Transforming Big Data into Collective Awareness;"Integrating social and sensor networks can transform big data, if treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems. The Web extra at http://youtu.be/xSoAvMNZSL8 is a video in which author Jeremy Pitt expands on his article ""Transforming Big Data into Collective Awareness"" and discusses how integrating social and sensor networks can transform big data, if it's treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems.";2013;Jeremy Pitt;10.1109/MC.2013.153;Magazines;1558-0814;
ieee_20221205_08_11_42;A big data implementation based on Grid computing;Big Data is a term defining data that has three main characteristics. First, it involves a great volume of data. Second, the data cannot be structured into regular database tables and third, the data is produced with great velocity and must be captured and processed rapidly. Oracle adds a fourth characteristic for this kind of data and that is low value density, meaning that sometimes there is a very big volume of data to process before finding valuable needed information. Big Data is a relatively new term that came from the need of big companies like Yahoo, Google, Facebook to analyze big amounts of unstructured data, but this need could be identified in a number of other big enterprises as well in the research and development field. The framework for processing Big Data consists of a number of software tools that will be presented in the paper, and briefly listed here. There is Hadoop, an open source platform that consists of the Hadoop kernel, Hadoop Distributed File System (HDFS), MapReduce and several related instruments. Two of the main problems that occur when studying Big Data are the storage capacity and the processing power. That is the area where using Grid Technologies can provide help. Grid Computing refers to a special kind of distributed computing. A Grid computing system must contain a Computing Element (CE), and a number of Storage Elements (SE) and Worker Nodes (WN). The CE provides the connection with other GRID networks and uses a Workload Management System to dispatch jobs on the Worker Nodes. The Storage Element is in charge with the storage of the input and the output of the data needed for the job execution. The main purpose of this article is to present a way of processing Big Data using Grid Technologies. For that, the framework for managing Big Data will be presented along with the way to implement it around a grid architecture.;2013;Dan Garlasu;10.1109/RoEduNet.2013.6511732;Conferences;2068-1038;978-1-4673-6114-9
ieee_20221205_08_11_42;Big Data in Neonatal Intensive Care;"The effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions. The Web extra at http://youtu.be/OIQBCboQs0g is a video in which author Carolyn McGregor expands on her article ""Big Data in Neonatal Intensive Care"" and discusses how the effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions.";2013;Carolyn McGregor;10.1109/MC.2013.157;Magazines;1558-0814;
ieee_20221205_08_11_42;Governing Big Data: Principles and practices;As data-intensive decision making is being increasingly adopted by businesses, governments, and other agencies around the world, most organizations encountering a very large amount and variety of data are still contemplating and assessing their readiness to embrace “Big Data.” While these organizations devise various ways to deal with the challenges such data brings, the impact and importance of Big Data to information quality and governance programs should not be underestimated. Drawing upon implementation experiences of early adopters of Big Data technologies across multiple industries, this paper explores the issues and challenges involved in the management of Big Data, highlighting the principles and best practices for effective Big Data governance.;2013;P. Malik;10.1147/JRD.2013.2241359;Journals;0018-8646;
ieee_20221205_08_11_42;IBM Streams Processing Language: Analyzing Big Data in motion;The IBM Streams Processing Language (SPL) is the programming language for IBM InfoSphere® Streams, a platform for analyzing Big Data in motion. By “Big Data in motion,” we mean continuous data streams at high data-transfer rates. InfoSphere Streams processes such data with both high throughput and short response times. To meet these performance demands, it deploys each application on a cluster of commodity servers. SPL abstracts away the complexity of the distributed system, instead exposing a simple graph-of-operators view to the user. SPL has several innovations relative to prior streaming languages. For performance and code reuse, SPL provides a code-generation interface to C++ and Java®. To facilitate writing well-structured and concise applications, SPL provides higher-order composite operators that modularize stream sub-graphs. Finally, to enable static checking while exposing optimization opportunities, SPL provides a strong type system and user-defined operator models. This paper provides a language overview, describes the implementation including optimizations such as fusion, and explains the rationale behind the language design.;2013;M. Hirzel;10.1147/JRD.2013.2243535;Journals;0018-8646;
ieee_20221205_08_11_42;Big Data text-oriented benchmark creation for Hadoop;Massive-scale Big Data analytics is representative of a new class of workloads that justifies a rethinking of how computing systems should be optimized. This paper addresses the need for a set of benchmarks that system designers can use to measure the quality of their designs and that customers can use to evaluate competing systems offerings with respect to commonly performed text-oriented workflows in Hadoop™. Additions are needed to existing benchmarks such as HiBench in terms of both scale and relevance. We describe a methodology for creating a petascale data-size text-oriented benchmark that includes representative Big Data workflows and can be used to test total system performance, with demands balanced across storage, network, and computation. Creating such a benchmark requires meeting unique challenges associated with the data size and its often unstructured nature. To be useful, the benchmark also needs to be sufficiently generic to be accepted by the community at large. Here, we focus on a text-oriented Hadoop workflow that consists of three common tasks: categorizing text documents, identifying significant documents within each category, and analyzing significant documents for new topic creation.;2013;A. Gattiker;10.1147/JRD.2013.2240732;Journals;0018-8646;
ieee_20221205_08_11_42;GPFS-SNC: An enterprise cluster file system for Big Data;A new class of data-intensive applications commonly referred to as Big Data applications (e.g., customer sentiment analysis based on click-stream logs) involves processing massive amounts of data with a focus on semantically transforming the data. This class of applications is massively parallel and well suited for the MapReduce programming framework that allows users to perform large-scale data analyses such that the application execution layer handles the system architecture, data partitioning, and task scheduling. In this paper, we introduce GPFS-SNC (General Parallel File System for Shared Nothing Clusters), a scalable file system that operates over a cluster of commodity machines and direct-attached storage and meets the requirements of analytics and traditional applications that are typically used together in analytics solutions. The architecture extends an existing enterprise cluster file system to support these emerging classes of workloads by applying five innovative optimizations: 1) locality awareness to allow compute jobs to be scheduled on nodes where the data resides, 2) metablocks that allow large and small block sizes to co-exist in the same file system to meet the needs of different types of applications, 3) write affinity that allows applications to dictate the layout of files on different nodes in order to maximize both write and read bandwidth, 4) pipelined replication to maximize use of network bandwidth for data replication, and 5) distributed recovery to minimize the effect of failures on ongoing computation.;2013;R. Jain;10.1147/JRD.2013.2243531;Journals;0018-8646;
ieee_20221205_08_11_42;Understanding system design for Big Data workloads;This paper explores the design and optimization implications for systems targeted at Big Data workloads. We confirm that these workloads differ from workloads typically run on more traditional transactional and data-warehousing systems in fundamental ways, and, therefore, a system optimized for Big Data can be expected to differ from these other systems. Rather than only studying the performance of representative computational kernels, and focusing on central-processing-unit performance, this paper studies the system as a whole. We identify three major phases in a typical Big Data workload, and we propose that each of these phases should be represented in a Big Data systems benchmark. We implemented our ideas on two distinct IBM POWER7® processor-based systems that target different market sectors, and we analyze their performance on a sort benchmark. In particular, this paper includes an evaluation of POWER7 processor-based systems using MapReduce TeraSort, which is a workload that can be a “stress test” for multiple dimensions of system performance. We combine this work with a broader perspective on Big Data workloads and suggest a direction for a future benchmark definition effort. A number of methods to further improve system performance are proposed.;2013;H. P. Hofstee;10.1147/JRD.2013.2242674;Journals;0018-8646;
ieee_20221205_08_11_42;Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost;"Finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The first Web extra at http://youtu.be/B2RlkoNjrzA is a video in which author Paul Tallon expands on his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and discusses how finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The second Web extra at http://youtu.be/g0RFa4swaf4 is a video in which author Paul Tallon discusses the supplementary material to his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and how projection models can help individuals responsible for data handling plan for and understand big data storage issues.";2013;Paul P. Tallon;10.1109/MC.2013.155;Magazines;1558-0814;
ieee_20221205_08_11_42;Bootstrapping smart cities through a self-sustainable model based on big data flows;"We have a clear idea today about the necessity and usefulness of making cities smarter, the potential market size, and trials and tests. However, it seems that business around Smart Cities is having difficulties taking off and is thus running short of projected potentials. This article looks into why this is the case and proposes a procedure to make smart cities happen based on big data exploitation through the API stores concept. To this end, we first review involved stakeholders and the ecosystem at large. We then propose a viable approach to scale business within that ecosystem. We also describe the available ICT technologies and finally exemplify all findings by means of a sustainable smart city application. Over the course of the article, we draw two major observations, which are seen to facilitate sustainable smart city development. First, independent smart city departments (or the equivalent) need to emerge, much like today's well accepted IT departments, which clearly decouple the political element of the improved city servicing from the underlying technologies. Second, a coherent three-phase smart city rollout is vital, where in phase 1 utility and revenues are generated; in phase 2 only-utility service is also supported; and in phase 3, in addition, a fun/leisure dimension is permitted.";2013;Ignasi Vilajosana;10.1109/MCOM.2013.6525605;Magazines;1558-1896;
ieee_20221205_08_11_42;Big Data's Big Unintended Consequences;"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article ""Big Data's Big Unintended Consequences"" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.";2013;Marcus R. Wigan;10.1109/MC.2013.195;Magazines;1558-0814;
ieee_20221205_08_11_42;Big Data: New Opportunities and New Challenges [Guest editors' introduction];"We can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldn't blind ourselves to the possible irreversibility of changes-whether good or bad-to society. The first Web extra at http://youtu.be/24czULRCI9c is an audio recording in which Katina Michael at the University of Wollongong discusses the June 2013 Computer magazine special issue on ""Big Data: New Opportunities and New Challenges,"" introducing the special issue, the guest editors, the authors, the articles, and the IEEE Society on Social Implications of Technology (SSIT). The second Web extra at http://youtu.be/9zpFqEDydDA is an audio recording in which Katina Michael at the University of Wollongong talks about the IEEE Society on the Social Implications of Technology (SSIT), IEEE Technology and Society (T&S) magazine, and the International Symposium on Technology and Society (ISTAS). The third Web extra at http://youtu.be/mn_9YHV2RGQis an audio recording in which Katina Michael at the University of Wollongong discusses how we can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldnít blind ourselves to the possible irreversibility of changes-whether good or bad-to society.";2013;Katina Michael;10.1109/MC.2013.196;Magazines;1558-0814;
ieee_20221205_08_11_42;Efficient and secure Cloud storage for handling big data;The term “Cloud” has been used historically as a metaphor for the internet. It is one of the most active application for enterprise. It has been more and more accepted by enterprises which can take advantage of low cost, fast deployment and elastic scaling. Due to demand of large volume of data processing in enterprises, huge amount of data are generated and dispersed on internet around the globe. Currently, storing the data safely and efficiently on Cloud is one of the biggest challenge in Cloud computing. There is no guarantee that data stored on Cloud is securely protected. We propose a method to build a trusted computing environment by providing a secure platform in a Cloud computing system. The proposed method allows users to store data safely and efficiently in the Cloud. It solves the problem of handling big data and security issues using encryption and compression technique while uploading data to the Cloud storage.;2012;Arjun Kumar;;Conferences;;978-1-4673-0876-2
ieee_20221205_08_11_42;Scalable single linkage hierarchical clustering for big data;"Personal computing technologies are everywhere; hence, there are an abundance of staggeringly large data sets-the Library of Congress has stored over 160 terabytes of web data and it is estimated that Facebook alone logs nearly a petabyte of data per day. Thus, there is a pertinent need for systems by which one can elucidate the similarity and dissimilarity among and between groups in these big data sets. Clustering is one way to find these groups. In this paper, we extend the scalable Visual Assessment of Tendency (sVAT) algorithm to return single-linkage partitions of big data sets. The sVAT algorithm is designed to provide visual evidence of the number of clusters in unloadable (big) data sets. The extension we describe for sVAT enables it to also then efficiently return the data partition as indicated by the visual evidence. The computational complexity and storage requirements of sVAT are (usually) significantly less than the O(n2) requirement of the classic single-linkage hierarchical algorithm. We show that sVAT is a scalable instantiation of single-linkage clustering for data sets that contain c compact-separated clusters, where c ≪ n; n is the number of objects. For data sets that do not contain compact-separated clusters, we show that sVAT produces a good approximation of single-linkage partitions. Experimental results are presented for both synthetic and real data sets.";2013;Timothy C. Havens;10.1109/ISSNIP.2013.6529823;Conferences;;978-1-4673-5500-1
ieee_20221205_08_11_42;Data stream mining to address big data problems;Today, the IT world is trying to cope with “big data” problems (data volume, velocity, variety, veracity) on the path to obtaining useful information. In this paper, we present implementation details and performance results of realizing “online” Association Rule Mining (ARM) over big data streams for the first time in the literature. Specifically, we added Apriori and FP-Growth algorithms for stream mining inside an event processing engine, called Esper. Using the system, these two algorithms were compared over LastFM social music site data and by using tumbling windows. The better-performing FP-Growth was selected and used in creation of a real-time rule-based recommendation engine. Our most important findings show that online association rule mining can generate (1) more rules, (2) much faster and more efficiently, and (3) much sooner than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as “George Harrison⇒Beatles”. We hope that our findings can shed light on the design and implementation of other big data analytics systems in the future.;2013;Erdi Ölmezoğulları;10.1109/SIU.2013.6531483;Conferences;;978-1-4673-5561-2
ieee_20221205_08_11_42;Panel: Big data for the public;"Summary form only given. While data are now being produced and collected on unprecedented scales, most of the ""big data"" remain inaccessible or difficult to use by the public.";2013;;10.1109/ICDE.2013.6544803;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_11_42;Machine learning on Big Data;Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.;2013;Tyson Condie;10.1109/ICDE.2013.6544913;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_11_42;Big data integration;The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.;2013;Xin Luna Dong;10.1109/ICDE.2013.6544914;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_11_42;Workload management for Big Data analytics;Parallel database systems and MapReduce systems are essential components of today's infrastructure for Big Data analytics. These systems process multiple concurrent workloads consisting of complex user requests, where each request is associated with an (explicit or implicit) service level objective.;2013;Ashraf Aboulnaga;10.1109/ICDE.2013.6544915;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_11_42;Very fast estimation for result and accuracy of big data analytics: The EARL system;Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets (a.k.a. `big data') can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in big data systems (e.g., Hadoop). Therefore, we propose a nonparametric accuracy estimation method and system to speedup big data analytics. Our framework is called EARL (Early Accurate Result Library) and it works by predicting the learning curve and choosing the appropriate sample size for achieving the desired error bound specified by the user. The error estimates are based on a technique called bootstrapping that has been widely used and validated by statisticians, and can be applied to arbitrary functions and data distributions. Therefore, this demo will elucidate (a) the functionality of EARL and its intuitive GUI interface whereby first-time users can appreciate the accuracy obtainable from increasing sample sizes by simply viewing the learning curve displayed by EARL, (b) the usability of EARL, whereby conference participants can interact with the system to quickly estimate the sample sizes needed to obtain the desired accuracies or response times, and then compare them against the accuracies and response times obtained in the actual computations.;2013;Nikolay Laptev;10.1109/ICDE.2013.6544928;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_11_42;Towards an Optimized Big Data Processing System;Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.;2013;Bogdan Ghit;10.1109/CCGrid.2013.53;Conferences;;978-1-4673-6465-2
ieee_20221205_08_11_42;A Holistic Architecture for the Internet of Things, Sensing Services and Big Data;Wireless Sensor Networks (WSNs) increasingly enable the interaction of the physical world with services, which may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). This potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which provides a set of abstractions for the different types of sensors and services, enabling them to take advantage of Big Data and cloud technologies and which is termed holistic as it caters for the data flow from sensors through to services. The architecture has been designed for implementation on a resource constrained node and to be extensible to server environments, shown in this paper where we present a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase data store.;2013;David Tracey;10.1109/CCGrid.2013.100;Conferences;;978-1-4673-6465-2
ieee_20221205_08_11_42;Getting an Intuition for Big Data;"IEEE Software Editor-in-Chief Forrest Shull discusses the importance of building reliable systems to interpret big data. In addition, he discusses the IBM Impact 2013 Unconference; the Software Engineering Institute's SATURN 2013 conference in which the IEEE Software Architecture in Practice Award went to Simon Brown of Coding the Architecture, for his presentation titled ""The Conflict between Agile and Architecture: Myth or Reality"" and the IEEE Software New Directions Award went to Darryl Nelson of Raytheon for his presentation titled, ""Next-Gen Web Architecture for the Cloud Era."" He also welcomes Professor Rafael Prikladnicki of the Computer Science School at PUCRS, Brazil, and Chief Software Economist Walker Royce of IBM's Software Group to the IEEE Software Advisory Board. The first Web extra at http://youtu.be/JrQorWS5m6w is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Paul Zikopoulos, Director--IBM Information Management Technical Professionals, Competitive Database, and Big Data at IBM, about the potentials of mining big data. Zikopoulos will deliver a keynote at Software Experts Summit 2013 on 17 July in Redmond, Washington. The second Web extra at http://youtu.be/NHHThAeONv8 is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Catherine Plaisant and Megan Monroe of the University of Maryland Human-Computer Interaction Laboratory about big data information visualization and its applications to software development. The third Web extra at http://youtu.be/NqXE0ewoTKA is a video overview of the IBM Impact 2013 Unconference, sponsored by IEEE Software magazine, an event specifically designed for developers that featured Grady Booch and Tim O'Reilly as keynote speakers.";2013;Forrest Shull;10.1109/MS.2013.76;Magazines;1937-4194;
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.84;Magazines;1937-4194;
ieee_20221205_08_11_42;Data mining with big data;Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.;2014;Xindong Wu;10.1109/TKDE.2013.109;Journals;2326-3865;
ieee_20221205_08_11_42;Big Data Analytics: Perspective Shifting from Transactions to Ecosystems;Understanding the flow and interrelated nature of institutions and business entities' processes and exchanges helps researchers develop and apply big data analytics techniques more effectively from an ecosystem-based perspective (rather than individual transactions and components).;2013;Daniel Zeng;10.1109/MIS.2013.40;Magazines;1941-1294;
ieee_20221205_08_11_42;Artificial Intelligence and Big Data;AI Innovation in Industry is a new department for IEEE Intelligent Systems, and this paper examines some of the basic concerns and uses of AI for big data (AI has been used in several different ways to facilitate capturing and structuring big data, and it has been used to analyze big data for key insights).;2013;;10.1109/MIS.2013.39;Magazines;1941-1294;
ieee_20221205_08_11_42;Big data analysis of irregular operations: Aborted approaches and their underlying factors;Procedures such as Missed Approaches and Holding Patterns are designed into Air Traffic Control procedures to provide a safe manner for flights to temporarily exit the airspace or the traffic flow when irregular operations occur. These procedures serve as “pressure release valves” and in this way are symptoms of the occurrence of infrequent phenomena that impact efficiency and safety margins. The occurrence of these procedures is not currently tracked by airlines or Air Navigation Service Providers (ANSP) due to the inability to identify these situations using the existing time-stamped event data (i.e. OOOI data) that is the basis for NAS performance analysis today. This paper describes a Big Data analysis of surveillance track data to establish the frequency of occurrence of Aborted Approaches, and an analysis of voluntary pilot/air traffic controller reports to establish factors leading to Aborted Approaches. Aborted Approaches include a Go Around for a Missed Approach as well as a turn off the final approach segment prior to the Missed Approach Point (MAP). Analysis of 21 days of surveillance track data for approaches at ORD identified a 7.4 in 1000 frequency of approaches resulting in an Aborted Approach. Daily Aborted Approaches ranged from 0 per day to 21 per 1000 approaches per day. Eighty percent of the Aborted Approaches involved a turn off the final approach segment prior to the MAP. An analysis of 467 voluntary pilot/air traffic controller reports from all U.S. airports identified factors leading to aborted approaches: (1) 48% airplane issues (e.g. onboard failure, unstable approach), (2) 27% traffic separation issues, (3) 16% weather (e.g. ceiling, visibility, crosswind), (4) 5% runway issues, and (5) 4% flightcrew-ATC interaction issues. These results suggest mitigation strategies to reduce the high variance in daily occurrences through procedure modification, training and equipment design.;2013;Lance Sherry;10.1109/ICNSurv.2013.6548548;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_11_42;Cross-platform aviation analytics using big-data methods;This paper identifies key aviation data sets for operational analytics, presents a methodology for application of big-data analysis methods to operational problems, and offers examples of analytical solutions using an integrated aviation data warehouse. Big-data analysis methods have revolutionized how both government and commercial researchers can analyze massive aviation databases that were previously too cumbersome, inconsistent or irregular to drive high-quality output. Traditional data-mining methods are effective on uniform data sets such as flight tracking data or weather. Integrating heterogeneous data sets introduces complexity in data standardization, normalization, and scalability. The variability of underlying data warehouse can be leveraged using virtualized cloud infrastructure for scalability to identify trends and create actionable information. The applications for big-data analysis in airspace system performance and safety optimization have high potential because of the availability and diversity of airspace related data. Analytical applications to quantitatively review airspace performance, operational efficiency and aviation safety require a broad data set. Individual information sets such as radar tracking data or weather reports provide slices of relevant data, but do not provide the required context, perspective and detail on their own to create actionable knowledge. These data sets are published by diverse sources and do not have the standardization, uniformity or defect controls required for simple integration and analysis. At a minimum, aviation big-data research requires the fusion of airline, aircraft, flight, radar, crew, and weather data in a uniform taxonomy, organized so that queries can be automated by flight, by fleet, or across the airspace system.;2013;Tulinda Larsen;10.1109/ICNSurv.2013.6548579;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_11_42;Predictive analytics with aviation big data;"Current archive is 50 billion records and growing - Approximately 34 million elements per day - \textasciitilde 1GB/day; Sheer volume of raw surveillance data makes analytics process very difficult; The raw data runs through a series of processes before it can be used for analytics; Next Steps - Continue application of predictive and prescriptive analytics - Big data visualization.";2013;Paul Comitz;10.1109/ICNSurv.2013.6548645;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_11_42;On interference-aware provisioning for cloud-based big data processing;Recent advances in cloud-based big data analysis offers a convenient mean for providing an elastic and cost-efficient exploration of voluminous data sets. Following such a trend, industry leaders as Amazon, Google and IBM deploy various of big data systems on their cloud platforms, aiming to occupy the huge market around the globe. While these cloud systems greatly facilitate the implementation of big data analysis, their real-world applicability remains largely unclear. In this paper, we take the first steps towards a better understanding of the big data system on the cloud platforms. Using the typical MapReduce framework as a case study, we find that its pipeline-based design intergrades the computational-intensive operations (such as mapping/reducing) together with the I/O-intensive operations (such as shuffling). Such computational-intensive and I/O-intensive operations will seriously affect the performance of each other and largely reduces the system efficiency especially on the low-end virtual machines (VMs). To make the matter worse, our measurement also indicates that more than 90 % of the task-lifetime is in the shadow of such interference. This unavoidably reduces the applicability of cloud-based big data processing and makes the overall performance hard to predict. To address this problem, we re-model the resource provisioning problem in the cloud-based big data systems and present an interference-aware solution that smartly allocates the MapReduce jobs to different VMs. Our evaluation result shows that our new model can accurately predict the job completion time across different configurations and significantly improve the user experience for this new generation of data processing service.;2013;Yi Yuan;10.1109/IWQoS.2013.6550282;Conferences;1548-615X;978-1-4799-0588-1
ieee_20221205_08_11_42;Case of small-data analysis for ion implanters in the era of big-data FDC;This paper presents a case study of constructing process models based on physical mechanisms of semiconductor manufacturing tools in attempts to predict behaviours of process conditions. Actual measurements from the processing tools are always corrupted with noises and crunching huge volumes of temporal traces of status variables very often fail to pinpoint the accurate fault conditions, not to mention any of their efficient classifications, should abnormal conditions really exist. The current fashion of moving into massive big data computing is yet to distill concrete correlations among tool conditions and impacts on process results of semiconductor devices. As an alternative before the foolproof maturity of big data cracking, and in contrast to the conventional black-box approach of statistical regressions, we take a fundamental view in constructing physical model of the ion implantation process for a flywheel implanter, first to calculate the motion trajectories and subsequently, the implantation dosage on the wafer. We summarize the underlying solution techniques in principles and leave the specific details of parameter calibrations to individual field practitioners.;2013;Keung Hui;10.1109/ASMC.2013.6552752;Conferences;2376-6697;978-1-4673-5006-8
ieee_20221205_08_11_42;Heading towards big data building a better data warehouse for more data, more speed, and more users;As a new company, GLOBALFOUNDRIES is aggressively agile and looking at ways to not just mimic existing semiconductor manufacturing data management but to leverage new technologies and advances in data management without sacrificing performance or scalability. Being a global technology company that relies on the understanding of data, it is important to centralize the visibility and control of this information, bringing it to the engineers and customers as they need it. Currently, the factories are employing the best practices and data architectures combined with business intelligence analysis and reporting tools. However, the expected growth in data over the next several years and the need to deliver more complex data integration for analysis will easily stress the traditional tools beyond the limits of the traditional data infrastructure. The manufacturing systems vendors need to offer new solutions based on Big Data concepts to reach the new level of information processing that work well with other vendor offerings. In this paper, we will show where we are and where we are heading to manage the increasing needs for handling larger amounts of data with faster as well as secure access for more users.;2013;Raymond Gardiner Goss;10.1109/ASMC.2013.6552808;Conferences;2376-6697;978-1-4673-5006-8
ieee_20221205_08_11_42;A comparative study of enterprise and open source big data analytical tools;In this paper, we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same. The Transaction Processing Council (TPC) has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment, amount of data that can be processed, decision making capabilities, ease of use, energy and time consumed, and the pricing.;2013;Udaigiri Chandrasekhar;10.1109/CICT.2013.6558123;Conferences;;978-1-4673-5757-9
ieee_20221205_08_11_42;Leveraging Big Data Analytics to Reduce Healthcare Costs;The healthcare sector deals with large volumes of electronic data related to patient services. This article describes two novel applications that leverage big data to detect fraud, abuse, waste, and errors in health insurance claims, thus reducing recurrent losses and facilitating enhanced patient care. The results indicate that claim anomalies detected using these applications help private health insurance funds recover hidden cost overruns that aren't detectable using transaction processing systems. This article is part of a special issue on leveraging big data and business analytics.;2013;Uma Srinivasan;10.1109/MITP.2013.55;Magazines;1941-045X;
ieee_20221205_08_11_42;Big Data and Transformational Government;The big data phenomenon is growing throughout private and public sector domains. Profit motives make it urgent for companies in the private sector to learn how to leverage big data. However, in the public sector, government services could also be greatly improved through the use of big data. Here, the authors describe some drivers, barriers, and best practices affecting the use of big data and associated analytics in the government domain. They present a model that illustrates how big data can result in transformational government through increased efficiency and effectiveness in the delivery of services. Their empirical basis for this model uses a case vignette from the US Department of Veterans Affairs, while the theoretical basis is a balanced view of big data that takes into account the continuous growth and use of such data. This article is part of a special issue on big data and business analytics.;2013;Rhoda C. Joseph;10.1109/MITP.2013.61;Magazines;1941-045X;
ieee_20221205_08_11_42;Business Process Analytics Using a Big Data Approach;"Continuous improvement of business processes is a challenging task that requires complex and robust supporting systems. Using advanced analytics methods and emerging technologies--such as business intelligence systems, business activity monitoring, predictive analytics, behavioral pattern recognition, and ""type simulations""--can help business users continuously improve their processes. However, the high volumes of event data produced by the execution of processes during the business lifetime prevent business users from efficiently accessing timely analytics data. This article presents a technological solution using a big data approach to provide business analysts with visibility on distributed process and business performance. The proposed architecture lets users analyze business performance in highly distributed environments with a short time response. This article is part of a special issue on leveraging big data and business analytics.";2013;Alejandro Vera-Baquero;10.1109/MITP.2013.60;Magazines;1941-045X;
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MCG.2013.62;Magazines;1558-1756;
ieee_20221205_08_11_42;Moving big data to the cloud;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geodispersed data into the cloud, for processing using a MapReducelike framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1/l+λ κ/λ with a lookahead window of l, where κ and λ are system parameters of similar magnitude.;2013;Linquan Zhang;10.1109/INFCOM.2013.6566804;Conferences;0743-166X;978-1-4673-5945-0
ieee_20221205_08_11_42;Comparative performance analysis of a Big Data NORA problem on a variety of architectures;Non Obvious Relationship Analysis (NORA) is one of the most stressing classes of Big Data Analytics problems. This paper proposes a reference NORA problem that is representative of real problems, and can rationally scale to very large sizes. It then develops a highly concurrent implementation that can run on large systems. Each step of this implementation is sized in terms of how much of four different resources (CPU, memory, disk, and network) might be used. From this, a parameterized model projecting both execution time and utilizations is used to identify the “tall poles” in performance. The parameters are then modified to represent several different target systems, from a large cluster typical of today to variations in an advanced architecture where processing has been moved into memory. A “thought experiment” then uses this model to discover the parameters of a system that would provide both a near 100X speedup, but with a balanced design where no resource is badly over or under utilized.;2013;Peter M. Kogge;10.1109/CTS.2013.6567199;Conferences;;978-1-4673-6402-7
ieee_20221205_08_11_42;Big data, deep data, and the effect of system architectures on performance;"Summary form only given. “Big Data” traditionally refers to some combination of high volume of data, high velocity of change, and/or wide variety and complexity of the underlying data. Solving such problems has evolved into using paradigms like MapReduce on large clusters of compute nodes. More recently, a growing number of “Deep Data” problems have arisen where it is the relationships between objects, and not necessarily the collections of objects, that are important, and for which the traditional implementation techniques are unsatisfactory. This talk addresses a study of a class of such “challenge problems” first formulated by David Bayliss of LexisNexis, and what are their execution characteristics on both current and future architectures. The goal is to discover, to at least a first order approximation, what are the tall poles preventing a speedup of their solution. A variety or architectures are considered, ranging from standard server blades in large scale configurations, to emerging variations that leverage simpler and more energy efficient chip sets, through systems built on 3D chip stacks, and on to new architectures that were designed from the ground up to “follow the links.” Such architectures are considered for two variants of such problems: a traditional partitioned data approach where data is “pre-boiled” to provide fast response, and one that uses very large graphs in very large shared memories. The results are not necessarily intuitive; the bottlenecks in such problems are not where current systems have the bulk of their capabilities or costs, nor where obvious near term upgrades will have major effects. Instead, it appears that only highly scalable memory-intensive architectures offer the potential for truly major gains in application performance.";2013;Peter M. Kogge;10.1109/CTS.2013.6567201;Conferences;;978-1-4673-6402-7
ieee_20221205_08_11_42;Big data: A review;Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.;2013;Seref Sagiroglu;10.1109/CTS.2013.6567202;Conferences;;978-1-4673-6402-7
ieee_20221205_08_11_42;Addressing big data issues in Scientific Data Infrastructure;Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.;2013;Yuri Demchenko;10.1109/CTS.2013.6567203;Conferences;;978-1-4673-6402-7
ieee_20221205_08_11_42;A disk based stream oriented approach for storing big data;This paper proposes an extension to the generally accepted definition of Big Data and from this extended definition proposes a specialized database design for storing high throughput data from low-latency sources. It discusses the challenges a financial company faces with regards to processing and storing data and how existing database technologies are unsuitable for this niche task. A prototype database called CakeDB is built using a stream oriented, disk based storage design and insert throughput tests are conducted to demonstrate how effectively such a design would handle high throughput data as per the use case.;2013;Peter Membrey;10.1109/CTS.2013.6567204;Conferences;;978-1-4673-6402-7
ieee_20221205_08_11_42;An application-aware approach to systems support for big data [Keynote address];Summary form only given. Everyday 2.5 quintillion (2.5×1018, or 2.5 million trillion) bytes of data are created by people. This data comes from everywhere: from traditional scientific computing and on-line transactions, to popular social network and mobile applications. Data produced in the last two years alone amounts to 90% of the data in the world today! This phenomenal growth and ubiquity of data has ushered in an era of “Big Data”, which brings with it new challenges as well as opportunities. In this talk, I will first discuss big data challenges facing computer and storage systems research, brought on by the huge volume, high velocity, great variety and veracity with which digital data are being produced in the world. I will first introduce some new and ongoing programs at NSF that are relevant to Big Data and to ASAP. I will then present research being conducted in my research group that seeks a scalable systems and application-aware approach to addressing some of the challenges, from the many core and storage architectures to the systems and up to the applications.;2013;Hong Jiang;10.1109/ASAP.2013.6567537;Conferences;1063-6862;978-1-4799-0494-5
ieee_20221205_08_11_42;From green computing to big-data learning: A kernel learning perspective [Keynote address];Summary form only given. The SVM learning model has been successfully applied to an enormously broad spectrum of application domains and has become a main stream of the modern machine learning technologies. Unfortunately, along with its success and popularity, there also raises a grave concern on it suitability for big data learning applications. For example, in some biomedical applications, the sizes may be hundreds of thousands. In social media application, the sizes could be easily in the order of millions. This curse of dimensionality represents a new challenge calling for new learning paradigm as well as application-specific parallel and distributed hardware and software. This talk will explore cost-effective design on kernel-based machine learning and classification for big data learning applications. It will present a recursive tensor based classification algorithm, especially amenable to systolic/wavefront array processors, which may potentially expedite realtime prediction speed by orders of magnitude. For time-series analysis, with nonstationary environment, it is vital to develop time-adaptive learning algorithms so as to allow incremental and active learning. The talk will tackle the active learning problems from two kernel-induced perspectives, one in intrinsic space and another in empirical space. The talk will show, if time permits, an algorithmic example highlighting the application of Map-Reduce technologies to supervised kernel (Slackmin) learning under a parallel and distributed processing framework.;2013;Sun-Yuan Kung;10.1109/ASAP.2013.6567539;Conferences;1063-6862;978-1-4799-0494-5
ieee_20221205_08_11_42;Big Data in 10 Years;Summary form only given, as follows. The complete panel presentation was not made available for publication as part of the conference proceedings. There is a lot of excitement about “Big Data” which is at the intersection of the ongoing explosion in data (volumes, variety, and velocity at which it arrives and must be acted upon), the dramatic increase in cost-effective memory capacities, and the maturation of scale-out processing technologies. Huge investments are being made, and there are great expectations for the gains to be had and the range of applications that will be transformed by new data-driven approaches. What does the future hold? Will the changes indeed be transformative, and if so, what will some of the main changes be? What domains are likely to benefit the most? Or is this just a case of unrealistic expectations waiting to be debunked by reality? We will ask panelists drawn from diverse backgrounds to offer their opinions and, hopefully, to get into violent arguments!;2013;Raghu Ramakrishnan;10.1109/IPDPS.2013.124;Conferences;1530-2075;978-0-7695-4971-2
ieee_20221205_08_11_42;Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that address the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more. This Special Issue therefore aims to publish high quality, original, unpublished research papers in the broad area of Cloud Computing and Big Data, and thus presents a platform for scientists and scholars to share their observations and research results in the field.;2013;;10.1109/TST.2013.6574681;Journals;1007-0214;
ieee_20221205_08_11_42;Authorized Public Auditing of Dynamic Big Data Storage on Cloud with Efficient Verifiable Fine-Grained Updates;"Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the verification is done by a trusted third party, this verification process is also called data auditing, and this third party is called an auditor. However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof of integrity of certain file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads. In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of frequent small updates, such as applications in social media and business transactions.";2014;Chang Liu;10.1109/TPDS.2013.191;Journals;2161-9883;
ieee_20221205_08_11_42;Modeling of system of systems via data analytics — Case for “Big Data” in SoS;Large data has been accumulating in all aspects of our lives for quite some time. Advances in sensor technology, the Internet, wireless communication, and inexpensive memory have all contributed to an explosion of “Big Data”. System of Systems (SoS) integrate independently operating, non-homogeneous systems to achieve a higher goal than the sum of the parts. Today's SoS are also contributing to the existence of unmanageable “Big Data”. Recent efforts have developed a promising approach, called “Data Analytics”, which uses statistical and computational intelligence (CI) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro-computing, evolutionary computation, Bayesian networks, etc. to reduce the size of “Big Data” to a manageable size and apply these tools to a) extract information, b) build a knowledge base using the derived data, and c) eventually develop a non-parametric model for the “Big Data”. This paper attempts to construct a bridge between SoS and Data Analytics to develop reliable models for such systems. A photovoltaic energy forecasting problem of a micro grid SoS will be offered here for a case study of this modeling relation.;2013;Barnabas K. Tannahill;10.1109/SYSoSE.2013.6575263;Conferences;;978-1-4673-5595-7
ieee_20221205_08_11_42;Scaling challenges of packaging in the Era of Big Data;The exascale computing is required in the Era of Big Data. In order to achieve this demand, new technology innovation must be required and packaging scaling including 3D-IC with TSV (Through Silicon Vias) is one of most promising technology. To increase the total bandwidth, the fine pitch die to die interconnection is necessary. Micro-bumping, thermally enhanced underfill and advanced interposer technologies are one of the key technologies. Material selection for reliable fine-pitch interconnection has become a critical challenge in 3D chip stacking. Underfill material between die to die is also very important to reduce the total packaging stress and to enhance the vertical thermal conductivity. Low CTE high density organic substrate is emerging technology for 2.5D structure.;2013;Yasumitsu Orii;;Conferences;0743-1562;978-1-4673-5226-0
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.245;Magazines;1558-0814;
ieee_20221205_08_11_42;Detecting unfolding crises with Visual Analytics and Conceptual Maps Emerging phenomena and big data;Detecting the emergence of a political crisis is a key goal of security informatics. Big data provides us with valuable information on the many socio-economic indicators of crisis dynamics, ranging from unemployment to the trustworthiness of political institutions. However, it is currently challenging to link information on these factors in order for analysts to assess the possible directions of a conflict. At present, while some solutions offer theoretical frameworks for understanding those indicators in the abstract, these frameworks cannot easily be operationalised to the level needed for automatic processing of big data streams. Alternative solutions do automatically code political events, but only offer a high level picture that cannot support the analysis of deeper conflict processes. In this paper, we combine Visual Analytics with Concept Maps to support analysts in monitoring conflicts. Visual Analytics allows the interactive visual exploration of data, while Concept Maps keep this exploration focused by linking data patterns (e.g., occurrence and frequency of keywords) to underlying dynamics (e.g., coordination of activism, salience of violence). We illustrate the potential of our approach through a discussion of how it could be used to study the on-going Syrian crisis. While this approach still requires validation with analysts, we fully specify the technical structure of our approach and exemplify its use to detect shifts in political stability.;2013;Simon F. Pratt;10.1109/ISI.2013.6578819;Conferences;;978-1-4673-6212-2
ieee_20221205_08_11_42;Big Data Security Hardening Methodology Using Attributes Relationship;"Recently developments in network, mining and data store technology have heightened the need for big data and big data security. In this paper, we focus on the big data's characteristic which takes seriously the analysis of value than the data itself. We express the relationship between attributes using nodes and edges. Through this, we propose a big data security hardening methodology by selecting ""protect attributes"" from attributes relationship graph.";2013;Sung-Hwan Kim;10.1109/ICISA.2013.6579427;Conferences;2162-9048;978-1-4799-0602-4
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.287;Magazines;1558-0814;
ieee_20221205_08_11_42;Storage Challenge: Where Will All That Big Data Go?;Big data creates numerous exciting possibilities for organizations, but first they must figure out where they're going to store all that information.;2013;Neal Leavitt;10.1109/MC.2013.326;Magazines;1558-0814;
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.324;Magazines;1558-0814;
ieee_20221205_08_11_42;High productivity processing - Engaging in big data around distributed computing;The steadily increasing amounts of scientific data and the analysis of `big data' is a fundamental characteristic in the context of computational simulations that are based on numerical methods or known physical laws. This represents both an opportunity and challenge on different levels for traditional distributed computing approaches, architectures, and infrastructures. On the lowest level data-intensive computing is a challenge since CPU speed has surpassed IO capabilities of HPC resources and on the higher levels complex cross-disciplinary data sharing is envisioned via data infrastructures in order to engage in the fragmented answers to societal challenges. This paper highlights how these levels share the demand for `high productivity processing' of `big data' including the sharing and analysis of `large-scale science data-sets'. The paper will describe approaches such as the high-level European data infrastructure EUDAT as well as low-level requirements arising from HPC simulations used in distributed computing. The paper aims to address the fact that big data analysis methods such as computational steering and visualization, map-reduce, R, and others are around, but a lot of research and evaluations still need to be done to achieve scientific insights with them in the context of traditional distributed computing infrastructures.;2013;Morris Riedel;;Conferences;;978-953-233-076-2
ieee_20221205_08_11_42;Social-Network-Sourced Big Data Analytics;Very large datasets, also known as big data, originate from many domains. Deriving knowledge is more difficult than ever when we must do it by intricately processing this big data. Leveraging the social network paradigm could enable a level of collaboration to help solve big data processing challenges. Here, the authors explore using personal ad hoc clouds comprising individuals in social networks to address such challenges.;2013;Wei Tan;10.1109/MIC.2013.100;Magazines;1941-0131;
ieee_20221205_08_11_42;A Discussion of Privacy Challenges in User Profiling with Big Data Techniques: The EEXCESS Use Case;User profiling is the process of collecting information about a user in order to construct their profile. The information in a user profile may include various attributes of a user such as geographical location, academic and professional background, membership in groups, interests, preferences, opinions, etc. Big data techniques enable collecting accurate and rich information for user profiles, in particular due to their ability to process unstructured as well as structured information in high volumes from multiple sources. Accurate and rich user profiles are important for applications such as recommender systems, which try to predict elements that a user has not yet considered but may find useful. The information contained in user profiles is personal and thus there are privacy issues related to user profiling. In this position paper, we discuss user profiling with big data techniques and the associated privacy challenges. We also discuss the ongoing EU-funded EEXCESS project as a concrete example of constructing user profiles with big data techniques and the approaches being considered for preserving user privacy.;2013;Omar Hasan;10.1109/BigData.Congress.2013.13;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Engineering Privacy for Big Data Apps with the Unified Modeling Language;This paper describes proposed privacy extensions to UML to help software engineers to quickly visualize privacy requirements, and design privacy into big data applications. To adhere to legal requirements and/or best practices, big data applications will need to apply Privacy by Design principles and use privacy services, such as, and not limited to, anonymization, pseudonymization, security, notice on usage, and consent for usage. We extend UML with ribbon icons representing needed big data privacy services. We further illustrate how privacy services can be usefully embedded in use case diagrams using containers. These extensions to UML help software engineers to visually and quickly model privacy requirements in the analysis phase, this phase is the longest in any software development effort. As proof of concept, a prototype based on our privacy extensions to Microsoft Visio's UML is created and the utility of our UML privacy extensions to the Use Case Diagram artifact is illustrated employing an IBM Watson-like commercial use case on big data in a health sector application.;2013;Dawn N. Jutla;10.1109/BigData.Congress.2013.15;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Consistent Process Mining over Big Data Triple Stores;'Big Data' techniques are often adopted in cross-organization scenarios for integrating multiple data sources to extract statistics or other latent information. Even if these techniques do not require the support of a schema for processing data, a common conceptual model is typically defined to address name resolution. This implies that each local source is tasked of applying a semantic lifting procedure for expressing the local data in term of the common model. Semantic heterogeneity is then potentially introduced in data. In this paper we illustrate a methodology designed to the implementation of consistent process mining algorithms in a `Big Data' context. In particular, we exploit two different procedures. The first one is aimed at computing the mismatch among the data sources to be integrated. The second uses mismatch values to extend data to be processed with a traditional map reduce algorithm.;2013;Antonia Azzini;10.1109/BigData.Congress.2013.17;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud;Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.;2013;Farhana Zulkernine;10.1109/BigData.Congress.2013.18;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Towards a Quality-centric Big Data Architecture for Federated Sensor Services;As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.;2013;Lakshmish Ramaswamy;10.1109/BigData.Congress.2013.21;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Approximate Incremental Big-Data Harmonization;The needs of `big data analytics' increasingly require IT organizations to ingest, process, and extract business insights from ever larger volumes of data that arrive far more rapidly than before, as well as from new sources such as social media, mobile devices, and sensors. However, in order to extract insights from diverse information feeds from multiple, often unrelated sources, these first need to be correlated or harmonized to a common level of granularity. We formally define this commonly arising data harmonization problem. We show how to correlate disparate data sources using map-reduce, but in an approximate and/or incremental manner as often required in practice. We motivate our techniques through a real-life enterprise data-harmonization case study for which we describe our performance results on big-data technologies, namely, Map Reduce, Hadoop and PIG.;2013;Puneet Agarwal;10.1109/BigData.Congress.2013.24;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Countering the Concept-Drift Problem in Big Data Using iOVFDT;How to efficiently uncover the knowledge hidden within massive and big data remains an open problem. One of the challenges is the issue of 'concept drift' in streaming data flows. Concept drift is a well-known problem in data analytics, in which the statistical properties of the attributes and their target classes shift over time, making the trained model less accurate. Many methods have been proposed for data mining in batch mode. Stream mining represents a new generation of data mining techniques, in which the model is updated in one pass whenever new data arrive. This one-pass mechanism is inherently adaptive and hence potentially more robust than its predecessors in handling concept drift in data streams. In this paper, we evaluate the performance of a family of decision-tree-based data stream mining algorithms. The advantage of incremental decision tree learning is the set of rules that can be extracted from the induced model. The extracted rules, in the form of predicate logics, can be used subsequently in many decision-support applications. However, the induced decision tree must be both accurate and compact, even in the presence of concept drift. We compare the performance of three typical incremental decision tree algorithms (VFDT [2], ADWIN [3], iOVFDT [4]) in dealing with concept-drift data. Both synthetic and real-world drift data are used in the experiment. iOVFDT is found to produce superior results.;2013;Hang Yang;10.1109/BigData.Congress.2013.25;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Challenges of Privacy Protection in Big Data Analytics;The big data paradigm implies that almost every type of information eventually can be derived from sufficiently large datasets. However, in such terms, linkage of personal data of individuals poses a severe threat to privacy and civil rights. In this position paper, we propose a set of challenges that have to be addressed in order to perform big data analytics in a privacy-compliant way.;2013;Meiko Jensen;10.1109/BigData.Congress.2013.39;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Techniques for Graph Analytics on Big Data;Graphs enjoy profound importance because of their versatility and expressivity. They can be effectively used to represent social networks, web search engines and genome sequencing. The field of graph pattern matching has been of significant importance and has wide-spread applications. Conceptually, we want to find subgraphs that match a pattern in a given graph. Much work has been done in this field with solutions like Subgraph Isomorphism and Regular Expression matching. With Big Data, scientists are frequently running into massive graphs that have amplified the challenge that this area poses. We study the speedup and communication behavior of three distributed algorithms for inexact graph pattern matching. We also study the impact of different graph partitionings on runtime and network I/O. Our extensive results show that the algorithms exhibit excellent scalable behavior and min-cut partitioning can lead to improved performance under some circumstances, and can drastically reduce the network traffic as well.;2013;M. Usman Nisar;10.1109/BigData.Congress.2013.78;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Service-Generated Big Data and Big Data-as-a-Service: An Overview;With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.;2013;Zibin Zheng;10.1109/BigData.Congress.2013.60;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Big Data Infrastructure for Active Situation Awareness on Social Network Services;Awareness computing aims at our final goal in computer science to simulate human's awareness and cognition. Awareness of social network knowledge in everyday life is actively enabled by big data society. In this paper, we investigate infrastructure for big data analytics for social network services, and propose TF-IDF calculation on big data infrastructure to be aware of social relations on social networks.;2013;Incheon Paik;10.1109/BigData.Congress.2013.61;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Storage Mining: Where IT Management Meets Big Data Analytics;The emerging paradigm shift to cloud based data center infrastructures imposes remarkable challenges to IT management operations, e.g., due to virtualization techniques and more stringent requirements for cost and efficiency. On one hand, the voluminous data generated by daily IT operations such as logs and performance measurements contain abundant information and insights which can be leveraged to assist the IT management. On the other hand, traditional IT management solutions cannot consume and exploit the rich information contained in the data due to the daunting volume, velocity, variety, as well as the lack of scalable data mining and machine learning frameworks to extract insights from such raw data. In this paper, we present our on-going research thrust of designing novel IT management solutions by leveraging big data analytics frameworks. As an example, we introduce our project of Storage Mining, which exploits big data analytics techniques to facilitate storage cloud management. The challenges are discussed and our proof-of-concept big data analytics framework is presented.;2013;Yang Song;10.1109/BigData.Congress.2013.66;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;Distributed Stochastic Aware Random Forests -- Efficient Data Mining for Big Data;Some top data mining algorithms, as ensemble classifiers, may be inefficient to very large data set. This paper makes an initial proposal of a distributed ensemble classifier algorithm based on the popular Random Forests for Big Data. The proposed algorithm aims to improve the efficiency of the algorithm by a distributed processing model called MapReduce. At the same time, our proposed algorithm aims to reduce the randomness impact by following an algorithm called Stochastic Aware Random Forests - SARF.;2013;Joaquim Assunção;10.1109/BigData.Congress.2013.68;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;The Knowledge Service Project in the Era of Big Data;The integration of industrialization and IT application is going to be one of the Chinese new economy development strategies in the future. Therefore, how to make Big Data useful in generating significant productivity improvement in industries has already become one of the most important issues. This paper outlines the platform of knowledge service based on big data processing techniques, which have guided the implementation of the integration of industrialization and IT application in Shenyang. And some challenges we met during implementation of the project were also discussed.;2013;Dongfeng Cai;10.1109/BigData.Congress.2013.70;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_11_42;A Novel Use of Big Data Analytics for Service Innovation Harvesting;Service innovation has assumed considerable significance with the growth of the services sectors of economies globally, yet progress has been slow in devising carefully formulated, systematic techniques to under pin service innovation. This paper argues that a novel approach to big data analytics offers interesting solutions in this space. The paper argues that the use of big data analytics for generating enterprise service insights is often ignored (while the extraction of insights about customers, the market and the enterprise context has received considerable attention). The paper offers a set of techniques (collectively referred to as innovation harvesting) which leverage big data in various forms, including object state sensor data, behaviour logs as well large-scale sources of open data such as the web to mine service innovation insights. The paper also outlines how systematic search might help overcome the limitations of big data analytics in this space.;2013;Aditya K. Ghose;10.1109/ICSSI.2013.45;Conferences;;978-0-7695-4985-9
ieee_20221205_08_11_42;Innovation as the strategic driver of sustainability: big data knowledge for profit and survival;Innovation has long been a central strategic focus of firms, and sustainability has recently become such a focus. We posit that innovation-across the value chain, in strategy, and in business models-is the central element of any truly sustainable business. Linking the theoretical models of Market Orientation (MO) and the Resource Based View of the Firm (RBV), purposive search directed through a Knowledge Based View (KBV) offers a schematic outline for how and where applications of big data analytics can facilitate innovation for long-term sustainability of the firm-for survival, profit, and dynamic fit with the changing environment.;2013;Mariann Jelinek;10.1109/EMR.2013.2259978;Journals;1937-4178;
ieee_20221205_08_11_42;Efficient global portfolios: Big data and investment universes;In this analysis of the risk and return of stocks in the United States and global markets, we apply several portfolio construction and optimization techniques to U.S. and global stock universes. We find that (1) mean-variance techniques continue to produce portfolios capable of generating excess returns above transaction costs and statistically significant asset selection, (2) optimization techniques minimizing expected tail loss are statistically significant in portfolio construction, and (3) global markets offer the potential for greater returns relative to risk than domestic markets. In this experiment, mean-variance, enhanced-index-tracking techniques, and mean-expected tail-loss methodologies are examined. Global equity data and the vast quantity (and quality) of the data relative to U.S. equity modeling have been discussed in the literature. We estimate expected return models in the U.S. and global equity markets using a given stock-selection model and generate statistically significant active returns from various portfolio construction techniques.;2013;J. B. Guerard;10.1147/JRD.2013.2272483;Journals;0018-8646;
ieee_20221205_08_11_42;Storm System Database: A Big Data Approach to Moving Object Databases;Rainfall data is often collected by measuring the amount of precipitation collected in a physical container at a site. Such methods provide precise data for those sites, but are limited in granularity to the number and placement of collection devices. We use radar images of storm systems that are publicly available and provide rainfall estimates for large regions of the globe, but at the cost of loss of precision. We present a moving object database called Storm DB that stores decibel measurements of rain clouds as moving regions, i.e., we store a single rain cloud as a region that changes shape and position over time. Storm DB is a prototype system that answers rain amount queries over a user defined time duration for any point in the continental United States. In other words, a user can ask the database for the amount of rainfall that fell at any point in the US over a specified time window. Although this single query seems straightforward, it is complicated due to the expected size of the dataset: storm clouds are numerous, radar images are available in high resolution, and our system will collect data over a large timeframe, thus, we expect the number and size of moving regions representing storm clouds to be large. To implement our proposed query, we bring together the following concepts: (i) image processing to retrieve storm clouds from radar images, (ii) interpolation mechanisms to construct moving regions with infinite temporal resolution from region snapshots, (iii) transformations to compute exact point in moving polygon queries using 2-dimensional rather than 3-dimensional algorithms, (iv) GPU algorithms for massively parallel computation of the duration that a point lies inside a moving polygon, and (v) map/reduce algorithms to provide scalability. The resulting prototype lays the groundwork for building big data solutions for moving object databases.;2013;Brian Olsen;10.1109/COMGEO.2013.30;Conferences;;978-0-7695-5012-1
ieee_20221205_08_11_42;Efficient Online Sharing of Geospatial Big Data Using NoSQL XML Databases;Summary form only given: Today a huge amount of geospatial data is being created, collected and used more than ever before. The ever increasing observations and measurements of geo-sensor networks, satellite imageries, point clouds from laser scanning, geospatial data of Location Based Services (LBS) and location-based social networks has become a serious challenge for data management and analysis systems. Traditionally, Relational Database Management Systems (RDBMS) were used to manage and to some extent analyze the geospatial data. Nowadays these systems can be used in many scenarios but there are some situations when using these systems may not provide the required efficiency and effectiveness. More specifically when the geospatial data has high volume, high frequency of change (in both data content and data structure) and variety of structures, the conventional data storage systems cannot provide needed efficiency in online systems in terms of performance and scalability. In these situations, NoSQL solutions can provide the efficiency necessary for applications using geospatial data. This paper provides an overview of the characteristics of geospatial big data, possible solutions for managing and processing them. Then the paper provides an overview of the major types of NoSQL solutions, their advantages and disadvantages and the challenges they present in managing geospatial big data. Then the paper elaborates on serving geospatial data using standard geospatial web services with a NoSQL XML database as a backend.;2013;Pouria Amirian;10.1109/COMGEO.2013.34;Conferences;;978-0-7695-5012-1
ieee_20221205_08_11_42;Big Data: Unleashing information;Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.;2013;James M. Tien;10.1109/ICSSSM.2013.6602615;Conferences;2161-1904;978-1-4673-4434-0
ieee_20221205_08_11_42;Store, schedule and switch - A new data delivery model in the big data era;The big data era is posing unprecedented challenges on the existing network infrastructure. In today's networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service level requirements. The so called elephant data, which may be less sensitive to transfer delay, compete precious network resources with mice data, in most cases from interactive and delay sensitive applications. Consequently, the Quality of Service (QoS) of interactive applications is hard to provision, and the utility of network is low. We propose a new data transfer model to complement the existing per-packet forwarding paradigm. In the new data transfer model, a service level requirement is assigned (by the data source) to each big data transfer request. Instead of transferring these data on per-packet bases immediately upon entering the network, the network stores the data until it find necessary, or enough network resource is available for that transfer. The scheduled data delivery is realized through the use of dynamic circuit switching. We also present some preliminary simulation results of SSS networks.;2013;Weiqiang Sun;10.1109/ICTON.2013.6602860;Conferences;2162-7339;978-1-4799-0683-3
ieee_20221205_08_11_42;Study on Big Data Center Traffic Management Based on the Separation of Large-Scale Data Stream;The network of traditional data center has been usually designed and constructed for the provision of user's equal access of data centre's resource or data. Therefore, network administrators have a strong tendency to manage user traffic from the viewpoint that the traffic has a similar size and characteristics. But, the emersion of big data begins to make data centers have to deal with 1015 byte-data transfer at once. Such a big data transfer can cause problems in network traffic management in the existed data center. And, the tiered network architecture of the legacy data center magnifies the magnitude of the problems. One of the well-known big data in science is from large hadron collider such as LHC in Swiss CERN. CERN LHC generates multi-peta byte data per year. From our experience of CERN data service, this paper showed the impact of network traffic affected by large-scale data stream using NS2 simulation, and then, suggested the evolution direction based on separating of large-scale data stream for the big data center's network architecture.;2013;Hyoung Woo Park;10.1109/IMIS.2013.104;Conferences;;978-0-7695-4974-3
ieee_20221205_08_11_42;Split File Model for Big Data in Low Throughput Storage;The demand for low-cost, large-scale storage is increasing. Recently, several low-throughput storage services such as the Pogo plug Cloud have been developed. These services are based on Amazon Glacier. They have low throughput, but low cost and large capacity. Therefore, these services are suitable for backups or archiving big data and can be used instead of offline storage tiers. To utilize such low throughput storage efficiently, we need tools for effective deduplication and resumable transfers, amongst others. We propose a split file model that can represent big data efficiently in low throughput storage. In the split file model, a large file is divided into many small parts, which are stored in a directory. We have developed tool commands to support the use of split files in a transparent way. Using these commands, replicated data is naturally excluded and effective shallow copying is supported. In this paper, we describe the split file model in detail and evaluate an implementation thereof.;2013;Minoru Uehara;10.1109/CISIS.2013.48;Conferences;;978-0-7695-4992-7
ieee_20221205_08_11_42;Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds;Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.;2013;Weiyi Shang;10.1109/ICSE.2013.6606586;Conferences;1558-1225;978-1-4673-3073-2
ieee_20221205_08_11_42;How deep data becomes big data;We present some problems and solutions for situations when compound and semantically rich nature of data records, such as scientific articles, creates challenges typical for big data processing. Using a case study of named entity matching in SONCA system we show how big data problems emerge and how they are solved by bringing together methods from database management and computational intelligence.;2013;Marcin Szczuka;10.1109/IFSA-NAFIPS.2013.6608465;Conferences;;978-1-4799-0348-1
ieee_20221205_08_11_42;Big-data integration methodologies for effective management and data mining of petroleum digital ecosystems;"Petroleum industries' big data characterize heterogeneity and they are often multidimensional in nature. In the recent past, explorers narrate petroleum system, as an ecosystem, in which elements and processes are constantly interacted and communicated each other. Exploration is one of the key super-type data dimensions of petroleum ecosystem, (including seismic dimension), exhibiting high degree of heterogeneity, sequence identity and structural similarity; this is especially the case for, elements and processes that are unique to petroleum systems of South East Asia. Existing approaches of petroleum data organizations have limitations in capturing and integrating petroleum systems data. An alternative method uses ontologies and does not rely on keywords or similarity metrics. The conceptual framework of petroleum ontology (PO) is to promote reuse of concepts and a set of algebraic operators for querying petroleum ontology instances. This ontology-based fine-grained multidimensional data structuring adapts to warehouse metadata modeling. The data integration process facilitates to metadata models, which are deduced for Indonesian sedimentary basins, and is useful for data mining and subsequent data interpretation including geological knowledge mapping.";2013;Shastri L Nimmagadda;10.1109/DEST.2013.6611345;Conferences;2150-4946;978-1-4799-0784-7
ieee_20221205_08_11_42;Data migration ecosystem for big data invited paper;Data Migration is the process of moving data from a system or systems to a new environment. Often, it is a sub-activity of a business application deployment. Big data is defined as data that is huge, has heterogeneous data dictionaries and involves complex manipulation. Due to nature of the process complexity and its resources hungry approach in migrating Big Data, special attention is required to have a proven methodology and ecosystem to govern the process. The Data Migration Ecosystem for Big Data is the productive set of interacting processes, practices and environments, to collect data from one location, storage medium, or hardware/software system, to cleanse, transform and transfer it to another. The processes and practices are governed by rules and disciplines, with the goal of ensuring information is complete, of high accuracy and consistent. This paper is based on our experience in migrating data for a Malaysia government agency, which involves approximately 1 billion rows of data from 31 heterogeneous sources / systems. Some of the data migrated was created in the seventies (1970), for which the business logic has since been enhanced or changed. The challenge is further complicated by available data being from proprietary databases that are non-RDMS compliance and includes data that is manually maintained in Microsoft Excel spreadsheets.;2013;Koong Wah Yan;10.1109/DEST.2013.6611352;Conferences;2150-4946;978-1-4799-0784-7
ieee_20221205_08_11_42;Big data: Issues, challenges, tools and Good practices;Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.;2013;Avita Katal;10.1109/IC3.2013.6612229;Conferences;;978-1-4799-0190-6
ieee_20221205_08_11_42;Wearable monitors on babies: Big data saving little people;"Today 8% of Canadian babies are born premature and internationally the average is 10% These early births, are responsible for three quarters of all infant deaths in Canada. Premature infants together with ill term infants are cared for in Neonatal Intensive Care U nits (NICUs) internationally contain state of th e art medical equipment to monitor and provide life support, resulting in a significant Big Data environment. In addition, graduates of neonatal intensive care may be discharged with medical devices to support continued monitoring as ambulatory patients in and outside the ho me setting. In both NICU and ambulatory contexts wearable patient monitoring has many social implications. This research presents an assessment of the social implications of Big Data solutions for criti cal care within the context of the Artemis project that is enabling Big Data solutions for: 1) Real-ti me processing of complex intensive care physiological signals for new and earlier condition onset detection; 2) new approaches to physiological data analysis to support clinical research; and 3) cloud computing/services computing to provide rural and remote communities with greater options for a dvanced critical care within their own community healthcare facilities.";2013;Carolyn McGregor;10.1109/ISTAS.2013.6613120;Conferences;2158-3404;978-1-4799-1242-1
ieee_20221205_08_11_42;HireSome-II: Towards Privacy-Aware Cross-Cloud Service Composition for Big Data Applications;Cloud computing promises a scalable infrastructure for processing big data applications such as medical data analysis. Cross-cloud service composition provides a concrete approach capable for large-scale big data processing. However, the complexity of potential compositions of cloud services calls for new composition and aggregation methods, especially when some private clouds refuse to disclose all details of their service transaction records due to business privacy concerns in cross-cloud scenarios. Moreover, the credibility of cross-clouds and on-line service compositions will become suspicional, if a cloud fails to deliver its services according to its “promised” quality. In view of these challenges, we propose a privacy-aware cross-cloud service composition method, named HireSome-II (History record-based Service optimization method) based on its previous basic version HireSome-I. In our method, to enhance the credibility of a composition plan, the evaluation of a service is promoted by some of its QoS history records, rather than its advertised QoS values. Besides, the k-means algorithm is introduced into our method as a data filtering tool to select representative history records. As a result, HireSome-II can protect cloud privacy, as a cloud is not required to unveil all its transaction records. Furthermore, it significantly reduces the time complexity of developing a cross-cloud service composition plan as only representative ones are recruited, which is demanded for big data processing. Simulation and analytical results demonstrate the validity of our method compared to a benchmark.;2015;Wanchun Dou;10.1109/TPDS.2013.246;Journals;2161-9883;
ieee_20221205_08_11_42;Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that addresses the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more.;2013;;10.1109/TST.2013.6616527;Journals;1007-0214;
ieee_20221205_08_11_42;Inconsistencies in big data;We are faced with a torrent of data generated and captured in digital form as a result of the advancement of sciences, engineering and technologies, and various social, economical and human activities. This big data phenomenon ushers in a new era where human endeavors and scientific pursuits will be aided by not only human capital, and physical and financial assets, but also data assets. Research issues in big data and big data analysis are embedded in multi-dimensional scientific and technological spaces. In this paper, we first take a close look at the dimensions in big data and big data analysis, and then focus our attention on the issue of inconsistencies in big data and the impact of inconsistencies in big data analysis. We offer classifications of four types of inconsistencies in big data and point out the utility of inconsistency-induced learning as a tool for big data analysis.;2013;Du Zhang;10.1109/ICCI-CC.2013.6622226;Conferences;;978-1-4799-0781-6
ieee_20221205_08_11_42;Is privacy still an issue in the era of big data? — Location disclosure in spatial footprints;Geospatial data that were once difficult to obtain are now readily available to the public with the development of geospatial technologies. The ubiquitous use of social networking and location-based services enables easy sharing of personal stories among many people. With or without awareness of location disclosure, some users reveal a considerable amount of geopersonal information to the general public. Privacy issues have been raised again in the GIScience community. This study shows that home and work places may be inferred from georeferenced tweets of heavy Twitter users. Is privacy still an issue in the era of big data when people freely share blogs, photos, videos, and spatial footprints over the Internet? Or do people willingly decide to give out location information in order to gain benefits from the disclosure? After a discussion on possible reasons for people to share spatial footprints, this paper invites more research on perception of geoprivacy.;2013;Linna Li;10.1109/Geoinformatics.2013.6626191;Conferences;2161-0258;978-1-4673-6227-6
ieee_20221205_08_11_42;BigLS - The 1st International Workshop on Big Data in Life Sciences;With the ever-increasing volume, velocity and variety biological and biomedical data collections continue to pose new challenges and increasing demands on computing and data management. The inherent complexity of this big data has forced us to rethink how we collect, store, combine and analyze it.;2013;Ananth Kalyanaraman;10.1109/ICCABS.2013.6629242;Conferences;;978-1-4799-0716-8
ieee_20221205_08_11_42;Workflow-driven programming paradigms for distributed analysis of biological big data;Scientific workflows have been used as a programming model to automate scientific tasks ranging from short pipelines to complex workflows that span across heterogeneous data and computing resources. While utilization of scientific workflow technologies varies slightly across different scientific disciplines, all informatics and computational science disciplines provide a common set of attributes to facilitate and accelerate workflow-driven research. Scientific workflows provide assembly of complex processing easily in local or distributed environments via rich and expressive programming models. Scientific workflows enable transparent access to diverse resources ranging from local clusters and traditional supercomputers to elastic and heterogeneous Cloud resources. Scientific workflows support incorporation of multiple software tools including domain specific tools for standard processing to custom generalized workflows and middleware tools that can be reused in various contexts. Scientific workflows often collect provenance information on workflow entities, e.g., workflow definitions, their executions and run time parameters, and, in turn, assure a level of reproducibility while enabling referencing and replicating results. While doing all these, scientific workflows often foster an open-source, open-access and standards-driven community development model based on sharing and collaborations. Cyberinfrastructure platforms and gateways commonly employ scientific workflows to bridge the gap between the infrastructure and users needs. While capturing and communicating the scientific process formally, workflows ensure flexibility, synergy between users, provide optimized usage of resources, increase reuse and ensure compliance with system specific data models and community-driven standards. Currently, scientific workflows are used widely in life sciences at different stages of end-to-end data lifecycle from generation to analysis and publication of biological data. The data handled by such workflows can be produced by sequencers, sensor networks, medical imaging instruments and other heterogeneous resources at significant rates at decreasing costs making the analysis and archival of such data a 'big data' challenge. Additionally, these new biological data resources are making new and exciting research in areas including metagenomics and personalized medicine possible. However, the analysis of big biological data is still very costly requiring new scalable computational models and programming paradigms to be applied to biological analysis. Although, some new paradigms exists for analysis of big data, application of these best practices to life sciences is still in its infancy. Scientific workflows can act as a scaffold and help speed this process up via combination of existing programming models and computational models with the challenges of biological problems as reusable blocks. In this talk, I will talk about such an approach that builds upon distributed data parallel patterns, e.g., MapReduce, and underlying execution engines, e.g., Hadoop, and matches the computational requirements of bioinformatics tools with such patterns and engines. The results of the presented approach is developed as a part of the bioKepler (bioKepler.org) module and can be downloaded to work within the release 2.4 of the Kepler scientific workflow system (kepler-project.org).;2013;Ilkay Altintas;10.1109/ICCABS.2013.6629243;Conferences;;978-1-4799-0716-8
ieee_20221205_08_11_42;Breaking the boundary for whole-system performance optimization of big data;MapReduce plays an critical role in finding insights in Big Data. The performance optimization of MapReduce programs is challenging because it requires a comprehensive understanding of the whole system including both hardware layers (processors, storages, networks and etc), and software stacks (operating systems, JVM, runtime, applications and etc). However, most of the existing performance tuning and optimization are based on empirical and heuristic attempts. It remains a blank on how to build a systematical framework which breaks the boundary of multiple layers for performance optimization. In this paper, we propose a performance evaluation framework by correlating performance metrics from different layers, which provides insights to efficiently pinpoint the performance issue. This framework is composed of a series of predefined patterns. Each pattern indicates one or more potential issues. The behavior of a MapReduce program is mapped to the corresponding resource utilization. The framework provides a holistic approach which allows users at different levels of experience to conduct MapReduce program performance optimization. We use Terasort benchmark running on a 10-node Power7R2 cluster as a real case to show how this framework improves the performance. By this framework, we finally get the Terasort result improved from 47 mins to less than 8 mins. In addition to the best practice on performance tuning, several key findings are summarized as valuable workload analysis for JVM, MapReduce runtime and application design.;2013;Yan Li;10.1109/ISLPED.2013.6629278;Conferences;;978-1-4799-1234-6
ieee_20221205_08_11_42;The role of big data in improving power system operation and protection;This paper focuses on the use of extremely large data sets in power system operation, control, and protection, which are difficult to process with traditional database tools and often termed big data. We will discuss three aspects of using such data sets: feature extraction, systematic integration for power system applications, and examples of typical applications in the utility industry. The following analytics tasks based on big data methodology are elaborated upon: corrective, predictive, distributed and adaptive. The paper also outlines several research topics related to asset management, operation planning, realtime monitoring and fault detection/protection that present new opportunities but require further investigation.;2013;Mladen Kezunovic;10.1109/IREP.2013.6629368;Conferences;;978-1-4799-0199-9
ieee_20221205_08_11_42;Big Data's Risks and Opportunities for ICT Agriculture;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address these issues and to create a common background to be applied in the new NEDO project in which Sojo University is an active research member.";2013;Dennis A. Ludena R;10.1109/IIAI-AAI.2013.60;Conferences;;978-1-4799-2134-8
ieee_20221205_08_11_42;Virtual Dataspace -- A Service Oriented Model for Scientific Big Data;The massive, distributed, heterogeneous and diverse features of big data have raised challenges to traditional data management systems. As the development and innovation of Data Space, virtual data space (VDS) model is proposed for big data management. Local ontologies are created from data sources. Then the local ontologies are mapped and formed a global ontology. Based on this, access log and user feedback are considered for data evolution. At last, a material scientists-oriented service (materials scholar assistant) is introduced as the application case of VDS.;2013;Wei Lin;10.1109/EIDWT.2013.5;Conferences;;978-1-4799-2140-9
ieee_20221205_08_11_42;On use of big data for enhancing network coverage analysis;Proliferation of data services has made it mandatory for operators to be able identify geographical regions with 3G connectivity discontinuity in a scalable and cost-efficient manner. The currently used methods for such analysis are either costly — such as in drive tests, partly unreliable — such as in network simulation approaches, or are not precise enough — such as in base station key performance indicators (KPI) based approaches. In this paper, towards addressing these inadequacies, we propose a 3G coverage analysis method that makes use of “big data” processing schemes and the vast amounts of network data logged in mobile operators. In the proposed scheme, the BSSAP mobility and radio resource management messages between the BSS and MSC nodes of the operator network are processed to identify inter-technology handovers from 3G (WCDMA) access to 2G (EDGE, GPRS, GSM). Demonstrative examples show that the proposed mechanism produces accurate and precise results, outperforming the base station KPI-based approach.;2013;Ömer Faruk Çelebi;10.1109/ICTEL.2013.6632155;Conferences;;978-1-4673-6425-6
ieee_20221205_08_11_42;Survey of Research on Big Data Storage;With the development of cloud computing and mobile Internet, the issues related to big data have been concerned by both academe and industry. Based on the analysis of the existed work, the research progress of how to use distributed file system to meet the challenge in storing big data is expatiated, including four key techniques: storage of small files, load balancing, copy consistency, and de-duplication. We also indicate some key issues need to concern in the future work.;2013;Xiaoxue Zhang;10.1109/DCABES.2013.21;Conferences;;978-0-7695-5060-2
ieee_20221205_08_11_42;Big data on small screens;Data is one of the integral assets of the organization. There are many companies which have the huge amount of data but the challenge lies in how to make it more usable. In this 21st century the world around us is rapidly changing and smartphones are tightly coupled to the users. Customers mainly concentrate on using smartphone devices to access telephonic services. Whereas now it's the time for companies to make use of this available big data from the customers. This big data can be used in one or the other form in understanding the behavior of the customers. In this paper author would present the usage of the big data on smart phones to provide sophisticated services to the customers in real time. Also author would explore how this big data on small screens can make a difference in major events like US Presidential Elections.;2013;Aditya R Desai;10.1109/ICACCI.2013.6637287;Conferences;;978-1-4799-2659-6
ieee_20221205_08_11_42;HPCS 2013 Keynotes: Tuesday keynote: Big process for big data;"These keynotes discuss the following: Big Process for Big Data; Killer-Mobiles: The Way Towards Energy Efficient High Performance Computers?; The UberCloud HPC Experiment - Paving the Way to HPC as a Service; and High Performance Fault Tolerance / Resilience at Extreme Scale.";2013;Ian Foster;10.1109/HPCSim.2013.6641378;Conferences;;978-1-4799-0837-0
ieee_20221205_08_11_42;Big data analytics as a service: Exploring reuse opportunities;As data scientists, we live in interesting times. Data has been the No. 1 fast growing phenomenon on the Internet for the last decade. Big data analytics have the potential to reveal deep insights hidden by big data that exceeds the processing capacity of existing systems, such as peer influence among customers, revealed by analyzing shoppers' transactions, social and geographical data. In the past 40 years, data was primarily used to record and report business activities and scientific events, and in the next 40 years data will be used also to derive new insights, to influence business decisions and to accelerate scientific discovery. The key challenge is to provide the right platforms and tools to make reasoning of big data easy and simple. In this keynote talk, I will explore reuse opportunities and challenges from multiple dimensions towards delivering big data analytics as a service. I will illustrate by example the importance and challenges of utilizing programmable algorithm abstractions for many seemingly domain-dependent data analytics tasks. Another reuse opportunity is to exploit unconventional data structures and big data processing constructs to simplify and speed up the big data processing.;2013;Ling Liu;10.1109/IRI.2013.6642438;Conferences;;978-1-4799-1050-2
ieee_20221205_08_11_42;Implementation of the Big Data concept in organizations - possibilities, impediments and challenges;This paper is devoted to the analysis of the Big Data phenomenon. It is composed of seven parts. In the first, the growing role of data and information and their rapid increase in the new socio-economical reality, are discussed. Next, the notion of Big Data is defined and the main sources of growth of data are characterized. In the following part of the paper the most significant possibilities linked with Big Data are presented and discussed. The next part is devoted to the characterization of tools, techniques and the most useful data in the context of Big Data initiatives. In the following part of the paper the success factors of Big Data initiatives are analyzed, followed by an analysis of the most important problems and challenges connected with Big Data. In the final part of the paper, the most significant conclusions and suggestions are offered.;2013;Janusz Wielki;;Conferences;;978-83-60810-52-1
ieee_20221205_08_11_42;Applying big data and linked data concepts in supply chains management;"One of the contemporary problems, and at the same time a big opportunity, in business networks of supply chains are the issues associated with the vast amounts of data arising there. The data may be utilized by the decision support systems in logistics; nevertheless, often there is an information integration problem. The problems with information interchange are related to issues with exchange between independently designed data systems. The networked supply chains will need appropriate IT architectures to support the cooperating business units utilizing structured and unstructured big data and the mechanisms to integrate data in heterogeneous supply chains. In this paper we analyze the capabilities of the big data technology architectures with cloud computing under usage of Linked Data in business process management in supply chains to cope with unstructured near-time data and data silos problems. We present our approach on a 4PL (Fourth-party Logistics) integrator business process example.";2013;Silva Robak;;Conferences;;978-83-60810-52-1
ieee_20221205_08_11_42;Using Big Data and predictive machine learning in aerospace test environments;"It is estimated that in 2012 most mid-size companies in the USA generate the equivalent data of the US Library of Congress in 1 year. As a company, Wal-Mart creates the equivalent of 50 million filing cabinets worth of data every hour. While these numbers seem incredible, the trend for most companies is an increasing volume of data generation and storage. Test Data generated by Automatic Test Equipment (ATE) in R&D, manufacturing and Repair environments is no exception to this increased volume of data. The challenge of this enormous amount of Test Data is how to provide people with effective ways to make decisions from it. Data visualization through charts, graphs and reports has been, historically, one of the more effective ways to provide actionable intelligence because humans can readily make decisions based on patterns and comparisons. But as data volume goes up, even this method is reaching its limits. When one starts to combine large datasets like Manufacturing Test Data and Repair Data together, data visualization becomes problematic. More sophisticated algorithmic, machine learning and predictive approaches become critical. In this paper, we will explore the experiences of using predictive algorithms on ""Big Data"" from both Manufacturing Test and Repair Test environments in the complex mission critical aerospace industry. By effectively using datasets from different functional areas, we will be looking at applying SPC techniques to answer new questions about the correlation of Repair test data and manufacturing data with the end goal to predict number of returns in the future and minimize product escapes.";2013;Tom Armes;10.1109/AUTEST.2013.6645085;Conferences;1558-4550;978-1-4673-5681-7
ieee_20221205_08_11_42;Embedded Analytics and Statistics for Big Data;Embedded analytics and statistics for big data have emerged as an important topic across industries. As the volumes of data have increased, software engineers are called to support data analysis and applying some kind of statistics to them. This article provides an overview of tools and libraries for embedded data analytics and statistics, both stand-alone software packages and programming languages with statistical capabilities.;2013;Panos Louridas;10.1109/MS.2013.125;Magazines;1937-4194;
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.130;Magazines;1937-4194;
ieee_20221205_08_11_42;Location: A Feature for Service Selection in the Era of Big Data;This paper introduces a service selection model with the service location considered. The location of a service represents its position in the network, which determines the transmission cost of calling this service in the composite service. The more concentrated the invoking services are, the less transmission time the composite service costs. On the other hand, the more and more popular big data processing services, which need to transfer mass data as input, make the effect much more obvious than ever before. Therefore, it is necessary to introduce service location as a basic feature in service selection. The definition and membership functions of service location are presented in this paper. After that, the optimal service selection problem is represented as an optimization problem under some reasonable assumptions. A shortest-path based algorithm is proposed to solve this optimization problem. At last, the case of railway detection is studied for better understanding of our model.;2013;Luo Zhiling;10.1109/ICWS.2013.75;Conferences;;978-0-7695-5025-1
ieee_20221205_08_11_42;pLSM: A Highly Efficient LSM-Tree Index Supporting Real-Time Big Data Analysis;Big Data boosts the development of data management and analysis in database systems but it also poses a challenge to traditional database. NoSQL databases are provided to deal with the new challenges brought by Big Data because of its high performance, storage, scalability and availability. In NoSQL databases, it is an essential requirement to provide scalable and efficient index services for real-time data analysis. Most existing index solutions focus on improving write throughput, but at the cost of poor read performance. We designed a new plug-in system PuntStore with pLSM (Punt Log Structured Merge Tree) index engine. To improve read performance, Cache Oblivious Look-ahead Array (COLA) is adopted in our design. We also presented a novel compact algorithm in bulk deletion to support migration of data from temporary storage to data warehouse for further analysis.;2013;Jin Wang;10.1109/COMPSAC.2013.40;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_11_42;Big Data -- Opportunities and Challenges Panel Position Paper;This paper summarizes opportunities and challenges of big data. It identifies important research directions and includes a number of questions that have been debated by the panel.;2013;Elisa Bertino;10.1109/COMPSAC.2013.143;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_11_42;Analytics over Big Data: Exploring the Convergence of DataWarehousing, OLAP and Data-Intensive Cloud Infrastructures;This paper explores the convergence of Data Warehousing, OLAP and data-intensive Cloud Infrastructures in the context of so-called analytics over Big Data. The paper briefly reviews some state-of-the-art proposals, highlights open research issues and, finally, it draws possible research directions in this scientific field.;2013;Alfredo Cuzzocrea;10.1109/COMPSAC.2013.152;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_11_42;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.370;Magazines;1558-0814;
ieee_20221205_08_11_42;Data Evolution Analysis of Virtual DataSpace for Managing the Big Data Lifecycle;New challenge about the constantly changing of associated data in big data management has arisen, which leads to the issue of data evolution. In this paper, a data evolution model of Virtual Data Space (VDS) is proposed for managing the big data lifecycle. Firstly, the concept of data evolution cycle is defined, and the lifecycle process of big data management is described. Based on these, the data evolution lifecycle is analyzed from the data relationship, the user requirements, and the operation behavior. Secondly, the classification and key concepts about the data evolution process are described in detail. According to this, the data evolution model is constructed by defining the related concepts and analyzing the data association in VDS, for the capture and tracking of dynamic data in the data evolution cycle. Then we discuss the cost problem about data dissemination and change. Finally, as the application case, the service process of dynamic data in the field of materials science is described and analyzed. We verify the validity of data evolution modeling in VDS by the comparison of traditional database, data space, and VDS. It shows that this analysis method is efficient for the data evolution processing, and very suitable for the data-intensive application and the real-time dynamic service.;2013;Xin Cheng;10.1109/IPDPSW.2013.57;Conferences;;978-0-7695-4979-8
ieee_20221205_08_11_42;A Workflow Framework for Big Data Analytics: Event Recognition in a Building;This paper studies event recognition in a building based on the patterns of power consumption. It is a big challenge to identify what kinds of events happened in a building without additional devices such as camera and motion sensors, etc. Instead, we learn when and how the events happened from the historical record of power consumption and apply the lesson into the design of an event recognition system (ERS). The ERS will find out abnormal power usage to avoid wasting power, which leads to the energy savings in a building. The ERS involves big data analytics with a large size of dataset collected in a real time. Such a data intensive system is usually viewed as a workflow. A workflow management is a significant task of the system requiring data analysis in terms of the system scalability to maintain high throughput or fast speed analysis. We propose a workflow framework that allows users to perform remote and parallel workflow execution, whose tasks are efficiently scheduled and distributed in cloud computing environment. We run the ERS as a target system for the proposed framework with power consumption data (whose size is approximately 20GB or more) collected from each of over 240 rooms in a building at Dept. of Engineering, Tokyo University in 2011. We show that the proposed framework accelerates the speed of data analysis by providing scaling infrastructure and parallel processing feature utilizing cloud computing technologies. We also share our experience and results on the big data analytics and discuss how the studies contribute to achieve Green Campus.;2013;Chonho Lee;10.1109/SERVICES.2013.29;Conferences;2378-3818;978-0-7695-5024-4
ieee_20221205_08_11_42;Social Issues of Big Data and Cloud: Privacy, Confidentiality, and Public Utility;"Business people and academia are now excited about Big Data and Cloud Computing as the new and most innovative means for enhancing productivity and customer satisfaction. Simultaneously, there are strong concerns about privacy not only among privacy advocates but among consumers in general, and how to strike a right balance is the main theme in every field of science. However, it is quite strange that very little attention has been paid to the concept of confidentiality, which must be the core element of privacy. This paper first tries to analyze the following two dichotomies as a basis for possible policy considerations: (1) privacy approach in the United States versus confidentiality approach in the United Kingdom, though they share the same common law tradition, and (2) clear demarcation between Information Service and Telecommunications in the United States, dating back to the Computer Inquiry in the 1970s. This paper also analyzes the features of the Cloud and discusses the possibility of treating it as a new type of Public Utility, namely Information Utility. This hypothesis should be rejected, because there are crucial differences in market structures, regardless of clear similarities in service features. Instead, this paper emphasizes the necessity of protecting confidentiality as an industrial norm. Taking into account the long tradition of free market for computing industries, self-regulation is basically preferable to government regulation. But from a different viewpoint of ""nudge"", a hybrid combination of libertarianism and paternalism, this paper concludes by proposing five short recommendations including fair contract terms as well as unbundling confidentiality from privacy.";2013;Koichiro Hayashi;10.1109/ARES.2013.66;Conferences;;978-0-7695-5008-4
ieee_20221205_08_11_42;Towards an efficient routing web processing service through capturing real-time road conditions from big data;The rapidly growing number of crowdsourcing platforms generates huge volumes of volunteered geographic information (VGI), which requires analysis to reveal their potential. The huge volumes of data appear as an opportunity to improve various applications, including routing and navigation services. How existing techniques for dealing with Big Data could be useful for the analysis of VGI remains an open question, since VGI differs from traditional data. In this paper, we focus on examining the latest developments and issues associated with big data from the perspective of the analysis of VGI. This paper notably presents our new architecture for exploiting Big VGI in event service processing in support to optimization of routing service. In addition, our study highlights the opportunities that are created by the emergence of Big VGI and crowdsourced data on improving routing and navigation services, as well as the challenges that remain to be addressed to make this a reality. Finally, avenues for future research on the next generation of collaborative routing and navigation services are presented.;2013;Mohamed Bakillah;10.1109/CEEC.2013.6659463;Conferences;;978-1-4799-0383-2
ieee_20221205_08_11_42;A big data file transfer tool for tablet-class machines;A big data file transport (BDFT) protocol is presented that minimize overheads associated with packet streaming in Java. The BDFT protocol relies on block 1-data transfers, and the elimination of unnecessary data copying between the application layer and the send socket in the standard Java IO model. The implementation uses the Java New IO (NIO) and the Zerocopy libraries. Several experiments are described and results compared against the standard Java IO - a stream-based file transport protocol. The motivation for this study is the development of a client/server big data file transport protocol for tablet-class client machines that rely on the Java Remote Method Invocation (RMI) package for distributed computing.;2013;Tevaganthan Veluppillai;10.1109/IDAACS.2013.6663011;Conferences;;978-1-4799-1427-2
ieee_20221205_08_11_42;A Big Data application framework for consumer behavior analysis;More than ever before, the amount of data about consumers, suppliers and products has been exploding in today consumer world referred as “Big Data”. In addition, more data is available to the consumer world from multiple sources including social network platforms. In order to deal with such amount of data, a new emerging technology “Big Data Analytics” is explored and employed for analyzing consumer behaviors and searching their information needs. Specifically, this paper proposes a Big Data application framework for analyzing consumer behaviors by using topological data structure, co-occurrence methodology and Markov chain theory. First, the consumer related data is translated into a topological data structure. Second, using topological relationships, a co-occurrence matrix is formed to deduce Markov chain model for consumer behavior analysis. Finally, some simulation results are shown to confirm the effectiveness of the proposed framework.;2013;Thi Thi Zin;10.1109/GCCE.2013.6664813;Conferences;2378-8143;978-1-4799-0890-5
ieee_20221205_08_11_42;A successful application of big data storage techniques implemented to criminal investigation for telecom;With the emerging of digital convergence, lots of communication services are generated, and the quantity of data grows rapidly. We face the scalability issue to deal with call data records (CDRs) and so are the other telecom companies. This research uses police CDR query as an example with an intension to increase system execution efficiency and scalability and to reduce total cost by applying cloud service. The implementation applying distributed parallel database (Hive), distributed computing (Hadoop MapReduce), and distributed file system (Hadoop HDFS) will be introduced by a simulation to evaluate the execution efficiency of a query from CDRs. The factors influencing query efficiency, such as the settings of data block size and partition size in HDFS, will also be explored. The experimental results show that applying the big data processing technologies to execute queries from a huge amount of CDRs could improve the system execution efficiency significantly and reduce cost.;2013;Ju-Chi Tseng;;Conferences;;978-4-8855-2279-6
ieee_20221205_08_11_42;Beyond Big Data?;What can we expect to find beyond Big Data? And how can we exploit Big Data to get there?;2013;Judith Bayard Cushing;10.1109/MCSE.2013.102;Magazines;1558-366X;
ieee_20221205_08_11_42;Leveraging Big Data and Business Analytics [Guest editors' introduction];"Big data projects and programs provide tremendous opportunity for organizations looking to transform their operations, innovate in their markets, and better serve their customers; however, these initiatives must be based on sound approaches and principles and not fads or empty vendor claims. This special issue aims to promote a better understanding of big data to foster wider deployment of big data approaches and a new era of business analytics capabilities.";2013;Sunil Mithas;10.1109/MITP.2013.95;Magazines;1941-045X;
ieee_20221205_08_11_42;Proper orthogonal decomposition based parallel compression for visualizing big data on the K computer;The development of supercomputers has greatly help us to carry on large-scale computing for dealing with various problems through simulating and analyzing them. Visualization is an indispensable tool to understand the properties of the data from supercomputers. Especially, interactive visualization can help us to analyze data from various viewpoints and even to find out some local small but important features. However, it is still difficult to interactively visualize such kind of big data directly due to the slow file I/O problem and the limitation of memory size. For resolving these problems, we proposed a parallel compression method to reduce the data size with low computational cost. Furthermore, the fast linear decompression process is another merit for interactive visualization. Our method uses proper orthogonal decomposition (POD) to compress data because it can effectively extract important features from the data and the resulting compressed data can also be linearly decompressed. Our implementation achieves high parallel efficiency with a binary load-distributed approach, which is similar to the binary-swap image composition used in parallel volume rendering [2]. This approach allows us to effectively utilize all the processing nodes and reduce the interprocessor communication cost throughout the parallel compression calculations. Our test results on the K computer demonstrate superior performance of our design and implementation.;2013;Chongke Bi;10.1109/LDAV.2013.6675169;Conferences;;978-1-4799-1659-7
ieee_20221205_08_11_42;RUBA: Real-time unstructured big data analysis framework;We are greeting “Big Data Generation”. As ICT technology is developing, the volume of data is incredibly growing and many works to deal with a big data are underway. In this paper, we proposed a novel framework for real-time unstructured big data analysis, such as a movie, sound, text and image data. Our proposed framework provides functions of a real-time analysis and dynamic modification for unstructured big data analysis. We have implemented the object monitoring system as a test system which is applied our framework, and we have confirmed each functions and the availability of our framework.;2013;Jaein Kim;10.1109/ICTC.2013.6675410;Conferences;2162-1241;978-1-4799-0698-7
ieee_20221205_08_11_42;Efficient and Customizable Data Partitioning Framework for Distributed Big RDF Data Processing in the Cloud;Big data business can leverage and benefit from the Clouds, the most optimized, shared, automated, and virtualized computing infrastructures. One of the important challenges in processing big data in the Clouds is how to effectively partition the big data to ensure efficient distributed processing of the data. In this paper we present a Scalable and yet customizable data PArtitioning framework, called SPA, for distributed processing of big RDF graph data. We choose big RDF datasets as our focus of the investigation for two reasons. First, the Linking Open Data cloud has put forwards a good number of big RDF datasets with tens of billions of triples and hundreds of millions of links. Second, such huge RDF graphs can easily overwhelm any single server due to the limited memory and CPU capacity and exceed the processing capacity of many conventional data processing software systems. Our data partitioning framework has two unique features. First, we introduce a suite of vertexcentric data partitioning building blocks to allow efficient and yet customizable partitioning of large heterogeneous RDF graph data. By efficient, we mean that the SPA data partitions can support fast processing of big data of different sizes and complexity. By customizable, we mean that the SPA partitions are adaptive to different query types. Second, we propose a selection of scalable techniques to distribute the building block partitions across a cluster of compute nodes in a manner that minimizes inter-node communication cost by localizing most of the queries on distributed partitions. We evaluate our data partitioning framework and algorithms through extensive experiments using both benchmark and real datasets. Our experimental results show that the SPA data partitioning framework is not only efficient for partitioning and distributing big RDF datasets of diverse sizes and structures but also effective for processing big data queries of different types and complexity.;2013;Kisung Lee;10.1109/CLOUD.2013.63;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_11_42;Toward an Ecosystem for Precision Sharing of Segmented Big Data;As the amount of data created and stored by organizations continues to increase, attention is turning to extracting knowledge from that raw data, including making some data available outside of the organization to enable crowd analytics. The adoption of the MapReduce paradigm has made processing Big Data more accessible, but is still limited to data that is currently available, often only within an organization. Fine-grained control over what information is shared outside an organization is difficult to achieve with Big Data, particularly in the MapReduce model. We introduce a novel approach to sharing that enables fine-grained control over what data is shared. Users submit analytics tasks that run on infrastructure near the actual data, reducing network bottlenecks. Organizations allow access to a logical version of their data created at runtime by filtering and transforming the actual data without creating storage-intensive stale copies, and resellers can further segment or augment this data to provide added value to analytics tasks. A loosely-coupled ecosystem driven by web services allows for discovery and sharing with a flexible, secure environment that limits the knowledge those running analytics need to have about the actual provider of the data. We describe a proof-of-concept implementation of the various components required to realize this ecosystem, and present a set of experiments to demonstrate feasibility, showing advantageous performance versus storage trade-offs.;2013;Mark Shtern;10.1109/CLOUD.2013.131;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_11_42;Tape Cloud: Scalable and Cost Efficient Big Data Infrastructure for Cloud Computing;Magnetic tapes have been a primary medium of backup storage for a long time in many organizations. In this paper, the possibility of establishing an inter-network accessible, centralized, tape based data backup facility is evaluated. Our motive is to develop a cloud storage service that organizations can use for long term storage of big data which is typically Write-Once-Read-Many. This Infrastructure-as-a-Service (IaaS) cloud can provide the much needed cost effectiveness in storing huge amounts of data exempting client organizations from high infrastructure investments. We make an attempt to understand some of the limitations induced by the usage of tapes by studying the latency of tape libraries in scenarios most likely faced in the backing up process in comparison to its hard disk counterpart. The result of this study is an outline of methods to overcome these limitations by adopting novel tape storage architectures, filesystem, schedulers to manage data transaction requests from various clients and develop faster ways to retrieve requested data to extend the applications beyond backup. We use commercially available tapes and a tape library to perform latency tests and understand the basic operations of tape. With the optimistic backing of statistics that suggests the extensive usage of tapes to this day and in future, we propose an architecture to provide data backup to a large and diverse client base.;2013;Varun S. Prakash;10.1109/CLOUD.2013.129;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_11_42;Moving Big Data to The Cloud: An Online Cost-Minimizing Approach;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible or secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geo-dispersed data into the cloud, for processing using a MapReduce-like framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms: an online lazy migration (OLM) algorithm and a randomized fixed horizon control (RFHC) algorithm , for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. Careful comparisons among these online and offline algorithms in realistic settings are conducted through extensive experiments, which demonstrate close-to-offline-optimum performance of the online algorithms.;2013;Linquan Zhang;10.1109/JSAC.2013.131211;Journals;1558-0008;
ieee_20221205_08_11_42;Social Genome: Putting Big Data to Work for Population Informatics;Data-intensive research using distributed, federated, person-level datasets in near real time has the potential to transform social, behavioral, economic, and health sciences--but issues around privacy, confidentiality, access, and data integration have slowed progress in this area. When technology is properly used to manage both privacy concerns and uncertainty, big data will help move the growing field of population informatics forward.;2014;Hye-Chung Kum;10.1109/MC.2013.405;Magazines;1558-0814;
ieee_20221205_08_11_42;Influence maximization for Big Data through entropy ranking and min-cut;As Big Data becomes prevalent, the traditional models from Data Mining or Data Analysis, although very efficient, lack the speed necessary to process problems with data sets in the range of million samples. Therefore, the need for designing more efficient and faster algorithms for these new types of problems. Specifically, from the field of social network analysis, we have the influence maximization problem. This is a problem with many possible applications in advertising, marketing, social studies, etc, where we have representations of influences by large scale graphs. Even though, the optimal solution of this problem, the minimum set of graph nodes which can influence a maximum set of nodes, is a NP-Hard problem, it is possible to devise an approximated solution to the problem. In this paper, we have proposed a novel algorithm for influence maximization analysis. This algorithm consist in two phases: the first one is an entropy based node ranking where entropy ranking is used to determine node importance in a directed weighted influence graph. The second phase computes the minimum cut using a novel metric. To test the propose algorithm, experiments were performed in several popular data sets to evaluate performance and the seed quality over the influences.;2013;Agustin Sancen-Plaza;10.4108/icst.collaboratecom.2013.254119;Conferences;;978-1-936968-92-3
ieee_20221205_08_11_42;Real-time collaborative planning with big data: Technical challenges and in-place computing (invited paper);There is increasing collaboration in new generation supply chain planning applications, where participants across a supply chain analyze and plan on a big volume of sales data over the internet together. To achieve real-time collaborative planning over big data, we have developed an unconventional technology, BigObject, based on an in-place computing approach in two ways. First, instead of moving (big) data around, move (small) code to where data resides for execution. Second, organize the complexity by determining the basic functional units (objects) for computing in the same sense that macromolecules are determined for living cells. The term ”in-place” indicates that data is in residence in memory space and ready for computing. BigObject is an in-place computing system, designed for storing and computing multidimensional data. Our experiment shows that in-place computing approach outperforms traditional computing approach in two orders of magnitude.;2013;Wenwey Hseush;10.4108/icst.collaboratecom.2013.254100;Conferences;;978-1-936968-92-3
ieee_20221205_08_11_42;A Sketch of Big Data Technologies;This paper outlines the recent developed information technologies in big data. The basic principles and theories, concepts and terminologies, methods and implementations, and the status of research and development in big data are depicted. The paper also highlights the technical challenges and major difficulties. The number of key technologies required to handle big data are deliberated. They include big data acquisition, pre/post-processing, data storage and distribution, networks, and analysis and mining, etc. At last, the development trend in big data technologies is addressed for discussion.;2013;Zaiying Liu;10.1109/ICICSE.2013.13;Conferences;2330-9857;978-0-7695-5118-0
ieee_20221205_08_11_42;An Iterative Hierarchical Key Exchange Scheme for Secure Scheduling of Big Data Applications in Cloud Computing;As the new-generation distributed computing platform, cloud computing environments offer high efficiency and low cost for data-intensive computation in big data applications. Cloud resources and services are available in pay-as-you-go mode, which brings extraordinary flexibility and cost-effectiveness as well as zero investment in their own computing infrastructure. However, these advantages come at a price-people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated Key Exchange (AKE) is essential to a security system that is based on high efficiency symmetric-key encryption. With virtualization technology being applied, existing key exchange schemes such as Internet Key Exchange (IKE) becomes time-consuming when directly deployed into cloud computing environment. In this paper we propose a novel hierarchical key exchange scheme, namely Cloud Background Hierarchical Key Exchange (CBHKE). Based on our previous work, CBHKE aims at providing secure and efficient scheduling for cloud computing environment. In our new scheme, we design a two-phase layer-by-layer iterative key exchange strategy to achieve more efficient AKE without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud computing environment, efficiency of the proposed scheme is dramatically superior to its predecessors CCBKE and IKE schemes.;2013;Chang Liu;10.1109/TrustCom.2013.65;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_11_42;Combining Top-Down and Bottom-Up: Scalable Sub-tree Anonymization over Big Data Using MapReduce on Cloud;In big data applications, data privacy is one of the most concerned issues because processing large-scale privacy-sensitive data sets often requires computation power provided by public cloud services. Sub-tree data anonymization, achieving a good trade-off between data utility and distortion, is a widely adopted scheme to anonymize data sets for privacy preservation. Top-Down Specialization (TDS) and Bottom-Up Generalization (BUG) are two ways to fulfill sub-tree anonymization. However, existing approaches for sub-tree anonymization fall short of parallelization capability, thereby lacking scalability in handling big data on cloud. Still, both TDS and BUG suffer from poor performance for certain value of k-anonymity parameter if they are utilized individually. In this paper, we propose a hybrid approach that combines TDS and BUG together for efficient sub-tree anonymization over big data. Further, we design MapReduce based algorithms for two components (TDS and BUG) to gain high scalability by exploiting powerful computation capability of cloud. Experiment evaluations demonstrate that the hybrid approach significantly improves the scalability and efficiency of sub-tree anonymization scheme over existing approaches.;2013;Xuyun Zhang;10.1109/TrustCom.2013.235;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_11_42;Big Data Real-Time Processing Based on Storm;As the growth of Internet, Cloud Computing, Mobile Network and Internet of Things is increasing rapidly, Big Data is becoming a hot-spot in recent years. Big Data Processing is involved in our daily life such as mobile devices, RFID and wireless sensors, which aims at dealing with billions of users' interactive data. At the same time, real-time processing is eagerly needed in integrated system. In this paper, several technologies associated with real-time big data processing are introduced, among which the core technology called Storm is emphasized. An entire system is built based on Storm, associated with RabbitMQ, NoSQL and JSP. To ensure the practical applicability and high efficiency, a simulation system is established and shows acceptable performance in various expressions using data sheet and Ganglia. It is proved that the big data real-time processing based on Storm can be widely used in various computing environment.;2013;Wenjie Yang;10.1109/TrustCom.2013.247;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_11_42;A Universal Storage Architecture for Big Data in Cloud Environment;With the rapid development of the Internet of Things and Electronic Commerce, we have entered the era of big data. The characteristics, such as great amount and heterogeneousity, of big data bring the challenge to the storage and analytics. The paper presented a universal storage architecture for big data in cloud environment. We use clustering analysis to divide the cloud nodes into multiple clusters according to the communication cost between different nodes. The cluster with the strongest computing power is selected to provide the universal storage and query interface for users. Each of other clusters is responsible for storing the data of a particular model, such as relational data, key-value data, and document data and so on. Experiments show that our architecture can store all kinds of heterogeneous big data and provide users with unified storage and query interface for big data easily and quickly.;2013;Qingchen Zhang;10.1109/GreenCom-iThings-CPSCom.2013.96;Conferences;;978-0-7695-5046-6
ieee_20221205_08_11_42;IOT-StatisticDB: A General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things;In large scale Internet of Things (IoT) systems, statistical analysis is a crucial technique for transforming data into knowledge and for obtaining overall information about the physical world. However, most existing statistical analysis methods for sensor sampling data are implemented outside the database kernel and focus on specialized analytics, making them unsuited for the IoT environment where both the data types and the statistical queries are diverse. To solve this problem, we propose a General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things (IOT-StatisticDB) in this paper. In IOT-StatisticDB, statistical functions are performed through statistical operators inside the DBMS kernel, so that complicated statistical queries can be expressed in the standard SQL format. Besides, statistical analysis is executed in a distributed and parallel manner over multiple servers so that the performance can be greatly improved, which is confirmed by the experiments.;2013;Zhiming Ding;10.1109/GreenCom-iThings-CPSCom.2013.104;Conferences;;978-0-7695-5046-6
ieee_20221205_08_11_42;Big Data Analytics for Security;"Big data is changing the landscape of security tools for network monitoring, security information and event management, and forensics; however, in the eternal arms race of attack and defense, security researchers must keep exploring novel ways to mitigate and contain sophisticated attackers.";2013;Alvaro A. Cárdenas;10.1109/MSP.2013.138;Magazines;1558-4046;
ieee_20221205_08_11_42;Rapid Scanning of Spectrograms for Efficient Identification of Bioacoustic Events in Big Data;Acoustic sensing is a promising approach to scaling faunal biodiversity monitoring. Scaling the analysis of audio collected by acoustic sensors is a big data problem. Standard approaches for dealing with big acoustic data include automated recognition and crowd based analysis. Automatic methods are fast at processing but hard to rigorously design, whilst manual methods are accurate but slow at processing. In particular, manual methods of acoustic data analysis are constrained by a 1:1 time relationship between the data and its analysts. This constraint is the inherent need to listen to the audio data. This paper demonstrates how the efficiency of crowd sourced sound analysis can be increased by an order of magnitude through the visual inspection of audio visualized as spectrograms. Experimental data suggests that an analysis speedup of 12× is obtainable for suitable types of acoustic analysis, given that only spectrograms are shown.;2013;Anthony Truskinger;10.1109/eScience.2013.25;Conferences;;978-0-7695-5083-1
ieee_20221205_08_11_42;Catching the wave: Big data in the classroom;Many diverse domains-in the sciences, engineering, healthcare, and homeland security-have been grappling with the analysis of “Big Data,” which has become shorthand to represent extremely large amounts of diverse types of data. A recent Gartner report predicts that around 4.4 million IT jobs globally will be created by 2015 to support Big Data, with 1.9 million of those jobs in the United States. Therefore, understanding approaches and techniques for handling and analyzing Big Data from diverse domains has become crucial for not only in computing but also engineering students. The mini-workshop will make use of active and collaborative learning exercises to introduce faculty in computer science, software engineering, and other disciplines to concepts and techniques involved in managing and analyzing Big Data. Approaches for incorporating Big Data into the engineering and computing curricula will also be presented.;2013;Carol J. Romanowski;10.1109/FIE.2013.6684855;Conferences;2377-634X;978-1-4673-5261-1
ieee_20221205_08_11_42;The Convergence of Big Data and Mobile Computing;In recent years, we have witnessed the influx of data driven by the exponential use of digital video, music, and mobile applications. The combination of the internet, mobile technologies and social media applications has been the most influential factor in the emergence of Big Data paradigm. Given that online activities of mobile users constitute the major contributors to the Big Data analytics, which significantly benefit various business enterprises, it is also interesting to look at how the information derived may also be useful for mobile users, and how it can be delivered in the most effective, and efficient manner. These will be the key points of discussion in this paper.;2013;Agustinus Borgy Waluyo;10.1109/NBiS.2013.15;Conferences;2157-0426;978-1-4799-2509-4
ieee_20221205_08_11_42;A Big Data Approach for a New ICT Agriculture Application Development;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address the impact of these issues in the novel nutrition-based vegetable production and distribution system project in which Sojo University is an active member.";2013;R. Dennis A. Ludena;10.1109/CyberC.2013.30;Conferences;;978-0-7695-5106-7
ieee_20221205_08_11_42;Big Data a Sure Thing for Telecommunications: Telecom's Future in Big Data;Big Data is made for telecommunications. No other industry has access to the wealth of information about their customers the way communications service providers (CSPs) do. Big data can be seen as the CSPs' most valuable asset. It puts them in a key position to win the battle for customers and generate new revenue streams - provided they can get their acts together.;2013;Rob Van Den Dam;10.1109/CyberC.2013.32;Conferences;;978-0-7695-5106-7
ieee_20221205_08_11_42;Big Data Analytics in the Public Sector: Improving the Strategic Planning in World Class Universities;"This paper presents an application in Information Fusion related to Analytics and fusion of Big Data. The approach involves the analysis of specific aspects of the management of a Brazilian university and the proposition of a new framework that may be useful mainly for universities focused on improving their strategic planning while having excellence in operational execution. The framework intends to be used for the development of new decision support systems that integrate ""past data"" with ""real time data"". The research methodology emphasizes induction with the collection of qualitative data in order to move from observed facts to theory. The main result is the framework itself while future work may involve the development and the validation of the new system.";2013;Joni A. Amorim;10.1109/CyberC.2013.33;Conferences;;978-0-7695-5106-7
ieee_20221205_08_11_42;A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud;The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.;2013;Xuyun Zhang;10.1109/CGC.2013.24;Conferences;;978-0-7695-5114-2
ieee_20221205_08_11_42;Visual Analytics for Big Data Using R;The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.;2013;Aziz Nasridinov;10.1109/CGC.2013.96;Conferences;;978-0-7695-5114-2
ieee_20221205_08_11_42;A distribute parallel approach for big data scale optimal power flow with security constraints;This paper presents a mathematical optimization framework for security-constrained optimal power flow (SCOPF) computations. The SCOPF problem determines the optimal control of power systems under constraints arising from a set of postulated contingencies. This problem is challenging due to the significantly large problem size, the stringent real-time requirement and the variety of numerous post-contingency states. In order to solve the resultant big data scale optimization problem with manageable complexity, the alternating direction method of multipliers (ADMM) is utilized. The SCOPF is decomposed into independent subproblems correspond to each individual pre-contingency and post-contingency case. Those subproblems are solved in parallel on distributed nodes and coordinated through dual (prices) variables. As a result, the algorithm is implemented in a distributive and parallel fashion. Numerical tests validate the effectiveness of the proposed algorithm.;2013;Lanchao Liu;10.1109/SmartGridComm.2013.6688053;Conferences;;978-1-4799-1526-2
ieee_20221205_08_11_42;Towards Service-Oriented Enterprise Architectures for Big Data Applications in the Cloud;Applications with Service-oriented Enterprise Architectures in the Cloud are emerging and will shape future trends in technology and communication. The development of such applications integrates Enterprise Architecture and Management with Architectures for Services & Cloud Computing, Web Services, Semantics and Knowledge-based Systems, Big Data Management, among other Architecture Frameworks and Software Engineering Methods. In the present work in progress research, we explore Service-oriented Enterprise Architectures and application systems in the context of Big Data applications in cloud settings. Using a Big Data scenario, we investigate the integration of Services and Cloud Computing architectures with new capabilities of Enterprise Architectures and Management. The underlying architecture reference model can be used to support semantic analysis and program comprehension of service-oriented Big Data Applications. Enterprise Services Computing is the current trend for powerful large-scale information systems, which increasingly converge with Cloud Computing environments. In this paper we combine architectures for services with cloud computing. We propose a new integration model for service-oriented Enterprise Architectures on basis of ESARC - Enterprise Services Architecture Reference Cube, which is our previous developed service-oriented enterprise architecture classification framework, with MFESA - Method Framework for Engineering System Architectures - for the design of service-oriented enterprise architectures, and the systematic development, diagnostics and optimization of architecture artifacts of service-oriented cloud-based enterprise systems for Big Data applications.;2013;Alfred Zimmermann;10.1109/EDOCW.2013.21;Conferences;2325-6605;978-1-4799-3048-7
ieee_20221205_08_11_42;Strategic Alignment of Cloud-Based Architectures for Big Data;Big Data is an increasingly significant topic for management and IT departments. In the beginning, Big Data applications were large on premise installations. Today, cloud services are used increasingly to implement Big Data applications. This can be done on different ways supporting different strategic enterprise goals. Therefore, we develop a framework that enumerates the alternatives for implementing Big Data applications using cloud-services and identify the strategic goals supported by these Alternatives. The created framework clarifies the options for Big Data initiatives using cloud-computing and thus improves the strategic alignment of Big Data applications.;2013;Rainer Schmidt;10.1109/EDOCW.2013.22;Conferences;2325-6605;978-1-4799-3048-7
ieee_20221205_08_11_42;Security — A big question for big data;Summary form only given. Big data implies performing computation and database operations for massive amounts of data, remotely from the data owner's enterprise. Since a key value proposition of big data is access to data from multiple and diverse domains, security and privacy will play a very important role in big data research and technology. The limitations of standard IT security practices are well-known, making the ability of attackers to use software subversion to insert malicious software into applications and operating systems a serious and growing threat whose adverse impact is intensified by big data. So, a big question is what security and privacy technology is adequate for controlled assured sharing for efficient direct access to big data. Making effective use of big data requires access from any domain to data in that domain, or any other domain it is authorized to access. Several decades of trusted systems developments have produced a rich set of proven concepts for verifiable protection to substantially cope with determined adversaries, but this technology has largely been marginalized as “overkill” and vendors do not widely offer it. This talk will discuss pivotal choices for big data to leverage this mature security and privacy technology, while identifying remaining research challenges.;2013;Roger Schell;10.1109/BigData.2013.6691547;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Communication efficient algorithms for fundamental big data problems;Big Data applications often store or obtain their data distributed over many computers connected by a network. Since the network is usually slower than the local memory of the machines, it is crucial to process the data in such a way that not too much communication takes place. Indeed, only communication volume sublinear in the input size may be affordable. We believe that this direction of research deserves more intensive study. We give examples for several fundamental algorithmic problems where nontrivial algorithms with sublinear communication volume are possible. Our main technical contribution are several related results on distributed Bloom filter replacements, duplicate detection, and data base join. As an example of a very different family of techniques, we discuss linear programming in low dimensions.;2013;Peter Sanders;10.1109/BigData.2013.6691549;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;P-DOT: A model of computation for big data;"In response to the high demand of big data analytics, several programming models on large and distributed cluster systems have been proposed and implemented, such as MapRe-duce, Dryad and Pregel. However, compared with high performance computing areas, the basis and principles of computation and communication behavior of big data analytics is not well studied. In this paper, we review the current big data computational model DOT and DOTA, and propose a more general and practical model p-DOT (p-phases DOT). p-DOT is not a simple extension, but with profound significance: for general aspects, any big data analytics job execution expressed in DOT model or BSP model can be represented by it; for practical aspects, it considers I/O behavior to evaluate performance overhead. Moreover, we provide a cost function implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, and demonstrate the effectiveness of the function through several experiments.";2013;Tao Luo;10.1109/BigData.2013.6691551;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Elastic algorithms for guaranteeing quality monotonicity in big data mining;When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.;2013;Rui Han;10.1109/BigData.2013.6691553;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Building a generic platform for big sensor data application;The drive toward smart cities alongside the rising adoption of personal sensors is leading to a torrent of sensor data. While systems exist for storing and managing sensor data, the real value of such data is the insight which can be generated from it. However there is currently no platform which enables sensor data to be taken from collection, through use in models to produce useful data products. The architecture of such a platform is a current research question in the field of Big Data and Smart Cities. In this paper we explore five key challenges in this field and provide a response through a sensor data platform “Concinnity” which can take sensor data from collection to final product via a data repository and workflow system. This will enable rapid development of applications built on sensor data using data fusion and the integration and composition of models to form novel workflows. We summarize the key features of our approach, exploring how it enables value to be derived from sensor data efficiently.;2013;Chun-Hsiang Lee;10.1109/BigData.2013.6691559;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;clusiVAT: A mixed visual/numerical clustering algorithm for big data;Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.;2013;Dheeraj Kumar;10.1109/BigData.2013.6691561;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Algebraic dataflows for big data analysis;Analyzing big data requires the support of dataflows with many activities to extract and explore relevant information from the data. Recent approaches such as Pig Latin propose a high-level language to model such dataflows. However, the dataflow execution is typically delegated to a MapRe-duce implementation such as Hadoop, which does not follow an algebraic approach, thus it cannot take advantage of the optimization opportunities of PigLatin algebra. In this paper, we propose an approach for big data analysis based on algebraic workflows, which yields optimization and parallel execution of activities and supports user steering using provenance queries. We illustrate how a big data processing dataflow can be modeled using the algebra. Through an experimental evaluation using real datasets and the execution of the dataflow with Chiron, an engine that supports our algebra, we show that our approach yields performance gains of up to 19.6% using algebraic optimizations in the dataflow and up to 39.1% of time saved on a user steering scenario.;2013;Jonas Dias;10.1109/BigData.2013.6691567;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Robot: An efficient model for big data storage systems based on erasure coding;It is well-known that with the explosive growth of data, the age of big data has arrived. How to save huge amounts of data is of great importance to both industry and academia. This paper puts forward a solution based on coding technologies in big data system that store a lot of cold data. By studying existing coding technologies and big data systems, we can not only maintain the system's reliability, but also improve the security and the utilization of storage systems. Due to the remarkable reliability and space saving rate of coding technologies, importing coding schema in to big data systems becomes prerequisite. In our presented schema, the storage node is divided into several virtual nodes to keep load balancing. By setting up different virtual node storage groups for different codec server, we can ensure system availability. And by utilizing the parallel decoding computing of the node and the block of data, we can also reduce the system recovery time when data is corrupted. Additionally, different users set different coding parameters can improve the robustness of big data storage systems. We configure various data block m and calibration block k to improve the utilization rate in the quantitative experiments. The results shows that parallel decoding speed can rise up two times than the past serial decoding speed. The encoding efficiency with ICRS coding is 34.2% higher than using CRS and 56.5% more than using RS coding equally. The decoding rate by using ICRS is 18.1% higher than using CRS and 31.1% higher than using RS averagely.;2013;Chao Yin;10.1109/BigData.2013.6691569;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Multilevel Active Storage for big data applications in high performance computing;Given the growing importance of supporting dataintensive sciences and big data applications, an effective HPC I/O solution has become a key issue and has attracted intensive attention in recent years. Active storage has been shown effective in reducing data movement and network traffic as a potential new I/O solution. Existing prototypes and systems, however, are primarily designed for read-intensive applications. In addition, they generally assume that offloaded processing kernels have small computational demands, which makes this solution a poor fit for data-intensive operations that have significant computational demands, including write-intensive operations. In this research, we propose a new Multilevel Active Storage (MAS) solution. The new MAS design can support and handle both read- and write-intensive operations, as well as complex operations that have considerable computational demands. Experimental tests have been carried out and confirmed that the MAS approach is feasible and outperformed existing approaches. The new multilevel active storage design has a potential to deliver a high performance I/O solution for big data applications in HPC.;2013;Chao Chen;10.1109/BigData.2013.6691570;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;GPU accelerated item-based collaborative filtering for big-data applications;"Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.";2013;Chandima Hewa Nadungodage;10.1109/BigData.2013.6691571;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Using pattern-models to guide SSD deployment for Big Data applications in HPC systems;"Flash-memory based Solid State Drives (SSDs) embrace higher performance and lower power consumption compared to traditional storage devices (HDDs). These benefits are needed in HPC systems, especially with the growing demand of supporting Big Data applications. In this paper, we study placement and deployment strategies of SSDs in HPC systems to maximize the performance improvement, given a practical fixed hardware budget constraint. We propose a pattern-model approach to guide SSD deployment for HPC systems through two steps; characterizing workload and mapping deployment strategy. The first step is responsible for characterizing the access patterns of the workload and the second step contributes the actual deployment recommendation for Parallel File System (PFS) configuration combining with an analytical model. We have carried out initial experimental tests and the results confirmed that the proposed approach can guide placement of SSDs in HPC systems for accelerating data accesses. Our research will be helpful in guiding designs and developments for Big Data applications in current and projected HPC systems including exascale systems.";2013;Junjie Chen;10.1109/BigData.2013.6691592;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Efficient large graph pattern mining for big data in the cloud;Mining big graph data is an important problem in the graph mining research area. Although cloud computing is effective at solving traditional algorithm problems, mining frequent patterns of a massive graph with cloud computing still faces the three challenges: 1) the graph partition problem, 2) asymmetry of information, and 3) pattern-preservation merging. Therefore, this paper presents a new approach, the cloud-based SpiderMine (c-SpiderMine), which exploits cloud computing to process the mining of large patterns on big graph data. The proposed method addresses the above issues for implementing a big graph data mining algorithm in the cloud. We conduct the experiments with three real data sets, and the experimental results demonstrate that c-SpiderMine can significantly reduce execution time with high scalability in dealing with big data in the cloud.;2013;Chun-Chieh Chen;10.1109/BigData.2013.6691618;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A Higher-order data flow model for heterogeneous Big Data;We introduce a data flow model that supports highly parallelisable design patterns and also has useful properties for analysing data serially over extended time periods without requiring traditional Big Data computing facilities. The model ranges over a class of higher-order relations which are sufficiently expressive to represent a wide variety of unstructured, semi-structured and structured data. Using JSONMatch, our web service implementation of the model, we show that the combination of this model and higher-order representation provides a powerful and extensible framework that is particularly well suited to analysing Big Variety data in a web application context.;2013;Simon Price;10.1109/BigData.2013.6691624;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Breaking the Arc: Risk control for Big Data;"The use of Big Data technologies and analytics have the potential to revolutionise the world. The mass instrumentation of the planet and society is providing intelligence that is not only enhancing our personal lives, but also opening up new opportunities for addressing some of key environmental, social and economic challenges of the 21st century. Unfortunately, as with all technology, there is the potential for misuse; in the case of personal data the ability to gather, enrich and mine at extreme pace and volume could result in societal-scale privacy intrusions. We apply a model for identity across cyber and physical spaces to the question of risk control for personal-data in the context of big data analytics. Using a graphical model for identity we reflect on the response options we have and how such risk controls may or may not be effective.";2013;Duncan Hodges;10.1109/BigData.2013.6691630;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation;"The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10-6) and very poor (102) selectivity, and short (seconds) to long (hours) job duration.";2013;Tim Hegeman;10.1109/BigData.2013.6691631;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Big data analytics on high Velocity streams: A case study;"Big data management is often characterized by three Vs: Volume, Velocity and Variety. While traditional batch-oriented systems such as MapReduce are able to scale-out and process very large volumes of data in parallel, they also introduce some significant latency. In this paper, we focus on the second V (Velocity) of the Big Data triad; We present a case-study where we use a popular open-source stream processing engine (Storm) to perform real-time integration and trend detection on Twitter and Bitly streams. We describe our trend detection solution below and experimentally demonstrate that our architecture can effectively process data in real-time - even for high-velocity streams.";2013;Thibaud Chardonnens;10.1109/BigData.2013.6691653;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Visualization and rhetoric: Key concerns for utilizing big data in humanities research: A case study of vaccination discourses: 1918-1919;Visualization of data mining results is the linchpin of successful research in the humanities that uses computational techniques. This paper describes efforts to utilize “big data” in a case study of news reporting on vaccination before, during, and after the 1918 influenza pandemic, focusing primarily on the conventions underlying methods of data extraction, data visualization practices, and the rhetorical impact of visualization design choices on researchers' observations and interpretive decisions. Purposeful attention to visualization and the methodological conventions that are embedded in particular visualization practices will allow humanists to have more confidence in their interpretations of big data, a key element in the acceptance of data mining as a valuable method for humanities research.;2013;Kathleen Kerr;10.1109/BigData.2013.6691666;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Humanities ‘big data’: Myths, challenges, and lessons;This paper argues that there have always been `big data' in the humanities, and challenges commonly held myths in this regard. It does so by discussing the case of transnational research on dispersed communities. Concluding, it examines the lessons humanities and sciences can learn from each other.;2013;Amalia S. Levi;10.1109/BigData.2013.6691667;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Bibliographic records as humanities big data;Most discussion hitherto of big data in the humanities has assumed that it is characterized by its heterogeneous nature. This paper examines the extent to which bibliographic records generated by libraries represent a more homogenous form of humanities big data, more closely related to the observational big data generated by scientific data. It is suggested from an examination of the British Library catalogue that, while superficially bibliographic records appear to be created according to consistent standards and form a more homogenous dataset, close examination reveals that bibliographical records often go through a marked process of historical development. However, the critical methods require to disaggregate such data are perhaps analogous to those used in some scientific disciplines.;2013;Andrew Prescott;10.1109/BigData.2013.6691670;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A concept of Generic Workspace for Big Data Processing in Humanities;Big Data challenges often require application of new data processing paradigms (like MapReduce), and corresponding software solutions (e. g. Hadoop). This trend causes a pressure on both cyber-infrastructure providers (to quickly integrate new services) and infrastructure users (to quickly learn to use new tools). In this paper we present the concept of DARIAH Generic Workspace for Big Data Processing in eHumanities which alleviates the aforementioned problems. It establishes a common integration layer, thus enables a quick integration of new services, and by providing unified interfaces, allows the users to start using new tools without learning their internal details. We describe the overall architecture and implementation details of the working prototype. The presented concept is generic enough to be applied in other emerging cyber-infrastructures for humanities.;2013;Jedrzej Rybicki;10.1109/BigData.2013.6691672;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A case study on entity Resolution for Distant Processing of big Humanities data;At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.;2013;Weijia Xu;10.1109/BigData.2013.6691678;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Business model canvas perspective on big data applications;Large and complex data that becomes difficult to be handled by traditional data processing applications triggers the development of big data applications which have become more pervasive than ever before. In the era of big data, data exploration and analysis turned into a difficult problem in many sectors such as the smart routing and health care sectors. Companies which can adapt their businesses well to leverage big data have significant advantages over those that lag this capability. The need for exploring new approaches to address the challenges of big data forces companies to shape their business models accordingly. In this paper, we summarize and share our findings regarding the business models deployed in big data applications in different sectors. We analyze existing big data applications by taking into consideration the core elements of a business (via business model canvas) and present how these applications provide value to their customers by making profit out of using big data.;2013;F. Canan Pembe Muhtaroğlu;10.1109/BigData.2013.6691684;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Understanding the value of (big) data;This paper acts as a primer on an economic outlook at the value and pricing of big data. We introduce a simple taxonomy, discuss rights to access and analyze the case of big data as a common pool resource.;2013;Koutroumpis Pantelis;10.1109/BigData.2013.6691691;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Memory system characterization of big data workloads;Two recent trends that have emerged include (1) Rapid growth in big data technologies with new types of computing models to handle unstructured data, such as map-reduce and noSQL (2) A growing focus on the memory subsystem for performance and power optimizations, particularly with emerging memory technologies offering different characteristics from conventional DRAM (bandwidths, read/write asymmetries). This paper examines how these trends may intersect by characterizing the memory access patterns of various Hadoop and noSQL big data workloads. Using memory DIMM traces collected using special hardware, we analyze the spatial and temporal reference patterns to bring out several insights related to memory and platform usages, such as memory footprints, read-write ratios, bandwidths, latencies, etc. We develop an analysis methodology to understand how conventional optimizations such as caching, prediction, and prefetching may apply to these workloads, and discuss the implications on software and system design.;2013;Martin Dimitrov;10.1109/BigData.2013.6691693;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;An ensemble MIC-based approach for performance diagnosis in big data platform;The era of big data has began. Although applications based on big data bring considerable benefit to IT industries, governments and social organizations, they bring more challenges to the management of big data platforms which are the fundamental infrastructures due to the complexity, variety, velocity and volume of big data. To offer a healthy platform for big data applications, we propose a novel signature-based performance diagnosis approach employing MIC invariants between performance metrics. We formalize the performance diagnosis as a pattern recognition problem. The normal state of a big data application is used to train a set of MIC (Maximum Information Criterion) invariants. One performance problem occurred in the big data application is identified by a unique binary tuple consisted by a set violations of MIC invariants. All the signatures of performance problems form a diagnosis knowledge database. If the KPI (Key Performance Indicator) of the big data application deviates its normal region, our approach can identify the real culprits through looking for similar signatures in the signature database. To detect the deviation of the KPI, we propose a new metric named unpredictability based on ARIMA model. And considering the variety of big data applications, we build an ensemble performance diagnosis approach which means a unique ARIMA model and a unique set of MIC invariants are built for a specific kind of application. Through experiment evaluation in a controlled environment running a state of the art big data benchmark, we find our approach can pinpoint the real culprits of performance problems in an average 83% precision and 87% recall which is better than a correlation based and single model based performance diagnosis.;2013;Pengfei Chen;10.1109/BigData.2013.6691701;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;The implications from benchmarking three big data systems;Along with today's data explosion and application diversification, a variety of hardware platforms for data centers are emerging and are attracting interests from both industry and academia. The existing hardware platforms represent a wide range of implementation approaches, and different hardware have different strengths. In this paper, we conduct comprehensive evaluations on three representative data center systems based on BigDataBench, which is a benchmark suite for benchmarking and ranking systems running big data applications. Then we explore the relative performance of the three implementation approaches with different big data applications, and provide strong guidance for the data center system construction. Through our experiments, we has inferred that a data center system based on specific hardware has different performance in the context of different applications and data volumes. When we construct a system, we can take into account not only the performance or energy consumption of the pure hardwares, but also the application-level characteristics. Data scale, application type and complexity should be considered comprehensively when researchers or architects plan to choose fundamental components for their data center system.;2013;Jing Quan;10.1109/BigData.2013.6691706;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A characterization of big data benchmarks;Recently, big data has been evolved into a buzzword from academia to industry all over the world. Benchmarks are important tools for evaluating an IT system. However, benchmarking big data systems is much more challenging than ever before. First, big data systems are still in their infant stage and consequently they are not well understood. Second, big data systems are more complicated compared to previous systems such as a single node computing platform. While some researchers started to design benchmarks for big data systems, they do not consider the redundancy between their benchmarks. Moreover, they use artificial input data sets rather than real world data for their benchmarks. It is therefore unclear whether these benchmarks can be used to precisely evaluate the performance of big data systems. In this paper, we first analyze the redundancy among benchmarks from ICTBench, HiBench and typical workloads from real world applications: spatio-temporal data analysis for Shenzhen transportation system. Subsequently, we present an initial idea of a big data benchmark suite for spatio-temporal data. There are three findings in this work: (1) redundancy exists in these pioneering benchmark suites and some of them can be removed safely. (2) The workload behavior of trajectory data analysis applications is dramatically affected by their input data sets. (3) The benchmarks created for academic research cannot represent the cases of real world applications.;2013;Wen Xiong;10.1109/BigData.2013.6691707;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A big data analytics framework for scientific data management;The Ophidia project is a research effort addressing big data analytics requirements, issues, and challenges for eScience. We present here the Ophidia analytics framework, which is responsible for atomically processing, transforming and manipulating array-based data. This framework provides a common way to run on large clusters analytics tasks applied to big datasets. The paper highlights the design principles, algorithm, and most relevant implementation aspects of the Ophidia analytics framework. Some experimental results, related to a couple of data analytics operators in a real cluster environment, are also presented.;2013;Sandro Fiore;10.1109/BigData.2013.6691720;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Scalable sentiment classification for Big Data analysis using Naïve Bayes Classifier;A typical method to obtain valuable information is to extract the sentiment or opinion from a message. Machine learning technologies are widely used in sentiment classification because of their ability to “learn” from the training dataset to predict or support decision making with relatively high accuracy. However, when the dataset is large, some algorithms might not scale up well. In this paper, we aim to evaluate the scalability of Naïve Bayes classifier (NBC) in large datasets. Instead of using a standard library (e.g., Mahout), we implemented NBC to achieve fine-grain control of the analysis procedure. A Big Data analyzing system is also design for this study. The result is encouraging in that the accuracy of NBC is improved and approaches 82% when the dataset size increases. We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput.;2013;Bingwei Liu;10.1109/BigData.2013.6691740;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Frequent Itemset Mining for Big Data;Frequent Itemset Mining (FIM) is one of the most well known techniques to extract knowledge from data. The combinatorial explosion of FIM methods become even more problematic when they are applied to Big Data. Fortunately, recent improvements in the field of parallel programming already provide good tools to tackle this problem. However, these tools come with their own technical challenges, e.g. balanced data distribution and inter-communication costs. In this paper, we investigate the applicability of FIM techniques on the MapReduce platform. We introduce two new methods for mining large datasets: Dist-Eclat focuses on speed while BigFIM is optimized to run on really large datasets. In our experiments we show the scalability of our methods.;2013;Sandy Moens;10.1109/BigData.2013.6691742;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;A look at challenges and opportunities of Big Data analytics in healthcare;Big Data analytics can revolutionize the healthcare industry. It can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. This paper provides an overview of Big Data, applicability of it in healthcare, some of the work in progress and a future outlook on how Big Data analytics can improve overall quality in healthcare systems.;2013;Raghunath Nambiar;10.1109/BigData.2013.6691753;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;BIG DATA infrastructures for pharmaceutical research;Big Data is an emerging paradigm covering production, collection, processing, analysis, access and presentation of huge data-sets: Big Data infrastructures represent an opportunity to approach this paradigm on an organizational level. We describe challenges and opportunities of big data infrastructures for the pharmaceutical industry. Pharmaceutical research and product development are huge investments and require intense exploration and analysis of data. Future trends show that pharmaceutical companies need to develop new methods of data and information processing to quicker and more precisely respond to changing markets and the need of patients and providers. The individual case and patients will become more influential in healthcare delivery and in consequence for the rationale behind these decisions. Our approach of a platform for semantic exploitation of BIG DATA supports a knowledge based infrastructure for deep analysis of clinical information from structured sources and clinical narratives. This infrastructure is able to pave the way towards the necessary big data-management possibilities. Example applications for cancer research will be given.;2013;Christian Seebode;10.1109/BigData.2013.6691759;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Big data solutions for predicting risk-of-readmission for congestive heart failure patients;Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.;2013;Kiyana Zolfaghar;10.1109/BigData.2013.6691760;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Big spatial data mining;In this paper, spatial data mining is discussed in the context of big data. Firstly, we elaborate the fact that spatial data plays a primary role in big data, attracting academic community, business industry and governments. Secondly, the adverse of spatial data mining is discussed, such as much garbage, heavy pollution and its difficulties in utilization. Finally, we dissect the value in spatial big data, expound the techniques to discover knowledge from spatial big data, and investigate the transformation from knowledge into data intelligences.;2013;Wang Shuliang;10.1109/BigData.2013.6691764;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;IntegrityMR: Integrity assurance framework for big data analytics and management applications;Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).;2013;Yongzhi Wang;10.1109/BigData.2013.6691780;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Exploring big data in small forms: A multi-layered knowledge extraction of social networks;Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.;2013;Yun Wei Zhao;10.1109/BigData.2013.6691784;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Tile based visual analytics for Twitter big data exploratory analysis;New tools for raw data exploration and characterization of “big data” sets are required to suggest initial hypotheses for testing. The widespread use and adoption of web-based geo maps have provided a familiar set of interactions for exploring extremely large geo data spaces and can be applied to similarly large abstract data spaces. Building on these techniques, a tile based visual analytics system (TBVA) was developed that demonstrates interactive visualization for a one billion point Twitter dataset. TBVA enables John Tukey-inspired exploratory data analysis to be performed on massive data sets of effectively unlimited size.;2013;Daniel Cheng;10.1109/BigData.2013.6691787;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Risk adjustment of patient expenditures: A big data analytics approach;For healthcare applications, voluminous patient data contain rich and meaningful insights that can be revealed using advanced machine learning algorithms. However, the volume and velocity of such high dimensional data requires new big data analytics framework where traditional machine learning tools cannot be applied directly. In this paper, we introduce our proof-of-concept big data analytics framework for developing risk adjustment model of patient expenditures, which uses the “divide and conquer” strategy to exploit the big-yet-rich data to improve the model accuracy. We leverage the distributed computing platform, e.g., MapReduce, to implement advanced machine learning algorithms on our data set. In specific, random forest regression algorithm, which is suitable for high dimensional healthcare data, is applied to improve the accuracy of our predictive model. Our proof-of-concept framework demonstrates the effectiveness of predictive analytics using random forest algorithm as well as the efficiency of the distributed computing platform.;2013;Lin Li;10.1109/BigData.2013.6691790;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Big data for business managers — Bridging the gap between potential and value;Given the surge of interest in research, publication and application on Big Data over the last few years, the potential of Big Data seems to be well-established now across businesses. However, in most of the business implementations Big Data still seem to be struggling to deliver the promised value (ROI). Such results despite using the market leading Big Data solutions and talented deployment team are forcing the business managers to think what needs to be done differently. This paper lays down the framework for business managers to understand Big Data processes. Besides providing a business overview of Big Data core components, the paper presents several questions that the managers must ask to assess the effectiveness of their Big Data processes. This paper is based on the analysis of several Big Data projects that never delivered and comparison against successful ones. The hypothesis is developed based on public information and is proposed as the first step for business managers keen on effectively leveraging Big Data.;2013;Anmol Rajpurohit;10.1109/BigData.2013.6691794;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Access control for big data using data content;Conventional database access control models have difficulties in dealing with big data, especially for the features of volume, variety and velocity. To address the problem, we introduce the Content-based Access Control (CBAC) model for content-centric information sharing. As a complement to conventional models, CBAC makes access control decisions based on the content similarity between user credentials and data content dynamically. We present an enforcement mechanism for CBAC exploiting Oracle's Virtual Private Database (VPD). Experimental results show that CBAC makes reasonable access control decision with a small overhead.;2013;Wenrong Zeng;10.1109/BigData.2013.6691798;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Knowledge cubes — A proposal for scalable and semantically-guided management of Big Data;A Knowledge Cube, or cube for short, is an intelligent and adaptive database instance capable of storing, analyzing, and searching data. Each cube is established based on semantic aspects, e.g., (1) Topical, (2) Contextual, (3) Spatial, or (4) Temporal. A cube specializes in handling data that is only relevant to the cube's semantics. Knowledge cubes are inspired by two prime architectures: (1) Dataspaces that provides an abstraction for data management where heterogeneous data sources can co-exist and it requires no prespecified unifying schema, and (2) Linked Data that provides best practices for publishing and interlinking structured data on the web. A knowledge cube uses Linked Data as its main building block for its data layer and encompasses some of the data integration abstractions defined by Dataspaces. In this paper, knowledge cubes are proposed as a semantically-guided data management architecture, where data management is influenced by the data semantics rather than by a predefined scheme. Knowledge cubes support the five pillars of Big Data also known as the five V's, namely Volume, Velocity, Veracity, Variety, and Value. Interesting opportunities can be leveraged when learning the semantics of the data. This paper highlights these opportunities and proposes a strawman design for knowledge cubes along with the research challenges that arise when realizing them.;2013;Amgad Madkour;10.1109/BigData.2013.6691800;Conferences;;978-1-4799-1293-3
ieee_20221205_08_11_42;Towards Network Reduction on Big Data;"The increasing ease of data collection experience and the increasing availability of large data storage space lead to the existence of very large datasets that are commonly referred as ""Big Data"". Such data not only take over large amount of database storage, but also increase the difficulties for data analysis due to data diversity, which, also makes the datasets seemingly isolated with each other. In this paper, we present a solution to the problem that is to build up connections among the diverse datasets, based upon their similarities. Particularly, a concept of similarity graph along with a similarity graph generation algorithm were introduced. We then proposed a similarity graph reduction algorithm that reduces vertices of the graph for the purpose of graph simplification.";2013;Xing Fang;10.1109/SocialCom.2013.103;Conferences;;978-0-7695-5137-1
ieee_20221205_08_11_42;Big Data and Policy Design for Data Sovereignty: A Case Study on Copyright and CCL in South Korea;The purpose of this paper is as follows. First, I am trying to conceptualize big data as a social problem. Second, I would like to explain the difference between big data and conventional mega information. Third, I would like to recommend the role of the government for utilization of big data as a policy tools. Fourth, while referring to copyright and CCL(Creative Commons License) cases, I would like to explain the regulation for big data on data sovereignty. Finally, I would like to suggest a direction of policy design for big data. As for the result of this study, policy design for big data should be distinguished from policy design for mega information to solve data sovereignty issues. From a law system perspective, big data is generated autonomously. It has been accessed openly and shared without any intention. In market perspective, big data is created without any intention. Big data can be changed automatically in case of openness with reference feature such as Linked of Data. Some policy issues such as responsibility and authenticity should be raised. Big data is generated in a distributed and diverse way without any concrete form in technology perspective. So, we need a different approach.;2013;Hyejung Moon;10.1109/SocialCom.2013.165;Conferences;;978-0-7695-5137-1
ieee_20221205_08_11_42;Next Big Thing in Big Data: The Security of the ICT Supply Chain;In contemporary society, with supply chains becoming more and more complex, the data in supply chains increases by means of volume, variety and velocity. Big data rise in response to the proper time and conditions to offer advantages for the nodes in supply chains to solve prewiously difficult problems. For any big data project to succeed, it must first depend on high-quality data but not merely on quantity. Further, it will become increasingly important in many big data projects to add external data to the mix and companies will eventually turn from only looking inward to also looking outward into the market, which means the use of big data must be broadened considerably. Hence the data supply chains, both internally and externally, become of prime importance. ICT (Information and Telecommunication) supply chain management is especially important as supply chain link the world closely and ICT supply chain is the base of all supply chains in today's world. Though many initiatives to supply chain security have been developed and taken into practice, most of them are emphasized in physical supply chain which is addressed in transporting cargos. The research on ICT supply chain security is still in preliminary stage. The use of big data can promote the normal operation of ICT supply chain as it greatly improve the data collecting and processing capacity and in turn, ICT supply chain is a necessary carrier of big data as it produces all the software, hardware and infrastructures for big data's collection, storage and application. The close relationship between big data and ICT supply chain make it an effective way to do research on big data security through analysis on ICT supply chain security. This paper first analyzes the security problems that the ICT supply chain is facing in information management, system integrity and cyberspace, and then introduces several famous international models both on physical supply chain and ICT supply chain. After that the authors describe a case of communication equipment with big data in ICT supply chain and propose a series of recommendations conducive to developing secure big data supply chain from five dimensions.;2013;Tianbo Lu;10.1109/SocialCom.2013.172;Conferences;;978-0-7695-5137-1
ieee_20221205_08_11_42;Analysis of Big Data Technologies and Method - Query Large Web Public RDF Datasets on Amazon Cloud Using Hadoop and Open Source Parsers;Extremely large datasets found in Big Data projects are difficult to work with using conventional databases, statistical software, and visualization tools. Massively parallel software, such as Hadoop, running on tens, hundreds, or even thousands of servers is more suitable for Big Data challenges. Additionally, in order to achieve the highest performance when querying large datasets, it is necessary to work these datasets at rest without preprocessing or moving them into a repository. Therefore, this work will analyze tools and techniques to overcome working with large datasets at rest. Parsing and querying will be done on the raw dataset - the untouched Web Data Commons RDF files. Web Data Commons comprises five billion pages of web pages crawled from the Internet. This work will analyze available tools and appropriate methods to assist the Big Data developer in working with these extremely large, semantic RDF datasets. Hadoop, open source parsers, and Amazon Cloud services will be used to data mine these files. In order to assist in further discovery, recommendations for future research will be included.;2013;Ted Garcia;10.1109/ICSC.2013.49;Conferences;;978-0-7695-5119-7
ieee_20221205_08_11_42;An improved BP algorithm over out-of-order streams for big data;Due to the difficulty of getting the association rules over out-of-order streams for big data, a new improved BP algorithm based on dynamic adjustment is proposed. We firstly use a dynamic adaptive structural adjustment mechanism to change the network training structure according to the environmental requirements, which can automatically remove invalid training node, and optimize the iterative training process. Secondly, we adjust three factors (i.e. learning index, momentum factor and scaling factor) during the learning process to speed up the learning response, and to enhance the stability of the network. Simulation results show that compared with traditional BP algorithm, this algorithm can get more convergence times,the convergence rate can be improved effectively, and finally obtain the association rules over out-of-order data streams.;2013;Kun Wang;10.1109/ChinaCom.2013.6694712;Conferences;;978-1-4799-1406-7
ieee_20221205_08_11_42;Advanced analytics for harnessing the power of smart meter big data;Smart meters or advanced metering infrastructure (AMI) are being deployed in many countries around the world. Smart meters are the basic building block of the smart grid and governments have invested vast amounts in smart meter deployment targeting wide economic, social and environmental benefits. The key functionality of the smart meter is the capture and transfer of data relating to the consumption (electricity, gas) and events such as power quality and meter status. Such capability has also resulted in the generation of an unprecedented data volume, speed of collection and complexity, which has resulted in the so called big data challenge. To realize the hidden value and power in such data, it is important to use the appropriate tools and technology which are currently being called advanced analytics. In this paper we define a smart metering landscape and discuss different technologies available for harnessing the smart meter captured data. Main limitations and challenges with existing techniques with big data are also highlighted and several future directions in smart metering are presented.;2013;Damminda Alahakoon;10.1109/IWIES.2013.6698559;Conferences;;978-1-4799-1135-6
ieee_20221205_08_11_42;The Oklahoma PetaStore: Big data on a small budget;In the era of Big Data, research productivity can be highly sensitive to the availability of large scale, long term storage. Unfortunately, most mass storage systems are prohibitively expensive at scales appropriate for individual institutions rather than national centers. Furthermore, many researchers won't adopt any centralized technology that they perceive as more expensive than what their research teams could do on their own. This poster examines a combination of business model and technology that addresses these concerns in a comprehensive way, distributing the costs among a funding agency, the institution and the research teams, thereby reducing the challenges faced by each.;2013;Patrick Calhoun;10.1109/CLUSTER.2013.6702629;Conferences;2168-9253;978-1-4799-0898-1
ieee_20221205_08_11_42;HcBench: Methodology, development, and characterization of a customer usage representative big data/Hadoop benchmark;"Big Data analytics using Map-Reduce over Hadoop has become a leading edge paradigm for distributed programming over large server clusters. The Hadoop platform is used extensively for interactive and batch analytics in ecommerce, telecom, media, retail, social networking, and being actively evaluated for use in other areas. However, to date no industry standard or customer representative benchmarks exist to measure and evaluate the true performance of a Hadoop cluster. Current Hadoop micro-benchmarks such as HiBench-2, GridMix-3, Terasort, etc. are narrow functional slices of applications that customers run to evaluate their Hadoop clusters. However, these benchmarks fail to capture the real usages and performance in a datacenter environment. Given that typical datacenter deployments of Hadoop process a wide variety of analytic interactive and query jobs in addition to batch transform jobs under strict Service Level Agreement (SLA) requirements, performance benchmarks used to evaluate clusters must capture the effects of concurrently running such diverse job types in production environments. In this paper, we present the methodology and the development of a customer datacenter usage representative Hadoop benchmark ""HcBench"" which includes a mix of large number of customer representative interactive, query, machine learning, and transform jobs, a variety of data sizes, and includes compute, storage 110, and network intensive jobs, with inter-job arrival times as in a typical datacenter environment. We present the details of this benchmark and discuss application level, server and cluster level performance characterization collected on an Intel Sandy Bridge Xeon Processor Hadoop cluster.";2013;Vikram A. Saletore;10.1109/IISWC.2013.6704672;Conferences;;978-1-4799-0553-9
ieee_20221205_08_11_42;Characterizing the efficiency of data deduplication for big data storage management;The demand for data storage and processing is increasing at a rapid speed in the big data era. Such a tremendous amount of data pushes the limit on storage capacity and on the storage network. A significant portion of the dataset in big data workloads is redundant. As a result, deduplication technology, which removes replicas, becomes an attractive solution to save disk space and traffic in a big data environment. However, the overhead of extra CPU computation (hash indexing) and IO latency introduced by deduplication should be considered. Therefore, the net effect of using deduplication for big data workloads needs to be examined. To this end, we characterize the redundancy of typical big data workloads to justify the need for deduplication. We analyze and characterize the performance and energy impact brought by deduplication under various big data environments. In our experiments, we identify three sources of redundancy in big data workloads: 1) deploying more nodes, 2) expanding the dataset, and 3) using replication mechanisms. We elaborate on the advantages and disadvantages of different deduplication layers, locations, and granularities. In addition, we uncover the relation between energy overhead and the degree of redundancy. Furthermore, we investigate the deduplication efficiency in an SSD environment for big data workloads.;2013;Ruijin Zhou;10.1109/IISWC.2013.6704674;Conferences;;978-1-4799-0553-9
ieee_20221205_08_11_42;Big data and humanitarian supply networks: Can Big Data give voice to the voiceless?;"Billions of US dollars are spent each year in emergency aid to save lives and alleviate the suffering of those affected by disaster. This aid flows through a humanitarian system that consists of governments, different United Nations agencies, the Red Cross movement and myriad non-governmental organizations (NGOs). As scarcer resources, financial crisis and economic inter-dependencies continue to constrain humanitarian relief there is an increasing focus from donors and governments to assess the impact of humanitarian supply networks. Using commercial (`for-profit') supply networks as a benchmark; this paper exposes the counter-intuitive competition dynamic of humanitarian supply networks, which results in an open-loop system unable to calibrate supply with actual need and impact. In that light, the phenomenon of Big Data in the humanitarian field is discussed and an agenda for the `datafication' of the supply network set out as a means of closing the loop between supply, need and impact.";2013;Asmat Monaghan;10.1109/GHTC.2013.6713725;Conferences;;978-1-4799-2401-1
ieee_20221205_08_11_42;A predictive tool for nonattendance at a specialty clinic: An application of multivariate probabilistic big data analytics;The field of data analytics has recently garnered significant attention as a means of improving service, outcome and cost in healthcare. Health informatics tools to date are largely rules-based, static in knowledge-gathering capability, and are not well incorporated within standard clinical processes. We introduce a multivariate analysis tool, built at the user-client interface, directly from data extracted from thousands of EMRs, for automated decision making. The predictive algorithms created to date have been implemented in a variety of clinical scenarios, including ER triage of chest pain, selection of optimal diagnostic testing, and management of chronic illnesses. This technology has now been applied to prediction of patient compliance with scheduled appointments in a medical specialty clinic. Approximately 16% of scheduled VA patients do not show for their scheduled appointments for Gastroenterology clinic. This results in misallocation of provider resources and inefficiencies in health care delivery. The improvement in quality of care, patient outcomes and cost is potentially immense.;2013;Victor Levy;10.1109/CEWIT.2013.6713760;Conferences;;978-1-4799-2546-9
ieee_20221205_08_11_42;KASR: A Keyword-Aware Service Recommendation Method on MapReduce for Big Data Applications;Service recommender systems have been shown as valuable tools for providing appropriate recommendations to users. In the last decade, the amount of customers, services and online information has grown rapidly, yielding the big data analysis problem for service recommender systems. Consequently, traditional service recommender systems often suffer from scalability and inefficiency problems when processing or analysing such large-scale data. Moreover, most of existing service recommender systems present the same ratings and rankings of services to different users without considering diverse users' preferences, and therefore fails to meet users' personalized requirements. In this paper, we propose a Keyword-Aware Service Recommendation method, named KASR, to address the above challenges. It aims at presenting a personalized service recommendation list and recommending the most appropriate services to the users effectively. Specifically, keywords are used to indicate users' preferences, and a user-based Collaborative Filtering algorithm is adopted to generate appropriate recommendations. To improve its scalability and efficiency in big data environment, KASR is implemented on Hadoop, a widely-adopted distributed computing platform using the MapReduce parallel processing paradigm. Finally, extensive experiments are conducted on real-world data sets, and results demonstrate that KASR significantly improves the accuracy and scalability of service recommender systems over existing approaches.;2014;Shunmei Meng;10.1109/TPDS.2013.2297117;Journals;2161-9883;
ieee_20221205_08_11_42;A Time Efficient Approach for Detecting Errors in Big Sensor Data on Cloud;Big sensor data is prevalent in both industry and scientific research applications where the data is generated with high volume and velocity it is difficult to process using on-hand database management tools or traditional data processing applications. Cloud computing provides a promising platform to support the addressing of this challenge as it provides a flexible stack of massive computing, storage, and software services in a scalable manner at low cost. Some techniques have been developed in recent years for processing sensor data on cloud, such as sensor-cloud. However, these techniques do not provide efficient support on fast detection and locating of errors in big sensor data sets. For fast data error detection in big sensor data sets, in this paper, we develop a novel data error detection approach which exploits the full computation potential of cloud platform and the network feature of WSN. Firstly, a set of sensor data error types are classified and defined. Based on that classification, the network feature of a clustered WSN is introduced and analyzed to support fast error detection and location. Specifically, in our proposed approach, the error detection is based on the scale-free network topology and most of detection operations can be conducted in limited temporal or spatial data blocks instead of a whole big data set. Hence the detection and location process can be dramatically accelerated. Furthermore, the detection and location tasks can be distributed to cloud platform to fully exploit the computation power and massive storage. Through the experiment on our cloud computing platform of U-Cloud, it is demonstrated that our proposed approach can significantly reduce the time for error detection and location in big data sets generated by large scale sensor network systems with acceptable error detecting accuracy.;2015;Chi Yang;10.1109/TPDS.2013.2295810;Journals;2161-9883;
ieee_20221205_08_11_42;Attribute Relationship Evaluation Methodology for Big Data Security;There has been an increasing interest in big data and big data security with the development of network technology and cloud computing. However, big data is not an entirely new technology but an extension of data mining. In this paper, we describe the background of big data, data mining and big data features, and propose attribute selection methodology for protecting the value of big data. Extracting valuable information is the main goal of analyzing big data which need to be protected. Therefore, relevance between attributes of a dataset is a very important element for big data analysis. We focus on two things. Firstly, attribute relevance in big data is a key element for extracting information. In this perspective, we studied on how to secure a big data through protecting valuable information inside. Secondly, it is impossible to protect all big data and its attributes. We consider big data as a single object which has its own attributes. We assume that a attribute which have a higher relevance is more important than other attributes.;2013;Sung-Hwan Kim;10.1109/ICITCS.2013.6717808;Conferences;;978-1-4799-2845-3
ieee_20221205_08_11_42;Secured e-health data retrieval in DaaS and Big Data;Big Data is one of rising IT trends such as cloud computing, social networking or ubiquitous computing. Big Data can offer beneficial scenarios in the e-health arena. However, one of the scenarios can be that Big Data needs to be kept secured for a long time in order to gain its benefits such as finding cures for infectious diseases and keeping patients' privacy. From this connection, it is beneficial to analyze Big Data to make meaningful information while the data are stored in a secure manner. Thus, the analysis of various database encryption techniques is essential. In this study, we simulated 3 types of technical environments such as Plain-text, Microsoft Built-in Encryption, and custom Advanced Encryption Standard using Bucket Index in Data-as-a-Service. The results showed that custom AES-DaaS has faster range query response time than MS built-in encryption. In addition, while carrying out the scalability test, we acknowledged there are performance thresholds according to physical IT resources. Therefore, for the purpose of efficient Big Data management in e-health, it is noteworthy to examine its scalability limits as well even if it is under cloud computing environment. Furthermore, when designing an e-health database, both patients' privacy and system performance needs to be dealt as top priorities.;2013;David Shin;10.1109/HealthCom.2013.6720677;Conferences;;978-1-4673-5800-2
ieee_20221205_08_11_42;Applying Sensor Web strategies to Big Data earth observations;Although the focus of the Sensor Web has been somewhat limited to a single architectural view in the form of web services and service oriented architectures our experience has shown in a number of projects that this is not always the most effective solution, especially when deal with Big Data. The correct approach is then to hold to the overall vision of the Sensor Web as a way of gaining access to and organising sensors and sensor data and to use the appropriate architectural patterns and strategies in overcoming the challenges presented. Using these other approaches is not necessarily conflict with an open systems view nor is it non web centric.;2013;Terence L. van Zyl;10.1109/IGARSS.2013.6721278;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_11_42;Big Data and the bright future of simulation — The case of agent-based modeling;Big Data may be an ugly word describing a diverse reality, but it also points to a bright future for simulation in general, and Agent-Based Modeling (ABM) in particular. As companies are struggling to make sense of the staggering amounts of data they have been amassing, data-driven simulation will be the backbone of how value is discovered and captured from data. Drawing from successful applications of ABM, I will make it explicit that what used to be an afterthought of simulation modeling (calibration) is now the cornerstone of the Big Data edifice.;2013;Eric Bonabeau;10.1109/WSC.2013.6721399;Conferences;1558-4305;978-1-4799-2077-8
ieee_20221205_08_11_42;Big Data Framework;We are constantly being told that we live in the Information Era - the Age of BIG data. It is clearly apparent that organizations need to employ data-driven decision making to gain competitive advantage. Processing, integrating and interacting with more data should make it better data, providing both more panoramic and more granular views to aid strategic decision making. This is made possible via Big Data exploiting affordable and usable Computational and Storage Resources. Many offerings are based on the Map-Reduce and Hadoop paradigms and most focus solely on the analytical side. Nonetheless, in many respects it remains unclear what Big Data actually is, current offerings appear as isolated silos that are difficult to integrate and/or make it difficult to better utilize existing data and systems. Paper addresses this lacunae by characterising the facets of Big Data and proposing a framework in which Big Data applications can be developed. The framework consists of three Stages and seven Layers to divide Big Data application into modular blocks. The aim is to enable organizations to better manage and architect a very large Big Data application to gain competitive advantage by allowing management to have a better handle on data processing.;2013;Firat Tekiner;10.1109/SMC.2013.258;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_11_42;MapReduce Based Method for Big Data Semantic Clustering;Big data analysis is very hot in cloud computing environments. How to automatically map heterogeneous data with the same semantics is one of the key problems in big data analysis. A big data clustering method based on the MapReduce framework is proposed in this paper. Big data are decomposed into many data chunks for parallel clustering, which is implemented by Ant Colony. Data elements are moved and clustered by ants according to the presented criterion. The proposed method is compared with the MapReduce framework based k-means clustering algorithm on a great amount of practical data. Experimental results show that the proposal is much effective for big data clustering.;2013;Jie Yang;10.1109/SMC.2013.480;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_11_42;A Reduction Algorithm for the Big Data in 3D Surface Reconstruction;As big data acquisition and storage becomes increasingly affordable, especially in the modern range sensing technology for the scans of complex objects, it is a challenge to reconstruct the surface of 3D geometric model effectively and precisely. In this paper, we describe a reduction method for the big data with noises in the 3D surface reconstruction based on partition of unity, Hermite radial basis functions, and sparse regularization. The proposed method not only provides an approach for pruning some redundant data according to the sparsity, but also contains a good robustness to the noises. This approach can be regarded as one of effective methods for processing big data. Experimental results are also provided.;2013;Jianwei Zhao;10.1109/SMC.2013.824;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_11_42;Road traffic big data collision analysis processing framework;With the advancement of sensor technologies, big data processing becomes a new paradigm for large scale information processing. Big data comes from different sources, such as from traffic information, social sites, mobile phone GPS signals and so on. In this paper, we propose a new architecture for distributed processing that enables big data processing on the road traffic data and its related information analysis. We applied Hadoop and HBase that can store and analyze real-time collision data in a distributed processing framework. This framework is designed as flexible and scalable framework using distributed CEP that process massive real-time traffic data and ESB that integrates other services. We tested the proposed framework on road traffic data on a 45-mile section of I-880N freeway CA, USA. By integrating freeway traffic big data and collision data over a ten-year period (1TB Size), we obtained the collision probability.;2013;Duckwon Chung;10.1109/ICAICT.2013.6722733;Conferences;;978-1-4673-6419-5
ieee_20221205_08_11_42;VegaIndexer: A Distributed composite index scheme for big spatio-temporal sensor data on cloud;With the prevalence of data-intensive geospatial applications, massive spatio-temporal sensor data are obtained and the big data have posed grand challenges on existing index methods based on spatial databases due to their intrinsic poor scalability and retrieval efficiency. Motivated by the deficiencies, in this paper, we propose a distributed composite spatio-temporal index scheme called VegaIndexer for efficiently answering queries from large collections of space-time sensor data. Firstly, we present a distributed spatio-temporal indexing architecture based on cloud platform which consists of global index and local index. Moreover, we propose Multi-version Distributed enhanced R+ (MDR+) tree algorithm for accelerating data retrieval and spatio-temporal query efficiency. Furthermore, we design a MapReduce-based parallel processing approach of batch constructing indices for big spatiotemporal sensor data. In addition, we implement VegaIndexer middleware on top of the leading cloud platform, i.e., Hadoop and associated NoSQL database. The experimental experiments show that VegaIndexer outperforms the index methods of typical spatial databases.;2013;Yunqin Zhong;10.1109/IGARSS.2013.6723126;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_11_42;Game theory applied to big data analytics in geosciences and remote sensing;This paper introduces the basic concepts of game theory and outlines the mechanisms for applying game theory models to big data analytics and decision making in the field of geosciences and remote sensing. The author proposes the use of strategic, competitive game theory models for the purpose of spectral band grouping when exploiting hyperspectral imagery. The proposed system uses conflict data filtering based on mutual entropy and a strategy interaction process of multiple band groups in a conflict environment, the goal of which is to maximize the payoff benefit of multiple groups of the whole system. The proposed system uses the Nash equilibrium as the means to find a steady state solution to the band grouping problem, and implements the model under the assumption that all players are rational. The author uses the proposed band grouping as a component in a multi-classifier decision fusion (MCDF) system for automated ground cover classification with hyperspectral imagery. The paper provides experimental results demonstrating that the proposed game theoretic approach significantly outperforms the comparison methods.;2013;Lori Mann Bruce;10.1109/IGARSS.2013.6723733;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_11_42;A cloud-based architecture for Big-Data analytics in smart grid: A proposal;A Smart Grid is an enhanced version of electric grid in which the demand and supply are balanced to meet the customers need. The paper deals with the formation of a cloud-based Smart Grid for analyzing the Bid-Data and taking decisions to balance the demand of customer needs. The proposed formation of smart grid will deal with Big Data set which will contain the data regarding the power usage patterns of customers, historic weather data of the location, the current demand and supply details. The grid will operate on the data being fetched from the cloud storage. The paper also focuses on smart grid being framed with the renewable energy sources.;2013;M. Mayilvaganan;10.1109/ICCIC.2013.6724168;Conferences;;978-1-4799-1595-8
ieee_20221205_08_11_42;Forecasting consumer behavior with innovative value proposition for organizations using big data analytics;The term `Big Data' is used to represent collection of such a huge amount of data that it becomes impossible to manage and process data using conventional database management tools. Big Data is defined by three important parameters `Volume' - Size of Data, `Velocity' - Speed of increase of data and `Variety' - Type of Data. Big data analytics is the process of analyzing this ever growing Big Data. The goal of every organization is to maximize its value for its stake holders. The paper aims to demonstrate that Big data analytics can be used as a catalyst for generating and increasing value for organizations by improving various business parameters. Furthermore, by utilizing case studies the paper also aims to establish that big data analytics supports creation, enhancement and improvement of various business services to significantly improve customer experience as well as value creation for organizations.;2013;Ankur Balar;10.1109/ICCIC.2013.6724280;Conferences;;978-1-4799-1595-8
ieee_20221205_08_11_42;Security Analytics: Big Data Analytics for cybersecurity: A review of trends, techniques and tools;The rapid growth of the Internet has brought with it an exponential increase in the type and frequency of cyber attacks. Many well-known cybersecurity solutions are in place to counteract these attacks. However, the generation of Big Data over computer networks is rapidly rendering these traditional solutions obsolete. To cater for this problem, corporate research is now focusing on Security Analytics, i.e., the application of Big Data Analytics techniques to cybersecurity. Analytics can assist network managers particularly in the monitoring and surveillance of real-time network streams and real-time detection of both malicious and suspicious (outlying) patterns. Such a behavior is envisioned to encompass and enhance all traditional security techniques. This paper presents a comprehensive survey on the state of the art of Security Analytics, i.e., its description, technology, trends, and tools. It hence aims to convince the reader of the imminent application of analytics as an unparalleled cybersecurity solution in the near future.;2013;Tariq Mahmood;10.1109/NCIA.2013.6725337;Conferences;;978-1-4799-1287-2
ieee_20221205_08_11_42;Big data platform development with a domain specific language for telecom industries;This paper introduces a system that offer a special big data analysis platform with Domain Specific Language for telecom industries. This platform has three main parts that suggests a new kind of domain specific system for processing and visualization of large data files for telecom organizations. These parts are Domain Specific Language (DSL), Parallel Processing/Analyzing Platform for Big Data and an Integrated Result Viewer. In addition to these main parts, Distributed File Descriptor (DFD) is designed for passing information between these modules and organizing communication. To find out benefits of this domain specific solution, standard framework of big data concept is examined carefully. Big data concept has special infrastructure and tools to perform for data storing, processing, analyzing operations. This infrastructure can be grouped as four different parts, these are infrastructure, programming models, high performance schema free databases, and processing-analyzing. Although there are lots of advantages of Big Data concept, it is still very difficult to manage these systems for many enterprises. Therefore, this study suggest a new higher level language, called as DSL which helps enterprises to process big data without writing any complex low level traditional parallel processing codes, a new kind of result viewer and this paper also presents a Big Data solution system that is called Petaminer.;2013;Cüneyt Şenbalcı;10.1109/HONET.2013.6729768;Conferences;1949-4106;978-1-4799-2568-1
ieee_20221205_08_24_39;Medical diagnostic and data quality;"The spread of electronic use of data in various areas has pushed the importance of data quality to a higher level. Data quality has syntactic and semantic components; the syntactic component is relatively easy to achieve if supported by tools (either off-the-shelf or our own), while the semantic component requires more research. In many cases such data come from different sources, are distributed across enterprises and are at different quality levels. Special attention needs to be paid to data upon which critical decisions are met, such as medical data for example. The starting point for research is in our case the risk of the medical area. We focus on the semantic component of medical data quality.";2002;T. Welzer;10.1109/CBMS.2002.1011361;Conferences;1063-7125;0-7695-1614-9
ieee_20221205_08_24_39;Multilayer perceptron discrimination of software quality in a biomedical data analysis system;Biomedical data analysis typically involves large data sets, a diverse user base and intensive visualization procedures. These features place stringent quality demands upon software development and performance for both the architect and supervisor. An invaluable tool for software supervisors would be the automatic qualitative grading of software objects in terms of their extensibility, reusability, clarity and efficiency. This paper examines the quality assessment of software objects by a multilayer perceptron in relation to a gold standard provided by two independent software architects.;2002;M.D. Alexiuk;10.1109/CCECE.2002.1013039;Conferences;0840-7789;0-7803-7514-9
ieee_20221205_08_24_39;The evolution of power quality data acquisition systems-triggering to capture power quality events;25 years ago, capturing multiple channels of simultaneous data required multiple oscilloscopes or a very specialized, expensive, and customized data acquisition system. In order to capture power quality events, very sophisticated methods were used to invoke simultaneous triggering across all recording channels. Normally, triggering was accomplished using overvoltage or overcurrent conditions and typically, you only had one shot. In addition, there was no easy way to get an oscilloscope to trigger on an undervoltage event (like a sag, for example). Today, all of that has changed. We are now able to record multiple channels of simultaneous data using many types of triggering mechanisms. This paper discusses the evolution of triggering power quality instrumentation and the advanced triggering systems available for portable and fixed panel mounted meters.;2002;D. Carnovale;10.1109/PAPCON.2002.1015146;Conferences;;0-7803-7446-0
ieee_20221205_08_24_39;Using Landsat TM data to aid the assessment of long-term trends in lake water quality in New Hampshire lakes;Assessing long-term trends in lake water quality is an important aspect of lake management. In the state of New Hampshire (NH), volunteer monitors working with the NH Lakes Lay Monitoring Program (LLMP) have measured lake quality parameters in many of the state's lakes for one or two decades. These measurements have served as the data bank for both policy decisions and scientific research on lake health. Because of the promise of improved remote sensing platforms for gathering water quality observations on a consistent basis across the state, we examine the potential relationships between remotely sensed observations and in situ water quality measurements, using Landsat TM imagery at a resolution of 30 m. Strong relationships between water clarity (Secchi disk transparency) and Landsat TM bands 1 and 3 have been determined for lakes in the Upper Midwest region. Using the long-term measurements of water quality data in NH lakes (water clarity, chlorophyll a, dissolved organic color, aquatic plant extent), we examine the usefulness beyond the original region of study of the empirical relationships between Landsat TM bands and Secchi disk transparency developed for Midwestern lakes. TM scenes taken during the period 1993-1999 are compared to water quality measurements for approximately 50 lakes in NH. The ability of 30 m spatial resolution imagery to capture temporal and spatial trends in lake water quality in our region is examined.;2002;A.L. Schloss;10.1109/IGARSS.2002.1026880;Conferences;;0-7803-7536-X
ieee_20221205_08_24_39;Requirement specifications for an enterprise level collaborative, data collection, quality management and manufacturing tool for an EMS provider;Numerous software applications that serve as effective tools in solving localized issues for an electronics manufacturing service (EMS) provider are currently available. However, there are very few products available in the market for an EMS provider that offer an integrated solution to issues ranging from the production floor to supplier quality. Applications with features including that of manufacturing execution systems (MES) and shop floor control systems would be examples of middle layer systems. This paper discusses the requirement specifications developed for a middle layer software tool to support collaborative manufacturing, extend shop floor visibility, monitor product yield and defects, and improve unit level traceability. Also, the software tool would assist in increasing operational efficiency by reducing paperwork of shop floor personnel, helping to locate reference designators and finding part numbers, and reducing debug time. The application interfaces with SMT, tests and inspection equipment for effective product and process monitoring. This paper also discusses the issues that are of concern to quality department personnel and customers such as the non-conformance materials report (NCMR) and tracking of corrective action requests associated with them and providing root cause analysis capability for NCMRs. The requirement specifications detailed here were developed to meet the needs of both high and low volume facilities involved in PCB assembly and subsequent box-build activities. This paper delineates the functionality required in an integrated system at an EMS provider for it to be an effective tool for manufacturing, improving quality and integrating data from facilities worldwide.;2002;S. Bahl;10.1109/IEMT.2002.1032741;Conferences;;0-7803-7301-4
ieee_20221205_08_24_39;Control Data Corporation's Government Systems Group Standard Software Quality Program;The authors describe the necessity of developing the Government Systems Group standard Software Quality Program (SQP), the background in developing the SQP, the advantages of the SQP, the components of the SQP, and the highlights of the SQP. The goal of the standard SQP was to develop common and reusable quality processes. The SQP will produce quality products, while the plan offers the advantages of compliancy, reusability, efficiency, effectiveness, consistency, cost savings, and portability. The components of the SQP include the policy, organization, plan, and handbook. The main elements of the SQP, which reflects government standards DOD-STD-2167A and DOD-STD-2168 for software development projects, are discussed. This standard SQP was developed using the total quality management process methodologies. The influence that the Software Engineering Institute's Capability Assessment had on developing and implementing this standard SQP is also discussed.<>;1990;G. Redig;10.1109/NAECON.1990.112846;Conferences;;
ieee_20221205_08_24_39;Quality Control Procedures in Processing Oceanographic Data;The National Oceanographic Data Center (NODC) receives significant volumes of oceanographic environmental data for processing. These data vary from the conventional ocean station data and expendable bathythermograph data to a wide variety of biological and pollution data. The need for a more objective environmental quality control became evident as the volume of data continued to increase. At the same time, the resources-enriched computer capability and large climatological files of fully processed data-for developing such a system became available. The NODC procedures, prior to the development of environmental models, were based primarily on the initial quality control by data donors and subjective analysis by NODC oceanographers. This study describes the use of ocean models for quality controlling ocean serial and XBT data.;1981;I. Perlroth;10.1109/OCEANS.1981.1151615;Conferences;;
ieee_20221205_08_24_39;Environmental Data Quality Estimates for Ocean Data Buoys;A summary of specific data quality estimates for the major buoy systems now operationally deployed by the NOAA Data Buoy Office (NDBO) is provided. Quality estimates are included for four distinct buoy types: (1) The Prototype Environmental Buoy (PEB) - deep ocean moored, (2) the Engineering Experimental Phase Buoy (EEP) - deep ocean moored, (3) the 5.0m Discus Buoy - shallow ocean moored, and (4) the NOMAD Buoy - shallow or deep ocean moored. The quality estimates presented are a result of in situ and laboratory testing, computer modeling, and analytical evaluations. Total quality estimates, or total system errors, and some examples of total system error breakdowns into component errors, are provided for some general environmental situations. Detailed quality estimates such as these are important since efficient uses of data from measurement platforms such as buoys are dependent on a comprehensive understanding of the accuracy of the data.;1976;G. Withee;10.1109/OCEANS.1976.1154197;Conferences;;
ieee_20221205_08_24_39;Circulatory survey data quality assurance;The National Ocean Survey is developing a data quality assurance program for its circulatory surveys in collaboration with the NOAA Office of Ocean Technology and Engineering Services (OTES). The present emphasis is on East Coast circulatory surveys conducted by the 133-foot NOAA ship FERREL using Grundy Model 9021 current meters. The methodology for estimating total measurement uncertainties is described, including extensive instrument test and evaluation, quality control field checks, error source identification and estimation, and quality assurance analyses. The results of the 1979 Gulf At-Sea Performance Experiment (GASP) on the Louisiana inner shelf are summarized.;1982;H. Frey;10.1109/CCM.1982.1158434;Conferences;;
ieee_20221205_08_24_39;A binary image preprocessor for document quality improvement and data reduction;A new binary image-enhancement operation is proposed to suppress noise in documents, introduced, e.g., by poor-quality copying. As opposed to approaches previously published, the new technique does not erode characters, symbols, drawing elements, etc, and thus improves the intelligibility of documents. In addition, it is shown that the new preprocessing method reduces significantly the data volume necessary to represent documents in compressed form. Finally, a low-cost hardware implementation is proposed, suitable of being integrated, e.g., in future scanning devices.;1986;F. Wahl;10.1109/ICASSP.1986.1169274;Conferences;;
ieee_20221205_08_24_39;Use of lightning location systems data in integrated systems for power quality monitoring;Voltage sags are one of major concerns in power quality, as they cause serious malfunction of a large number of apparatuses. There are a number of reasons for voltage sags in distribution networks: there is some evidence, however, that in electrical systems located in regions with high value of isokeraunic level, lightning can cause the majority of voltage sags. Data from lightning location systems, which provide an estimation of both lightning flash location and return-stroke current amplitude, can then be used to understand whether lightning is indeed the real cause of circuit breaker operation during thunderstorms-which means, in turn, of voltage sags-or not. Due to the complexity of the problem, the information coming from LLS (lighting location and estimate of lighting current amplitude) are, in general, not enough to infer the origin for voltage sags. It is necessary to suitably integrate them with data from system monitoring, e.g. relevant to the intervention of circuit breakers in primary substations, and with simulation results from of accurate models for computing lighting-induced voltages on complex power networks.;2002;C.A. Nucci;10.1109/TDC.2002.1178458;Conferences;;0-7803-7525-4
ieee_20221205_08_24_39;Mind your P's and Q's [power-quality events capture, data acquisition system];This paper reviews the evolution of data-acquisition systems and the use of triggering to capture power-quality events. Triggers characterize the events and provide unique methods of data sorting. They make capturing data easier. They make sorting data easier. They make analysis easier and faster. The drawback is that they may lead to fast/false (oversimplified) conclusions. There is also the danger that, if setup incorrectly, the event may be missed.;2003;D. Carnovale;10.1109/MIA.2003.1180950;Magazines;1558-0598;
ieee_20221205_08_24_39;Data quality and grounding considerations for a medical facility;Medical facilities as well as most other commercial, industrial, and educational facilities depend heavily on telecommunications, data, and computer networks. These often experience problems with signal quality and noise because of the high-speed, low-level signals. In addition, there are serious issues that relate to electrical safety as well as computer grounding and communications. Topics such as the 11 different ground systems, connections to ground, avoiding potential difference, avoiding circulating currents, and wiring methods are discussed. Three areas specific to networks require special attention. These are equipment bonding, isolated earth ground, and isolated power systems.;2002;M.O. Durham;10.1109/MWSCAS.2002.1186828;Conferences;;0-7803-7523-8
ieee_20221205_08_24_39;Development of a real-time data quality monitoring system using embedded intelligence;Rule-based reasoning and case-based reasoning have emerged as two important and complimentary reasoning methodologies in the field of artificial intelligence (AI). This paper describes the development of a real-time data quality monitoring system (CORMS AI) using case-based and rule-based reasoning. CORMS AI was developed to augment an existing decision support system (CORMS Classic) for monitoring the quality of environmental data and information and their respective computer based systems for use in NOAA Ocean Service's oceanographic operational products.;2002;T. Bethem;10.1109/OCEANS.2002.1191909;Conferences;;0-7803-7534-3
ieee_20221205_08_24_39;Quality control of ocean temperature and salinity profile data;The objective of data quality control (QC) is to ensure consistency within a single data set or throughout a historical database. This is a vital step to be taken with any data set before scientific analysis can be proceed. A redesign of data quality procedures for temperature and salinity profile data undertaken by the Naval Oceanographic Office (NAVOCEANO) during the past year has resulted in new data validation standards and procedures. New data QC and editing software has been developed and is being applied to all new and archived shipboard and aircraft temperature and salinity profile data collected or held by NAVOCEANO. The initial QC effort (Level 1 effort) is in progress. Level 1 effort is an assessment of the approximately 8 million profiles held in the NAVOCEANO archives for outlying and invalid profiles by comparison of profiles from relevant environments. The subsequent Level 2 effort entails more detailed investigations and further analyses for editing the previously flagged profiles. The MATLAB-based data edit programs use Master Oceanographic Observation Data Set (MOODS) 2000, a revised internal standard data file format, which will ease the use and exchange of data. The new file structure also stores numerous data quality flags and histories of collections for incorrect or missing metadata, deviations from climatological statistics, and profile data quality.;2002;K.P. Grembowicz;10.1109/OCEANS.2002.1192144;Conferences;;0-7803-7534-3
ieee_20221205_08_24_39;Power quality disturbance data compression, detection, and classification using integrated spline wavelet and S-transform;In this paper, power quality transient data are compressed and stored for analysis and classification purposes. From the compressed data set, original data are reconstructed and then analyzed using a modified wavelet transform known as S-transform. Compression techniques using splines are performed through signal decomposition, thresholding of wavelet transform coefficients, and signal reconstruction. Finally, the authors present compression results using splines and examine the application of splines compression in power quality monitoring to mitigate against data-communication and data-storage problems. Since S-transform has better time frequency and localization property, power quality disturbances are detected and then classified in a superior way than the recently used wavelet transform.;2003;P.K. Dash;10.1109/TPWRD.2002.803824;Journals;1937-4208;
ieee_20221205_08_24_39;Air quality data remediation by means of ANN;We present an application of neural networks to air quality time series remediation. The focus has been set on photochemical pollutants, and particularly on ozone, considering statistical correlations between precursors and secondary pollutants. After a preliminary study of the phenomenon, we tried to adapt a predictive MLP (multi layer perceptron) network to fulfill data gaps. The selected input was, along with ozone series, ozone precursors (NO/sub x/) and meteorological variables (solar radiation, wind velocity and temperature). We then proceeded in selecting the most representative periods for the ozone cycle. We ran all tests for a 80-hours validation set (the most representative gap width in our data base) and an accuracy analysis with respect to gap width as been performed too. In order to maximize the process automation, a software tool has been implemented in the Matlab/spl trade/ environment. The ANN validation showed generally good results but a considerable instability in data prediction has been found out. The re-introduction of predicted data as input of following simulations generates an uncontrolled error propagation scarcely highlighted by the error autocorrelation analysis usually performed.;2002;G. Latini;10.1109/ICONIP.2002.1198163;Conferences;;981-04-7524-1
ieee_20221205_08_24_39;Water quality retrievals from combined Landsat TM data and ERS-2 SAR data in the Gulf of Finland;This paper presents the applicability of combined Landsat Thematic Mapper and European Remote Sensing 2 synthetic aperture radar (SAR) data to turbidity, Secchi disk depth, and suspended sediment concentration retrievals in the Gulf of Finland. The results show that the estimated accuracy of these water quality variables using a neural network is much higher than the accuracy using simple and multivariate regression approaches. The results also demonstrate that SAR is only a marginally helpful to improve the estimation of these three variables for the practical use in the study area. However, the method still needs to be refined in the area under study.;2003;Yuanzhi Zhang;10.1109/TGRS.2003.808906;Journals;1558-0644;
ieee_20221205_08_24_39;Video quality objective metric using data hiding;"In this paper a non-reference objective video quality metric is proposed. The quality metric is obtained by means of a non-conventional use of data hiding technique. Test data are embedded in an MPEG-2 video; the basic assumption is that the data embedded undergo under the same degradation as the host video. To analyze the performance of the system, a comparison between the results obtained using this metric and the perceived mean annoyance values was performed. The annoyance values were obtained through a psychophysical experiment, which measured the threshold and mean annoyance values of compressed videos.";2002;M.C.Q. Farias;10.1109/MMSP.2002.1203346;Conferences;;0-7803-7713-3
ieee_20221205_08_24_39;A variable rate channel quality feedback scheme for 3G wireless packet data systems;An expanded effort is underway to support the evolution of the UMTS and cdma2000-1x standards to meet the rapidly developing needs associated with wireless Internet applications. A number of performance enhancing technologies are proposed to ensure high peak and average packet data rates while supporting circuit-switched voice and packet data on the same spectrum. These techniques include adaptive modulation and coding (AMC), hybrid ARQ (H-ARQ) and fat-pipe scheduling. In order to enable these techniques downlink channel quality feedback (CQF) through explicit uplink signaling is necessary. Frequent CQF results in good estimates of downlink channel quality at the base station, which, in turn, improves downlink system performance. However, this comes at the expense of larger uplink signaling overhead, thereby impacting the overall uplink capacity. Infrequent CQF, on the other hand, reduces this signaling overhead in the uplink at the expense of larger errors in the channel quality estimates available at the base station, thereby leading to system performance degradation. In this paper, we present a variable rate CQF scheme that significantly reduces uplink signaling overhead without affecting downlink system performance. In addition to simulation results demonstrating the performance of the proposed scheme, we also compare the reduction in uplink signaling overhead of the said scheme with other known methods.;2003;A. Das;10.1109/ICC.2003.1204494;Conferences;;0-7803-7802-4
ieee_20221205_08_24_39;The effect of the elevation of GPR antennas on data quality;In many GPR-measurement situations it is possible to measure with the antennas of the GPR-system in contact with medium containing the unknown objects. In this paper, we have tested the performance of a commercial Pulse Ekko 1000 system at various heights above a sand-surface. We show measurements over the same area with the antennas of this near-surface GPR system at four different heights above the surface. We show that the decrease in signal to noise ratio is small for two-way distances between antenna and surface is smaller than one third of the dominant wavelength. For larger distances, the signal quality decreases rapidly for data processing purposes and object detectability.;2003;R. Bloemenkamp;10.1109/AGPR.2003.1207319;Conferences;;90-76928-04-5
ieee_20221205_08_24_39;Survey of medical data from data acquisition to high quality patient treatment;"The European Clinical Database EuCliD/spl reg/ has been developed as a tool for supervising selected quality indicators of about 200 European dialysis centres. EuCliD/spl reg/ is a Lotus Notes/spl reg/ based database currently containing medical data of more than 22,000 dialysis patients from 220 dialysis centres in 10 European countries. Data evaluation is performed with statistical tools like SPSS. EuCliD/spl reg/ is used as a part of the Quality Management System of Fresenius Medical Care (FMC) and its Continuous Quality Improvement (CQI) program. Each participating dialysis centre receives benchmarking reports in regular intervals (currently every half year). The benchmark for all quality parameters is the weighted mean of the corresponding data of all centres. Quality parameters are selected according to the ""Quality Pyramid"" and the European Best Practice Guidelines (EBPG) or other accepted guidelines (for instance DOQI). An obvious impact of our work on the quality of the treatments could be observed. This concerns important outcome predictors like K*t/V and haemoglobin concentration as well as the outcome itself expressed in survival rates.";2003;H. Steil;10.1109/ITAB.2003.1222558;Conferences;;0-7803-7667-6
ieee_20221205_08_24_39;OpenGL volumizer: a toolkit for high quality volume rendering of large data sets;We present the OpenGL Volumizer API for interactive, high-quality, scalable visualization of large volumetric data sets. Volumizer provides a high-level interface to OpenGL hardware to allow application writers and researchers to visualize multiple gigabytes of volumetric data. Use of multiple graphics pipes scales rendering performance and system resources including pixel-fill rate and texture memory size. Volume roaming and multi-resolution volume rendering provide alternatives for interactive visualization of volume data. We combine the concepts of roaming and multi-resolution to introduce 3D Clip-Textures, an efficient technique to visualize arbitrarily large volume data by judiciously organizing and paging them to local graphics resources from storage peripherals. Volumetric shaders provide an interface for high quality volume rendering along with implementing new visualization techniques. This paper gives an overview of the API along with a discussion of large data visualization techniques used by Volumizer.;2002;P. Bhanirantka;10.1109/SWG.2002.1226509;Conferences;;0-7803-7641-2
ieee_20221205_08_24_39;Data quality management improvement;Summary form only given. Nowadays, more and more organizations are realizing the importance of their data, because it can be considered as an important asset present in nearly all business organizational processes. The traditional point of view of quality in information systems has been focused on software quality. The basis of our proposal is to consider information as a result of a data management process, which can be supported by the software running in the information system. Our aim is to optimize the data management process (DMP) in order to assure data quality. For this, we have just drawn up a framework based on maturity levels (such as CMMI) with specific and generic data quality goals, which are achieved by executing the corresponding activities.;2003;I. Caballero;10.1109/AICCSA.2003.1227489;Conferences;;0-7803-7983-7
ieee_20221205_08_24_39;Using repertory grids to test data quality and experts' hunches;The 'theorise-inquire' technique is described, which supports the testing of both experts' hunches and the quality of data sources. This technique is useful for the identification of data sources and data gaps by domain experts. We describe and illustrate the use of group contrasts, an analysis technique that allows an expert to explore and interpret repertory grids interactively to find significant contrasting relationships between attributes and test these against data sources.;2003;S. Stumpf;10.1109/DEXA.2003.1232120;Conferences;1529-4188;0-7695-1993-8
ieee_20221205_08_24_39;A quantitative analysis of the data acquisition requirements for measuring power quality phenomena;Measuring power quality phenomena poses great challenges to instruments designers as well as the users. The many phenomena having different characteristics call for different and possibly contradictory requirements on the data acquisition system. This article reviews the requirements needed for accurate measurement of power quality phenomena. It focuses on the sampling method and the digitization process. The results show that careful selection of data acquisition hardware is paramount to ensuring accurate measurements.;2003;Shiun Chen;10.1109/TPWRD.2003.817521;Journals;1937-4208;
ieee_20221205_08_24_39;High-quality two-level volume rendering of segmented data sets on consumer graphics hardware;One of the most important goals in volume rendering is to be able to visually separate and selectively enable specific objects of interest contained in a single volumetric data set, which can be approached by using explicit segmentation information. We show how segmented data sets can be rendered interactively on current consumer graphics hardware with high image quality and pixel-resolution filtering of object boundaries. In order to enhance object perception, we employ different levels of object distinction. First, each object can be assigned an individual transfer function, multiple of which can be applied in a single rendering pass. Second, different rendering modes such as direct volume rendering, iso-surfacing, and non-photorealistic techniques can be selected for each object. A minimal number of rendering passes is achieved by processing sets of objects that share the same rendering mode in a single pass. Third, local compositing modes such as alpha blending and MIP can be selected for each object in addition to a single global mode, thus enabling high-quality two-level volume rendering on GPUs.;2003;M. Hadwiger;10.1109/VISUAL.2003.1250386;Conferences;;0-7803-8120-3
ieee_20221205_08_24_39;Quality assessment of traversability maps from aerial LIDAR data for an unmanned ground vehicle;In this paper we address the problem of assessing quantitatively the quality of traversability maps computed from data collected by an airborne laser range finder. Such data is used to plan paths for an unmanned ground vehicle (UGV) prior to the execution of long range traverses. Little attention has been devoted to the problem we address in this paper. We use a unique data set of geodetic control points, real robot navigation data, ground LIDAR (light detection and ranging) data and aerial imagery, collected during a week long demonstration to support our work.;2003;N. Vandapel;10.1109/IROS.2003.1250645;Conferences;;0-7803-7860-1
ieee_20221205_08_24_39;High-quality vehicle trajectory generation from video data based on vehicle detection and description;Vehicle trajectories contain rich information on microscopic phenomena such as car following and lane changing. Despite many efforts to retrieve reliable trajectories from video images, previous approaches do not give high enough quality of trajectories that can be used in microscopic analysis. We introduce a new vehicle tracking approach based on a model-based 3-D vehicle detection and description algorithm. The proposed algorithm uses a probabilistic line feature grouping method to detect vehicles with little computation. A dynamic programming algorithm is proposed for fast reasoning. We present the system implementation and the vehicle detection and tracking results.;2003;Zu Whan Kim;10.1109/ITSC.2003.1251944;Conferences;;0-7803-8125-4
ieee_20221205_08_24_39;Enabling data quality notification in cooperative information systems through a Web-service based architecture;"Cooperative information systems (CISs) are often characterized by a high degree of data replication; as an example, in an e-government scenario, the personal data of citizens are stored by almost all administrations. In such scenarios, organizations typically provide the same information with distinct quality levels and this enables providing users with data of the highest available quality. Furthermore, the comparison of data values might be used to enforce a general improvement of data quality in all organizations. In the DaQuinCIS project (Aguilera et al., 1999), we propose an architecture for the management of data quality in CISs; this architecture allows the diffusion of data and related quality and exploits data replication to improve the overall quality of cooperative data. In this paper, we present an overview of a component of the DaQuinCIS architecture, namely the quality notification service (QNS), which is used to inform interested users when changes in quality values occur within the CIS. QNS can be used to control the quality of critical data, e.g. to keep track of quality changes and to be always aware when quality degrades under a certain threshold. The interaction between the QNS and its users follows the publish/subscribe paradigm: a user willing to be notified for quality changes subscribes to the QNS by submitting the features of the events to be notified for, through a specific subscription language. When a change in quality occurs, an event is published by the QNS i.e., all the users which have a consistent subscription receive a notification. However, as shown in the paper by Marchetti et al. (2003), currently available pub/sub infrastructures do not allow to meet all the requirements that a QNS implementation should satisfy, in particular scaling to a large number of users and coping with platform heterogeneity. QNS addresses both these problems through a layered architecture that: (i) encapsulates the technological infrastructure specific of each organization; (ii) adopts the standard Web-service technology to implement inter-organization communications; and (iii) embeds solutions and algorithms (namely, merge subscriptions and diffusion trees) to reduce the use of physical and computational resources. The remainder of this paper is organized as follows: we first introduce some preliminary concepts and the QNS specification; then we motivate and describe the internal architecture of the service. Due to the lack of space, explanations are given at a very high abstraction level. Interested readers can find technical and formal details, as well as running examples, in the paper by Scannapieco et al. (2003).";2003;C. Marchetti;10.1109/WISE.2003.1254505;Conferences;;0-7695-1999-7
ieee_20221205_08_24_39;Use of IEC 61850 object models for power system quality/security data exchange;This paper covers the use of the object models and services of the international standard IEC 61850 for information exchange to support power system quality and security functions. IEC 61850 has many features that make it cost effective, easy to implement and maintain and it may be readily extended to include new devices of interest to the electric utility industry. The object models incorporate basic data types for the common formats used in power system information exchange. Further, the object models are self-defining such that software and/or protocol changes are not required when configuring new and advanced power system applications. Also covered is the status of UCA/sup /spl reg// international, the users group that has assisted with the development of the object models and the related documents.;2003;A. Apostolov;10.1109/QSEPDS.2003.159813;Conferences;;2-85873-015-6
ieee_20221205_08_24_39;IEEE Recommended Practice for the Transfer of Power Quality Data;This recommended practice defines a file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner. The format is designed to represent all power quality phenomena identified in IEEE Std 1159 TM -1995, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to minimize disk space and transmission times. The utilization of globally unique identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.;2004;;10.1109/IEEESTD.2004.94416;Standards;;978-0-7381-3579-3
ieee_20221205_08_24_39;Welding quality monitoring and management system based on data mining technology;"In allusion to automatic arc welding production, this paper focuses on the establishment of on-line quality monitoring system based on data mining (DB) technology. The system is set up with client/server architecture. In the server, Shewhart control chart and decision tree are applied to monitor the production and supply decision-making based on space series DB; whilst Shewhart control chart and multivariate statistical process control (MSPC) are applied to give quality indication of welding process based on time series DB. The system realizes the on-line quality monitoring for each produced parts as well as the quality management for the whole workshop production.";2003;Chun-Hua Zhang;10.1109/ICMLC.2003.1264433;Conferences;;0-7803-8131-9
ieee_20221205_08_24_39;Power quality XML markup language for enhancing the sharing of power quality data;The growing attention on power quality related problems has prompted the need to share power quality information between measurement equipments, and analysis and simulation tools. However, existing equipment designs are still very much proprietary and are rarely catered for the exchange of information between one another. Contrasting data format and communication protocol has often obstructed the sharing of information. This paper describes an attempt to exploit a well-known Internet technology, the XML, to facilitate the sharing of power quality information. Based on the well-known power quality data interchange format (PQDIF), a new power quality XML markup language is proposed. The definition of the of markup language is described in details with illustrative examples language is described in details with illustrative examples of several XML-based power quality documents. Two possible data-exchanging scenarios, including one over the Internet, are also demonstrated in the paper.;2003;S. Chen;10.1109/PES.2003.1267388;Conferences;;0-7803-7989-6
ieee_20221205_08_24_39;Power quality data mining using soft computing and wavelet transform;This paper presents a new approach to power quality data mining using a modified wavelet transform for feature extraction of power disturbance signal data and a fuzzy multilayer perceptron network to generate the rules and classify the patterns. The choice of modified wavelet transform known as multiresolution s-transform is essential for processing very short duration nonstationary time series data from transient disturbances occurring on an electric supply network as they can not be handled by conventional Fourier and other transform methods for extraction of relevant features pertinent for data mining applications. The trained fuzzy neural network infers the output class membership value of an input pattern and a certainty measure is also presented to facilitate rule generation. Using the electric supply network disturbance data obtained from numerical algorithms and MATLAB software, the paper presents transient disturbance pattern classification scores. A knowledge discovery approach is also highlighted in the paper to convert raw power disturbance signal data to knowledge in the form of an answer module to the queries by the end-users. The pattern classification approach used in this paper can also be applied to speech, cardiovascular system and other medical and engineering databases.;2003;P.K. Dash;10.1109/TENCON.2003.1273392;Conferences;;0-7803-8162-9
ieee_20221205_08_24_39;2-D analysis and compression of power-quality event data;This paper introduces a novel two-dimensional (2-D) representation of the power quality event data. 2-D discrete-time wavelet transform is applied to the 2-D representation of real-life event data. The proposed representation and the transform is tested in terms of both event analysis and data compression. The experimental results indicate that the 2-D transform of the event data outperforms the results obtained by conventional one-dimensional (1-D) wavelet transform-based methods.;2004;O.N. Gerek;10.1109/TPWRD.2003.823197;Journals;1937-4208;
ieee_20221205_08_24_39;Factors affecting the signal quality in optical data transmission and estimation method for BER and SNR;Optical communication in fiber and photonic devices is subject to power loss and signal distortion due to linear and nonlinear phenomena. We provide an analysis of factors that affect the signal quality and we describe a method to estimate the signal bit error rate based on eye diagram sampling and statistical estimates.;2004;S.V. Kartalopoulos;10.1109/ITCC.2004.1286721;Conferences;;0-7695-2108-8
ieee_20221205_08_24_39;NEXRAD data quality by spectral processing. Spectral processing on NCAR's S-Pol radar;The WSR-88D radar system (NEXRAD) is developing a phase coded pulsing scheme using spectral processing that will improve data quality by removing overlaid echo contamination. A prototype spectral processor has been implemented on NCAR's S-Pol research radar for validation testing. Spectral processing will simultaneously enhance the anomalous propagation clutter mitigation technique, which uses a fuzzy logic based radar echo classifier, and allow future data quality enhancements.;2003;J. Keeler;10.1109/IGARSS.2003.1294435;Conferences;;0-7803-7929-2
ieee_20221205_08_24_39;Study on the quality of hyperspectral vegetation data observed in the field;A measurement model on spectra quality is presented through a bigram composed of a spectra quality grade and a metadata integrality grade. Quantitative describing datasets and qualitative describing datasets of spectra quality are extracted with spectra enveloping line analysis, spectra line profile analysis, principles of relative parameters matching and spectra prior knowledge. The quality grade is converted from subordinative degree of eigenpoints and eigenvalues from quantitative datasets. The metadata integrality grade is obtained by visiting each node in a multicross tree by which metadata about vegetation spectra is organized. The two grades make up a bigram by which one can evaluate vegetation spectra quality.;2003;Yanmin Shuai;10.1109/IGARSS.2003.1294875;Conferences;;0-7803-7929-2
ieee_20221205_08_24_39;Study on the spectral quality preservation derived from multisensor image fusion techniques between JERS-1 SAR and Landsat TM data;The advantage of multisensor data fusion stems from the fact that the use of multiple types of sensors increases the accuracy with which a quantity can be observed or characterized. The response of radar is more a function of geometry and structure than surface reflection as occurs in the optical wavelengths. A suitable fusion method has to be chosen with respect to the used spectral characteristic of the multispectral bands and the intended application. This paper describes a comparative study of multisensor image fusion techniques in preserving spectral quality of the fused images. Image fusion techniques applied in this study are: wavelet, intensity-hue-saturation (IHS), principal component analysis (PCA), and high pass filtering (HPF). With these image fusion techniques, a higher spatial resolution JERS-1 SAR is fused with Landsat TM data. The merging process is carried out at the pixel level and the comparison of the resulting images is explained based on the measurement in preserving spectral quality of the fused images. Assessment of the spectral quality is performed by graphical and statistical methods between the original TM image and the fused images. The factors computed to qualify the fused images are: mean, standard deviation, coefficient correlation, and entropy. With a visual inspection, wavelet and PCA techniques seem to be better than the other techniques. PCA provided the greatest improvement with an average entropy of about 5.119 bits/pixel.;2003;Rokhmatuloh;10.1109/IGARSS.2003.1295228;Conferences;;0-7803-7929-2
ieee_20221205_08_24_39;CALDEA: a data quality model based on maturity levels;In these days, most organizations have realised, at last, so important as their software is their data, because it can be considered as an important asset present in all business organizational processes. Our aim is to optimise the data management process (DMP) in order to assure data quality. For this, we have just drawn a framework based on maturity levels - as CMMIs ones -with specific and generic data quality goals, which are achieved by executing the corresponding activities.;2003;I. Caballero;10.1109/QSIC.2003.1319125;Conferences;;0-7695-2015-4
ieee_20221205_08_24_39;A data mining approach to objective speech quality measurement;"Existing objective speech quality measurement algorithms still fall short of the measurement accuracy that can be obtained from subjective listening tests. We propose an approach that uses statistical data mining techniques to improve the accuracy of auditory-model based quality measurement algorithms. We present the design of a novel measurement algorithm using the multivariate adaptive regression splines (MARS) method. A large set of speech distortion features is first created. MARS is used to find a small set of features that provide the best estimate (""model"") of speech quality. One appeal of the approach is that the model size can scale with the amount of speech data available for learning. In our simulations, the new algorithm furnishes significant performance improvement over PESQ (perceptual evaluation of speech quality).";2004;Wei Zha;10.1109/ICASSP.2004.1326022;Conferences;1520-6149;0-7803-8484-9
ieee_20221205_08_24_39;Soundness and quality of semantic retrieval in DNA-based memories with abiotic data;Associative memories based on DNA-affinity have been proposed based in L. M. Adleman (1994) and E. Baum (1995). Previously, we have quantified the quality of retrieval of genomic information in simulation as stated in M. Garzon et al. (2003). Here, the ability of two types of DNA-based memories to store abiotic data and retrieve semantic information is evaluated for soundness and compared to state-of-the-art symbolic methods available, such as LSA (latent semantic analysis) of T. K. Landauer et al. Their ability is poor when performed without a proper compaction procedure. However, when the corpus is summarized through a selection protocol based on PCR or a training procedure on J. Chen et al. (2004) to extract useful information, their performance is much closer to that of LSA, according to human expert ratings. These results are expected to improve and scale up when actual DNA molecules are employed in real test tubes, currently a feasible goal.;2004;A. Neel;10.1109/CEC.2004.1331126;Conferences;;0-7803-8515-2
ieee_20221205_08_24_39;High quality isosurface generation from volumetric data and its application to visualization of medical CT data;We propose a method for generating an isosurface from volumetric data sampled with a face-centered cubic lattice. The display quality of the isosurface obtained by our method is greatly enhanced because it generates many good aspect ratio triangle patches. We applied the method to visualization of a colonic wall from medical data. We experimentally compared the resulting surface of our method with those of existing methods, showing the effectiveness of our method.;2004;T. Takahashi;10.1109/ICPR.2004.1334633;Conferences;1051-4651;0-7695-2128-2
ieee_20221205_08_24_39;Data compression technique in recording electric arc furnace voltage and current waveforms for tracking power quality;A data compression technique is used to enhance the storage capability of electric power quality instruments in recording electric arc furnace voltage and currents waveforms, where voltage flicker and harmonic current distortion are major power quality disturbances. The compression approaches by multi-resolution analysis with threshold coding and vector quantization coding are compared. The digital filters are designed by the discrete wavelet transform. The results from spectrum analysis show high compression ratios while keeping low information loss. From the calculation results, the reconstructed voltage waveforms using threshold coding almost keep the same voltage flicker values. The vector quantization coding is better for current waveforms to keep harmonic distortion values. From the simulation results of the measurement data, the memory requirement is reduced to 20.4% while power quality characteristics are almost kept.;2003;Chi-Jui Wu;10.1109/TDC.2003.1335253;Conferences;;0-7803-8110-6
ieee_20221205_08_24_39;Research on the application of data-mining for quality analysis in petroleum refining industry;High product quality is the main target of the petroleum refining industry. It is widely admitted that there are some limitations of traditional product-quality-monitoring methods. Data-mining (DM) is a method to get useful information, which other regular methods cannot find, from enormous data. Data warehouse (DW) is the best way to store and manage massive enterprise data and provide a strong support to data analysis methods. This paper presents a new framework to deal with quality analysis, which combines the soft sensor, DM and DW. It promises to overcome the limitations of soft sensor and apply soft sensor in quality analysis in the petroleum refining industry.;2004;Jiang Chen;10.1109/WCICA.2004.1342326;Conferences;;0-7803-8273-0
ieee_20221205_08_24_39;Water quality classification of lakes using 250-m MODIS data;The traditional method used in the water quality classification of Finnish lakes includes the collection of water samples from lakes and their analysis in laboratory conditions. The classification is based on statistical analysis of water quality parameter values and on expert opinion. It is possible to acquire similar information by using radiance values measured with the Earth Observing System Terra/Aqua Moderate Resolution Imaging Spectroradiometer (MODIS). In this letter, the classification accuracy with MODIS data is about 80%. Only about 0.2% of the 20 391 pixels were misclassified by two or more classes, as a four-class classification system is used.;2004;S. Koponen;10.1109/LGRS.2004.836786;Journals;1558-0571;
ieee_20221205_08_24_39;Data processing development in the field of power quality monitoring;The presented paper deals with the problems of electrical power quality estimation, especially in multi-disturbances conditions. The synergy effect of different kinds of supply voltage disturbances have been pointed out and it have been described in relation to additional temperature rise of induction motor windings. The practical consequences for power quality analysis have been singled out. Proposed solution has been laid and commented on.;2004;P. Gnacinski;10.1109/IMTC.2004.1351394;Conferences;1091-5281;0-7803-8248-X
ieee_20221205_08_24_39;The CMS electromagnetic calorimeter: results on crystal measurements, quality control and data management in the Rome Regional Center;The barrel of the CMS electromagnetic calorimeter is currently under construction and will contain 61200 PbWO/sub 4/ crystals. Half of them are being fully characterized for dimensions, optical properties and light yield in the INFN-ENEA Regional Center near Rome. We describe the setup of an automatic quality control system for the crystal measurements and the present results on their qualification, as well as the REDACLE project, which has been developed to control and ease the production process. As it will not be possible to precalibrate the whole calorimeter, the crystal measurements and quality checks performed of the Regional Center will be crucial to provide a basis for fast in-situ calibration with particles. REDACLE is at the same time a fast database and a data managements system, where the database and the workflow structures are decoupled, in order to obtain the best flexibility.;2004;S. Costantini;10.1109/IMTC.2004.1351443;Conferences;1091-5281;0-7803-8248-X
ieee_20221205_08_24_39;Data quality and grounding considerations in a mixed-use facility;Most industrial, commercial, and educational facilities depend heavily on telecommunications, data, and computer networks. In many petrochemical installations, the functions of all three types of facilities are combined into one location. These facilities often experience problems with signal quality and noise on communication lines because of the proximity of high-speed, low-level signals to higher power lines. Additionally, electrical safety and various code issues must be addressed. Topics such as multiple ground systems, connections to ground, potential difference, circulating currents, and wiring methods are discussed. Eleven different systems must be bonded together. Three areas specific to networks require special attention. These are equipment bonding, isolated earth ground, and isolated power systems. Finally, cathodic protection systems have a unique impact on grounding systems.;2004;M.O. Durham;10.1109/PCICON.2004.1352768;Conferences;0090-3507;0-7803-8698-1
ieee_20221205_08_24_39;A study on the method to reduce data transfer in multi-quality VOD service;In the VOD service, the low resolution video data is expected for content searching, and the high resolution video data is expected for browsing. We have presented the effect of spatial video scalability encoding in cache utilization. In this study, we report comprehensive experiments on cases of various updating procedures, and the effects of differential data transferring. As a result, if a time-cycle of downloading, transcoding and browsing is too short for all clients to download contents within, the scalability encoding method can improve the quality of service, in both of data size and consumed time.;2004;S. Suzuki;10.1109/MWSCAS.2004.1353979;Conferences;;0-7803-8346-X
ieee_20221205_08_24_39;The necessity of assuring quality in software measurement data;Software measurement data is often used to model software quality classification models. Related literature has focussed on developing new classification techniques and schemes with the aim of improving classification accuracy. However, the quality of software measurement data used to build such classification models plays a critical role in their accuracy and usefulness. We present empirical case studies, which demonstrate that despite using a very large number of diverse classification techniques for building software quality classification models, the classification accuracy does not show a dramatic improvement. For example, a simple lines-of-code based classification performs comparatively to some other more advanced classification techniques such as neural networks, decision trees, and case-based reasoning. Case studies of the NASA JM1 and KC2 software measurement datasets (obtained through the NASA Metrics Data Program) are presented. Some possible reasons that affect the quality of a software measurement dataset include presence of data noise, errors due to improper software data collection, exclusion of software metrics that are better representative software quality indicators, and improper recording of software fault data. This study shows, through an empirical study, that instead of searching for a classification technique that perform well for given software measurement dataset, the software quality and development teams should focus on improving the quality of the software measurement dataset.;2004;T.M. Khoshgoftaar;10.1109/METRIC.2004.1357896;Conferences;1530-1435;0-7695-2129-0
ieee_20221205_08_24_39;On data aggregation quality and energy efficiency of wireless sensor network protocols - extended summary;"In-network data gathering and data fusion are essential for the efficient operation of wireless sensor networks. While most existing data gathering routing protocols addressed the issue of energy efficiency, few of them, however, have considered the quality of the implied data aggregation process. In this work, an information model for sensed data is first formulated. A new metric for evaluating data aggregation process, data aggregation quality (DAQ), is formally derived. DAQ does not assume any prior knowledge on values or on statistical distributions of sensing data, and may be applied to most data gathering protocols. Next, two new protocols are proposed: the enhanced LEACH and the clustered PEGASIS, enhanced from two major existing protocols: the cluster-based LEACH and the chain-based PEGASIS. By carefully accounting for listening energy, energy efficiency of all four protocols is evaluated. In addition, DAQ is applied to evaluate their data aggregation process. It is found that, while chain-based protocols are more energy efficient than cluster-based protocols, they however suffer from poor data aggregation quality. DAQ may be readily applied to most of continuous data gathering protocols; it is therefore significant to future development of sensor network protocols.";2004;T. Pham;10.1109/BROADNETS.2004.51;Conferences;;0-7695-2221-1
ieee_20221205_08_24_39;An integrated system for estimating crop quality based on remotely sensed imagery and GIS data;A method for estimating grain quality of winter wheat using Landsat Thematic Mapper data and GIS data is presented, and it gave the development and application of the Geographical Information Application system for estimating wheat grain quality. The system, supported by COMGIS, JModel-Base Management System and other advanced Spatial Information Technologies, can deal with the fusion analysis of remotely sensed data, GIS data and other multisources data, flexible management of models and knowledge;2004;Yuchun Pan;10.1109/IGARSS.2004.1368635;Conferences;;0-7803-8742-2
ieee_20221205_08_24_39;Water quality monitoring in Taihu Lake using MODIS image data;Remote sensing technique has been widely applied in water quality monitoring, since it can provide both spatial and temporal information needed to detect the changes of water quality. However, inland water monitoring using remote sensing technique is still experimental, and its development depends on improved remote sensors with higher spectral and spatial resolution. The purpose of this paper is to apply MODIS image data to inland lake water quality monitoring, and then to provide a MODIS-based procedure for regional inland water quality monitoring. After the correlation analysis between band combinations and water parameters such as chlorophyll-a and suspended sediment, we proposed an empirical algorithm based on certain MODIS bands. The study showed that MODIS image data were useful for water quality monitoring in Taihu Lake.;2004;Lingya Zhu;10.1109/IGARSS.2004.1369749;Conferences;;0-7803-8742-2
ieee_20221205_08_24_39;Studies on methods for quality assessment of crop spectral data;In the process of measurement, a number of factors will affect the quality of data. Therefore, data must be verified and assessed before their applications. This work discussed some methods for quality assessment of crop spectral data, which include methods of analysis of spectral characteristics, statistical test and spectral simulation. The method of spectral analysis compares measured spectra with the reference spectrum, and analyzes the location of wave crest, wave trough and the shape, intensity of spectral reflectance curves. The method of statistical test consists of shape similarity test and intensity test. The shape similarity test analyzes the correlation between measured spectral data and the reference spectral datum over the special wavelength range and assesses quality of the measured data by the correlation coefficients. The intensity test calculates the mean value and standard deviation of spectral data, and forms a spectral zone around the mean value. We consider it as the abnormal one if one spectral curve goes beyond the spectrum zone. The method of spectral simulation mainly compares measured spectra with simulated spectra by combined PROSPECT-SAIL model. Results show these methods of quality assessment are feasible.;2004;Xuehong Zhang;10.1109/IGARSS.2004.1369949;Conferences;;0-7803-8742-2
ieee_20221205_08_24_39;Study on data models of image quality assessment for the Chinese-Brazil Earth Resources Satellite;Analysis on Image Characteristic is a key step to understand and make good use of remote sensing image. Data models were employed to analyze for image quality of the Chinese-Brazil Earth Resources Satellite (CBERS-2) and LANDSAT/TM in Li-jiang region, Yunnan province. They include normalized model, univariate image statistics model, the ratio of inner and mutual variance model and spatial resolution model. The normalized model can be used to convert data into same physical quantity in order to make data be comparability. The other models can be used to open out the classified capability and information of data in supervised classification. With the study on data model, a method to analyze and assess remote sensing data was tried to bring forward;2004;Huang Miao-Fen;10.1109/IGARSS.2004.1369991;Conferences;;0-7803-8742-2
ieee_20221205_08_24_39;Road detection statistics for automated quality control of GIS data;We examine the use of road detection in VHR satellite images to automate the process of quality assessment of digital road network data. An important aspect is the emphasis on accuracy and reliability of the system. Although road detection has been studied for more than a decade, it is often difficult to assess what performance can be expected over datasets other than the images published. We propose a system to train a road detector based on image examples of typical roads. The system calculates the optimal detection parameters and estimates the performance over the dataset in terms of detection rate and degree of fragmentation. The methodology relies on error propagation and image statistics, and is generic in nature. By showing image examples, variations due to shadow, activity on the road, weather conditions etc. can be taken into account when estimating the expected performance.;2004;W. Goernan;10.1109/IGARSS.2004.1370280;Conferences;;0-7803-8742-2
ieee_20221205_08_24_39;A near optimal approach to quality of service data replication scheduling;This paper describes an approach to real-time decision-making for quality of service based scheduling of distributed asynchronous data replication. The proposed approach addresses uncertainty and variability in the quantity of data to replicate over low bandwidth fixed communication links. A dynamic stochastic knapsack is used to model the acceptance policy with dynamic programming optimization employed to perform offline optimization. The obtained optimal values of the input variables are used to build and train a multilayer neural network. The obtained neural network weights and configuration can be used to perform near optimal accept/reject decisions in real-time. Offline processing is used to establish the initial acceptance policy and to verify that the system continues to perform near-optimally. The proposed approach is implemented via simulation enabling the evaluation of a variety of scenarios and refinement of the scheduling portion of the model. The preliminary results are very promising.;2004;K. Adams;10.1109/WSC.2004.1371539;Conferences;;0-7803-8786-4
ieee_20221205_08_24_39;Analysis of data compression methods for power quality events;The use of digital technology for data acquisition and process of electric power quality events results in the growth the increase of available data , which need to be transferred and stored to be post, processed. Therefore much research has recently been carried out in the area of data compression in order to reduce the size of the files that contain the stored data and reduce the communication time. In this paper different data compression methods are reviewed and their application to the power quality area is studied.;2004;F. Lorio;10.1109/PES.2004.1372851;Conferences;;0-7803-8465-2
ieee_20221205_08_24_39;Robust quality management for differentiated imprecise data services;Several applications, such as Web services and e-commerce, are operating in open environments where the workload characteristics, such as the load applied on the system and the worst-case execution times, are inaccurate or even not known in advance. This implies that transactions submitted to a real-time database cannot be subject to exact schedulability analysis given the lack of a priori knowledge of the workload. In this paper we propose an approach, based on feedback control, for managing the quality of service of real-time databases that provide imprecise and differentiated services, given inaccurate workload characteristics. For each service class, the database operator specifies the quality of service requirements by explicitly declaring the precision requirements of the data and the results of the transactions. The performance evaluation shows that our approach provides reliable quality of service even in the face of varying load and inaccurate execution time estimates.;2004;M. Amirijoo;10.1109/REAL.2004.49;Conferences;1052-8725;0-7695-2247-5
ieee_20221205_08_24_39;Statistical quality of service guarantee for temporal consistency of real-time data objects;In this paper, we study the problem of temporal consistency maintenance where a certain degree of temporal inconsistency is tolerable. We propose a suite of statistical more-less (SML) approaches to tradeoff of quality of service (QoS) of temporal consistency against the number of supported transactions. We begin with a base-line algorithm, SML-BA, which provides the requested QoS of temporal consistency. We then propose SML with optimization (SML-OPT) to further improve the QoS by better utilizing the excessive CPU capacity. Finally, we enhance SML-OPT with a slack reclaiming scheme (SML-SR). The reclaimed slacks are used to process jobs whose required computation time is larger than the guaranteed computation time. Simulation experiments are conducted to compare the performance of these schemes (SML-BA, SML-OPT and SML-SR) together with the deterministic more-less and half-half schemes. Our results show that the SML schemes are effective in trading off the schedulability of transactions and the QoS guaranteed. Moreover, SML-SR performs best and offers a significant QoS improvement over SML-BA and SML-OPT.;2004;Kam-Yiu Lam;10.1109/REAL.2004.52;Conferences;1052-8725;0-7695-2247-5
ieee_20221205_08_24_39;A Web Services Application for the Data Quality Management in the B2B Networked Environment;Characteristics of Web services such as interoperability and platform independence make Web service a promising technique to manage data quality effectively in inter-organizational information exchanges. In this paper, we describe an application of Web services for managing data quality in the B2B information exchange that is typically characterized by large volumes of information, widely distributed data sources, and frequent information interchanges. In such environments, it is important that organizations are able to evaluate the quality of information they get from other organizations. We propose a framework for managing data quality in inter-organizational settings using the information product approach. We highlight the requirements for data quality management and the developing Web service standards to show why Web services offer a unique, yet simple platform for managing data quality in inter-organizational settings.;2005;G. Shankaranarayanan;10.1109/HICSS.2005.62;Conferences;1530-1605;0-7695-2268-8
ieee_20221205_08_24_39;Why CRM Efforts Fail? A Study of the Impact of Data Quality and Data Integration;This paper reports the results of a study into the implementation of data-driven customer relationship management (CRM) strategies. Despite its popularity, there is still a significant failure rate of CRM projects. A combination of survey and interviews/case studies research approach was used. It is found that CRM implementers are not investing enough efforts in improving data quality and data integration processes to support their CRM applications.;2005;F. Missi;10.1109/HICSS.2005.695;Conferences;1530-1605;0-7695-2268-8
ieee_20221205_08_24_39;Efficient channel quality feedback schemes for adaptive modulation and coding of packet data;Adaptive modulation and coding (AMC) is a powerful technique to improve throughput especially of wireless packet-oriented channels. In general such link adaptation schemes rely on feedback from the receiver, which allows selecting the appropriate modulation and coding. In this paper we evaluate the performance of different channel quality feedback schemes and propose a general performance metric, that considers data throughput, energy efficiency, channel resource consumption, as well as control overhead. At the example of the high speed downlink packet access (HSDPA) channel for UMTS FDD, we show that channel quality feedback schemes that consider the burstiness of packet data are able to provide notably higher efficiency than the cyclic feedback currently standardized.;2004;M. Dottling;10.1109/VETECF.2004.1400221;Conferences;1090-3038;0-7803-8521-7
ieee_20221205_08_24_39;SIQuA: server-aware image quality adaptation for optimizing server latency and capacity in wireless image data services [mobile radio];This paper investigates the problem of providing guaranteed server-side latency while performing dynamic image adaptation. We introduce a server-aware image quality adaptation (SIQuA) technique, used to optimize the runtime image adaptation process so as to provide guaranteed server-side latency with a maximized quality of image service level. SIQuA utilizes dynamic server conditions (server traffic loads and server resource availability), and optimizes both the quality of image serviced and the server-side latency required for runtime image adaptation, simultaneously, through judicious tuning of image compression parameters. We demonstrate that SIQuA can significantly reduce the server-side latency with a minimal impact on the image quality, thereby enabling efficient wireless image services with greatly reduced overall service costs.;2004;Dong-Gi Lee;10.1109/VETECF.2004.1400529;Conferences;1090-3038;0-7803-8521-7
ieee_20221205_08_24_39;On improving voice quality degraded by packet loss in data networks;"In voice over data networks, packet loss can have a major impact on perceived voice quality. The impact of packet loss on perceived voice quality depends on several factors, including loss pattern, codec type and packet size. Many techniques have been developed to recover lost packets. A newly developed recovery technique, ""switched recovery technique"" (SRT), employs a suitable recovery technique from known techniques to estimate the lost packet content (silence/voiced/unvoiced), depending on the preceding and following packets that were received correctly. The proposed technique was tested with different codecs, such as LD-CELP, LPC-10, and variable rate coders. Two loss models, random model and burst model, have been used to simulate the packet loss";2004;M.E. Nasr;10.1109/AFRICON.2004.1406632;Conferences;;0-7803-8605-1
ieee_20221205_08_24_39;Transduction and typicalness for quality assessment of individual classifications in machine learning and data mining;In the past, machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper, we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier. We perform a series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary TCM-NN method as well as with kernel density estimation. We show that the proposed method significantly outperforms density estimation methods, and how it may be used to improve their performance.;2004;M. Kukar;10.1109/ICDM.2004.10089;Conferences;;0-7695-2142-8
ieee_20221205_08_24_39;Dynamic end-to-end image adaptation for guaranteed quality of service in wireless image data services;The paper investigates total end-to-end service variability issues in providing dynamic image adaptation services to wireless end users. While most existing image adaptation techniques focus on improving wireless transmission latency by adapting content-rich images to the time varying bandwidth availability of wireless networks, we have found dynamic service load variations at the server also significantly affect end-to-end service latency. We introduce a dynamic end-to-end image adaptation technique that optimizes the runtime image adaptation process so as to provide guaranteed total service latency (including both server processing latency and wireless transmission latency) with a maximized image quality. Through judicious tuning of application-layer image compression parameters, the proposed technique dynamically optimizes both the image quality and each component of service latency required for wireless image data services simultaneously depending on service variability issues existing at both server-side and network-side. Experimental results under synthetic service workloads demonstrate that our proposed dynamic end-to-end image adaptation can provide guaranteed service latency with a minimal impact on the image quality loss.;2005;Dong-Gi Lee;10.1109/WCNC.2005.1424909;Conferences;1558-2612;0-7803-8966-2
ieee_20221205_08_24_39;Application of data mining and artificial modeling for coagulant dosage of water treatment plants corresponding to water quality;Shortage of water is gradually accelerated because a high standard of living is required and water resources are rapidly run dry. Therefore, effective water treatment is necessary to retain the required quality and amount of water. The general treatment includes coagulation, flocculation, filtering, and disinfections. Coagulation, flocculation, and disinfections are major parts of the water treatment processes. In this paper, new automatic algorithm is proposed for coagulation that is one of the water treatment processes. The proposed method is showing how to determine the coagulant soil and amounts using data mining techniques.;2004;Hyeon Bae;10.1109/IECON.2004.1432134;Conferences;;0-7803-8730-9
ieee_20221205_08_24_39;Mosaicing of medical video-endoscopic images: data quality improvement and algorithm testing;This conlribution deals with the construction of a cartography of the internal surface of organs based on image sequences acquired during clinical video-endoscopic exams. This paper describes a metrological approach to adapt the images acquisition conditions and the pre-processing tools to the evaluation of our specifically developed registration and mosaicing algorithm. A 3D-micrometric positioning system and an endoscope's tip holding structure was used to acquire sequences of images following a specific protocol. Forty images of a test scene (bladder planar photography) were acquired along a determined displacement looped path. An algorithm for correcting non-linear radial distortion caused by the endoscope was implemented. This algorithm computes projective (camera) and polynomial (distortion) transformations. The optimization process regislers the corrected distorted pattern image with the non-distorted one. Mutual information was used as a measure of similarity and stochastic gradient descent method for optimization. The visual quality of the images was then improved by a shading correction and by removing the optical fibers pattern from the images (specific low-pass filtering). A low-pass filter was used for obtaining the background that is subtracted from every image. The registration algorithm based on mutual inrormation as similarity measure and stochastic gradient descent as optimization method was used to obtain the transformation parameters that were applied to the images for building a mosaic (cartography). Finally, these parameters were applied to a similar sequence of images of a dots pattern (in place of the photography) allowing us to compute significant error parameters for the quality of the mosaicing. The results of the various processes are presented and the evaluation of the performancc of the cartography is discussed.;2004;R. Miranda-Luna;10.1109/ICEEE.2004.1433942;Conferences;;0-7803-8531-4
ieee_20221205_08_24_39;Quality assurance experience data base;An outline is given of a method and a tool developed to ensure permanent software quality evaluation and to create an experience database for future quality assurance tasks. The authors mainly focus on the benefits of using such a tool and its evolution in an industrial environment for global quality assurance of multisite large-scale software projects.<>;1991;M. Defamie;10.1109/ISSRE.1991.145370;Conferences;;0-8186-2143-5
ieee_20221205_08_24_39;Tuning a paper mill SCADA system using quality control data;Various paper qualities are automatically monitored on-line in a paper mill. These qualities are indirectly measured and used by the SCADA System to keep the product in trim. The same qualities are also directly manually measured from paper taken from the finished reels. This data is processed by the quality control section. Both the data from the computer and the quality control data are stored and comparisons made. Although vast amounts of automatically and manually collected data are continuously available it is difficult to confirm that the two sets of data are compatible. The doubt about the validity of the existing data and the proposal to increase the machine speed by one third caused the company to investigate the automatically collected data. This paper describes that investigation and the conclusions drawn.<>;1990;P. Cheek;;Conferences;;0-86341-704-3
ieee_20221205_08_24_39;A study on data quality management maturity model;Previously many studies on data quality have been focused on the realization and evaluation of data value quality and data service quality. But those studies showed poor data value quality and poor data service quality caused by the poor data structure. Meanwhile, in this study we focused on meta data management, that is to say, data structure quality. Especially, data quality management maturity model is introduced as a way of maturity model. And then it is empirically proved to show the improvement of the data quality as the data management matures.;2005;Rye Kyung-seok;10.1109/ICACT.2005.245923;Conferences;;
ieee_20221205_08_24_39;Protection device monitoring using data from power quality monitoring devices;In this paper, a model based expert system method was developed for monitoring the operation of protection devices based on the data obtained from power quality monitoring devices placed at the substations. First, models of physical protection devices were developed, and then actual power quality monitoring data were fed to the models to get an estimate of protection device operation. Test results, based on the actual data, show that, the proposed method can identify which protection device has operated to clear an observed fault, especially when there are multiple protection devices on the system.;2005;M. Baran;10.1109/PES.2005.1489300;Conferences;1932-5517;0-7803-9157-8
ieee_20221205_08_24_39;On improving voice quality degraded by packet loss in data networks;"In voice over data networks, packet loss can have a major impact on perceived voice quality. The impact of packet loss on perceived voice quality depends on several factors, including loss pattern, codec type and packet size. Many techniques were developed to recover lost packets. In this paper a new recovery technique called ""switched recovery technique"", SRT, which is developed by employing the suitable recovery technique from known techniques to an estimate of the lost packet content (silence/voiced/unvoiced) depending on preceding and following packets that received correctly. The proposed technique was tested with different codecs such as LD-CELP, LPC-10, and variable rate coders. Two models of loss have been used in this study, random model and burst model to simulate the packet loss";2005;M.E. Nasr;10.1109/NRSC.2005.194032;Conferences;;977-503183-4
ieee_20221205_08_24_39;An experimental study of the effects of contextual data quality and task complexity on decision performance;The effects of information quality and the importance of information have been reported in the information systems (IS) literature. However, little has been learned about the impact of data quality (DQ) on decision performance. This study explores the effects of contextual DQ and task complexity on decision performance. To examine the effects of contextual DQ and task complexity, a laboratory experiment was conducted. Based on two levels of contextual DQ and two levels of task complexity, this study had a 2/spl times/2 factorial design. The dependent variables were problem-solving accuracy and time. The results demonstrated that the effects of contextual DQ on decision performance were significant. The findings suggest that decision makers can expect to improve their decision performance by enhancing contextual DQ. This research extends a body of research examining the effects of factors that can be tied to human decision-making performance.;2005;W. Jung;10.1109/IRI-05.2005.1506465;Conferences;;0-7803-9093-8
ieee_20221205_08_24_39;Automated quality control of tropical cyclone winds through data mining;The analysis of tropical cyclones (TC) depends heavily on the quality of the incoming data set. With the advances in technology, the sizes of these data sets also increase. There is a great demand for an efficient and effective unsupervised quality control tool. Towards such a demand, data mining algorithms like spatial clustering and specialized distance measures can be applied to perform this task. This paper reports our findings on the studies on utilizing a density-based clustering algorithm with three different distance measures on a series of TC data sets.;2005;N.C. Carrasco;10.1109/IRI-05.2005.1506478;Conferences;;0-7803-9093-8
ieee_20221205_08_24_39;Objective video quality metric based on data hiding;In this paper, a new no-reference (NR) objective metric based on data hiding is proposed. The metric has the advantage of being fast and not requiring knowledge of the original video contents. The proposed method uses a spread-spectrum embedding algorithm to embed a mark (binary image) into video frames. A t the receiver, the mark is extracted and a measure of its degradation is used to estimate the quality of the video. We used data gathered from psychophysical experiments to help in the design of the video quality assessment system. We evaluated the visibility and annoyance of the impairments caused by the embedding algorithm and estimated the 'best' mark strength for a particular video. The performance of the proposed metric is estimated by measuring its ability to predict the total squared error (TSE) of the host video and the mean observer score (MOS) obtained from naive subjects in a psychophysical experiment. Experimental results show that the proposed metric had a good performance and a good correlation with the MOS.;2005;M.C.Q. Farias;10.1109/TCE.2005.1510512;Journals;1558-4127;
ieee_20221205_08_24_39;Quality assessment using data hiding on perceptually important areas;In this paper, we present a no-reference video quality metric that blindly estimates the quality of a video. The proposed approach makes use of a data hiding technique to embed a fragile mark into perceptually important areas of the video frame. To estimate the importance of an area, we take into account three perceptual features that are known to attract visual attention: motion, contrast, and color. At the receiver, the mark is extracted from the perceptually important areas of the decoded video. Then, a quality measure of the video is obtained by computing the degradation of the extracted mark. Simulation results indicate that the proposed video quality metric outperforms standard peak signal to noise ratio (PSNR) in estimating the perceived quality of a video. Additionally, results from a subjective experiment show that the metric output values increase monotonically with the mean annoyance scores gathered from the human observers.;2005;M. Carli;10.1109/ICIP.2005.1530613;Conferences;2381-8549;0-7803-9134-9
ieee_20221205_08_24_39;Prefiltered Gaussian reconstruction for high-quality rendering of volumetric data sampled on a body-centered cubic grid;In this paper a novel high-quality reconstruction scheme is presented. Although our method is mainly proposed to reconstruct volumetric data sampled on an optimal body-centered cubic (BCC) grid, it can be easily adapted lo the conventional regular rectilinear grid as well. The reconstruction process is decomposed into two steps. The first step, which is considered to be a preprocessing, is a discrete Gaussian deconvolution performed only once in the frequency domain. Afterwards, the second step is a spatial-domain convolution with a truncated Gaussian kernel, which is used to interpolate arbitrary samples for ray casting. Since the preprocessing is actually a discrete prefiltering, we call our technique prefiltered Gaussian reconstruction (PGR). It is shown that the impulse response of PGR well approximates the ideal reconstruction kernel. Therefore the quality of PGR is much higher than that of previous reconstruction techniques proposed for optimally sampled data, which are based on linear and cubic box splines adapted to the BCC grid. Concerning the performance, PGR is slower than linear box-spline reconstruction but significantly faster than cubic box-spline reconstruction.;2005;B. Csebfalvi;10.1109/VISUAL.2005.1532810;Conferences;;0-7803-9462-3
ieee_20221205_08_24_39;Evaluating and improving integration quality for heterogeneous data sources using statistical analysis;This paper considers the problem of integrating heterogeneous semi-structured data sources with the purpose of estimating integration quality (IQ). Integration of such data sources leads to results with unpredictable trustworthiness and none of the existing methods is capable of accounting for the uncertainty which is accumulated over all of the integration steps and which affects integration quality. To compute the uncertainties we suggest using a well-established statistical method Latent Class Analysis (LCA). This method allows to analyze the influence of the latent factors associated with the real-world entities on the set of data. We show on examples how the proposed approach can be used for evaluating and improving IQ giving an important tool to the users concerned with the data's trustworthiness.;2005;E. Altareva;10.1109/IDEAS.2005.25;Conferences;1098-8068;0-7695-2404-4
ieee_20221205_08_24_39;An adaptive reverse link data rate control scheme based on channel quality for CDMA system;The mechanism of the reverse link (RL) data rate control in CDMA system is studied, and a new scheme is presented to enhance the efficiency, which is based not only on probabilistic model, but also on a variable signifying the competitive ability of the mobile station (MS). This variable is related to MS' reverse pilot transmit power, which can reflect the reverse channel quality in some sense. With this new mechanism, The MS enjoying better channel quality is easier in raising its data transmission rate while the MS undergoing worse channel quality is more likely to reduce its rate. Since MS' reverse pilot transmit power is a variable known by MS itself, no extra signaling overhead is required from BS. Through dynamic system level simulation for cdma2000 release D where H-ARQ is employed on RL, the results show that the system throughput on RL is improved by the proposed scheme with the rise over thermal (RoT) value kept on a stable level.;2005;Qin Jie;10.1109/WCNM.2005.1544243;Conferences;2161-9654;0-7803-9335-X
ieee_20221205_08_24_39;Quality assessment of gene expression data;With the escalating amount of gene expression data being produced by microarray technology, one of important issues in the analysis of expression data is quality assessment, in which we want to know whether the one chip is artifactually high or low intensity relative to the majority of the chip. We propose a graphical tool implemented in R for visualizing distributions of two gene chips. Moreover, a statistical test based on chi-square test is employed to quantify degrees of array comparability for pairwise comparisons on a large number of arrays.;2005;Chen-An Tsai;10.1109/EITC.2005.1544369;Conferences;;0-7803-9328-7
ieee_20221205_08_24_39;Development of an on-line data quality monitor for the relativistic heavy-ion experiment ALICE;The on-line data monitoring tool developed for the coming ALICE experiment at LHC, CERN is presented. This monitoring tool which is a part of the ALICE-DAQ software framework, written entirely in C++ language, uses standard Linux tools in conjunction with the data display and analysis package ROOT, developed at CERN. It allows checking the consistency and quality of the data and correct functioning of the various sub-detectors either at run time or during off line by playing back the recorded raw data. After discussing the functionality and performance of this package, the experience gained during the test beam periods is also summarized.;2005;O. Cobanoglu;10.1109/RTC.2005.1547409;Conferences;;0-7803-9183-7
ieee_20221205_08_24_39;An Optimization Technique for the Evaluation of Eddy Current Inspection Data to Determine Weld Quality;This paper describes the design, engineering, assembly, testing and implementation of an eddy current inspection system to determine weld quality for a laser welding process. This system performs an in situ circumferential weld depth measurement of nuclear weapons primary components during fabrication. The goal of the inspection is to provide an accurate and repeatable estimate of the weld quality while minimizing contact with the part. Eddy current testing is a non-destructive testing (NDT) method used for interrogating conductive materials for defects and metallurgical characteristics. The technique is commonly used in the construction, aerospace, automotive, chemical processing, and power industries to examine structural components for cracks and voids and to study material properties such as hardness, embrittlement, permeability, and conductivity. This paper presents an analysis technique for the evaluation of eddy current data for the determination of weld quality;2005;K.W. Hench;10.1109/ICSMC.2005.1571365;Conferences;1062-922X;0-7803-9298-1
ieee_20221205_08_24_39;Analyzing software quality with limited fault-proneness defect data;Assuring whether the desired software quality and reliability is met for a project is as important as delivering it within scheduled budget and time. This is especially vital for high-assurance software systems where software failures can have severe consequences. To achieve the desired software quality, practitioners utilize software quality models to identify high-risk program modules: e.g., software quality classification models are built using training data consisting of software measurements and fault-proneness data from previous development experiences similar to the project currently under-development. However, various practical issues can limit availability of fault-proneness data for all modules in the training data, leading to the data consisting of many modules with no fault-proneness data, i.e., unlabeled data. To address this problem, we propose a novel semi-supervised clustering scheme for software quality analysis with limited fault-proneness data. It is a constraint-based semi-supervised clustering scheme based on the k-means algorithm. The proposed approach is investigated with software measurement data of two NASA software projects, JM1 and KC2. Empirical results validate the promise of our semi-supervised clustering technique for software quality modeling and analysis in the presence of limited defect data. Additionally, the approach provides some valuable insight into the characteristics of certain program modules that remain unlabeled subsequent to our semi-supervised clustering analysis.;2005;N. Seliya;10.1109/HASE.2005.4;Conferences;1530-2059;0-7695-2377-3
ieee_20221205_08_24_39;Quality of information for data fusion in net centric publish and subscribe architectures;This paper examines data fusion and target tracking issues involved within net centric publish and subscribe architectures with respect to the quality of information (QOI) provided to the end user. These architectures are commonly selected in shared knowledge environments to enable access to all information by all users. DoD, DOJ, FAA, and other government organizations are pushing net centric concepts to move from stovepiped, closed architectures with limited data accessibility toward open architectures where dissemination of new data sources is intrinsically supported. Although these net centric initiatives (such as DoD's GIG) promise tremendous aggregate bandwidth, sensor-to-user bandwidth, end-user processing power and storage capacity, and other real world issues are expected to impact operations. For example, the soldier of the future may require moving target updates on a bandwidth-limited wireless PDA. This paper examines how the Air Force Research Laboratory's Joint Battlespace Infosphere (JBI), a publish-and-subscribe system in conjunction with the OGC's sensor Web enablement (SWE) initiative, can be used to update clients in a quality-of-information (QOI) paradigm rather than a quality-of-service (QOS) paradigm. We conclude that net centric architectures can support operational flows that improve the quality of information while reducing resource usage over traditional stovepipe systems. We also conclude that net centric tool implementations should specifically address QOI.;2005;M.E. Johnson;10.1109/ICIF.2005.1591976;Conferences;;0-7803-9286-8
ieee_20221205_08_24_39;Feature Selection for Classification with Proteomic Data of Mixed Quality;In this paper we assess experimentally the performance of two state-of-the-art feature selection methods, called RFE and RELIEF, when used for classifying pattern proteomic samples of mixed quality. The data are generated by spiking human sera to artificially create differentiable sample groups, and by handling samples at different storage temperature. We consider two type of classifiers: support vector machines (SVM) and k-nearest neighbour (kNN). Results of leave-one-out cross validation (LOOCV) experiments indicate that RELIEF selects more stable feature subsets than RFE over the runs, where the selected features are mainly spiked ones. However, RFE outperforms RELIEF in terms of (average LOOCV) accuracy, both when combined with SVM and kNN. Perfect LOOCV accuracy is obtained by RFE combined with 1NN. Almost all the samples that are wrongly classified by the algorithms have high storage temperature. The results of experiments on this data indicate that when samples of mixed quality are analyzed computationally, feature selection of only relevant (spiked) features does not necessarily correspond to highest accuracy of classification.;2005;E. Marchiori;10.1109/CIBCB.2005.1594944;Conferences;;0-7803-9387-2
ieee_20221205_08_24_39;Design and Implementation of a Real-Time Control Platform for the Testing of Advanced Control Systems and Data Quality Management in the Wastewater Industry;This paper reports on the design and implementation of a software platform for a wastewater treatment plant (WWTP) for advanced control testing and data quality assessment. The platform can be integrated within an existing control structure, namely the plant SCADA system, with the purpose of providing tools for the implementation and testing of sophisticated control systems and data quality analysis for plant wide operation. The platform is being implemented and tested at a Scottish utility’s wastewater treatment plant, where several control algorithms are under research.;2003;A. Sanchez;10.1109/ICCA.2003.1595058;Conferences;;0-7803-7777-X
ieee_20221205_08_24_39;Test of distributed data quality monitoring of CMS tracker;The complexity of the HEP detectors for LHC (CMS tracker has more than 50 millions of electronic channels) makes the task of monitoring of the detector performance and of the data quality on-line challenging because of the large amount of data and information to be processed. The availability of the grid computing environment opens new possibilities to perform this task, by profiting from the distributed resources in remote sites. As the computing resources may be scarce in the CMS control room it is proposed to send a sizeable amount of tracker raw data to a specialized tier-2 center through an high-bandwidth connection. This center have to be able to analyze the data as soon as they arrive, doing a quasi-online monitoring and sending back to the control room the result of this analysis. The feasibility test performed using a grid farm of INFN (tier-2) in Bari is described.;2005;M. Santa Mennea;10.1109/NSSMIC.2005.1596389;Conferences;1082-3654;0-7803-9221-3
ieee_20221205_08_24_39;Knowledge Discovery in Power Quality Data Using Support Vector Machine and S-Transform;In this paper, we investigate the potential of support vector machines (SVMs) for power quality data mining in electrical power systems. Modified wavelet transform, known as S-transform, has been used to extract unique features of the various power quality disturbances. Feature vectors from S-transform analysis are used to train the SVM classifier. Various multi-class SVM algorithms have been applied on the power quality data under study and the directed acyclic graph (DAGSVM) algorithm is found to be performing well. A comparison between the DAGSVM method and the one based on artificial neural network demonstrates the efficiency of the SVM method in classifying PQ disturbances;2006;K. Vivek;10.1109/ITNG.2006.86;Conferences;;0-7695-2497-4
ieee_20221205_08_24_39;Quality, data, and TR929;To improve the ability to meet customers' needs, it is necessary to improve the quality of the technology and processes used to provide services. To improve quality there is a need to understand current performance and have a way to track change. Data are needed to meet these needs and consistent and common data are needed to make procurement decisions. The Reliability and Quality Measurement System for Telecommunications (RQMS) TR-TSY-000929 provides criteria for standardization of the data and their delivery. One must, however, be sensitive to the need to monitor these requirements and change them as necessary.<>;1991;K.M. Walling;10.1109/ICC.1991.162300;Conferences;;0-7803-0006-8
ieee_20221205_08_24_39;Towards a Quality Model for Effective Data Selection in Collaboratories;Data-driven scientific applications utilize workflow frameworks to execute complex dataflows, resulting in derived data products of unknown quality. We discuss our on-going research on a quality model that provides users with an integrated estimate of the data quality that is tuned to their application needs and is available as a numerical quality score that enables uniform comparison of datasets, providing a way for the community to trust derived data.;2006;Y.L. Simmhan;10.1109/ICDEW.2006.150;Conferences;;0-7695-2571-7
ieee_20221205_08_24_39;Data quality and grounding;Proper application of grounding systems, signal isolation, electrical safety, and power quality are key to the successful installation and operation of any electrical and data system, whether in a building or a plant. This paper necessitates an effective data quality and grounding design that considers the neutral, power grounding, bonding, shielding, and transient protection. With consistent design, installation, and maintenance of the grounding system, noise disturbances can be controlled.;2006;M.O. Durham;10.1109/MIA.2006.1628848;Magazines;1558-0598;
ieee_20221205_08_24_39;GeoExpert A Framework for Data Quality in Spatial Databases;Usage of very large sets of historical spatial data in knowledge discovery process became a common trend and in order to obtain better results from this knowledge discovery process the data should be of high quality. We proposed a framework for data quality assessment and cleansing tool for spatial data that integrates the spatial data visualization and analysis capabilities of the ARCGIS Engine, the reason and inference capability of an expert system. In this paper, we explain the core architecture of the framework and also the functionality of each module in the framework. We will explain the implementation details of the framework.;2005;A. Tadakaluru;10.1109/CIMCA.2005.1631527;Conferences;;0-7695-2504-0
ieee_20221205_08_24_39;Short term forecast of the quality of water in New York coastal zone using multispectral satellite data;One of the possible and probably most effective ways of solving the problem of monitoring the quality of water in ocean coastal areas is to use satellite measurements (remote sensing). Analysis of temporal and spatial structure of the inhomogeneities in coastal waters is an important step for the development of a model of the impurity transport and pollution forecast, but direct contact sensors cannot cover the necessary scales. Therefore, it is very important that WLR (normalized water leaving radiance) measurements made by satellites SeaWiFS, MODIS, and occasionally ASTER be used and analyzed. One of the most informative hydro-optical parameters is known to be the color index defined as the ratio of the WLR values in two spectral ranges, i.e. I(/spl lambda//sub 1/, /spl lambda//sub 2/) = WLR(/spl lambda//sub 1/)/WLR(/spl lambda//sub 2/). When calculating the color index I(/spl lambda//sub 1/, /spl lambda//sub 2/), partial compensation of the multiplicative measurement errors is taking place. For this reason, the influence of such factors as the visual angle, illumination of the ocean surface, sky color, etc. is substantially weakened, which enhances the informativity of the optical data on the upper ocean layer. This property of the color index is the basic prerequisite of its use aimed at determining multiple parameters characterizing the quality of water. Preliminary color index analyses to determine water characteristics in the New York coastal zone have shown that this parameter is quite relevant in this respect. Main mesoscale disturbances of the color index are concentrated near the coastline. Their spectral analysis has revealed two basic scales of such disturbances. The first one corresponds to the dimensions of heterogeneities within 20-30 km, the second one to 9-11 km. The first type of disturbances is identified as mesoscale eddies since their dimension corresponds to 2/spl pi/R/sub b/ (R/sub b/ is the baroclinic deformation radius). The second type of color index disturbances, with dimensions 9-11 km, manifests itself as the structures that move along the Lond Island coast in the western direction. The coherence of such disturbances quickly decreases with the distance from the coastline. These properties of the disturbances are characteristic for the entrapped waves and, as shown by the analysis, their parameters correspond well enough to the dispersion curves of the barotropic shelf waves. The role of such disturbances in the exchange processes remains so far not clear, and it may be a subject for further research. Consecutive digital images of the color index in the New York coastal zone may be a source of information on the development and dynamics of the mesoscale eddies. On this basis, methods of forecast of water quality changes may be developed.;2005;R. Khanbilvardi;10.1109/OCEANS.2005.1639816;Conferences;0197-7385;0-933957-34-3
ieee_20221205_08_24_39;Designing a quality oceanographic data processing environment;Oceanographic data are increasing by data types and volume making present methods of processing and determining quality a cumbersome task. The National Oceanic and Atmospheric Administration's (NOAA) Center for Operational Oceanographic Products and Services (CO-OPS) is developing an end-to-end, state-of-the-art data management system to ingest, quality control, analyze, and disseminate water velocity and related data. The benefits include streamlining preliminary analysis, thus allowing time and resources for more in-depth investigations of the physical phenomena, increasing consistency of results between users, and improving overall data quality. The design of the system architecture follows a planned structured methodology improving the quality of the software developed. Designing a Web-based modular system will allow flexibility so the system can accommodate new analyses, reports and plots as well as allow for future data types. Well-defined algorithms will be implemented determining the quality of both the data and the analyses. This data management system will provide oceanographers the means to study water velocity data using a wide suite of mathematical and graphical tools. This will allow users to focus on the analysis results rather than the process.;2005;C. Paternostro;10.1109/OCEANS.2005.1640149;Conferences;0197-7385;0-933957-34-3
ieee_20221205_08_24_39;Using Data Mining Technology to improve Manufacturing Quality - A Case Study of LCD Driver IC Packaging Industry;In recent year, because of the professional teamwork, to improve the qualification percentage of products, to accelerate the acknowledgement of product defects and to find out the solution, the LCD driver IC packaging factories have to establish an analysis mode for quality problems of product for more effective and quicker acquisition of needed information and to improve the customer’s satisfaction for information system. The past information system used neural network to improve the yield rate of production. In this research employs the star schema of data warehousing as the base of line analysis, and uses decision tree in data mining to establish a quality analysis system for the defects found in the production processes of package factories in order to provide an interface for problem analysis, enabling quick judgment and control over the cause of problem to shorten the time solving the quality problem. The result of research shows that the use of decision tree algorithm reducing the numbers of defected inner leads and chips has been improved, and using decision tree algorithm is more suitable than using neural network in quality problem classification and analysis of the LCD driver IC packaging industry.;2006;Ruey-Shun Chen;10.1109/SNPD-SAWN.2006.75;Conferences;;0-7695-2611-X
ieee_20221205_08_24_39;Accuracy Control in Compressed Multidimensional Data Cubes for Quality of Answer-based OLAP Tools;An innovative technique supporting accuracy control in compressed multidimensional data cubes is presented in this paper. The proposed technique can be efficiently used in QoA-based OLAP tools, where OLAP users/applications and DW servers are allowed to mediate on the accuracy of (approximate) answers, similarly to what happens in QoS-based systems for the quality of services. The compressed data structure KLSA, which implements the technique, is also extensively presented and discussed. We complement our analytical contributions with an experimental evaluation on several kinds of synthetic multidimensional data cubes, demonstrating the superiority of our approach in comparison with other similar techniques;2006;A. Cuzzocrea;10.1109/SSDBM.2006.10;Conferences;1551-6393;0-7695-2590-3
ieee_20221205_08_24_39;Transformer thermal modeling: improving reliability using data quality control;Eventually, all large transformers will be dynamically loaded using models updated regularly from field-measured data. Models obtained from measured data give more accurate results than models based on transformer heat-run tests and can be easily generated using data already routinely monitored. The only significant challenge to use these models is to assess their reliability and improve their reliability as much as possible. In this work, we use data-quality control and data-set screening to show that model reliability can be increased by about 50% while decreasing model prediction error. These results are obtained for a linear model. We expect similar results for the nonlinear models currently being explored.;2006;D.J. Tylavsky;10.1109/TPWRD.2005.864039;Journals;1937-4208;
ieee_20221205_08_24_39;A novel data hiding scheme for keeping high stego-image quality;The LSB-based data hiding scheme is to embed the secret data into the least significant bits of the pixel values in a cover image. When the size of the embedded secret data is bigger, this processing will degrade the quality of the stego-image so significantly as to catch the attention of hostile interceptors. To overcome this drawback, in this paper, we propose a high-capacity, high-quality data hiding scheme. First, the proposed scheme divides the pixel value range into several non-overlapping regions. For each spatial pixel value in the cover image, we embed one secret-bit into it by using the difference expansion technique in single pass, where the difference is calculated between the spatial pixel and the start or end of the region it is situated in. Owing to the use of the difference expansion technique, this processing is also suitable for multiple-layer embedding. Experimental results show significant improvement with respect to the embedding capacity. In addition, the distortion in the stego-image is almost invariable regardless of the increase of the load of embedded data.;2006;Chin-Chen Chang;10.1109/MMMC.2006.1651324;Conferences;1550-5502;1-4244-0028-7
ieee_20221205_08_24_39;On Creating a New Format for Power Quality and Quantity Data Interchange;IEEE 1159, a recommended practice for monitoring power quality has resulted in a consistent set of terms, definitions and guidelines for monitoring power quality. IEEE 1159 does not address the need however, for a consistent mechanism for transporting the output of these instruments to the end user. Each vendor has their own communications, control, and analysis software that is not compatible with any other vendor. In addition to the difficulty with exchangeability of monitored data, there is a similar difficulty in exchanging and comparing the results of computer simulations. Simulation tool vendors also use a variety of file formats and conventions appropriate for their programs. Given that today's power quality engineers need to transport the output of different vendors monitors and simulation programs to various database and analysis programs (also from multiple vendors), a need exists for a standard data interchange format. This paper describes the principal issues that were addressed in the development of the IEEE 1159.3-2003 Recommended Practice for the Transfer of Power Quality Data (also known as the power quality data interchange format - PQDIF);2006;Gunther;10.1109/TDC.2006.1668517;Conferences;2160-8563;0-7803-9194-2
ieee_20221205_08_24_39;An approach to geographical data quality evaluation;We present an approach to the evaluation of the quality of cadastral data that caters for the differing levels of quality required of various parameters in order to meet different goals. The key parameters associated with particular quality classes and their acceptable range of values are obtained through interview with expert users. The approach described is currently being applied in an experimental evaluation of the quality of cadastral map of the Republic of Latvia.;2006;A. Jansone;10.1109/DBIS.2006.1678486;Conferences;;1-4244-0345-6
ieee_20221205_08_24_39;"Monitoring and Improving the Quality of ODC Data using the ""ODC Harmony Matrices"": A Case Study";Orthogonal Defect Classification (ODC) is an advanced software engineering technique to provide in-process feedback to developers and testers using defect data. ODC institutionalization in a large organization involves some challenging roadblocks such as the poor quality of the collected data leading to wrong analysis. In this paper, we have proposed a technique (‘Harmony Matrix’) to improve the data collection process. The ODC Harmony Matrix has useful applications. At the individual defect level, results can be used to raise alerts to practitioners at the point of data collection if a low probability combination is chosen. At the higher level, the ODC Harmony Matrix helps in monitoring the quality of the collected ODC data. The ODC Harmony Matrix complements other approaches to monitor and enhances the ODC data collection process and helps in successful ODC institutionalization, ultimately improving both the product and the process. The paper also describes precautions to take while using this approach.;2006;N. Saraiya;10.1109/SERA.2006.52;Conferences;;0-7695-2656-X
ieee_20221205_08_24_39;Test Data Selection and Quality Estimation Based on the Concept of Essential Branches for Path Testing;A new coverage measure is proposed for efficient and effective software testing. The conventional coverage measure for branch testing has such defects as overestimation of software quality and redundant test data selection because all branches are treated equally. These problems can be avoided by paying attention to only those branches essential for path testing. That is, if one branch is executed whenever another particular branch is executed, the former branch is nonessential for path testing. This is because a path covering the latter branch also covers the former branch. Branches other than such nonessential branches will be referred to as essential branches.;1987;T. Chusho;10.1109/TSE.1987.233196;Journals;2326-3881;
ieee_20221205_08_24_39;Feature Selection for Data Driven Prediction of Protein Model Quality;Features selection to assess the accuracy o f a protein three-dimensional model, when only the protein sequence is known, is a challenging task because it is not clear which features are most important and how they should best be combined. We present the results of an information theory-based approach to select an optimal subset of features for the prediction of protein model quality. The optimal subset of features was calculated by means of a backward selection procedure, starting from a set of structural features belonging to the following three categories: atomic interactions, solvent accessibility, and secondary structure. Three statistical-learning approaches were evaluated to predict the quality of a protein model starting from an optimum subset of features. The performances of a probabilistic classifier modeled by means of a Kernel Probability Density Estimation method (KPDE) were compared with those of a feed-forward Artificial Neural Network (ANN) and a Support Vector Machine (SVM).;2006;A. Montuori;10.1109/IJCNN.2006.247365;Conferences;2161-4407;0-7803-9490-9
ieee_20221205_08_24_39;Fuzzy Clustering of Open-Source Software Quality Data: A Case Study of Mozilla;We present a fuzzy cluster analysis of software quality data extracted from the Mozilla open-source Web browser. This is a new dataset that combines object-oriented software quality metrics with the number of defects per code unit. We undertake a fuzzy cluster analysis of this dataset, which for the first time addresses the use of both hyperspherical and hyperellipsoidal fuzzy clusters (using the Gath-Geva algorithm) in software quality analysis. Using a Pareto analysis based on the fuzzy clusters, we were able to identify groups of modules having higher defect densities than would be found by merely ranking modules based on any single software metric.;2006;S. Dick;10.1109/IJCNN.2006.246954;Conferences;2161-4407;0-7803-9490-9
ieee_20221205_08_24_39;Comparison of I-V, CV, and chemical data for quality control studies of SiO/sub x/N/sub y/ films on Si;Capacitance-voltage (CV) and current-voltage (I-V) measurements for SiO/sub x/N/sub y/ films are compared with chemical data in order to provide some diagnostic capabilities in relating aberrant electrical characteristics with contaminants incorporated in the insulator film structure. In-process monitoring of film quality (utilizing electrical characteristics and chemical data) is especially critical in very large-scale integration (VLSI) processing control where the films are utilized both as an integral part of specific semiconductor device processing steps or as part of the semiconductor device structure.<>;1988;M.W. Huck;10.1109/66.17986;Journals;1558-2345;
ieee_20221205_08_24_39;Using data quality measures in decision-making algorithms;Four decision methods are compared to determine appropriate ways of using data quality measures. Separate studies are directed toward defining and measuring tactical data quality, and calibrating the measures to decision problems. The decision methods compared include Dempster's rule, the linear and logarithmic opinion pools, a fuzzy-logic algorithm and the Mycin certainty factor calculus. It is concluded that none of the four decision algorithms are fully satisfactory. Of the four algorithms, the linear opinion pool is the most likely to succeed in practice because it is the simplest.<>;1992;R.A. Dillard;10.1109/64.180410;Magazines;2374-9407;
ieee_20221205_08_24_39;Reliability and quality data in court;The author presents the types of quality assurance data and information that are frequently used in the courtroom for litigation involving contract disagreements between buyer and seller (as well as personal injuries of the user or bystander versus all of the sellers and manufacturers) or arguments between the manufacturer of an item and a subsequent manufacturer who incorporates the item, over the damage caused by the inoperability or premature failure of the initial product. The author discusses the importance of these types of data, how they are to be recorded, where they are supposed to be recorded, the importance of configuration management (not only of military but of commercial products), cross referencing dates, the use of serial numbers, and the use of model designations with and without modification due to internal changes not affecting form, fit, or function.<>;1988;R.M. Jacobs;10.1109/ARMS.1988.196452;Conferences;;
ieee_20221205_08_24_39;Probabilistic model for quality of service modelling in packet-switched data networks;"A probabilistic model for quality of service available to users for communication via packet-switched public data networks is presented. The concepts of network reliability, availability and congestion are discussed. It is shown that network congestion appears as a loss of availability with regard to data communication services provided to network users, and therefore the quality of service depends not only on equipment reliability and maintenance activity, but also on the congestion probability. After defining the network equivalent availability, a probabilistic model for its evaluation is presented. New concepts are introduced for network equivalent availability evaluation; they consider network component reliability, their maintainability, and the network congestion together.<>";1992;I. Popescu;10.1109/LCN.1992.228128;Conferences;;0-8186-3095-7
ieee_20221205_08_24_39;Drifting buoy data quality and performance assessment at the National Data Buoy Center;A description is given of the first large-scale, centralized, drifting-buoy data quality program, initiated in April 1988 by the National Data Buoy Center (NDBC), an agency within the US National Weather Service (NWS). This program is leading to improved meteorological observations in data-sparse oceanic areas and hence can improve operational numerical analyses and prognoses. All North American drifting buoy reports enter the Global Telecommunications System (GTS) in Washington, DC. The Service Argos US Global Processing Center places the reports in DRIBU format and sends them to the NWS IBM 4341 computer system, where the data are subjected to gross range and time-continuity checks before dissemination of the GTS. More stringent checks are then performed at NDBC by data analysts in near-real time. The close monitoring of drifting buoy sensor data permits analysis of system reliability and performance, which can be used in general evaluation of hardware quality.<>;1988;E.A. Meindl;10.1109/OCEANS.1988.23626;Conferences;;
ieee_20221205_08_24_39;1.5 GHz band wave propagation characteristics and data transmission quality from train to ground;The propagation characteristics, data transmission quality, and service area of a quasi-microwave railway communication system are described. Measurements of propagation path loss taken along a typical railway route and in tunnels are presented.<>;1992;H. Hayashi;10.1109/VETEC.1992.245442;Conferences;1090-3038;0-7803-0673-2
ieee_20221205_08_24_39;Data quality for telecommunications;"The importance of data in large databases to the operation of telecommunications networks has grown considerably. For example, all provisioning, maintenance, and billing operations are critically dependent on data and many new network services are based on real-time access to data. This makes data quality a major issue for the industry. The purpose of the present paper is to outline an approach for addressing at least part of this issue. This approach makes use of process management to focus attention on processes that create data and data tracking, one method to quantify the performance of such processes. The merits of this approach are compared to more traditional methods of ""cleaning"" databases. A joint AT&T/LEC process, by which special services access is ordered and provisioned, is used as an example.<>";1994;T.C. Redman;10.1109/49.272881;Journals;1558-0008;
ieee_20221205_08_24_39;Probing power quality data;A power quality problem can best be described as any variation in the electric power service resulting in misoperation or failure of end-use equipment. As customers seek to increase utilisation and efficiency, utilities strive to better understand power quality events and their effects on these customers. Utilities are creating special programs and organisations to deal with customer power quality needs. A problem shared by both parties is the need for improved methods in the collection, analysis, and reporting of very large amounts of measured power quality data. This article presents one method currently being used to characterise power quality levels on distribution systems throughout the United States. The method utilises a number of software systems, one of which is the data management and analysis program described in the article.<>;1994;W.W. Dabbs;10.1109/67.273780;Magazines;1558-4151;
ieee_20221205_08_24_39;Ultrasound data acquisition system design for collecting high quality RF data from beef carcasses in the slaughterhouse environment;Ultrasound has considerable potential to accurately and precisely grade beef. The ultimate goal is an automated and objective grading process. The current phase of the work does not involve construction of an ultrasound grading device but rather the determination of which of these ultrasound-related and derived quantities best correlate to each grade. In order to make this determination, an ultrasound data acquisition system capable of acquiring large amounts of high-quality RF data from beef carcasses has been constructed. The engineering concepts and construction details of an ultrasound data acquisition device to be used as a research tool for this application are presented.<>;1992;I.A. Hein;10.1109/ULTSYM.1992.276019;Conferences;;0-7803-0562-0
ieee_20221205_08_24_39;A transformation for ordering multispectral data in terms of image quality with implications for noise removal;A transformation known as the maximum noise fraction (MNF) transformation, which always produces new components ordered by image quality, is presented. It can be shown that this transformation is equivalent to principal components transformations when the noise variance is the same in all bands and that it reduces to a multiple linear regression when noise is in one band only. Noise can be effectively removed from multispectral data by transforming to the MNF space, smoothing or rejecting the most noisy components, and then retransforming to the original space. In this way, more intense smoothing can be applied to the MNF components with high noise and low signal content than could be applied to each band of the original data. The MNF transformation requires knowledge of both the signal and noise covariance matrices. Except when the noise is in one band only, the noise covariance matrix needs to be estimated. One procedure for doing this is discussed and examples of cleaned images are presented.<>;1988;A.A. Green;10.1109/36.3001;Journals;1558-0644;
ieee_20221205_08_24_39;An exact solution of the analytic equation of image quality from optimum quantization of microwave imaging data;An exact solution is presented of the analytic equation of image quality in terms of the entropy of the quantized aperture data. Previously published work provides an optimum quantization scheme that obtains the best image quality for a given distribution of complex aperture data and a given number of bits into which each complex data sample is to be quantized. An extension is provided to determine the number of bits required of the quantized signal for the image to achieve a given quality or, conversely, what quality of image can be obtained for a given number of quantized levels. It is shown that the theory imposes substantially no constraints on the distributions of amplitude and phase in the microwave data. It is also shown that the optimum quantization design derived from the theory is nearly scene-independent and may achieve real-time performance.<>;1989;Z. Liang;10.1109/29.31299;Journals;0096-3518;
ieee_20221205_08_24_39;Software quality measurement based on fault-detection data;We develop a methodology to measure the quality levels of a number of releases of a software product in its evolution process. The proposed quality measurement plan is based on the faults detected in field operation of the software. We describe how fault discovery data can be analyzed and reported in a framework very similar to that of the QMP (quality measurement plan) proposed by B. Hoadley (1986). The proposed procedure is especially useful in situations where one has only very little data from the latest release. We present details of implementation of solutions to a class of models on the distribution of fault detection times. The conditions under which the families: exponential, Weibull, or Pareto distributions might be appropriate for fault detection times are discussed. In a variety of typical data sets that we investigated one of these families was found to provide a good fit for the data. The proposed methodology is illustrated with an example involving three releases of a software product, where the fault detection times are exponentially distributed. Another example for a situation where the exponential fit is not good enough is also considered.<>;1994;S. Weerahandi;10.1109/32.317425;Journals;2326-3881;
ieee_20221205_08_24_39;The RASCALS SYSTEM: a tool for ERS-1 data product quality analysis and sensor performance assessment;RASCALS (Radar Altimeter and SCAtterometer Long-term Surveillance) is a software system dedicated to the monitoring and analysis of sensor performance and data product quality for the ERS-1 wind scatterometer and radar altimeter. It is operated by the ERS-1 Product Control Service at ESRIN/Earthnet. The system can access raw data and fast delivery products as well as instrument housekeeping data generated by the AMI wind mode, the AMI wave mode, and the radar altimeter. The functionality covered by the RASCALS system ranges from the graphical visualisation of all parameters provided in the data products up to specialized analysis functions supporting the geophysical validation of measurements. RASCALS provides two modes of operation: In the analysis mode, the operator can interactively display, investigate and process the relevant data. In the routine mode, the system periodically samples the data products available from radar altimeter and wind scatterometer and extracts from the engineering and geophysical data a set of statistical parameters, thus building up a database characterizing the long-term performance. This paper describes the functionality and applications of RASCALS.<>;1993;H. Eichenherr;10.1109/IGARSS.1993.322074;Conferences;;0-7803-1240-6
ieee_20221205_08_24_39;Improved underway ADCP data quality through precise heading measurement;Acoustic Doppler current profilers provide valuable data on ocean currents from ships whilst underway, and they are a valuable tool when used in conjunction with high resolution hydrography. One of the main difficulties in using ADCP data to estimate mean cross-track currents (and hence transport) is the error in the conversion from ship to Earth coordinates brought about by errors in the ship's gyrocompass. Errors in the cross-track velocity of tens of cm s/sup -1/ may occur at moderate passage speeds. A relatively new tool - 3D GPS - provides precise heading measurements that can be used to correct the ADCP data. This paper uses 3D GPS, gyro, and ADCP data gathered during Cruise 198 of RRS Discovery to illustrate gyro errors dependent on latitude, speed and heading, and their effect on ADCP data quality.<>;1993;G. Griffiths;10.1109/OCEANS.1993.325993;Conferences;;0-7803-1385-2
ieee_20221205_08_24_39;Data quality requirements analysis and modeling;A set or premises, terms, and definitions for data quality management are established, and a step-by-step methodology for defining and documenting data quality parameters important to users is developed. These quality parameters are used to determine quality indicators about the data manufacturing process, such as data source creation time, and collection method, that are tagged to data items. Given such tags, and the ability to query over them, users can filter out data having undesirable characteristics. The methodology provides a concrete approach to data quality requirements collection and documentation. It demonstrates that data quality can be an integral part of the database design process. A perspective on the migration towards quality management of data in a database environment is given.<>;1993;R.Y. Wang;10.1109/ICDE.1993.344012;Conferences;;0-8186-3570-3
ieee_20221205_08_24_39;Routine data quality control in a data centre. The example of the TOGA/WOCE Subsurface Centre;"The TOGA/WOCE Subsurface Data Centre was designed to assemble all the available temperature and salinity profiles of the ocean's upper layers in the intertropical area from 1985 according to the ""Tropical Ocean Global Atmosphere"" international program, and from 1990 onwards in the global ocean according to the ""World Ocean Circulation Experiment"". In June 1994, the database was archiving about 252000 profiles. In order to assure a high level of quality and to be in accordance with international requirements, the received data are controlled at the centre which insures time consistency, integrity and elimination of redundancy. The Centre's software must join powerful X/motif interactive displays with a relational database system manager to insure a response time compatible with routine work on large data sets. At present, the centre receives at least 300 profiles per day and can check and disseminate them in a few days.<>";1994;G. Maudire;10.1109/OCEANS.1994.364074;Conferences;;0-7803-2056-5
ieee_20221205_08_24_39;Transmission quality of 10 Mbits/s Ethernet data frames using DPSK in wireless indoor environment;This paper presents the transmission quality of 10 Mbits/s Ethernet data frames in wireless indoor environment by direct modulation of the data frames onto a 1.95 GHz carrier using differential phase shift keying. Rectangular patch antennas with a 3 dB beamwidth of 74/spl deg/ are used in both the transmitter and receiver. Outage probabilities measured in a laboratory of 15.6/spl times/16.3/spl times/2.6 meters using three different transmitter antenna heights and locations are presented. In the first experiment where the antenna height is 1.3 meters, an outage probability of around 13% is reported. This improves to 2% by re-locating the transmitter antenna height and position. A comparison with measured RMS indoor delay spread characteristics at a similar frequency is made.<>;1994;C.L. Law;10.1109/ICC.1994.368819;Conferences;;0-7803-1825-0
ieee_20221205_08_24_39;Administrative databases and the quality of data in a myocardial infarction database;The St. George's post-infarction survey covers approximately 1,000 patients and is used for research into risk factors. Criteria are required for prompt entering of manually entered data. Using dumb terminal emulation, links to the hospital's patient administration system are used to improve data quality. New wiring and central computer staff are not required. Data editing by an events committee or during statistical analysis can exaggerate errors. Purchasing recommendations are made for database software and investigations equipment, which assist database management, make audit checks possible, and avoid manual transcription.<>;1993;J.D. Poloniecki;10.1109/CIC.1993.378375;Conferences;;0-8186-5470-8
ieee_20221205_08_24_39;Improving the quality of heart rate variability spectra by imposing a constraint on the amount of ectopy in each data segment;"Heart rate variability (HRV) is primarily due to instability oscillations of homeostatic feedback loops. Each loop tends to oscillate at a frequency characteristic of the time required to traverse that loop. Discarding all segments with ectopy is impractical in a clinical setting, since the number of ectopy-free segments is low even in patients with moderate ectopy. Investigators are extending spectra into lower frequency ranges, which requires longer data segments, and further reduces the number of ectopy-free segments. A method is proposed to determine the level of ectopy above which data segments should be discarded. By enforcing an effective ectopy cutoff, the mean distance among the spectra making up the average periodogram is reduced, increasing the confidence in these spectra. This technique allows for ""quality control"" of the data upon which HRV spectral analysis is based.<>";1993;J.J. Chungath;10.1109/CIC.1993.378381;Conferences;;0-8186-5470-8
ieee_20221205_08_24_39;Quality-of-service for video and data in very-high-speed DQDB MANs;Simulation is used to investigate the quality of service that a high-speed DQDB metropolitan area network can provide in a mixed-traffic scenario where connection-oriented video services coexist with connectionless data services. The basic DQDB access protocol as well as two of its options, are considered in a network with 1 Gbit/s data rate, and 5 km spall. The performance metrics used for the quality of service assessment are the throughput, the average and variance of the message delay. Results are presented considering the whole network, as well as individual nodes, so that protocol fairness issues can be discussed.<>;1995;M. Ajmone Marsan;10.1109/MASCOT.1995.378678;Conferences;;0-8186-6902-0
ieee_20221205_08_24_39;Data reduction for high quality digital audio storage and transmission;The paper reviews the current state of the art in the field of data reduction for high and medium quality audio applications, with particular emphasis on the methods standardised by the ISO-MPEG as ISO 11172-3. It also summarises the recent ISO-MPEG work which is aimed at defining further enhancements to the original standard, known as MPEG 2. It is acknowledged that the ISO-MPEG approach is not the only one in existence, there being a number of alternative commercial coding systems for high quality audio. These are mentioned briefly.<>;1994;F.J. Rumsey;10.1049/cp:19941125;Conferences;;0-85296-630-X
ieee_20221205_08_24_39;Quality and reliability impact of defect data analysis;In the last decade we have seen a shift towards a broader application of information on IC manufacturing defects. Here an overview is given of the methods used to gather data on the defects with a focus on local defects in the interconnection layers. Next this information is applied to determine a model describing the geometrical aspects of such defects. This model is used to arrive at a definition of hard faults and soft faults and to derive a relationship between the relative number of occurrence for either fault. Because the electrical impact of some of the soft faults will be closely related to the behavior of small open circuits or gate-oxide shorts, this relationship is an indication for the extent of the quality and reliability problems.<>;1995;E. Bruls;10.1109/66.382275;Journals;1558-2345;
ieee_20221205_08_24_39;Fuzzy sensor data fusion for quality monitoring in concrete mixing plant;One of the most important problems to achieve the required quality in concrete manufacturing is the estimation of moisture in aggregates and in concrete mix. The authors present a data fusion and fuzzy logic on line real-time application devoted to optimize the concrete quality. The application is based on smart treatment of existing information coming from plant sensors and on other operators experience. The results obtained at present time with only a process partial mapping shows that an estimation of the aggregate water content can be obtained with a reproducibility better than one order of magnitude more that given by the best actually used systems, for the same recipe and materials. This approach allows control of the most important parameter, the total water amount, to maintain the best quality of the product.<>;1993;A. Boscolo;10.1109/IMTC.1993.382559;Conferences;;0-7803-1229-5
ieee_20221205_08_24_39;Continuous process improvements and the use of quality control methodologies in the data item description process;Over the past few years, the DID process at Air Force Material Command previously Air Force Logistics Command prior to the June 1992 merger with Air Force Systems Command, at Wright-Patterson AFB, Dayton, Ohio has undergone significant process improvements in an effort to reduce lead time and promote quality in the development of DIDs. Statistical Process Control methods were used to evaluate the individual steps in the existing DID process. A number of process changes were made, and a series of training-oriented guides geared toward promoting quality were distributed. These process improvements proved to be quite successful. The lead time to develop a DID went from 938 to 300 h, a 68% reduction. Additional steps are planned to further improve the DID process.<>;1993;B.J. Stanley;10.1109/AUTEST.1993.396316;Conferences;;0-7803-0646-5
ieee_20221205_08_24_39;Image quality, statistical and textural properties of SAREX data from the Tapajos test site;"The image quality of the SAREX data set from the Tapajos test site is examined in the light of the available CCRS calibration data. Noise removal and its effects on the statistical properties of the data is addressed. Mathematical modeling is used to predict HH/VV differences, incidence angle effects and general trends; these are compared to the SAREX date. Model results are close to observations up to near grazing angles of incidence. Observed large HH/VV differences seen at near grazing angles may be due to surface scattering from the top of the forest canopy allied with Brewster angle effects. Textural discrimination of surface cover types is investigated; preliminary results are presented.<>";1994;K.D. Grover;10.1109/IGARSS.1994.399341;Conferences;;0-7803-1497-2
ieee_20221205_08_24_39;X-SAR data and image quality;The first SIR-C/X-SAR mission was flown in April 1994. Part of the acquired SAR data have been downlinked, analyzed and processed to precision images at the German Processing and Archiving Facility (PAF) at DLR. This paper summarizes the first analysis results of raw data and image quality. It is shown that already during the mission using preliminary orbit information the product quality requirements have been surpassed.<>;1994;R. Bamler;10.1109/IGARSS.1994.399376;Conferences;;0-7803-1497-2
ieee_20221205_08_24_39;Measuring Data Abstraction Quality in Multiresolution Visualizations;Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks;2006;Qingguang Cui;10.1109/TVCG.2006.161;Journals;2160-9306;
ieee_20221205_08_24_39;Software Quality Imputation in the Presence of Noisy Data;The detrimental effects of noise in a dependent variable on the accuracy of software quality imputation techniques were studied. The imputation techniques used in this work were Bayesian multiple imputation, mean imputation, instance-based learning, regression imputation, and the REPTree decision tree. These techniques were used to obtain software quality imputations for a large military command, control, and communications system dataset (CCCS). The underlying quality of data was a significant factor affecting the accuracy of the imputation techniques. Multiple imputation and regression imputation were top performers, while mean imputation was ineffective;2006;Taghi M. Khoshgoftaar;10.1109/IRI.2006.252462;Conferences;;0-7803-9788-6
ieee_20221205_08_24_39;Data acquisition equipment for measurement of the parameters for electric energy quality;The paper presents the equipment used to measure some parameters that characterize the electric energy quality. The proposed equipment performs test and acquisition of analogue data (U and I) and numerical data. The sampled data are recorded when preset thresholds are exceeded by the analogical inputs or when the digital inputs states change. The fixed variant is supplementary provided with 2 analogue outputs and 8 numerical outputs. The operation of equipment is simulated and the corresponding software are exemplified for the case of a highly distorting consumer, a set of electric energy quality parameters being determined for this case;2006;G. Vladut;10.1109/AQTR.2006.254612;Conferences;;1-4244-0361-8
ieee_20221205_08_24_39;Data Quality Management using Business Process Modeling;The quality of data contained in the enterprise information systems has significant impact, both from the internal business decision-making perspective and the external regulatory and shareholder obligations. This paper addresses data quality assessment in business processes by proposing a modeling framework to quantify the data quality in an information processing system. We present a business process modeling framework for data quality analysis and develop the mathematical formulation for error propagation. This is overlaid with a business controls framework where the placement and effectiveness of the controls alter the propagation of errors. This framework enables the estimation and management of data quality when faced with changes in various aspects of the business process. It also allows the formulation of optimization problems that trade off the cost of business controls with the level or cost of the resultant data quality. We illustrate the modeling framework and analyses with a revenue management process;2006;Sugato Bagchi;10.1109/SCC.2006.41;Conferences;;0-7695-2670-5
ieee_20221205_08_24_39;Estimating Software Quality with Advanced Data Mining Techniques;Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.;2006;Matej Mertik;10.1109/ICSEA.2006.261275;Conferences;;0-7695-2703-5
ieee_20221205_08_24_39;Application of a Statistical Methodology to Simplify Software Quality Metric Models Constructed Using Incomplete Data Samples;"During the construction of a software metric model, incomplete data often appear in the data sample used for the construction. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable ""retrospectively"" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, the authors have earlier derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a ""goodness-of fit"" criterion. Whilst the methodology has been applied successfully to software effort metric models, it is applied only recently to software quality metric models which usually suffer from far more serious incomplete data. This paper documents the latter application based on a successful case study";2006;Victor K.y. Chan;10.1109/QSIC.2006.13;Conferences;2332-662X;0-7695-2718-3
ieee_20221205_08_24_39;Exploratory Visualization of Multivariate Data with Variable Quality;Real-world data is known to be imperfect, suffering from various forms of defects such as sensor variability, estimation errors, uncertainty, human errors in data entry, and gaps in data gathering. Analysis conducted on variable quality data can lead to inaccurate or incorrect results. An effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content, but also its quality attributes. While some research has been conducted on visualizing uncertainty in spatio-temporal data and univariate data, little work has been reported on extending this capability into multivariate data visualization. In this paper we describe our approach to the problem of visually exploring multivariate data with variable quality. As a foundation, we propose a general approach to defining quality measures for tabular data, in which data may experience quality problems at three granularities: individual data values, complete records, and specific dimensions. We then present two approaches to visual mapping of quality information into display space. In particular, one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions. The other solution is to superimpose the quality information within the data visualizations using additional visual variables. We also report on user studies conducted to assess alternate mappings of quality attributes to visual variables for the second method. In addition, we describe case studies that expose some of the advantages and disadvantages of these two approaches;2006;Zaixian Xie;10.1109/VAST.2006.261424;Conferences;;1-4244-0592-0
ieee_20221205_08_24_39;A framework for analysis of data quality research;Organizational databases are pervaded with data of poor quality. However, there has not been an analysis of the data quality literature that provides an overall understanding of the state-of-art research in this area. Using an analogy between product manufacturing and data manufacturing, this paper develops a framework for analyzing data quality research, and uses it as the basis for organizing the data quality literature. This framework consists of seven elements: management responsibilities, operation and assurance costs, research and development, production, distribution, personnel management, and legal function. The analysis reveals that most research efforts focus on operation and assurance costs, research and development, and production of data products. Unexplored research topics and unresolved issues are identified and directions for future research provided.<>;1995;R.Y. Wang;10.1109/69.404034;Journals;2326-3865;
ieee_20221205_08_24_39;New York City Automated Air-Quality Data Collection Network;The problem of air pollution, particularly in metropolitan areas, is a function of widely distributed sources of contaminants and meteorological conditions which prevail over a large geographical area. To effectively monitor such a large area, and to provide timely, accurate information at a reasonable cost, an automated data network was deemed mandatory. The initial system for monitoring, telemetering, and assembling air-quality data in New York City is composed of ten diverse monitoring locations, each containing instrumentation for the measurement of CO, SO2, particulates, temperature, wind speed, and wind direction. The central, or control point of the network, is at Air Pollution Control Department offices in downtown Manhattan. Display equipment, output devices, controls for the network, and the automatic equipment, including a small-scale integral computer, form the network master station. The system, as now installed, can accommodate up to 30 remote telemetry stations with up to 12 instruments at each location. Plans for extension of the system are now under consideration and include mobile field laboratories using radio telemetry and additional fixed locations.;1970;Robert R. Ryder;10.1109/TGE.1970.271379;Journals;0018-9413;
ieee_20221205_08_24_39;A Web-Based System to Monitor the Quality of Meta-Data in Web Portals;We present a Web-based system to monitor the quality of the meta-data used to describe content in Web portals. The system implements meta-data analysis using statistical, visualization and data mining tools. The Web-based system enables the site's editor to detect and correct problems in the description of contents, thus improving the quality of the Web portal and the satisfaction of its users. We have developed this system and tested it on a Portuguese portal for management executives;2006;Marcos Aurelio Domingues;10.1109/WI-IATW.2006.24;Conferences;;0-7695-2749-3
ieee_20221205_08_24_39;Data-driven Soft Sensor Approach For Quality Prediction in a Refinery Process;In petrochemical industry, the product quality encapsulates the commercial and operational performance of a manufacturing process. Usually, the product quality is measured in the analytical laboratory and it involves resources and considerable time delay. On-line prediction of quality using frequent process measurements would be beneficial in terms of operation and quality control. In this article, a novel soft sensor technology based on partial least squares (PLS) regression between process variables and quality variable is developed and applied to a refinery process for quality prediction. The modeling process is described, with emphasis on data preprocessing, PLS regression, multi-outliers' detection and variables selection in regression. Enhancement of PLS is also discussed to take into account the dynamics in the process data. The proposed approach is applied to data collected from a refinery process and its feasibility and performance are justified by comparison with laboratory data.;2006;D. Wang;10.1109/INDIN.2006.275785;Conferences;2378-363X;0-7803-9700-2
ieee_20221205_08_24_39;Identification of Load Power Quality Characteristics using Data Mining;The rapid increase in computer technology and the availability of large scale power quality monitoring data should now motivate distribution network service providers to attempt to extract information that may otherwise remain hidden within the recorded data. Such information may be critical for identification and diagnoses of power quality disturbance problems, prediction of system abnormalities or failure, and alarming of critical system situations. Data mining tools are an obvious candidate for assisting in such analysis of large scale power quality monitoring data. This paper describes a method of applying unsupervised and supervised learning strategies of data mining in power quality data analysis. Firstly underlying classes in harmonic data from medium and low voltage (MV/LV) distribution systems were identified using clustering. Secondly the link analysis is used to merge the obtained clusters into supergroups. The characteristics of these super-groups are discovered using various algorithms for classification techniques. Finally the a priori algorithm of association rules is used to find the correlation between the harmonic currents and voltages at different sites (substation, residential, commercial and industrial) for the interconnected supergroups;2006;Ali Asheibi;10.1109/CCECE.2006.277720;Conferences;0840-7789;1-4244-0038-4
ieee_20221205_08_24_39;Application of Data Mining Technology Based On BP Neural Network in Yarn Qualities Forecast;In this paper, the sample database for data mining is designed by analyzing historical data of raw cotton properly and spinning yarn quality. Based on the above analytic result, we design the BP neural network model for spinning yarn qualities prediction. And the model was used to forecast and analyze the spinning strength, unevenness value and filoplume. Finally, simulation and test result show that the method has excellent effect.;2006;Wang wen;10.1109/CHICC.2006.280887;Conferences;2161-2927;7-81077-802-1
ieee_20221205_08_24_39;X-SAR radiometric calibration and data quality;In April and October, 1994 the X-SAR was flown as part of the SIR-C/X-SAR space radar laboratory missions (SRL-1/2) on the Space Shuttle. Amongst other activities DLR is responsible for the calibration of all X-SAR data products and is running the German Processing and Archiving Facility (D-PAF). Calibration activities included three major parts. Before the first mission, the authors performed a detailed analysis of the overall system to localize the main error sources and developed algorithms and procedures to correct these errors. During the missions they concentrated their efforts on calibration campaigns at the Oberpfaffenhofen super test site. Post mission activities included the determination of the antenna pattern and the absolute calibration factor as well as detailed performance analyses. This paper describes the overall approach to radiometrically calibrate the X-SAR and provides information on system performance and data quality to users in the different application fields.<>;1995;M. Zink;10.1109/36.406670;Journals;1558-0644;
ieee_20221205_08_24_39;Tracking Spike-Amplitude Changes to Improve the Quality of Multineuronal Data Analysis;During extracellular electrophysiological recording experiments, the waveform of neuronal spikes recorded from a single neuron often changes. These spike-waveform changes make single-neuron identification difficult, particularly when the activities of multiple neurons are simultaneously recorded with a multichannel microelectrode, such as a tetrode or a heptode. We have developed a tracking method of individual neurons despite their changing spike amplitudes. The method is based on a bottom-up hierarchical clustering algorithm that tracks each neuron's spike cluster during temporally overlapping clustering periods. We evaluated this method by comparing spike sorting with and without cluster tracking of an identical series of multineuronal spikes recorded from monkey area-TE neurons responding to a set of visual stimuli. According to Shannon's information theory, errors in spike-amplitude tracking reduce the expected value of the amount of information about a stimulus set that is transferred by the spike train of a cluster. In this study, cluster tracking significantly increased the expected value of the amount of information transferred by a spike train (p<0.01). Additionally, the stability of the stimulus preference and that of the cross-correlation between clusters improved significantly (p<0.000001). We conclude that cluster tracking improves the quality of multineuronal data analysis;2007;Hidekazu Kaneko;10.1109/TBME.2006.886934;Journals;1558-2531;
ieee_20221205_08_24_39;QTOP-K: A novel Algorithm for mining high quality pattern-based clusters in GST Microarray Data;Pattern-based clustering is widely applied in bioinformatics and biomedical Recently, mining high quality pattern-based clusters has become an important research direction. However, the existing methods were neither efficient in large data set nor precise at measuring the quality of clusters. These problems have greatly limited the methods' application in large data set. This paper proposes a new algorithm, which can provide a more accurate measurement for the quality of clusters and sharply cut down the time for mining high quality patterned-based clusters compared with today's methods. Experiments are held on real data set and synthetic data set and the test result suggests that Qtop-k has made notable progress in the aforementioned problems;2006;Shuhui Chen;10.1109/ICCIAS.2006.294252;Conferences;;1-4244-0605-6
ieee_20221205_08_24_39;Contribution to Quality of Life: Cross-National Validation of New Metrics for Mobile Data Service Technology in Korea and Japan;Every technology, especially in ubiquitous computing area, should be geared to improve the quality of their users' life ultimately. In order to achieve this goal, the important pre-requisite is to measure the contribution of the technology to quality of life reliably and validly. This study provides a theoretical as well as an empirical basis for the development of better measures for the contribution of a mobile data service technology to the quality of its users' lives. The reliability and validity of the proposed metrics were verified through online surveys in Korea and Japan. The survey results also indicated that critical mobile data services could be identified based on the degree of contribution to the quality of life of mobile data-service users. Moreover, the critical services were found to be different between Korea and Japan;2006;Jinwoo Kim;10.1109/PICMET.2006.296734;Conferences;2159-5100;1-890843-14-8
ieee_20221205_08_24_39;Compression of Power Quality Disturbance Data Based on Energy Threshold and Adaptive Arithmetic Encoding;Recently, power quality issues have captured more attention. It is necessary to monitor power quality in order to analyze and evaluate it. The monitors will record huge data during disturbance. It is inconvenient for data storage and transmission. Compression of power quality disturbance data will save storage space efficiently and accelerate transmission speed. This paper proposes energy threshold method based on wavelet transform, and then integrates adaptive arithmetic encoding to compress disturbance data. The compression ratio is improved and performance is better. Four typical power quality disturbances including voltage sag, swell, interruption and transient impulse are used to test the proposed method, the validity is verified by simulation results.;2005;Jidong Wang;10.1109/TENCON.2005.300848;Conferences;2159-3450;0-7803-9311-2
ieee_20221205_08_24_39;Learning the Quality of Sensor Data in Distributed Decision Fusion;The problem of decision fusion has been studied for distributed sensor systems in the past two decades. Various techniques have been developed for either binary or multiple hypotheses decision fusion. However, most of them do not address the challenges that come with the changing quality of sensor data. In this paper we investigate adaptive decision fusion rules for multiple hypotheses within the framework of Dempster-Shafer theory. We provide a novel learning algorithm for determining the quality of sensor data in the fusion process. In our approach each sensor actively learns the quality of information from different sensors and updates their reliabilities using the weighted majority technique. Several examples are provided to show the effectiveness of our approach;2006;Bin Yu;10.1109/ICIF.2006.301632;Conferences;;0-9721844-6-5
ieee_20221205_08_24_39;Quality Control of Minerals Management Service - Oil Company ADCP Data at NDBC: A Successful Partnership Implementation;"The Minerals Management Service (MMS) requires that deep water oil drilling and production platforms in the northern Gulf of Mexico collect and provide current profile data to the National Data Buoy Center (NDBC). NDBC processes and displays the resulting currents on the NDBC website. NDBC has recently implemented quality control algorithms agreed upon by industry and the government. The resulting imagery and data, including quality control flags, are available on the publicly available NDBC website. The quality control algorithms and flags are presented and comparisons of the resulting files are described. Oil companies must collect current profile data when drilling wells or operating production platforms in water greater than 400 meters deep. They are required to collect the data at 20 minute intervals and transmit the data via FTP to NDBC. The data are received, decoded, and quality controlled at NDBC. The current profiles are then formatted in TEmperature Salinity and Current (TESAC) messages and transmitted over the Global Telecommunications System (GTS). The data are also viewed over the NDBC website as columnar listings and current vector stick plots. In order to determine the quality control algorithms for the current profiles, a committee of oil company, industry, and government representatives determined an approach that includes both individual bin (depth level) and profile algorithms. The algorithms take advantage of the fact that the Teledyne RDI Acoustic Doppler Current Profiler (ADCP) collects error velocity, percent good statistics for 3 and 4 beams, and correlation matrices and echo amplitudes for each beam. The algorithms described in this presentation were then implemented and flags generated for each quality control test. A total of nine flags are assigned within the NDBC database. The flags indicate good data (3), suspect data (2), or bad (1) data. Only bad data are not reproduced or plotted on the NDBC real-time webpage. Results from the implementation are being reviewed, but a quick look indicates that the algorithms are returning accurate descriptions of the ADCP data. The stick plots of ocean current with depth are much ""cleaner"" following the quality control implementation. The implementation of the quality control algorithms was delayed by Hurricanes Katrina and Rita, which impacted both the NDBC and the oil industry in the Gulf of Mexico. NDBC is now resubmitting past data files through the quality control algorithms to insure that all data at NDCB have been quality controlled. The results of this effort (including the quality control algorithms) are being shared with Integrated Ocean Observing System (IOOS) partners in an effort to standardize quality control of oceanographic data";2006;Richard L. Crout;10.1109/OCEANS.2006.307072;Conferences;0197-7385;1-4244-0115-1
ieee_20221205_08_24_39;Quality-Aware Sampling and Its Applications in Incremental Data Mining;We explore in this paper a novel sampling algorithm, referred to as algorithm PAS (standing for proportion approximation sampling), to generate a high-quality online sample with the desired sample rate. The sampling quality refers to the consistency between the population proportion and the sample proportion of each categorical value in the database. Note that the state-of-the-art sampling algorithm to preserve the sampling quality has to examine the population proportion of each categorical value in a pilot sample a priori and is thus not applicable to incremental mining applications. To remedy this, algorithm PAS adaptively determines the inclusion probability of each incoming tuple in such a way that the sampling quality can be sequential/preserved while also guaranteeing the sample rate close to the user specified one. Importantly, PAS not only guarantees the proportion consistency of each categorical value but also excellently preserves the proportion consistency of multivariate statistics, which will be significantly beneficial to various data mining applications. For better execution efficiency, we further devise an algorithm, called algorithm EQAS (standing for efficient quality-aware sampling), which integrates PAS and random sampling to provide the flexibility of striking a compromise between the sampling quality and the sampling efficiency. As validated in experimental results on real and synthetic data, algorithm PAS can stably provide high-quality samples with corresponding computational overhead, whereas algorithm EQAS can flexibly generate samples with the desired balance between sampling quality and sampling efficiency;2007;Kun-ta Chuang;10.1109/TKDE.2007.1005;Journals;2326-3865;
ieee_20221205_08_24_39;Assessment of the Quality of Teaching and Learning Based on Data Driven Evaluation Methods;"The data-driven decision support tool built around the SAS technology has been developed to support the evaluation and monitoring of the quality of educational process. The tool forms an integrated framework that can be used for managing of teaching and learning processes and for performing comparative studies in the participating institutions. The tool includes: the engine for the comparative statistical analysis of the quality of teaching, the mechanism for dissemination of analytical results and the reporting facility. The main aim of our study is to extract and to compare the information on the quality of courses and teaching obtained from the different sets of databases at the Faculty of Electronic Engineering (FEE) of the Wroclaw, University of Technology (WUT), Poland; the Faculty of Computer Science (FCS) at University of Las Palmas de Grand Canaria (ULPGC), Spain; the Faculty of Engineering, Software, the Polytechnic University of Upper Austria in Hagenberg, Austria; the Electrical and Computer Science Department at the University of Arizona in Tucson, USA and Software Engineering Group at the Faculty of Engineering University of Technology, Sydney (UTS), Australia. In this paper we describe the process involved, the methodology, the tools for the analysis; and we present the results of our study.";2006;Zenon Chaczko;10.1109/ITHET.2006.339723;Conferences;;1-4244-0406-1
ieee_20221205_08_24_39;Data Quality and Query Cost in Wireless Sensor Networks;This research is motivated by emerging, real-world wireless sensor network applications for monitoring and control. We examine the benefits and costs of caching data for such applications. We propose and evaluate several approaches to querying for, and then caching data in a sensor field data server. We show that for some application requirements (i.e., when delay drives data quality), policies that emulate cache hits by computing and returning approximate values for sensor data yield a simultaneous quality improvement and cost savings. This win-win is because when system delay is sufficiently important, the benefit to both query cost and data quality achieved by using approximate values outweighs the negative impact on quality due to the approximation. In contrast, when data accuracy drives quality, a linear trade-off between query cost and data quality emerges. We also identify caching and lookup policies for which the sensor field query rate is bounded when servicing an arbitrary workload of user queries. This upper bound is achieved by having multiple user queries share the cost of a sensor field query. Finally, we demonstrate that our results are robust to the manner in which the environment being monitored changes using two different sensor field models;2007;David Yates;10.1109/PERCOMW.2007.35;Conferences;;0-7695-2788-4
ieee_20221205_08_24_39;Data Quality of 3836 Polarimetric Radar and Its Observation of Melting-Layer;The technical status of the upgraded 3836 C-band dual-linear polarimetric Doppler radar (3836 even radar) is reported first. Then this study analyses its data quality and its detection ability. A model is established in which the bright band can be identified by applying the fuzzy logic method to the four polarimetric radar observation variables. The radar data are used to assess the result from the model and the variety features of different phased hydrometeors in the melting-layer bright band are also discussed;2006;Jun Wu Cao;10.1109/ICR.2006.343182;Conferences;;0-7803-9583-2
ieee_20221205_08_24_39;Quality Metrics for Object-Based Data Mining Applications;A new quality measurement for video sequences utilized in video retrieval systems and visual data mining applications is proposed. First, each frame of the sequence undergoes a segmentation step using extracted texture features from the gray-level cooccurrence matrix (GLCM) (Davis and Johns, 1979). Next, corresponding objects between adjacent frames are matched thus resulting in a 3-dimensional segmentation of the video into objects. Finally, color and texture features are extracted for each object in the sequence and provide the primary input in computing the quality measurement pertaining to the video. A low quality measurement may thus eliminate the possibility of the sequence being stored in a database retrieval system. The algorithm is tested on various types of video segments - pans, zooms, close-ups, and multiple objects' motion - with results included;2007;Mark Smith;10.1109/ITNG.2007.163;Conferences;;0-7695-2776-0
ieee_20221205_08_24_39;P2A-5 Effect of Contrast Microbubble Concentration on Quality of Echo Particle Image Velocimetry (Echo PIV) Data: Initial In Vitro Studies;We have recently developed a novel contrast-based ultrasonic particle image velocimetry (Echo PIV) technique to measure multi-component velocity vectors in opaque flows. In the technique, ultrasound contrast microbubbles are used as flow tracers, and digitally acquired backscattered RF-data are used with custom velocimetry analysis to obtain velocity vectors. The technique has been shown as useful in obtaining 2D flow velocity from various flow patterns, including blood flow. Well control of timely sensitive microbubble concentration in Echo PIV is very important to produce robust data quality. In this paper, effect of microbubble concentration on Echo PIV data quality was examined and a quantitative tool based on feature of Echo PIV image cross-correlation was developed to evaluate real time robustness of Echo PIV data quality induced by bubble concentration and image quality;2006;H. Zheng;10.1109/ULTSYM.2006.396;Conferences;1051-0117;1-4244-0202-6
ieee_20221205_08_24_39;Importance of Data Quality in Virtual Metrology;The purpose of VM is to enable the manufacturers to conjecture the wafer quality and deduce the causes of defects without performing physical metrology. VM requires a large amount of sensor data retrieved from production tools. However, inappropriateness and instability of the data collection system, which leads to incorrectness, fragment and asynchrony of data collected, may lead to inaccurate conjecture results. Hence, not only precision of the VM conjecture module but also quality of the collected data are essential to ensure accurate and stable VM results for improving production yield. In this work, the importance of data quality to VM is investigated. The data quality mechanism is proposed for data characteristic analysis, data anomaly detection, data cleaning, data normalization, and data reduction. Besides, the equipment of semiconductor chemical vapor deposition (CVD) is adopted as a practical example to illustrate the significance of data quality mechanism, and further verify feasibility and effectiveness of VM;2006;Yi-Ting Huang;10.1109/IECON.2006.347318;Conferences;1553-572X;978-1-5090-9155-3
ieee_20221205_08_24_39;Resources Allocation Optimization for Scalable Multimedia Data Subject to Quality of Service Contraints;Scalable multimedia data transmission are subject to specific constraints such as the quality of service (QoS) of sensitivity classes and the transmission rate (yielding a maximum size of each frame to send). Many scalable source decoders are used to discarding data than processing an erroneous stream. This featuring class structure is helpful to define a strategy that determines the maximum number of classes to send and deliver the suitable protection and transmission scheme (coding rates and modulation) to apply in accordance with the transmission constraints. It leads to the possible truncation of frame parts transmitted with an unequal error protection (UEP) scheme for severe channel conditions. In a MPEG-4 speech frames context, we compare our approach to other methods using equal error and existing UEP schemes. It results in a significant improvement of the peak SNR (PSNR) quality in poor channel transmission conditions;2006;Heykel Houas;10.1109/SPAWC.2006.346429;Conferences;1948-3252;0-7803-9711-8
ieee_20221205_08_24_39;Analysis of Power Quality Waveform for Data Transmission Efficiency over IEC 61850 Communication Standard;In power utilities, grid operators monitor power-quality phenomena such as voltage sag at several substations to obtain an overview of the general network state and evolution of power quality. Collections of these huge online data from disturbance recorders involved long and tedious downloading process, usage of Internet protocols consumed bandwidth and causes traffic congestion and collisions with other users of the wide area network. This paper describes an integrated application of power quality monitoring system for a modeled substation over the IEC 61850 standard developed in Java programming language. The application used wavelet compression technique for transmission of voltage sag waveform over IEC 61850 standard and then Internet to a control centre. Analysis done shows that the correct selection of mother wavelet can achieve over 90% data compression ability while preserving all significant power quality features required for power quality waveform analysis required by utilities.;2006;Bahisham Yunus;10.1109/PECON.2006.346639;Conferences;;1-4244-0274-3
ieee_20221205_08_24_39;Workflow Quality of Service Management using Data Mining Techniques;Organizations have been aware of the importance of quality of service (QoS) for competitiveness for some time. It has been widely recognized that workflow systems are a suitable solution for managing the QoS of processes and workflows. The correct management of the QoS of workflows allows for organizations to increase customer satisfaction, reduce internal costs, and increase added value services. In this paper we show a novel method, composed of several phases, describing how organizations can apply data mining algorithms to predict the QoS for their running workflow instances. Our method has been validated using experimentation by applying different data mining algorithms to predict the QoS of workflow;2006;Jorge Cardoso;10.1109/IS.2006.348466;Conferences;1941-1294;1-4244-0195-X
ieee_20221205_08_24_39;LANDSAT-4 MSS And Thematic Mapper Data Quality And Information Content Analysis;Landsat-4 Thematic Mapper and Multispectral Scanner data were analyzed to obtain information on data quality and information content. Geometric evaluations were performed to test band-to-band registration accuracy. Thematic Mapper overall system resolution was evaluated using scene objects which demonstrated sharp high contrast edge responses. Radiometric evaluation included detector relative calibration, effects of resampling, and coherent noise effects. Information content evaluation was carried out using clustering, principal components, transformed divergence separability measure, and numerous supervised classifiers on data from Iowa and Illinois. A detailed spectral class analysis (multispectral classification) was carried out on data from the Des Moines, Iowa area to compare the information content of the MSS and TM for a large number of scene classes.;1984;Paul E. Anuta;10.1109/TGRS.1984.350595;Journals;1558-0644;
ieee_20221205_08_24_39;Improvement in Tile Quality of Vidio Signals by Spatio-Temporal Data-Dependent Filtering;The restoration of video signals corrupted by the additive noise is important for getting the high quality video and the high compression ratio of video signals. In this paper, we propose a novel restoration method for video signals corrupted by the Gaussian noise. Conventional methods use the spatio-temporal filtering after the motion compensation. However, the accuracy of the motion compensation on degraded image is not satisfied. Therefore, in the proposed method, we introduce a spatial filter as the pre-filter for motion compensation. Furthermore, the proposed method makes has four filter choices (i.e., spatio-temporal filter, spatial filter, temporal filter, identity filter) depending on the local information. Thus, the proposed data dependent filter can reduce the additive noise while preserving the signal edge/detail and motion;2006;Shunsuke Saitoh;10.1109/ISPACS.2006.364826;Conferences;;0-7803-9733-9
ieee_20221205_08_24_39;"Comments on ""Water Quality Retrievals From Combined Landsat TM Data and ERS-2 SAR Data in the Gulf of Finland";A paper by Zhang , using a feedforward artificial neural network (ANN) for water quality retrievals from combined Thematic Mapper data and synthetic aperture radar data in the Gulf of Finland, has been published in this journal. This correspondence attempts to discuss and comment on the paper by Zhang The amount of data used in the paper by Zhang is not enough to determine the number of fitting parameters in the networks. Therefore, the models are not mathematically sound or justified. The conclusion is that ANN modeling should be used with care and enough data;2007;W. Sha;10.1109/TGRS.2007.895432;Journals;1558-0644;
ieee_20221205_08_24_39;Toward Assessing Data Quality of Ontology Matching on the Web;Nowadays the semantic Web is a leading Web technology, which enables semantic interoperability between structurally and semantically heterogeneous web sources and web users. Ontologies are a key of semantic interoperability and the main vehicle of the development of the semantic Web. One of the most challenging and important tasks of ontology engineering is integration of ontologies because with the purpose to built a common ontology for all Web sources and consumers in a domain. The present paper describes an approach of assessing data quality of ontology matching that allows evaluating correctness of mapping concepts and relationships from one ontological fragment to another. The purpose of correct mapping is to find identical and synonymous concepts and relationships in ontologies facilitating their integration to a common ontology, which serves as a base for attaining interoperability.;2007;Olga Vorochek;10.1109/CNSR.2007.68;Conferences;;0-7695-2835-X
ieee_20221205_08_24_39;System Requirements of Real Time Continuous Data Acquisition for Power Quality Monitoring;Data acquisition systems for power quality monitoring have complex requirements which also vary with the intended application. Overall, the complexity of an instrumentation system for power system monitoring is most dependent on the number of channels and sites which must be analysed, and the required time stamping accuracy. Three broad categories of system configurations are described with their basic parameters. The emphasis is on a discussion of hardware and software issues which are involved in a system specification.;2006;V. Kuhlmann;10.1109/UPEC.2006.367635;Conferences;;978-186135-342-9
ieee_20221205_08_24_39;Data-quality Guided Load Shedding for Expensive In-Network Data Processing;In situ wireless sensor networks, not only have to route sensed data from sources to destinations, but also have to filter and fuse observations to eliminate potentially irrelevant data. If data arrive faster to such fusion nodes than the speed with which they can consume the inputs, this will result in an overflow of input buffers. In this paper, we develop load shedding mechanisms which take into consideration both data quality and expensive nature of fusion operators. In particular, we present quality assessment models for objects and fusion operators and we highlight that such quality assessments may impose partial orders on objects.;2007;Lina Peng;10.1109/ICDE.2007.369003;Conferences;2375-026X;1-4244-0803-2
ieee_20221205_08_24_39;A Comprehensive Data Quality Methodology for Web and Structured Data;Measuring and improving data quality in an organization or in a group of interacting organizations is a complex task. Several methodologies have been developed in the past providing a basis for the definition of a complete data quality program applying assessment and improvement techniques in order to guarantee high data quality levels. Since the main limitation of existing approaches is their specialization on specific issues or contexts, this paper presents the comprehensive data quality (CDQ) methodology that aims at integrating and enhancing the phases, techniques and tools proposed by previous approaches. CDQ methodology is conceived to be at the same time complete, flexible and simple to apply. Completeness is achieved by considering existing techniques and tools and integrating them in a framework that can work in both intra and inter organizational contexts, and can be applied to all types of data. The methodology is flexible since it supports the user in the selection of the most suitable techniques and tools within each phase and in any context. Finally, CDQ is simple since it is organized in phases and each phase is characterized by a specific goal and techniques to apply. The methodology is explained by means of a running example.;2007;Carlo Batini;10.1109/ICDIM.2007.369236;Conferences;;1-4244-0682-X
ieee_20221205_08_24_39;Research on Air-quality Change with MODIS-derived AOD Data in Corresponding to 2008 Olympic Environment Monitoring Projects;Using aerosol optical depth (AOD) retrieved from temporal MODIS data to monitor region air-quality change. First, AOD images are retrieved from temporal MODIS data by a well-validated method. Based on four different regions in Beijing, AOD changing graphs are obtained from the retrieval images. The yearly graphs of AOD and PM10 in 2004 are compared. Results indicate that their total trends are good in accordance with each other. Meanwhile, the monthly averaged AOD graphs from June to September between 2003 and 2005 are compared at all the four regions. It turns out that monthly mean AOD in 2003 is higher than that of 2005 at most time, which implies air-quality is improved during 2003 and 2005.;2006;L. Liwei;10.1109/IGARSS.2006.388;Conferences;2153-7003;0-7803-9510-7
ieee_20221205_08_24_39;Data Quality Through Business Rules;Good quality data is a valuable asset for any enterprise in this information age. Though volumes of data have grown, their utility for decision making has been meagre. This is a result of poor quality data that enterprises generally possess. Data generation process goes through a number of stages from capture through exchange, integration, migration to storage and data can get corrupted at any stage. Information systems/business applications built with business rules embedded in the code are inadequate to overcome data quality problems. This paper presents a framework that enables enterprise develop business applications that externalize centralized business logic through business rules. Decoupling business rules from the application provides a number of benefits to enterprises including enhanced data quality conforming to defined rules and agility in responding to the ever changing demands of the business environment.;2007;Vivek Chanana;10.1109/ICICT.2007.375390;Conferences;;984-32-3394-8
ieee_20221205_08_24_39;Development of Analog Optohybrid Circuit for the CMS Inner Tracker Data Acquisition System: Project, Quality Assurance, Volume Production, and Final Performance;The tracker system of the compact muon solenoid (CMS) experiment, will employ approximately 40000 analog fiber-optic data and control links. The optical readout system is responsible for converting and transmitting the electrical signals coming out from the front-end to the outside counting room. Concerning the inner part of the Tracker, about 3600 analog optohybrid circuits are involved in this tasks. These circuits have been designed and successfully produced in Italy under the responsibility of INFN Perugia CMS group, completing the volume production phase by February 2005. Environmental features, reliability, and performances of the analog optohybrid circuits have been extensively tested and qualified. This paper reviews the most relevant steps of the manufacturing and quality assurance process: from prototypes to mass-production for the final use in the CMS data acquisition system.;2007;Daniel Ricci;10.1109/TEPM.2007.899101;Journals;1558-0822;
ieee_20221205_08_24_39;Integrating Data and Quality Space Interactions in Exploratory Visualizations;Data quality is an important topic for many fields because real-world data is rarely perfect. Analysis conducted on data of variable quality can lead to inaccurate or incorrect results. To avoid this problem, researchers have introduced visual elements and attributes into traditional visualization displays to represent data quality information in conjunction with the original data. However, little work thus far has focused on creating an interactive interface to enable users to explicitly explore that data quality information. In this paper, we propose a framework for the linkage between data space and quality space for multivariate visualizations. Moreover, we introduce two novel techniques, quality brushing and quality-series animation, to help users with the exploration of this linkage. A visualization technique specifically designed for the quality space, called the quality map, is proposed as a means to help users create and manipulate quality brushes. We present some interesting case studies to show the effectiveness of our approaches.;2007;Zaixian Xie;10.1109/CMV.2007.11;Conferences;;0-7695-2903-8
ieee_20221205_08_24_39;Heat Map Visualizations Allow Comparison of Multiple Clustering Results and Evaluation of Dataset Quality: Application to Microarray Data;Since clustering algorithms are heuristic, multiple clustering algorithms applied to the same dataset will typically not generate the same sets of clusters. This is especially true for complex datasets such as those from microarray time series experiments. Two such microarray datasets describing gene expression activities from regenerating newt forelimbs at various times following limb amputation were used in this study. A cluster stability matrix, which shows the number of times two genes appear in the same cluster, was generated as a heat map. This was used to evaluate the overall variation among the clustering algorithms and to identify similar clusters. A comparison of the cluster stability matrices for two related microarray experiments with different levels of precision was shown to be an effective basis for comparing the quality of the two sets of experiments. A pairwise heat map was generated to show which pairs of clustering algorithms grouped the data into similar clusters.;2007;John Sharko;10.1109/IV.2007.61;Conferences;1550-6037;0-7695-2900-3
ieee_20221205_08_24_39;Study on the Continuous Quality Improvement of Telecommunication Call Centers Based on Data Mining;Based on the study of the processes of telecommunication call centers, the service quality metrics of the call centers are put forward. And the mode of the continuous service quality improvement of the call centers based on data warehouse and data mining is studied. Then the process of the IVR (Interactive Voice Response) is analyzed and a mode for the efficiency improvement of IVR is put forward based on the exchange of the orders of the service items in the IVR. Then a service quality metrics of the agents, the ratio of recall in one hour, is put forward. This metrics can be used in the performance analysis of the agents. Furthermore, the model of the performance analysis and control of the ASA (Average Speed of Answer) based on data mining and SPC (Statistical Process Control) is put forward. At last, a method for forecasting the call arriving in is put forward based the time series analysis using dynamic data mining. The result certified that the efficiency and service quality of the telecommunication call center can be improved obviously using the method in this paper.;2007;He Shu-guang;10.1109/ICSSSM.2007.4280171;Conferences;2161-1904;1-4244-0885-7
ieee_20221205_08_24_39;Data Quality Evaluation Process and Methods of Natural Environment Conceptual Model in Simulation Systems;Currently in almost simulation systems applying environment models, data quality of natural environment is not sufficiently evaluated. In order to address this problem, the process and method of data quality evaluation are proposed for Natural Environment Conceptual Model (NECM). Firstly, a process of 4 steps is presented to evaluate data quality of NECM. Secondly, indexes and factors of data quality evaluation are given according to the characteristics of NECM. Thirdly, in order to meet the requirement of this data evaluation, the analytic hierarchy process (AHP) method is improved and then applied to evaluate the data quality. Finally, the results of data quality evaluation are given and analyzed;2006;Guobing Sun;10.1109/CESA.2006.4281678;Conferences;;7-900718-14-1
ieee_20221205_08_24_39;A Proposal for the Management of Mobile Network's Quality of Service (QoS) using Data Mining Methods;Today, the challenge for the service operators is not only to attract and subscribe new users but to retain already subscribed users. To gain a competitive edge over other service operators, the operating personnel have to measure the services provided to their users and the network performance in terms of Quality of Service (QoS) at regular periods. By analyzing the information in these measurements, they can manage the quality of service, which helps to improve their service and network performance. But due to the heavy increase in the number of users in recent years, they find it difficult to elicit essential information from such a large and complex data to manage the QoS using the existing methods. It is here that the recently developed and more powerful data mining methods come in handy. In this paper we proposed how data mining methods can be used to manage the mobile network QoS. We describe three data mining methods: Rough Set Theory, Classification and Regression Tree (CART), and Self Organizing Map (SOM).;2007;MPS Bhatia;10.1109/WOCN.2007.4284163;Conferences;2151-7703;1-4244-1005-3
ieee_20221205_08_24_39;Data Sharing Strategy for Guaranteeing Quality-of-Service in VoD Application;The phenomenal growth in the distributed multimedia applications has accelerated the popularity of Video-on-Demand (VoD) system. The vital task of multimedia applications is to satisfy diverse client's request for distinct video with confined resources by using assorted Quality-of-Service (QoS) procedures. In this paper a fusion of data sharing techniques like batching and recursive patching is applied in the local server for ensuring Quality-of-Service to the clients and enabling higher throughput. The network resources are apportioned appropriately using batching and the time difference between the requests is minified by recursive patching. The suggested algorithm renders the entire video to the clients using true VoD, near VoD using multicast or broadcast scheme depending on popularity of the video. The experimental results indicate that our approach accomplishes 2% reduction in blocking ratio and throughput is 10% -15% greater than the Poon 's strategy [15], which depicts that not only the resources are efficiently utilized but also a suitable Quality-of-Service is provided to each client.;2006;D. N. Sujatha;10.1109/ICISIP.2006.4286062;Conferences;;1-4244-0612-9
ieee_20221205_08_24_39;Predicting User-Perceived Quality Ratings from Streaming Media Data;Media stream quality is highly dependent on underlying network conditions, but identifying scalable, unambiguous metrics to discern the user-perceived quality of a media stream in the face of network congestion is a challenging problem. User-perceived quality can be approximated through the use of carefully chosen application layer metrics, precluding the need to poll users directly. We discuss the use of data mining prediction techniques to analyze application layer metrics to determine user-perceived quality ratings on media streams. We show that several such prediction techniques are able to assign correct (within a small tolerance) quality ratings to streams with a high degree of accuracy. The time it takes to train and tune the predictors and perform the actual prediction are short enough to make such a strategy feasible to be executed in real time and on real computer networks.;2007;Amy Csizmar Dalal;10.1109/ICC.2007.20;Conferences;1938-1883;1-4244-0353-7
ieee_20221205_08_24_39;Learning Fuzzy Linguistic Models from Low Quality Data by Genetic Algorithms;Incremental rule base learning techniques can be used to learn models and classifiers from interval or fuzzy-valued data. These algorithms are efficient when the observation error is small. This paper is about datasets with medium to high discrepancies between the observed and the actual values of the variables, such as those containing missing values and coarsely discretized data. We will show that the quality of the iterative learning degrades in this kind of problems, and that it does not make full use of all the available information. As an alternative, we propose a new implementation of a mutiobjective Michigan-like algorithm, where each individual in the population codifies one rule and the individuals in the Pareto front form the knowledge base.;2007;Luciano Sanchez;10.1109/FUZZY.2007.4295659;Conferences;1098-7584;1-4244-1210-2
ieee_20221205_08_24_39;An Empirical Study of the Classification Performance of Learners on Imbalanced and Noisy Software Quality Data;In the domain of software quality classification, data mining techniques are used to construct models (learners) for identifying software modules that are most likely to be fault-prone. The performance of these models, however, can be negatively affected by class imbalance and noise. Data sampling techniques have been proposed to alleviate the problem of class imbalance, but the impact of data quality on these techniques has not been adequately addressed. We examine the combined effects of noise and imbalance on classification performance when seven commonly-used sampling techniques are applied to software quality measurement data. Our results show that some sampling techniques are more robust in the presence of noise than others. Further, sampling techniques are affected by noise differently given different levels of imbalance.;2007;Chris Seiffert;10.1109/IRI.2007.4296694;Conferences;;1-4244-1500-4
ieee_20221205_08_24_39;MULS: A General Framework of Providing Multilevel Service Quality in Sequential Data Broadcasting;In recent years, data broadcasting becomes a promising technique to design a mobile information system with power conservation, high scalability and high bandwidth utilization. In many applications, the query issued by a mobile client corresponds to multiple items which should be accessed in a sequential order. In this paper, we study the scheduling approach in such a sequential data broadcasting environment. Explicitly, we propose a general framework referred to as MULS (standing for MUlti-Level Service) for an information system. There are two primary stages in MULS: on-line scheduling and optimization procedure. In the first stage, we propose an On- Line Scheduling algorithm (denoted by OLS) to allocate the data items into multiple channels. As for the second stage, we devise an optimization procedure SCI, standing for Sampling with Controlled Iteration, to enhance the quality of broadcast programs generated by algorithm OLS. Procedure SCI is able to strike a compromise between effectiveness and efficiency by tuning the control parameters. According to the experimental results, we show that algorithm OLS with procedure SCI outperforms the approaches in prior works prominently in both effectiveness (i.e., the average access time of mobile users) and efficiency (i.e., the complexity of the scheduling algorithm). Therefore, by cooperating algorithm OLS with procedure SCI, the proposed MULS framework is able to generate broadcast programs with flexibility of providing different service qualities under different requirements of effectiveness and efficiency: in the dynamic environment in which the access patterns and information contents change rapidly, the parameters used in SCI will perform online scheduling with satisfactory service quality. As for the static environment in which the query profile and the database are updated infrequently, larger values of parameters are helpful to generate an optimized broadcast program, indicating the advantageous feature of MULS.;2007;Hao-Ping Hung;10.1109/TKDE.2007.1072;Journals;2326-3865;
ieee_20221205_08_24_39;Data Compression of Power Quality Events Using the Slantlet Transform;The slantlet transform (SLT) is an orthogonal discrete wavelet transform (DWT) with two zero moments and with improved time localization. It also retains the basic characteristic of the usual filterbank such as octave band characteristic, a scale dilation factor of two and efficient implementation. However, the SLT is based on the principle of designing different filters for different scales unlike iterated filterbank approaches for the DWT. In this paper a novel approach for power quality data compression using the SLT is presented and its performance in terms of compression ratio (CR), percentage of energy retained and mean square error present in the reconstructed signals is assessed. Varieties of power quality events, which include voltage sag, swell, momentary interruption, harmonics, transient oscillation, and voltage flicker are used to test the performance of the new approach. Computer simulation results indicate that the SLT offers superior compression performance compared to the conventional DCT and the DWT based approaches.;2002;G. Panda;10.1109/MPER.2002.4311703;Magazines;1558-1705;
ieee_20221205_08_24_39;Power Quality Disturbance Data Compression, Detection, and Classification Using Integrated Spline Wavelet and S-Transform;In this paper power quality transient data are compressed and stored for analysis and classification purpose. From the compressed data set origtnal data are reconstructed and then analysed using a modified wavelet transform known as transform. Compression techniques using splines are performed through signal decomposition, thresholding of wavelet transform coefficients, and signal reconstruction. Finally we present compression results using splines and examine the application of splines compression in power quality monitoring to mitigate against data communication and data storage problems. Since Stransform has better time frequency and localisation property. power quality disturbances are detected and then classified in a superior way than the recently used wavelet transform.;2002;P. K. Dash;10.1109/MPER.2002.4312423;Magazines;1558-1705;
ieee_20221205_08_24_39;A Data Mining Algorithm for Monitoring PCB Assembly Quality;A pattern clustering algorithm is proposed in this paper as a statistical quality control technique for diagnosing the solder paste variability when a huge number of binary inspection outputs are involved. To accommodate this goal, a latent variable model is first introduced and incorporated into classical logistic regression model so that the interdependencies between measured physical characteristics and their relationship to the final solder defects can be explained. This probabilistic model also allows a maximum-likelihood principal component analysis (MLPCA) method to recognize the dimension of systematic causes contributing to solder paste variability. The correlated measurement variables are then projected onto the reduced latent space, followed by an appropriate clustering approach over the inspected solder pastes for variation interpretation and quality diagnosing. An application to a real stencil printing process demonstrates that this method facilitates in identifying the root causes of solder paste defects and thereby improving PCB assembly yield.;2007;Feng Zhang;10.1109/TEPM.2007.907576;Journals;1558-0822;
ieee_20221205_08_24_39;An EffectiveMulti-Layer Model for Controlling the Quality of Data;"Data mining aims to search for implicit, previously unknown, and potentially useful information that might be embedded in the data. It is well known that ""garbage in, garbage out"". Hence, to get meaningful mining results, a clean set of data is essential. In this paper, we propose an effective model for controlling the quality of data. Specifically, this three-layer model focuses on data validity and data consistency. To elaborate, the internal layer ensures that the observed data are valid and their values fall within reasonable ranges. The temporal layer ensures that data are consistent with their temporal behaviour. The spatial layer ensures that data are consistent with their spatial neighbours. A case study on applying our proposed model to real-life weather data for an agricultural application shows that our model is effective in controlling and improving data quality, and thus leading to better mining results. It is important to note the application of our proposed model is not confined to the weather data for agricultural applications. We also discuss, in this paper, how the proposed three-layer model can be effectively applicable to control the quality of data in some other real-life situations.";2007;Carson Kai-Sang Leung;10.1109/IDEAS.2007.4318086;Conferences;1098-8068;978-0-7695-2947-9
ieee_20221205_08_24_39;A Probabilistic Approach to Web Portal's Data Quality Evaluation;"Advances in technology and the use of the Internet have favoured the emergence of a large number of Web applications, including Web Portals. Web portals provide the means to obtain a large amount of information therefore it is crucial that the information provided is of high quality. In recent years, several research projects have investigated Web Data Quality; however none has focused on data quality within the context of Web Portals. Therefore, the contribution of this research is to provide a framework centred on the point of view of data consumers, and that uses a probabilistic approach for Web portal's data quality evaluation. This paper shows the definition of operational model, based in our previous work.";2007;Angelica Caro;10.1109/QUATIC.2007.10;Conferences;;978-0-7695-2948-6
ieee_20221205_08_24_39;An Empirical Study of China Quality Award on Firm's Market Value - Based on the Data from Chinese Stock Market;This paper empirically investigates the relation between China Quality Award and the market value of the firm by a sample of public listed companies that have won China Quality Award from 2001 to 2005 in the mainland of China. Our results show that the award winners experienced remarkably positive abnormal returns on the day of announcement ranging from 0.55% to 0.77% depending on the model used to generate the abnormal returns. According to Hendricks and Singhal (1996), winning a quality reward conveys the information about the systematic risk of the firm. However, we haven't found a statistically decrease in the equity and asset betas after the quality award announcement. Finally, the factors that affect abnormal returns are investigated, empirical results show that debt ratio of the firms and the award prestige have an significant impact on abnormal returns, however, the firm size doesn't play an important role on abnormal returns.;2007;Xiang-Zhi Bu;10.1109/WICOM.2007.1043;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_24_39;Application of Data Mining in Manufacturing Quality Data;Knowledge on product quality is one of the most important knowledge sources throughout the product lifecycle for the efficiency and effectiveness of product design decisions. To provide quality related knowledge, this paper proposed one data mining based knowledge discovery approach. This approach can extract knowledge on product quality from large volume of quality related manufacturing data. The effectiveness of this approach is illustrated and validated by an example adapted from literature. Finally, some conclusions and future works are discussed.;2007;Keqin Wang;10.1109/WICOM.2007.1318;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_24_39;Study on the Continuous Quality Improvement Systems of LED Packaging Based on Data Mining;LED is one of the most widely used components in electric products. And the LED packaging is a very important process between semiconductor manufacturers and the electric product manufacturers. Based on the analysis of the characteristics of the LED packaging processes, a quality control model based on SPC (statistical process control) and data mining is put forward. The data mining is used as the quality data analysis tool and the quality diagnosis method. Then an infrastructure of the integrated continuous quality improvement systems of the LED packaging is put forward. In this infrastructure, there are three layers of the data collection layer, the data analysis layer and the result viewer layer. Furthermore, the data warehouse of LED packaging is designed with the snowflake schema. A 3 layer yield rate SPC is studied and the decision tree method is used as a quality diagnosis method based on the designed data warehouse. Finally, a prototype of the continuous quality improvement system is developed.;2007;Shu-Guang He;10.1109/WICOM.2007.1378;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_24_39;Evaluating Spatial Data Quality in GIS Database;The quality of spatial data is often limited by the quality of their sources such as paper maps and satellite images. Spatial operations performed on database of geographical information systems (GIS) such as selection, projection, and Cartesian product, do not always work correctly because their accuracy and completeness depends on the quality of spatial data. The present paper suggests a methodology to evaluate two data quality characteristics - accuracy and completeness - of the spatial database. Four quantitative measures are introduced to assess the quality of spatial data. Their explicit forms are derived for a tuple, and four assumptions are presented where the measures can be evaluated efficiently by numerical calculation.;2007;Ying Su;10.1109/WICOM.2007.1463;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_24_39;Study on the Quality Improvement of Injection Molding in LED Packaging Processes Based on DOE and Data Mining;The LED (light emitting diode) packaging is a very important process between semiconductor manufacturers and the electric product manufacturers. And the injection molding process in LED packaging is critical to the quality of the final products. Based on the analysis of the injection molding processes, the main quality problems and their possible causes are studied. Then the RSM (response surface methodology) of DOE (design of experiment) is used for the optimization of the producing parameters. The optimized parameters, the mold temperature, the warm-up temperature, the screw pressure and the screw time, are found with DOE. In the running process, the CTQ (critical to quality) is controlled with SPC (Statistical process control) by different dimensions like product, product catalog, time and devices. Finally, a model of continuous quality improvement based on data mining is put forward. The association analysis is used for the parameter optimization in the running process.;2007;Shu-Guang He;10.1109/WICOM.2007.1626;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_24_39;Tuning anonymity level for assuring high data quality: an empirical study.;Preserving data privacy is posing new challenges to software engineering researchers. Current technologies can be too cumbersome, pervasive or costly to be successfully applied in dynamic and complex scenarios where data exchange occurs among a large number of applications. Anonymization techniques seem to be a promising candidate, even if preliminary investigations suggest that they could deteriorate the quality of data. An empirical study has been carried out in order to understand the relationship between the anonymization level and the degradation of data quality.;2007;Gerardo Canfora;10.1109/ESEM.2007.23;Conferences;1949-3789;978-0-7695-2886-1
ieee_20221205_08_24_39;Filtering, Robust Filtering, Polishing: Techniques for Addressing Quality in Software Data;Data quality is an important aspect of empirical analysis. This paper compares three noise handling methods to assess the benefit of identifying and either filtering or editing problematic instances. We compare a 'do nothing' strategy with (i) filtering, (ii) robust filtering and (Hi) filtering followed by polishing. A problem is that it is not possible to determine whether an instance contains noise unless it has implausible values. Since we cannot determine the true overall noise level we use implausible val.ues as a proxy measure. In addition to the ability to identify implausible values, we use another proxy measure, the ability to fit a classification tree to the data. The interpretation is low misclassification rates imply low noise levels. We found that all three of our data quality techniques improve upon the 'do nothing' strategy, also that the filtering and polishing was the most effective technique for dealing with noise since we eliminated the fewest data and had the lowest misclassification rates. Unfortunately the polishing process introduces new implausible values. We believe consideration of data quality is an important aspect of empirical software engineering. We have shown that for one large and complex real world data set automated techniques can help isolate noisy instances and potentially polish the values to produce better quality data for the analyst. However this work is at a preliminary stage and it assumes that the proxy measures of lity are appropriate.;2007;Gernot Liebchen;10.1109/ESEM.2007.70;Conferences;1949-3789;978-0-7695-2886-1
ieee_20221205_08_24_39;Data Quality - The Key Success Factor for Data Driven Engineering;As the scale and diversity of data grows in the digital arena, the complexities of data driven engineering grow multifold with it. The last several years have brought forth several new technologies to service this need - semantic Web, grid systems, Web service composition to mention a few. However, a fundamental underpinning of the success of these technologies resides in the quality of data that they can provide. Often the failure of a technology is attributed to its functionality when the real problem lies in the quality of data it uses and subsequently produces. In this paper, we highlight a need to embrace data quality considerations in all aspects of data driven engineering.;2007;Shazia Sadiq;10.1109/NPC.2007.176;Conferences;;978-0-7695-2943-1
ieee_20221205_08_24_39;Experience at Italian National Institute of Health in the quality control in telemedicine: tools for gathering data information and quality assessing;"The authors proposed a set of tools and procedures to perform a Telemedicine Quality Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers. The proposed tools were: the Informative Questionnaire (InQu), the Classification Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The InQu served to acquire the information about the examined TM product/service; the ClFo allowed to classify a TM product/service as belonging to one application area of TM. The TF was intended as a technical dossier of product and forced the TM supplier to furnish the only requested documentation of its product, so to avoid redundant information. The QACL was a checklist of requirements, regarding all the essential aspects of the telemedical applications, that each TM products/services must be met. The final assessment of the TM product/service was carried out via the QACL, by computing the number of agreed requirements: on the basis of this computation, a Quality Level (QL) was assigned to the telemedical application. Seven levels were considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality control of the telemedical applications and should be a guidance to all the TM practitioners, from the manufacturers to the expert evaluators. The quality control process procedures proposed thus could be adopted in future as routine procedures and could be useful in the assessing the TM delivering into the National Health Service versus the traditional face to face healthcare services.";2007;D. Giansanti;10.1109/IEMBS.2007.4352911;Conferences;1558-4615;978-1-4244-0788-0
ieee_20221205_08_24_39;Power Quality of Renewable Energy Systems Can Be Evaluated Using Simulation Data;The paper presents a comparative study of a renewable energy system using simulation data versus the data recorded for its implementation at the Hybrid Power System Test Bed (HPSTB) at the National Wind Technology Center, NREL. The simulation data were obtained from the model realized using RPM-SIM simulator. This study shows that under different conditions the power, voltage, and frequency traces of a simulated system follow closely those recorded. Consequently, it is concluded that the quality of power generated under different conditions can be evaluated using simulation data and that such simulation study can be used to develop the structure and control strategy of renewable energy system to meet power quality requirements.;2007;Jan T. Bialasiewicz;10.1109/ISIE.2007.4375001;Conferences;2163-5145;978-1-4244-0755-2
ieee_20221205_08_24_39;Quality Assessment of Affymetrix GeneChip Data using the EM Algorithm and a Naive Bayes Classifier;Recent research has demonstrated the utility of using supervised classification systems for automatic identification of low quality microarray data. However, this approach requires annotation of a large training set by a qualified expert. In this paper we demonstrate the utility of an unsupervised classification technique based on the Expectation-Maximization (EM) algorithm and naïve Bayes classification. On our test set, this system exhibits performance comparable to that of an analogous supervised learner constructed from the same training data.;2007;Brian E. Howard;10.1109/BIBE.2007.4375557;Conferences;;978-1-4244-1509-0
ieee_20221205_08_24_39;The ALICE-LHC Online Data Quality Monitoring Framework: Present and Future;ALICE is one of the experiments under installation at CERN Large Hadron Collider, dedicated to the study of Heavy-Ion Collisions. The final ALICE Data Acquisition system has been installed and is being used for the testing and commissioning of detectors. The Online Data Quality monitoring is an important part of the DAQ software framework (DATE). In this presentation we overview the implementation and usage experience of the interactive tool MOOD used for the commissioning period of ALICE and we present the architecture of the Automatic Data Quality Monitoring framework, a distributed application aimed to produce, collect, analyze, visualize and store monitoring data in a large, experiment wide scale.;2007;Filimon Roukoutakis;10.1109/RTC.2007.4382730;Conferences;;978-1-4244-0867-2
ieee_20221205_08_24_39;Data Quality Monitoring Framework for the ATLAS Experiment at the LHC;Data quality monitoring (DQM) is an important and integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results while data is being processed. When DQM occurs in the online environment, it provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to only have dependence on software that is common between online and offline (such as ROOT) and therefore is used in the offline framework as well. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.;2007;Corso-Radu;10.1109/RTC.2007.4382779;Conferences;;978-1-4244-0867-2
ieee_20221205_08_24_39;Refinement of a Tool to Assess the Data Quality in Web Portals;The Internet is now firmly established as an environment for the administration, exchange and publication of data. To support this, a great variety of Web applications have appeared, among these web portals. Numerous users worldwide make use of Web portals to obtain information for different purposes. These users, or data consumers, need to ensure that this information is suitable for the use to which they wish to put it. PDQM (portal data quality model) is a model for the assessment of portal data quality. It has been implemented in the PoDQA tool (portal data quality assessment tool), which can be accessed at http://podqa.webportalqualitv.com. In this paper we present the various refinements that it has been necessary to make in order to obtain a tool which is stable and able to make accurate and efficient calculations of the elements needed to assess the quality of the data of a Web portal.;2007;Angelica Caro;10.1109/QSIC.2007.4385501;Conferences;2332-662X;978-0-7695-3035-2
ieee_20221205_08_24_39;Improving grid monitoring with data quality assessment;As Grid emerges as a cyber-infrastructure for the next-generation of e-Science applications, monitoring Grid becomes a very significant task. A typical Grid application is composed of a large number of resources that can fail, including network, hardware and software. Even when monitoring information from all these components is accessible, it is hard to determine whether anomalies and failures during the execution are related to a particular job. However receiving intermediate results and interacting with applications play a key role for users in reality. Considering the complexity of implementation and the large scope the monitoring system covers, there is no doubt we will face incomplete and duplicate data in many applications. Overcoming data heterogeneity is a long standing problem in the Grid research communities. It will be a disaster to handle large amount of inaccurate information where the quality of data is very poor. Fortunately, a wide spectrum of applications exhibit strong dependencies among data samples, the readings of nearby sensors are generally correlated, and the components are connected with interactions. Such relations can be used for promoting the quality of the recorded data. This paper proposes a data cleaning approach oriented Grid monitoring model, which is based on modeling data dependencies based on entity relation graph. We bring effective data quality preprocessing approach into the Grid applications monitoring model, which is critical because many real-world Grid datasets are not perfect, but rather they contain missing, erroneous, duplicate data and other data quality problems.;2007;Wei Liu;10.1109/ISCIT.2007.4392260;Conferences;;978-1-4244-0977-8
ieee_20221205_08_24_39;Representing Data Quality for Streaming and Static Data;In smart item environments, multitude of sensors are applied to capture data about product conditions and usage to guide business decisions as well as production automation processes. A big issue in this application area is posed by the restricted quality of sensor data due to limited sensor precision as well as sensor failures and malfunctions. Decisions derived on incorrect or misleading sensor data are likely to be faulty. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In this paper, we present a flexible model for the efficient transfer and management of data quality for streaming as well as static data. We propose a data stream metamodel to allow for the propagation of data quality from the sensors up to the respective business application without a significant overhead of data. Furthermore, we present the extension of the traditional RDBMS metamodel to permit the persistent storage of data quality information in a relational database. Finally, we demonstrate a data quality metadata mapping to close the gap between the streaming environment and the target database. Our solution maintains a flexible number of DQ dimensions and supports applications directly consuming streaming data or processing data filed in a persistent database.;2007;Anja Klein;10.1109/ICDEW.2007.4400967;Conferences;;978-1-4244-0832-0
ieee_20221205_08_24_39;Scheduling Algorithm of Update Transactions and Quality of Service Management Based on Derived Data in Real-Time and Mobile Database Systems;The demand on mobile and real-time database application and service in distributed environment is becoming increasingly extensive, since the workload cannot be precisely predicted so that they can become overloaded possibly. Low bandwidth in mobile environment will lead to aborting or restarting for transactions because of competing for the limited system resources severely. In this paper, we propose a novel performance metric for example deadline miss ratio of transactions and data freshness based on the relationship of data items in the DAG (directed acyclic graph) to ensure the QoS (quality of service ) in mobile and real-time database systems. Feedback control architecture and MODDFT (modifying on-demand breath-first traversal algorithm) are proposed in our system to decrease miss ratio of transactions and improve data freshness. The MODDFT algorithm can guarantee the QoS performance will not be beyond the reference by database administrator through simulation experiments.;2007;Guohui Li;10.1109/FCST.2007.26;Conferences;2159-631X;978-0-7695-3036-9
ieee_20221205_08_24_39;A Statistical Approach to Volume Data Quality Assessment;Quality assessment plays a crucial role in data analysis. In this paper, we present a reduced-reference approach to volume data quality assessment. Our algorithm extracts important statistical information from the original data in the wavelet domain. Using the extracted information as feature and predefined distance functions, we are able to identify and quantify the quality loss in the reduced or distorted version of data, eliminating the need to access the original data. Our feature representation is naturally organized in the form of multiple scales, which facilitates quality evaluation of data with different resolutions. The feature can be effectively compressed in size. We have experimented with our algorithm on scientific and medical data sets of various sizes and characteristics. Our results show that the size of the feature does not increase in proportion to the size of original data. This ensures the scalability of our algorithm and makes it very applicable for quality assessment of large-scale data sets. Additionally, the feature could be used to repair the reduced or distorted data for quality improvement. Finally, our approach can be treated as a new way to evaluate the uncertainty introduced by different versions of data.;2008;Chaoli Wang;10.1109/TVCG.2007.70628;Journals;2160-9306;
ieee_20221205_08_24_39;Assistance ontology of quality control for enterprise model using data mining;There are many quality domains in which ideas and concepts about quality are represented. The intelligent discovery assistants ontology of data mining (DM) processes was presented to compose and select the large space and non-trivial interaction in quality control for enterprise. We use a prototype to show that quality control for enterprise model is using the virtual enterprise quality ontology. A simple, but typical DM process was presented in the paper, which included preprocessing data, applying a data-mining algorithm, and post processing the mining results. It provides users with systematic enumerations of valid DM processes, in order that important, potentially fruitful options are not overlooked and effective rankings of these valid processes by different criteria, to facilitate the choice of DM processes to execute. Deeply research in the quality and ontology area is realized in protege with the format of OWL. Assistance ontology has the function to help mining workers selecting the algorithm, how to help selecting algorithm, the one prerequisite is that establishes good data mining method ontology. The intelligent discovery assistants search and deduct in the quality ontology. Finally, a study case is given to explain the practical application with the fault diagnosis bases on ontology, and was given encouraging results.;2007;Xuhui Chen;10.1109/IEEM.2007.4419260;Conferences;2157-362X;978-1-4244-1529-8
ieee_20221205_08_24_39;Quality improvement of steel products by using multivariate data analysis;This paper describes quality improvement methods based on multivariate data analysis and their application to an industrial steel process. The PCA-LDA, which combines principal component analysis and linear discriminant analysis, is used for influential factor analysis of the qualitative quality variables. In addition, Data-Driven Quality Improvement (DDQI) is used to determine the optimal operating condition that can achieve the desired product quality under a given objective function and various constraints. The PCA-LDA and the DDQI can provide useful information to improve product quality. The experimental results show the effectiveness of the proposed methods.;2007;Yoshiaki Nakagawa;10.1109/SICE.2007.4421396;Conferences;;978-4-907764-27-2
ieee_20221205_08_24_39;Assessing Data Quality for Geographical Information System Databases;GIS is a tool for spatial-related data processing and decision making. Handling decision making under high quality data is a further extension of GIS functions. Therefore, it is of vital importance to assess data quality in GIS and to decide the fitness of data to user's particular applications. We present a methodology to determine two data quality characteristics - accuracy and completeness - that are of critical importance to decision makers. We examine how the quality metrics of source data affect the quality for information outputs produced using the relational algebra operations Selection, Projection, and Cartesian product. Our methodology can help users deciding the fitness of spatial data to their GIS applications according to the quality levels much efficiently.;2007;Su Ying;10.1109/ICMSE.2007.4421817;Conferences;2155-1855;978-7-88358-080-5
ieee_20221205_08_24_39;Data Quality Guidelines for GEOSS Consideration-The CEOS Working Group on Calibration and Validation (WGCV);The harmonization of operational data products and the creation of higher level information products such as global maps and time series (from different sensor sources) are required to satisfy the operational service requirements of the societal benefit areas as outlined in the GEOSS implementation plan. The CEOS Worldng Group on Calibration and Validation (WGCV) concentrates on defining standards and procedures aimed at allowing for the inter-comparison and ultimate utilization of data from all Earth observing platforms, both current and future. WGCV strives to establish common approaches to validation, calibration and data exchange formats to ensure effective cooperative use of all CEOS member space assets in addressing important global scale problems. This paper reviews the WGCV data assurance strategy, detailed system element requirements to guarantee data quality, and current WGCV activities for the generation and validation of products..;2007;S. Ungar;10.1109/IGARSS.2007.4422791;Conferences;2153-7003;978-1-4244-1212-9
ieee_20221205_08_24_39;DataFed: Mediated web services for distributed air quality data access and processing.;This is a report on the federated data system,DataFed for distributed air quality data. Data sources are federated by applying a universal, multi-dimensional data model. The physical and semantic homogenization is accomplished by wrapper services. Data processing is performed through web services, which themselves can be distributed. Data processing applications are created by the composition of the distributed service components. International Standards and Protocols are used to establish interoperability of the service components. In this work we have adapted the OGC Web services as a standard protocol for air quality data access. This report summarizes our experiences with the OGC W*S based service composition. It includes the results of our participation in the GALEON (geo-interface to atmosphere,land,earth,ocean netCDF) Interoperability experiment.;2007;Rudolf B. Husar;10.1109/IGARSS.2007.4423730;Conferences;2153-7003;978-1-4244-1212-9
ieee_20221205_08_24_39;Data system for the monitoring of power quality in the transmission substations supplying big consumers;During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.;2007;Fanica Vatra;10.1109/EPQU.2007.4424094;Conferences;2150-6655;978-84-690-9441-9
ieee_20221205_08_24_39;The Development of an Objective Metric for the Accessibility Dimension of Data Quality;This paper presents interim results with respect to an objective metric for the accessibility dimension of data quality. It builds on a conceptual framework developed in previous research. This framework examined the dimensions of data quality and in particular the accessibility dimension. A definition of the accessibility dimension was presented based on its objective attributes as outlined by both practitioners and academics. The findings suggest that it is possible to measure this dimension objectively. The attributes are measured in a controlled environment and have been compared with traditional survey methodologies. The results indicate a relationship between the level of data quality and each of the accessibility attributes examined in the controlled environment. The overall measure of data quality can be it is argued improved by measuring these objective attributes and applying the necessary improvements identified. Future research intends to examine all of these attributes, their impact on data quality and possible remedies for overall improved data quality.;2007;Owen Foley;10.1109/IIT.2007.4430434;Conferences;;978-1-4244-1841-1
ieee_20221205_08_24_39;The ALICE-LHC online Data Quality Monitoring framework;ALICE is one of the experiments under installation at CERN Large Hadron Collider, dedicated to the study of heavy- ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Data Quality Monitoring (DQM) is an important aspect of the online procedures for a HEP experiment. In this presentation we overview the architecture, implementation and usage experience of ALICE'S AMORE (Automatic MOnitoRing Environment), a distributed application aimed to collect, analyze, visualize and store monitoring data in a large, experiment wide scale. AMORE is interfaced to the DAQ software framework (DATE) and follows the publish-subscribe paradigm where a large number of batch processes execute detector-specific analysis on raw data samples and publish monitoring results on specialized servers. Clients connected to these servers have the ability to correlate, further analyze and visualize the monitoring data. Provision is taken to archive the most important results so that historic plots can be produced.;2007;Sylvain Chapeland;10.1109/NSSMIC.2007.4436318;Conferences;1082-3654;978-1-4244-0923-5
ieee_20221205_08_24_39;Guidelines for Setting Organizational Policies for Data Quality;From a process perspective, the tasks that individuals carry out within an organization are linked. These linkages are often documented as process flow diagrams that connect the data inputs and outputs of individuals. In such a connected setting, the differences among individuals in preference for data attributes such as timeliness, accuracy, etc., can cause data quality problems. For example, individuals at the head of a process flow may bear all the cost of capturing high quality data but may not receive all of the benefits although the rest of the organization benefits from their diligence. Consequently, these individuals, in absence of any managerial intervention, may not invest enough in data quality. In this research, solutions to this and similar organization data quality problems are proposed. The solutions focus on principles of reengineering, employee empowerment, decentralization of computing, and mechanisms to measure and reward individuals for their data quality efforts.;2008;Rajiv Dewan;10.1109/HICSS.2008.187;Conferences;1530-1605;978-0-7695-3075-8
ieee_20221205_08_24_39;HDQ: A meta-model for the quality improvement of heterogeneous data;In this paper, we outline the HDQ meta-model and exemplify its application to a realistic organizational scenario within a methodology for assessing and improving corporate data quality. The main aim of the HDQ meta-model is to support the modeling of the information resources used within an organization towards the application of comprehensive but yet lean methodologies of data quality improvement. The approach allowed by HDQ meta-model is aimed at addressing the scalable and incremental management of the quality of data represented in either structured, semi- structure or unstructured information sources.;2007;Daniele Barone;10.1109/ICDIM.2007.4444259;Conferences;;978-1-4244-1476-5
ieee_20221205_08_24_39;Technical method for service quality measurement and user’s service usage collection in wireless broadband data service;"In this study, the relationship between network access quality and RF quality parameter in the wireless networks which have different characteristics from wired networks when the dedicated terminals being connected to the high speed wireless data networks (In other words, broadband wireless access service networks) such as CDMA 1X EV-DO, WCDMA(HSDPA), or Mobile WiMax (WiBro) is analyzed. Also, this study proves that the analysis in connection with the users’ service usage patterns is available. This was possible by constructing and operating the broadband wireless access service Monitoring System (BWA Monitoring System) that collects user data in connection with the CM (Connection Manager) program of the respective terminal. And, analysis regarding the users’ network usage patterns was possible which could be utilized as reference in creating new business models; furthermore, the study shows that the SLA (Service Level Agreement) in the wireless network sector can be proved.";2007;Ju Yeoul Park;10.1109/SOFTCOM.2007.4446060;Conferences;;978-953-6114-95-5
ieee_20221205_08_24_39;Data Quality Monitoring Framework for the ATLAS Experiment at the LHC;Data quality monitoring (DQM) is an integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results to the shift personnel while data is being processed. In the online environment, DQM provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing the execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to have dependence only on software that is common between online and offline (such as ROOT) and therefore the same framework can be used in both environments. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.;2008;A. Corso-Radu;10.1109/TNS.2007.912884;Journals;1558-1578;
ieee_20221205_08_24_39;The ALICE-LHC Online Data Quality Monitoring Framework: Present and Future;ALICE is one of the experiments under installation at CERN Large Hadron Collider (LHC), dedicated to the study of heavy-ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Online data quality monitoring is an important part of the DAQ software framework, DATE. In this paper, we overview the implementation and usage experience of the interactive tool MOOD used for the commissioning period of ALICE, and we present the architecture of the automatic data quality monitoring framework, a distributed application aimed at producing, collecting, analyzing, visualizing, and storing monitoring data in a large experiment-wide scale.;2008;Filimon Roukoutakis;10.1109/TNS.2007.913942;Journals;1558-1578;
ieee_20221205_08_24_39;Delivering high quality, secure speech communication through low data rate 802.15.4 WPAN;IEEE 802.15.4 is a Wireless Personal Area Network (WPAN) for low data-rate and duty-cycle applications. How this simple network can also be used for high quality, secure speech communication is described in this paper. The key to achieve this lies in how the various features can be packed into the low bandwidth structure that has been allocated in this protocol. A low bandwidth high quality speech vocoder such as the Advanced Multiband Excitation (AMBE) from DVSI has been found suitable to be used in our evaluation. Guaranteed Time Slot (GTS) has been reserved in the superframe structure to ensure continuous and uninterrupted audio streaming. For high security, the inherently 802.15.4 built in Advanced Encryption Standard (AES128) and Message Integrity Code (MIC) 128 bits algorithms are utilized. At the end of the paper, a methodology to channel speech more efficiently by forming GTS dynamically is proposed.;2007;Lee Yong Hua;10.1109/ICTMICC.2007.4448593;Conferences;;978-1-4244-1094-1
ieee_20221205_08_24_39;Information quality mapping in resource-constrained multi-modal data fusion system over wireless sensor network with losses;One of the approaches to reduce the complexity of application adaptation for a particular sensor network installation is to separate the application completely from the information acquisition level of the sensor network. However, in this case the question arises if the information obtained is good enough for the application. In this paper we describe the possible metrics of information quality (IQ) in the sensor network. We present a framework which addresses the problem of satisfying the IQ in the case of a dynamic system with resource constraints and communication losses. The framework is based on the Dynamic Bayesian Network model. The framework is built on a base of a constraint optimization problem which takes into account all the levels of information processing, from measurement to aggregation to data delivery by the network.;2007;Andrei Tolstikov;10.1109/ICICS.2007.4449835;Conferences;;978-1-4244-0983-9
ieee_20221205_08_24_39;Channel Quality Variation as a Design Consideration for Wireless Data Link Protocols;The data link protocol HDL+, which has been proposed for incorporation into NATO STANAG 4538, employs an innovative combination of Type II Hybrid-ARQ techniques with real-time adaptation of signal constellation and code rate to achieve high throughput performance under a wide variety of channel conditions. The ionospheric HF channels for which HDL+ was designed exhibit important variations in channel quality (SNR, fading and multipath chcracteristics) at a variety of time scales from seconds to minutes. It appears plausible that the design characteristics of the HDL+ protocol couldprove especially valuable in coping with time-varying channel quality over these time scales. In this paper, we • Present and discuss a model of channel quality variation and a way of incorporating it into an ionospheric channel simulator suitable for performance characterization of HF communications waveforms and protocols • Provide overviews of HDL+ and of the STANAG 5066 data link protocol, a widely-used protocol based on conventional (not hybrid) ARQ techniques • Present and discuss comparative performance data for the HDL+ and STANAG 5066 protocols obtained under test conditions including channel quality variation.;2007;William Batts;10.1109/MILCOM.2007.4455231;Conferences;2155-7586;978-1-4244-1513-7
ieee_20221205_08_24_39;Implications of Reduced Measurement Data Sets on overall Microwave Tomographic Image Quality;Microwave tomographic approaches generally acquire measurement data from multi-directional field illuminations and reconstruct the images into dielectric property maps, or images, of the target zone. In general, this inversion process is ill-posed and ill-conditioned. Conventional wisdom suggests that when the amount of data collected can be increased, the image quality will be improved. Of course, there are often real costs involved in acquiring more data - especially with respect to the system hardware design. In fact, previous singular value decomposition studies by our group have indicated the returns from such added costs diminish rapidly with the number of antennas after a certain point. In addition, by implementing our log-magnitude/phase reconstruction algorithm, we have begun examining the actual pieces of measurement data to explore which pieces contain more valuable information. These experiments may have important implications for our reconstruction strategies and also open up new imaging opportunities.;2007;P.M. Meaney;10.1049/ic.2007.1311;Conferences;0537-9989;978-0-86341-842-6
ieee_20221205_08_24_39;Maximising Data Return: Towards a quality control strategy for Managing and Processing TRDI ADCP Data Sets from Moored Instrumentation;In this paper, we evaluate, by means of case studies, methods for quality controlling data obtained from Teledyne RDI ADCP measurements (ensembles and bins or depth cells) such that the data can be checked and acceptance (pass or fail) criteria established. The QA strategy applied utilizes the quality control criteria recorded for each beam and bin by the Teledyne RDI ADCP itself and which are stored within the binary data structure of the recorded file. These stored parameters include the 'Percentage Good Pings', ' Number of 3/4 Beam Solutions' and 'Correlation Magnitude' obtained from the Broadband Signal Processing. 'Error Velocity' values reported from comparison of the two independent vertical velocity measurements from each of the ADCPs beam pairs are also used, either as a percentage of the overall magnitude of current velocity, or as absolute values specific to the instrument deployment. Using a relational database approach to the problem of managing and manipulating current profiler data a quality parameter for each measurement bin may be assigned as the data is imported, and Standard Query Language (SQL) is subsequently used as a means of manipulating the data-set, allowing time-series to be extracted for a particular vertical window or temporal extent, subject to a quality threshold. In this paper we test two such QA strategies the first which weights the error velocity of each measurement bin and a second tiered QA strategy using a series of test data sets to evaluate the performance and ease of implementation of the two approaches.;2008;J. A. Taylor;10.1109/CCM.2008.4480848;Conferences;2160-7176;978-1-4244-1486-4
ieee_20221205_08_24_39;A Computing Model for Marketable Quality and Profitability of Corporations: Model Evaluation Based on Two Different Sources Data;In this paper, we introduce and evaluate a computing model for marketable quality and profitability of corporations. We discuss the model prediction of the turning and transition periods based on data from two different sources. By applying these real data of some leading manufacturing corporations in Japan, we analyze the model accuracy. From the analysis, we conclude that even there are some differences between two sources data, the proposed model give a good approximation and prediction of the turning and transition periods of Japanese economy.;2008;Valbona Barolli;10.1109/AINA.2008.59;Conferences;2332-5658;978-0-7695-3095-6
ieee_20221205_08_24_39;A Rate Control Method for Subjective Video Quality on Mobile Data Terminals;This paper proposes a rate control method providing the maximum subjective video quality using the minimum transmission rate for streaming video services on mobile data terminals. At that time, because user perceptive video quality differs greatly with video contents, we need to consider the control method with reference to them. In this paper, we conduct subjective assessment and analyze the relationship between video content and subjective video quality. The proposed scheme makes it possible to provide high-quality streaming services under the condition of using limited fixed-size monitors.;2008;Yuka Kato;10.1109/WAINA.2008.269;Conferences;;978-0-7695-3096-3
ieee_20221205_08_24_39;Quality-Aware Retrieval of Data Objects from Autonomous Sources for Web-Based Repositories;The goal of this paper is to develop a framework for designing good data repositories for Web applications. The central theme of our approach is to employ statistical methods to predict quality metrics. These prediction quantities can be used to answer important questions such as: How soon should the local repository be synchronized to have a quality of at least 90% precision with certain confidence level? Suppose the local repository was synchronized three days ago, how many objects could have been deleted at the remote source since then?;2008;Houtan Shirani-Mehr;10.1109/ICDE.2008.4497600;Conferences;2375-026X;978-1-4244-1837-4
ieee_20221205_08_24_39;The Scanning Data Collection Strategy for Enhancing the Quality of Electrical Impedance Tomography;This paper addresses a scanning data collection strategy in electrical impedance tomography (EIT) to enhance the quality of an impedance image by expanding the electrode number. This scanning EIT (SEIT) system rotates the electrode pairs at a small angle, and then, the measurement electrodes can scan around the circumference of a phantom tank. The numerical simulations examine the reconstructed result by using a cylindrical model with two conductivity varieties. The experimental results illustrate the reconstruction images with and without the SEIT from 2-D real measurement data. Compared with conventional EIT images, the images reconstructed from the scanning data collection strategy exhibit high resolution and are clearer. This paper provides a feasible configuration to reduce the noise and improve the resolution of the impedance image.;2008;Cheng-Ning Huang;10.1109/TIM.2007.915149;Journals;1557-9662;
ieee_20221205_08_24_39;Online power quality disturbances identification based on data stream technologies;Power quality disturbances identification is the important procedure for improving the power quality, and online application has actual value. An efficient method for power quality disturbances identification is presented in this paper. Wavelet decomposition is used for extracting the features of various disturbances, and decision tree in data mining is used for identifying the disturbances. For online application, sliding window model and one-pass scan algorithms for wavelet decompositions are used. This method has low cost in memory and run time, it can identify different disturbances in high accuracy and less time. Simulation experiment using several typical disturbances, swell, sag, interrupt, harmonic, transient impulse, transient oscillation, show the effectiveness of proposed method.;2007;Yinghui Kong;;Conferences;1947-1270;978-981-05-9423-7
ieee_20221205_08_24_39;A Contemporary Technique to Guarantee Quality of Service (QoS) for Heterogeneous Data Traffic;The upcoming high-speed networks are expected to support a wide variety of  real-time multimedia applications. However, the current Internet architecture offers mainly best-effort service and does not meet the requirements of future integrated services networks that will require guarantee for transferring heterogeneous data. There are many parameters involve in improving the performance of a computer network such as reliability, delay, jitter, bandwidth, etc. These parameters together determine the Quality of Service (QoS). The requirements of the above parameters will vary from one application to another application. Applications like file transfer, remote login, etc., will require high reliability. But, applications like audio, video, etc., will require low reliability, because they can tolerate errors. The objectives of this paper are to propose a technique to store the results of a data transfer in binary based on the above parameters, to compare the expected requirements with the actual requirements, to show performance degradation and to suggest ideas to minimize differences between expected requirements and actual requirements. Ultimately, the outcome of this paper will give better results to improve the performance of the network.;2008;Calduwel P. Newton;10.1109/ISA.2008.14;Conferences;;978-0-7695-3126-7
ieee_20221205_08_24_39;Data Quality and Query Cost in Pervasive Sensing Systems;This research is motivated by large-scale pervasive sensing applications. We examine the benefits and costs of caching data for such applications. We propose and evaluate several approaches to querying for, and then caching data in a sensor field data server. We show that for some application requirements (i.e., when delay drives data quality), policies that emulate cache hits by computing and returning approximate values for sensor data yield a simultaneous quality improvement and cost savings. This win-win is because when system delay is sufficiently important, the benefit to both query cost and data quality achieved by using approximate values outweighs the negative impact on quality due to the approximation. In contrast, when data accuracy drives quality, a linear trade-off between query cost and data quality emerges. We also identify caching and lookup policies for which the sensor field query rate is bounded when servicing an arbitrary workload of user queries. This upper bound is achieved by having multiple user queries share the cost of a single sensor field query. Finally, we demonstrate that our results are robust to the manner in which the environment being monitored changes using models for two different sensing systems.;2008;David Yates;10.1109/PERCOM.2008.117;Conferences;;978-0-7695-3113-7
ieee_20221205_08_24_39;A power-quality event data compression algorithm based on advanced Support Vector Machine;An algorithm using SVM (Support Vector Machine) regression for Power-Quality (PQ) event data compression is presented. the advanced SVM regression can learn dependency from an array of wavelet coefficients which is 2-D representation of PQ event data decomposed by 2-D discrete-time wavelet transform (2-D DWT), using fewer Support Vectors (SV) to represent the original data, thus, the data could be compressed based on this feature. Experiment results show the good performance of proposed algorithm in PQ event data compression, comparing with traditional SVM compression data under the same conditions.;2008;Wei-Yan Zheng;10.1109/DRPT.2008.4523751;Conferences;;978-7-900714-13-8
ieee_20221205_08_24_39;DPSK Data Quality Dependencies in Microring-Based Transmitter and Receiver;Ultra-small DPSK (de)modulators at 10 Gb/s, using silicon-based microrings, are proposed. Data quality of the microring-based DPSK is greatly dependent on operation conditions that can be optimized to improve eye-opening by up-to-7-dB.;2008;Lin Zhang;10.1109/OFC.2008.4528046;Conferences;;978-1-55752-588-1
ieee_20221205_08_24_39;A Methodology for Information Quality Assessment in Data Warehousing;This paper presents a methodology to determine two IQ characteristics-accuracy and comprehensiveness-that are of critical importance to data warehousing. This methodology can examine how the quality metrics of source information affect the quality for information outputs produced using the relational algebra operations selection, projection, and Cubic product. It can be used to determine how quality characteristics associated with diverse data sources affect the quality of the derived data. The study resulted in the development of a model of a data cube and an algebra to support IQ Assessment operations on this cube.;2008;Y. Su;10.1109/ICC.2008.1035;Conferences;1938-1883;978-1-4244-2075-9
ieee_20221205_08_24_39;Complete video quality preserving data hiding with reversible functionality;This paper proposes a novel data hiding method in the MPEG domain where the image quality of the modified video is completely preserved to that of the original (compressed) video. To our best knowledge, there is no data hiding method that completely preserves the video quality after data embedding, and this method is the first attempt of its kind. This method is also reversible where the modifications done during data embedding could be undone to restore the original video. This method is applicable not only to existing MPEG1/2 encoded video streams but also to the encoding process of MPEG video from sequence of raw pictures. The problem of filesize increase as a result of data embedding is addressed, and three independent solutions are presented to suppress the filesize increase while trading off with payload and coding efficiency. Basic performance of this method is verified through experiments on various existing MPEG1 encoded videos.;2008;KokSheik Wong;10.1109/ISCCSP.2008.4537375;Conferences;;978-1-4244-1688-2
ieee_20221205_08_24_39;The variation of power quality indices due to data analysis procedure;Power quality data is often reported using statistical confidence levels. This will exclude the most extreme data for a certain length of time depending on the interval over which the confidence level is applied. There is considerable conjecture as to the effect of applying statistical measures over different time intervals, e.g. several days, weeks or one year. If statistical confidence levels are applied over long intervals, the length of time not included in the statistical confidence interval is long. During such intervals disturbance levels may be continuously high and not be accounted for in the statistical parameter. This study investigates the effect different methods of aggregating data to a specific reporting period will have on the calculated index. Several data processing methods are trialled to evaluate the effect of using different aggregation intervals to produce an index to characterise disturbance levels for the whole year.;2007;Sean Elphick;10.1109/AUPEC.2007.4548030;Conferences;;978-0-646-49499-1
ieee_20221205_08_24_39;Data Quality in Traditional Chinese Medicine;Data quality is a key issue in medical informatics and bioinformatics. Although many researches could be found that discuss data quality in the area of health care and medicine, few literature exist that particularly focuses on data quality in the field of traditional Chinese medicine (TCM). Due to the high domain-specificity of data quality, it is of essential necessity to identify key dimensions of data quality in TCM. In this paper, based on TCM practice in past years, three data quality aspects are highlighted as key dimensions, including representation granularity, representation consistency, and completeness. Moreover, practical methods and techniques to handle data quality problems in these dimensions are also provided, showing how to enhance data quality in TCM field.;2008;Yi Feng;10.1109/BMEI.2008.268;Conferences;1948-2922;978-0-7695-3118-2
ieee_20221205_08_24_39;A High Efficient Quality Control Strategy for Wavelet-Based ECG Data Compression System;Maintaining retrieved signal with desired quality is crucial for ECG data compression. In this paper, a high efficient quality control strategy is proposed for wavelet-based ECG data compression. The strategy is based on a modified non-linear quantization scheme that can obtain a linear distortion behavior with respective to a control variable. The linear distortion characteristic supports the design of a linear control variable prediction algorithm. By using the MIT-BIH arrhythmia database, the experimental results show that the linear control variable prediction method can effectively improve the convergence speed than the previous literatures.;2008;Cheng-Tung Ku;10.1109/BMEI.2008.184;Conferences;1948-2922;978-0-7695-3118-2
ieee_20221205_08_24_39;Information Quality in Healthcare: Coherence of Data Compared between Organization's Electronic Patient Records;In this paper we present a case-based analysis of health care data quality problems in a situation, where data of diabetes patient are combined from different information systems. Nationally uniform integrated health care information systems shall become more important when meeting the demands of patient centered care in the future. During the development of several electronic health records it has become clear that the integration of the data is still challenging. Data collected in various systems can have quality faults, it can for instance be non-coherent or include contradictory information, or the desired data is completely missing, as proved to be in our case as well. The quality of the content of patient information and the process of data production constitute a central part of good patient care, and more attention should be paid to them.;2008;Merja Miettinen;10.1109/CBMS.2008.64;Conferences;1063-7125;978-0-7695-3165-6
ieee_20221205_08_24_39;The application study of ERP data quality assessment and improvement methodology;The problem of ERP data quality is studied and the model of ERP data quality assessment and improvement is established. The problem of ERP data quality can be detected and improved effectively by measuring ERP data quality. Finally, ERP data quality assessment and improvement methodology is verified by case study.;2008;Zhao Xiaosong;10.1109/ICIEA.2008.4582673;Conferences;2158-2297;978-1-4244-1718-6
ieee_20221205_08_24_39;Identifying learners robust to low quality data;Real world datasets commonly contain noise that is distributed in both the independent and dependent variables. Noise, which typically consists of erroneous variable values, has been shown to significantly affect the classification performance of learners. In this study, we identify learners with robust performance in the presence of low quality (noisy) measurement data. Noise was injected into five class imbalanced software engineering measurement datasets, initially relatively free of noise. The experimental factors considered included the learner used, the level of injected noise, the dataset used (each with unique properties), and the percentage of minority instances containing noise. No other related studies were found that have identified learners that are robust in the presence of low quality measurement data. Based on the results of this study, we recommend using the random forest learner for building classification models from noisy data.;2008;Andres Folleco;10.1109/IRI.2008.4583028;Conferences;;978-1-4244-2660-7
ieee_20221205_08_24_39;Signal Quality Measurements for cDNA Microarray Data;Concerns about the reliability of expression data from microarrays inspire ongoing research into measurement error in these experiments. Error arises at both the technical level within the laboratory and the experimental level. In this paper, we will focus on estimating the spot-specific error, as there are few currently available models. This paper outlines two different approaches to quantify the reliability of spot-specific intensity estimates. In both cases, the spatial correlation between pixels and its impact on spot quality is accounted for. The first method is a straightforward parametric estimate of within-spot variance that assumes a Gaussian distribution and accounts for spatial correlation via an overdispersion factor. The second method employs a nonparametric quality estimate referred to throughout as the mean square prediction error (MSPE). The MSPE first smoothes a pixel region and then measures the difference between actual pixel values and the smoother. Both methods herein are compared for real and simulated data to assess numerical characteristics and the ability to describe poor spot quality. We conclude that both approaches capture noise in the microarray platform and highlight situations where one method or the other is superior.;2010;Tracy L. Bergemann;10.1109/TCBB.2008.72;Journals;2374-0043;
ieee_20221205_08_24_39;An Advanced Quality Control System for the CEOP/CAMP In-Situ Data Management;The Coordinated Enhanced Observing Period (CEOP) was proposed in 1997 as an initial step for establishing an integrated observation system for the global water cycle. The Enhanced Observing Period was conducted from October 2002 to December 2004, with satellite data, in-situ data, and model output data collected and available for integrated analysis. Under the framework of CEOP, the CEOP Asia-Australia Monsoon Project (CAMP) was organized and provided the in-situ dataset in the Asian region. CAMP included 13 different reference sites in the Asian monsoon region during Phase 1 (October 2002 to December 2004). These reference sites were operated by individual researchers for their own research objectives. Therefore, the various sites' data had important differences in observational elements, data formats, recording intervals, etc. This usually requires substantial manual data processing to use these data for scientific research which consumes a great deal of researcher time and energy. To reduce the time and effort for data quality checking and format conversion, the CAMP Data Center (CDC) established a Web-based quality control (QC) system. This paper introduces this in-situ data management and quality control system for the Asian region data under the framework of CEOP.;2008;Katsunori Tamagawa;10.1109/JSYST.2008.927710;Journals;2373-7816;
ieee_20221205_08_24_39;Quality measurement for transmitted audio data using distribution of sub-band signals;Recently, the remarkable progress of network technology has increased the requirement for transmission of high quality multimedia data. By the trend, it has been issued to investigate an efficient methodology for quality measurement of transmitted multimedia data. In this paper, we propose a new audio quality measurement technique to substitute for a typical quality measurement tool, RMSE (Root Mean Squared Error). The proposed method modifies the variance of sub-band signals to perform the estimation of audio quality at the transmitter, the receiver is able to estimate the quality distortion of transmitted audio data by calculating the distance between the variance and the reference value representing the characteristics of sub-band signals, so called EVE (Estimated Variance Error). The proposed is as no reference technique, it does not require the original data to measure the audio quality. On the Gaussian noise channel with several standard deviations, we prove that the proposed scheme has good performance, and it is a novel alternative to RMSE.;2008;D.C. Nguyen;10.1109/RIVF.2008.4586350;Conferences;;978-1-4244-2380-4
ieee_20221205_08_24_39;A quality-threshold data summarization algorithm;As database sizes increase, semantic data summarization techniques have been developed, so that data mining algorithms can be run on the summarized set for the sake of efficiency. Clustering algorithms such as K-Means have popularly been used as semantic summarization methods where cluster centers become the summarized set. The goal of semantic summarization is to provide a summarized view of the original dataset such that the summarization ratio is maximized while the error (i.e., information loss) is minimized. This paper presents a new clustering-based data summarization algorithm, in which the quality of the summarized set can be controlled. The algorithm partitions a dataset into a number of clusters until the distortion of each cluster is less than a given threshold, thus guaranteeing the summarized set has less than a fixed amount of information loss. Based on the threshold, the number of clusters is automatically determined. The proposed algorithm, unlike traditional K-Means, adjusts initial centers based on the information about the data space discovered so far, thus significantly alleviating the local optimum effect. Our experiments show that our algorithm generates higher quality clusters than K-Means does and it also guarantees an error bound, an essential criterion for data summarization.;2008;Viet Ha-Thuc;10.1109/RIVF.2008.4586362;Conferences;;978-1-4244-2380-4
ieee_20221205_08_24_39;Evaluating Frequency Quality of Nordic System using PMU data;This paper focuses on analysing frequency quality of Nordic power system using measurements from phasor measurement units (PMU). The PMU data of one year long period is used which has very high time resolution (20 ms per sample) and is able to provide detailed information in evaluating frequency quality and its correlation with time. The results show that the frequency quality of the Nordic power system is not satisfactory according to the suggested requirements. The electricity market operation is found to be one of the major reasons behind. Based on the results, discussion of frequency control for future power system is made where several new technologies of interests are suggested to be investigated.;2008;Zhao Xu;10.1109/PES.2008.4596468;Conferences;1932-5517;978-1-4244-1906-7
ieee_20221205_08_24_39;A Computing Model for Marketable Quality and Profitability of Corporations: A Case Study Evaluation Using a New Sources Data;In this paper, we introduce and evaluate a computing model for marketable quality and profitability of corporations. We discuss the model prediction of the turning and transition period based on a new source data. By applying the real data of some leading manufacturing corporations in Japan we analyze the model accuracy. The analysis results show the proposed model give a good approximation and prediction of the turning and transition period of Japanese economy. By using the proposed model, we can obtain a boundary between sellers' and buyers' market.;2008;Heihachiro Fukuda;10.1109/CISIS.2008.82;Conferences;;978-0-7695-3109-0
ieee_20221205_08_24_39;An efficient data representation scheme for complete video quality preserving data hiding;This paper proposes an efficient data representation scheme to improve the performance of a data hiding method [1] in MPEG compressed domain. Even though [1] completely preserves the quality of the modified video to that of the original (compressed) video and [1] is reversible, [1] suffers from consistent filesize increase caused by data embedding. To suppress filesize increase, reverse zerorun length (RZL) is proposed to efficiently encode the message. RZL utilizes the statistics of the macroblocks with respect to [1], and the distance between two excited macroblocks is considered to encode a message segment. RZL simultaneously achieves high payload and high embedding efficiency, thus RZL is able to suppress the filesize increase caused by data embedding. We theoretically analyzed that RZL outperformsmatrix encoding for both payload and embedding efficiency for this particular data hiding method. Experiments are also carried out to verify the theoretically deduced results, and the observed results agree with the expected outcomes.;2008;KokSheik Wong;10.1109/ICME.2008.4607584;Conferences;1945-788X;978-1-4244-2571-6
ieee_20221205_08_24_39;Applicative solution for testing the quality of data services in mobile networks;Mobile operators nowadays have plenty of data services, since the introduction of GPRS and this number is increasing towards future 3G LTE mobile networks. There are IP-based services which were migrated from wired to mobile networks, such as HTTP, FTP and E-mail. But, also mobile specific services were created such as WAP and MMS. On the other side, behavior of mobile services is influenced from the environment, the terrain, interference, from protocols on different OSI layers, from user behavior, etc. Therefore, mobile operators need to examine their network and be assured that services are performing well. In this paper we describe an applicative solution for service quality testing system (SQTS), which provides efficient way for testing of all data services, including bearer as well as IP-based services in mobile networks from the user point of view, through a defined set of Key Performance Indicators. We define all SQTS functionalities as well as all needed networks nodes for its functioning. The proposed system is robust and scalable regarding definition of new services that will appear in future.;2008;Toni Janevski;10.1109/MELCON.2008.4618437;Conferences;2158-8481;978-1-4244-1633-2
ieee_20221205_08_24_39;A hybrid data mining approach to quality assurance of manufacturing process;Quality assurance (QA) is a process employed to ensure a certain level of quality in a product or service. One of the techniques in QA is to predict the product quality based on the product features. However, traditional QA techniques have faced some drawbacks such as heavily depending on the collection and analysis of data and frequently dealing with uncertainty processing. In order to improve the effectiveness during a QA process, a hybrid approach incorporated with data mining techniques such as rough set theory (RST), fuzzy logic (FL) and genetic algorithm (GA) is proposed in this paper. Based on an empirical case study, the proposed solution approach provides great promise in QA.;2008;Chun-Che Huang;10.1109/FUZZY.2008.4630465;Conferences;1098-7584;978-1-4244-1819-0
ieee_20221205_08_24_39;A comparative study of RBF neural network and SVM classification techniques performed on real data for drinking water quality;The control and monitoring of drinking water is becoming more and more interesting because of its effects on human life. Many techniques were developed in this field in order to ameliorate this process control attending to rigorous follow-ups of the quality of this vital resource. Several methods were implemented to achieve this goal. In this paper, a comparative study of two techniques resulting from the field of the artificial intelligence namely: RBF neural network (RBF-NN) and support vector machine (SVM), is presented. Developed from the statistical learning theory, these methods display optimal training performances and generalization in many fields of application, among others the field of pattern recognition. Applied as classification tools, these techniques should ensure within a multi-sensor monitoring system, a direct and quasi permanent control of water quality. In order to evaluate their performances, a simulation using real data, corresponding to the recognition rate, the training time, and the robustness, is carried out. To validate their functionalities, an application is presented.;2008;Mohamed Bouamar;10.1109/SSD.2008.4632856;Conferences;;978-1-4244-2206-7
ieee_20221205_08_24_39;Evolutive complex scheduling in interaction networks for quality improvment in geographical data base updating;The aim of this paper is to propose a scheduling processes for the mechanism of geographical data bases updating in term of improvement of its quality. Because of the complex interacting network of the components involved in each updating cycle of this mechanism, a traditional analytic solver cannot success in most cases. So we propose to use an evolutionary computation based on a genetic algorithm which is well adapted for the complex interaction network of the involved components. A genetic algorithm may allow to improve the updating quality in increasing the validation rate of a transaction.;2005;Hakima Kadri-Dahmani;10.1109/ICECS.2005.4633616;Conferences;;978-9972-61-100-1
ieee_20221205_08_24_39;TiSeG: A Flexible Software Tool for Time-Series Generation of MODIS Data Utilizing the Quality Assessment Science Data Set;Time series generated from remotely sensed data are important for regional to global monitoring, estimating long-term trends, and analysis of variations due to droughts or other extreme events such as El Nintildeo. Temporal vegetation patterns including phenological states, photosynthetic activity, or biomass estimations are an essential input for climate modeling or the analysis of the carbon cycle. However, long-term analysis requires accurate calibration and error estimation, i.e., the quality of the time series determines its usefulness. Although previous attempts of quality assessment have been made with NOAA-AVHRR data, a first rigorous concept of data quality and validation was introduced with the MODIS sensors. This paper presents the time-series generator (TiSeG), which analyzes the pixel-level quality-assurance science data sets of all gridded MODIS land (MODLand) products suitable for time-series generation. According to user-defined settings, the tool visualizes the spatial and temporal data availability by generating two indices, the number of invalid pixels and the maximum gap length. Quality settings can be modified spatially and temporally to account for regional and seasonal variations of data quality. The user compares several quality settings and masks or interpolates the data gaps. This paper describes the functionality of TiSeG and shows an example of enhanced vegetation index time-series generation with numerous settings for Germany. The example indicates the improvements of time series when the quality information is employed with a critical weighting between data quality and the necessary quantity for meaningful interpolation.;2008;RenÉ R. Colditz;10.1109/TGRS.2008.921412;Journals;1558-0644;
ieee_20221205_08_24_39;Diagnostic quality driven physiological data collection for personal healthcare;We believe that each individual is unique, and that it is necessary for diagnosis purpose to have a distinctive combination of signals and data features that fits the personal health status. It is essential to develop mechanisms for reducing the amount of data that needs to be transferred (to mitigate the troublesome periodically recharging of a device) while maintaining diagnostic accuracy. Thus, the system should not uniformly compress the collected physiological data, but compress data in a personalized fashion that preserves the “important” signal features for each individual such that it is enough to make the diagnosis with a required high confidence level. We present a diagnostic quality driven mechanism for remote ECG monitoring, which enables a notation of priorities encoded into the wave segments. The priority is specified by the diagnosis engine or medical experts and is dynamic and individual dependent. The system pre-processes the collected physiological information according to the assigned priority before delivering to the backend server. We demonstrate that the proposed approach provides accurate inference results while effectively compressing the data.;2008;David Jea;10.1109/IEMBS.2008.4649798;Conferences;1558-4615;978-1-4244-1815-2
ieee_20221205_08_24_39;Analysis of data quality and information quality problems in digital manufacturing;This work focuses on the increasing importance of data quality in organizations, especially in digital manufacturing companies. The paper firstly reviews related works in field of data quality, including definition, dimensions, measurement and assessment, and improvement of data quality. Then, by taking the digital manufacturing as research object, the different information roles, information manufacturing processes, influential factors of information quality, and the transformation levels and paths of the data/information quality in digital manufacturing companies are analyzed. Finally an approach for the diagnosis, control and improvement of data/information quality in digital manufacturing companies, which is the basis for further works, is proposed.;2008;K. Q. Wang;10.1109/ICMIT.2008.4654405;Conferences;;978-1-4244-2330-9
ieee_20221205_08_24_39;Data-Centric Prioritization in a Cognitive Radio Network: A Quality-of-Service Based Design and Integration;"The under-/un-utilized radio spectrum is an area of emphasis and potential in an effort to increase utilization. This paper introduces a new matrix into the frequency evaluation/determination methodology. It will reduce the number of decision and network collisions in a cognitive radio network environment. This is achieved via a new paradigm, data centric prioritization (DCP), the exploitation of the unique relationship between the transmitted application data type and a ""true best fit"" cognitive radio frequency decision in a cognitive radio community cluster. Since application data types during normal wireless device operations vary from user to user and utility to utility, so will the network performance requirements. The network must be able to seamlessly support this diversity and a cognitive radio device is best suited for this task. The concept of associating application-specific design requirements with the network dynamics of the frequency spectrum lends itself to a quality-of-service (QoS) methodology. This paper delves into the usage of a QoS methodology within the cognitive radio cognition cycle.";2008;Urban Wiggins;10.1109/DYSPAN.2008.62;Conferences;;978-1-4244-2016-2
ieee_20221205_08_24_39;A New Field-Data Based EAF Model for Power Quality Studies;A new electric arc furnace (EAF)-specific model based on field measurements of instantaneous furnace voltages and currents has been proposed. This model presents the dynamic behavior of the EAF system including all parts, i.e. the EAF transformer, the secondary circuit, the electrode movements and the arc. It consists of a cascade connected variable-resistance and -inductance combination to represent the time variation at the fundamental frequency, and a current source in parallel with it to inject the harmonics and interharmonics of the EAF current. The model takes into account several typical tap-to-tap periods of the specific EAF operation. This model is especially advantageous for power quality (PQ) analysis, and development of FACTS solutions for PQ problem mitigation of a given busbar supplying single- or multi-EAF installations. The validity of the proposed model has been verified by comparing EMTDC/PSCAD simulations of the model with the field measurements. The results obtained have shown quite satisfactory correlation between the proposed model and the actual EAF operation.;2008;Murat Gol;10.1109/08IAS.2008.335;Conferences;0197-2618;978-1-4244-2279-1
ieee_20221205_08_24_39;A quality-of-information-aware framework for data models in wireless sensor networks;Wireless sensor networks are used to monitor a given environment, such as indoor heating conditions or the micro-climate of glaciers. They offer a low-cost solution that provides a high data density. Usually the user of such a sensor network has a good idea of how, knowing the environment, the sensed values should behave. This idea can be expressed as a data model. Such models can be used to detect anomalies, compress data, or combine data from many inexpensive sensors to increase the quality of the measurements. This paper presents a framework to process arbitrary sensor-network data models. The framework can then be used to distribute the model processing into the wireless sensor network. Quality of information criteria are used to determine the performance of the models. A prototype of the framework is presented together with a comparison of two existing stochastic data model approaches for wireless sensor networks.;2008;Urs Hunkeler;10.1109/MAHSS.2008.4660118;Conferences;2155-6814;978-1-4244-2575-4
ieee_20221205_08_24_39;Data quality driven sensor reporting;Within the field of event driven data reporting from wireless sensor networks, reducing energy consumption is an ongoing problem. Using an applicationpsilas tolerance toward data imprecision (or, data quality) allows energy savings, via fewer messages sent, through the selection of how and when to send sensor readings. This paper is an early work that examines pushing or pulling data depending on which approach is expected to send fewer messages, based upon the recent history of application requests for a sensor and the changes of the sensor values predicted by application specific models. The simulation results indicate that our method is more efficient relative to the push only or pull only methods for situations where application request frequency or data change rate is variable or unknown.;2008;Doug Hakkarinen;10.1109/MAHSS.2008.4660121;Conferences;2155-6814;978-1-4244-2575-4
ieee_20221205_08_24_39;Towards data warehouse business quality through requirements elicitation;Data warehouses are mainly used to support decision-making based on the analysis of highly heterogeneous sources to extract, transform and aggregate data, as well as facilitating ad-hoc queries that retrieve the decisional information. Data warehouse development involves many knowledge-intensive activities, of which requirements elicitation is recognized as being crucial and difficult to model. This paper adapts the data warehouse requirements elicitation process, namely informational scenarios, to incorporate business quality at the requirements engineering level of the DW development. To accomplish this, we look at DW business quality mainly from the context of changing economic factors and environmental concerns.;2008;Anjana Gosain;10.1109/ICADIWT.2008.4664394;Conferences;;978-1-4244-2624-9
ieee_20221205_08_24_39;Achieving data warehouse quality using GDI approach;Data warehouses are complex systems that have to deliver highly-aggregated data from heterogeneous sources to decision makers. It is essential that we can assure the quality data warehouse in terms of data as well as the services provided by it. Therefore, we should use methods, models, techniques and tools to help us in designing and maintaining high quality DWs. In this paper we outline a general methodological framework for the quality of data warehouse systems, adapting the goal-decision-information approach. This model considers the quality of conceptual models to reach at specific decisions in data warehousing environment.;2008;Anjana Gosain;10.1109/ICADIWT.2008.4664399;Conferences;;978-1-4244-2624-9
ieee_20221205_08_24_39;Application of Attribute Recognition Model Based on Coefficient of Data-Driven to Evaluation Water Quality;In order to overcome the weakness of subjectivity in determining weight coefficient of fuzzy comprehensive evaluation method, an attributes recognition method based on coefficient of data-driven is built up, and the weight is determined by the data-mining thinking through comprehensive utilization of classification criteria information and sample information. Utilization of this model to evaluate the water quality of Fuqiao River reservoir verifies its feasibility. Through comparison with traditional fuzzy synthetic evaluation method and attribute recognition model based on coefficient of entropy, the attribute recognition model based on coefficient of data-driven can get more objective evaluation results.;2008;Zhihong Zou;10.1109/FSKD.2008.484;Conferences;;978-0-7695-3305-6
ieee_20221205_08_24_39;Application of Clustering Methods for Analysing of TTCN-3 Test Data Quality;"The use of the standardised testing notation, testing and test control notation (TTCN-3) language has increased continuously over the last years. Many test suites of large sizes covering different domains exist. Therefore, it becomes important to provide the TTCN-3 community with methods and tools to evaluate the quality of tests. This paper presents the idea of evaluating the quality of the test data stimuli by using a data clustering method and measuring the coverage related to data clusters. A cluster contains stimuli which are considered similar for the system under test (SUT) behaviour; that means that each stimuli within a cluster should provide similar results from the test point of view.";2008;Diana Vega;10.1109/ICSEA.2008.44;Conferences;;978-0-7695-3372-8
ieee_20221205_08_24_39;Factor analysis of power quality variation data on a distribution network;Continuous power quality (PQ) surveys of electricity networks generate large amounts of data that must be condensed for the purpose of interpretation and reporting. In this paper, summary indices for continuous PQ disturbances are calculated from distribution network data, and the relationship between these indices and the known physical characteristics of the site is investigated. Results from this survey prompted further analysis into the relationship between voltage harmonic distortion (THDv) levels and variation in load characteristics across the monitored sites. Accepted explanations for variation in THDv are evaluated against the observed levels and alternative explanations are proposed.;2008;G. Nicholson;10.1109/ICHQP.2008.4668769;Conferences;2164-0610;978-1-4244-1770-4
ieee_20221205_08_24_39;An extension of the relational model to intuitionistic fuzzy data quality attribute model;The model we suggest makes the data quality an intrinsic feature of an intuitionistic fuzzy relational database. The quality of the data is no more determined by the level of user complaints or ad hoc sql queries prior to the data load but it is stored explicitly in relational tables and could be monitored and measured regularly. The quality is stored on an attribute level basis in supplementary tables to the base user ones. The quality is measured along preferred quality dimensions and is represented by intuitionistic fuzzy degrees. To consider the preferences of the user with respect to the different quality dimensions and table attributes we create additional tables that contain the weight values. The user base tables are not intuitionistic fuzzy but we have to use an intuitionistic fuzzy RDBMS to represent and manipulate data quality measures.;2008;Diana Boyadzhieva;10.1109/IS.2008.4670520;Conferences;1941-1294;978-1-4244-1740-7
ieee_20221205_08_24_39;The research and development of data acquisition card for on-line monitoring system of power quality based on Windows CE;According to the requirements of stability and real-time in on-line monitoring system of power quality, a data acquisition card is designed based on TMS320C6713 and PCI9030, and use Windows CE embedded system as the software platform. In the process of data acquisition card driver development, we used the Jungopsilas driver development tool WinDriver. It provides user-mode library (API) access to hardware, avoids OS kernel calls and hardware operation, makes users quickly and efficiently develop a data acquisition card driver. The test results show the driver is excellent in operation with stable performance, various work indexes of data acquisition card meet the real-time requirements.;2008;Xiang-wu Yan;10.1109/ISIE.2008.4676916;Conferences;2163-5145;978-1-4244-1666-0
ieee_20221205_08_24_39;Query Quality of Service Management Based on Data Relationship over Real-Time Data Stream Systems;Many real-time applications and data services in distributed environments need to operate on continuous unbounded data streams with the development of large wired and wireless sensor network. Conventional one-time queries cannot be suitable to provide continues results as data and update stream into the system, and continuous queries represent a new paradigm for interacting with dynamically changing data. At the same time, many real-time applications have inherent timing constraints in their tasks, so providing data objects deadline guarantees for continuous queries over dynamic data streams is a challenging problem. A novel performance metric and quality of service management scheme continuous queries over dynamic data streams are proposed to guarantee system performance based on relationship of all updated data items. Experimental simulations demonstrate that the presented algorithm can guarantee the performance and decrease miss ratios of queries for dynamic workload fluctuations especially transient overloads.;2008;Jun Xiang;10.1109/WiCom.2008.1330;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_24_39;Modeling Information Quality Risk in Data Mining;Information quality (IQ) is a critical factor in the success of the data mining (DM). Therefore, it is essential to measure the risk of IQ in a data warehouse to ensure success in implementing DM. This paper presents a methodology to determine two IQ characteristics-accuracy and comprehensiveness-that are of critical importance to decision makers. This methodology can examine how the quality risks of source information affect the quality for information outputs produced using the relational algebra operations selection, projection, and Cubic product. It can be used to determine how quality risks associated with diverse data sources affect the quality of the derived data. The study resulted in the development of a model of a data cube and an algebra to support IQ risk operations on this cube. The model we present is simple and intuitive, and the algebra provides a means to concisely express complex DM queries.;2008;Ying Su;10.1109/WiCom.2008.2424;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_24_39;Improving Uncertain Data-Quality through Effective Use of Knowledge Base;Data quality issues have taken on increasing importance in recent years. In our research, we have discovered that data quality is uncertain because the perceived quality of the data is influenced by the task and that the same data may be viewed through two or more different quality lenses depending on the user and the user' task it is used for. In order to improve uncertain data-quality, we construct a knowledge base to treat with different expectations of data quality. This knowledge-based approach enhances the flexibility and extensibility of the application, and can improve uncertain data-quality according to the user's needs.;2008;Zhimao Huang;10.1109/WiCom.2008.2517;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_24_39;An Efficient Method of Data Quality using Quality Evaluation Ontology;In SOA (Service Oriented Architecture) and RTE (Real-Time Enterprise) environment, an assurance of data quality is important. Because we do not assure data accuracy among dynamic clustering data set. Traditional methodology for assuring data quality is data profiling and data auditing. However, that is needed lots of time and cost to analysis of metadata and business process for integrating system before evaluating data quality. In this paper, we propose an efficient methodology of assuring data quality with considering dynamic clustering data set. To extract evaluate rules for data quality, we use ontology that has meanings of each word in itself. We gain the relationship among word in ontology, and then make SQL to evaluate data accuracy, especially focused on data meaning.;2008;O-Hoon Choi;10.1109/ICCIT.2008.118;Conferences;;978-0-7695-3407-7
ieee_20221205_08_24_39;A High Quality Histogram Shifting Based Embedding Technique for Reversible Data Hiding;Reversible data hiding is a technique that embeds secret information into a host media without loss of host information. Ni et al.'s histogram shifting technique is a high-quality, reversible method for data embedding. However, their technique still suffers from undesirable distortion at low embedding rates and lack of a mechanism to control the stego-image quality, due to all pixels between the peak point and the minimum point have to be shifted one unit for data embedding. The proposed technique lowers the distortion performance at low embedding rates by scanning pixels from outer region toward the center of the host image, and chooses better locations for shifting histograms to embed data. The experimental results show that the proposed method significantly improves the quality of the stego image of histogram shifting technique, especially at low embedding rates.;2008;Wien Hong;10.1109/ISDA.2008.304;Conferences;2164-7151;978-0-7695-3382-7
ieee_20221205_08_24_39;Predicting Defect Content and Quality Assurance Effectiveness by Combining Expert Judgment and Defect Data - A Case Study;Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from his-torical projects. Due to the fact that many companies do not collect enough data for applying these approaches (es-pecially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method that combines commonly available measurement data and context-specific expert knowledge. To evaluate the method’s applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model pro-vides improved estimation accuracy when compared to applicable models based solely on data. The mean magni-tude of relative error (MMRE) determined by cross-validation is 29.6% compared to 76.5% obtained by the most accurate data-based model.;2008;Michael Kläs;10.1109/ISSRE.2008.43;Conferences;2332-6549;978-0-7695-3405-3
ieee_20221205_08_24_39;Towards a quality of service model for replicated data access;This paper introduces the notion of a quality of service model for read and write access to a replicated database or file system. The goal is to provide clients with a choice of service guarantees and to separate the specification of a client's needs from the mechanisms implemented to meet those needs. An analogy is drawn with the quality of service models being explored for communications protocols.<>;1995;D.B. Terry;10.1109/SDNE.1995.470455;Conferences;;0-8186-7092-4
ieee_20221205_08_24_39;An Efficient Method of Data Quality Evaluation Using Metadata Registry;This paper proposes MDRDP (Metadata Registry based on Data Profiling) to minimize the time and human resource for analyzing and extracting metadata as criteria standard for data profiling. MDRDP is based on MDR (Metadata Registry) which is used for an international standard of standardizing and managing metadata for information sharing in various fields. By MDRDP, we can evaluate data quality with authorize metadata using methodology of data profiling. MDR can guarantee the quality of metadata so that results of quality evaluation would improve.;2008;O-Hoon Choi;10.1109/ASEA.2008.46;Conferences;;978-0-7695-3432-9
ieee_20221205_08_24_39;The Development of Data Acquisition Card in Power Quality Monitoring System Based on Embedded System;In this paper, a data acquisition card in monitoring system of power quality is designed and realized according to system real-time performance and reliability requirements. It systematically introduces the hardware and software platforms, analyses device driver development of data acquisition card and its operation mechanism in embedded system. The test shows the driver is working stability, and work indexes meet the requirements.;2008;Xiang-wu Yan;10.1109/CSSE.2008.608;Conferences;;978-0-7695-3336-0
ieee_20221205_08_24_39;Reinforcing Records for Improving the Quality of Data Integration;In the data integration, the heterogeneity of sources leads to missing value and various expressions of the same value in records, which reduces the quality of data. In this paper, we propose a novel approach to reinforce records for the integrated data. By studying the functional dependency of attribute in schema of data integration, we discover the related attribute that determines the attribute with uncertain value. Then our approach exploits matching algorithms on the value of related attribute to associate different records. And the uncertain value will be reinforced with the consistent value in a certain record. We also propose algorithms for reinforcing dataset of data integration. The experiments based on the data of conference paper demonstrate the effectiveness and performance of our approach on improving the quality of data.;2008;Tiezheng Nie;10.1109/CSSE.2008.626;Conferences;;978-0-7695-3336-0
ieee_20221205_08_24_39;Comparison of Four Performance Metrics for Evaluating Sampling Techniques for Low Quality Class-Imbalanced Data;Erroneous attribute values can significantly impact learning from otherwise valuable data. The learning impact can be exacerbated by the class imbalanced training data. We investigate and compare the overall learning impact of sampling such data by using four distinct performance metrics suitable for models built from binary class imbalanced data. Seven relatively free of noise, class imbalanced software engineering measurement datasets were used. A novel noise injection procedure was applied to these datasets. We injected domain realistic noise into the independent and dependent (class) attributes of randomly selected instances to simulate lower quality measurement data. Seven well known data sampling techniques with the benchmark decision-tree learner C4.5 were used. No other related studies were found that have comprehensively investigated learning by sampling low quality binary class imbalanced data containing both independent and dependent corrupted attributes. Two sampling techniques (random undersampling and Wilson's editing) with better and more robust learning performances were identified. In contrast, all metrics concurred on the identification of the worst performing sampling technique (cluster-based oversampling).;2008;Andres Folleco;10.1109/ICMLA.2008.11;Conferences;;978-0-7695-3495-4
ieee_20221205_08_24_39;Quality control of fan beam scanning data processing with in vitro material;In vivo assessment of human body composition quantities from imaging/scanning systems in clinical research and public health has led to proliferation of indirect methods and techniques that deliver approximating values. The systems are based on morphological or biochemical models and assumptions. Since the development of dual-energy X-ray absorptiometry (DXA) for the bone density, mineral content and the detection of osteoporosis, it assesses lean tissue and fat also. This study conducts a quality control of DXA variables using direct dissection of 14 porcine hind legs as the criterion method. Results show good to excellent correlations between DXA and dissection data (r2 =0.75 to 0.99), but absolute indirect DXA and direct dissection values were significantly different (P<0.05). In addition DXA provides erroneous values for bone density and the data dimensions are morphological, not chemical values as claimed by the manufacturer. Validation and accuracy studies with intact whole bodies are advised.;2008;J.P. Clarys;10.1109/IEEM.2008.4737861;Conferences;2157-362X;978-1-4244-2630-0
ieee_20221205_08_24_39;Analysis of consumers’ requirements for data/information quality by using HOQ;Data/information quality (DQ/IQ*) has great impact on data consumers’ decisions. In order to provide high quality data/information, data consumers’ requirements for DQ/IQ have to be analyzed and identified. Right requirement identification is fundamental to data quality control activities including DQ/IQ measurement, evaluation, improvement, etc. Quality function deployment (QFD) and the house of quality (HOQ) are effective tools to translate consumers’ requirements into specific DQ dimensions for DQ improvement. This work briefly introduces QFD, HOQ and their constitutive elements. Data consumers are also examined. A methodology of applying HOQ in DQ/IQ, which includes five major steps, is described in details. Then an example of product design information quality is presented. By using the methodology, the weak points of DQ in industrial firms can be identified in the form of DQ dimensions in order to take actions to improve data/information quality.;2008;Keqin Wang;10.1109/IEEM.2008.4737862;Conferences;2157-362X;978-1-4244-2630-0
ieee_20221205_08_24_39;A Method for Measuring Data Quality in Data Integration;This paper reports our method on measuring data quality in data integration. Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. Data quality is crucial for operational data integration. We posit that data-integration need to handle the measure of data quality. So, measuring data quality in data integration is one of worthy research topics. This paper focuses on believability, a major aspect of quality. At first, the author analyzes the background and content of this paper, then description of dimensions of believability is given, and we present our approach for computing believability based on metadata, finally the summary and prospect are listed. In this method, we make explicit use of lineage-based measurements and develop a precise approach to measuring data quality.;2008;Lin Mo;10.1109/FITME.2008.146;Conferences;;978-0-7695-3480-0
ieee_20221205_08_24_39;Research on Information Quality Driven Data Cleaning Framework;Considering the limited extensibility and interactivity of the current data cleaning work, this paper proposes a new extensible and interactive data cleaning framework driven by information quality. The framework implements the formation of data quality analysis strategy, data transformation strategy and cleaning result assessment strategy. It also realizes the control of the cleaning process. The framework has the significant features of extensibility and interactivity.;2008;Hao Yan;10.1109/FITME.2008.126;Conferences;;978-0-7695-3480-0
ieee_20221205_08_24_39;Quantization Based Data Hiding Scheme for Efficient Quality Access Control of Images Using DWT via Lifting;This paper proposes a transform domain data-hiding scheme for quality access control of images. The original image is decomposed into tiles by applying n-level lifting-based discrete wavelet transformation (DWT). A binary watermark image (external information) is spatially dispersed using the sequence of number generated by a secret key. The encoded watermark bits are then embedded into all DWT-coefficients of nth-level and only in the high-high (HH) coefficients of the subsequent levels using dither modulation (DM) but without complete self-noise suppression. It is well known that due to insertion of external information, there will be degradation in visual quality of the host image (cover). The degree of deterioration depends on the amount of external data insertion as well as step size used for DM. If this insertion process is reverted, better quality of images can be accessed. To achieve that goal, watermark bits are detected using minimum distance decoder and the remaining self-noise due to information embedding is suppressed to provide better quality of image. The simulation results have shown the validity of this claim.;2008;Amit Phadikar;10.1109/ICVGIP.2008.23;Conferences;;978-0-7695-3476-3
ieee_20221205_08_24_39;Improved reconstructed image quality in a SPECT system with slit-slat collimation by combination of multiplexed and non-multiplexed data;The desirability of multiplexing in SPECT acquisition remains controversial. Allowing multiplexing is a way of optimizing system sensitivity, however multiplexing has been found to deteriorate the image quality due to the ambiguity in the origin of the projections. In this paper we explore the system designed specifically for brain SPECT based on multi slit-slat (MSS) collimator in which the slit spacing is varied to provide a mix of multiplexed (MX) and non-MX data. The degree of multiplexing for individual slits (DM) is fixed and we vary the fractional number of projections that are multiplexed (FMP). The results showed significant increase in Signal to noise ratio (SNR) due to the reduced noise level in the reconstructed images with insignificant effect on the recovery coefficient (RC) and spatial resolution up to high degrees of multiplexing and with less than 30% non-MX projections. We therefore conclude that multiplexing can be used to increase system sensitivity and therefore reduce noise in the image quality provided that mixed MX and non-MX data are acquired.;2008;Shelan Mahmood;10.1109/NSSMIC.2008.4774374;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_24_39;Online data quality monitoring tools of LHCb;LHCb is one of the four major experiments under completion at the Large Hadron Collider (LHC). One crucial aspect of the experiment is the online data quality monitoring, part of which project is the LHCb Histogram Presenter, a flexible tool dedicated to the visualisation of online histograms. Data quality monitoring is important, because it allows the verification of the detector performance: anomalies, such as missing values or unexpected distributions of summary statistical data can be indicators of a malfunctioning detector.;2008;O. Callot;10.1109/NSSMIC.2008.4774777;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_24_39;Commissioning of the ALICE-LHC online Data Quality Monitoring framework;ALICE is one of the experiments installed at CERN Large Hadron Collider, dedicated to the study of heavy-ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Data Quality Monitoring (DQM) is an important aspect of the online procedures for a HEP experiment. In this presentation we overview the commissioning and the integration of ALICE’s AMORE (Automatic MOnitoRing Environment), a custom-written distributed application aimed at providing DQM services in a large, experiment-wide scale.;2008;Filimon Roukoutakis;10.1109/NSSMIC.2008.4774928;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_24_39;Data quality monitor of the muon spectrometer tracking detectors of the ATLAS experiment at the Large Hadron Collider: First experience with cosmic rays;The Muon Spectrometer of the ATLAS experiment at the CERN Large Hadron Collider is completely installed and many data have been collected with cosmic rays in different trigger configurations. In the barrel part of the spectrometer, cosmic ray muons are triggered with Resistive Plate Chambers, RPC, and tracks are obtained joining segments reconstructed in three measurement stations equipped with arrays of high-pressure drift tubes, MDT. The data are used to validate the software tools for the data extraction, to assess the quality of the drift tubes response and to test the performance of the tracking programs. We present a first survey of the MDT data quality based on large samples of cosmic ray data selected by the second level processors for the calibration stream. This data stream was set up to provide high statistics needed for the continuous monitor and calibration of the drift tubes response. Track segments in each measurement station are used to define quality criteria and to assess the overall performance of the MDT detectors. Though these data were taken in not optimized conditions, when the gas temperature and pressure was not stabilized, the analysis of track segments shows that the MDT detector system works properly and indicates that the efficiency and space resolution are in line with the results obtained with previous tests with a high energy muon beam.;2008;M. Iodice;10.1109/NSSMIC.2008.4774959;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_24_39;Improving audio data quality and compression;High data quality at low bit rate is an essential goal that people want to achieve. It is necessary to transfer data at low bit rate so that the bandwidth of the medium can be utilized efficiently. In most of the speech coding techniques the goal of low bit rate transfer is achieved but the data quality is affected badly. The proposed technique is an attempt to improve the data quality at low bit rate as well as fast transmission of data. The proposed technique protects the data quality by applying Linear Predictive Coding-10 and achieves the low bit rate by applying Quadrature Mirror Filter. A comprehensive analysis is on the basis of given parameters as size, compression time, Signal to Noise Ratio, power, energy, power in air, energy in air, mean, standard deviation and intensity.;2008;Qaiser Naeem;10.1109/ICET.2008.4777524;Conferences;;978-1-4244-2211-1
ieee_20221205_08_24_39;Combining NSMS and High-Quality MPV-TD Data for UXO Discrimination;In this paper, a new physics-based approach for estimating a buried object's location and orientation is combined with the normalized surface magnetic source (NSMS) model to analyze high-quality, high-density multiaxis data provided by the Man-Portable Vector (MPV) time domain (TD) sensor. The NSMS is a very simple and robust technique for predicting the EMI responses of various objects. It is applicable to any combination of magnetic or electromagnetic induction data for any arbitrary homogeneous or heterogeneous 3D object or set of objects. The physics-based approach to estimate location assumes that the target exhibits a dipolar response and uses only two global values, the magnetic field vector H and the scalar magnetic potential psi, reconstructed at a set of points in space. To demonstrate the applicability of the NSMS, we first compare its predictions with dynamic MPV-TD measurements and then present the results of a blind-test analysis using multiaxis static MPV-TD data sets.;2008;F. Shubitidze;10.1109/IGARSS.2008.4778916;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_24_39;Data Assimilation Experiments Using Quality Controlled AIRS Version 5 Temperature Soundings;The AIRS Science Team Version 5 retrieval algorithm has been finalized and is now operational at the Goddard DAAC in the processing (and reprocessing) of all AIRS data. The AIRS Science Team Version 5 retrieval algorithm contains two significant improvements over Version 4: 1) Improved physics allows for use of AIRS observations in the entire 4.3 mum CO2 absorption band in the retrieval of temperature profile T(p) during both day and night. Tropospheric sounding 15 mum CO2 observations are now used primarily in the generation of cloud cleared radiances Rcirci. This approach allows for the generation of accurate values of Rcirci and T(p) under most cloud conditions. 2) Another very significant improvement in Version 5 is the ability to generate accurate case-by-case, level-by-level error estimates for the atmospheric temperature profile, as well as for channel-by-channel error estimates for Rcirci. These error estimates are used for quality control of the retrieved products. We have conducted forecast impact experiments assimilating AIRS temperature profiles with different levels of quality control using the NASA GEOS-5 data assimilation system. Assimilation of quality controlled T(p) resulted in significantly improved forecast skill compared to that obtained from analyses obtained when all data used operationally by NCEP, except for AIRS data, is assimilated. We also conducted an experiment assimilating AIRS radiances uncontaminated by clouds, as done operationally by ECMWF and NCEP. Forecast resulting from assimilated AIRS radiances were of poorer quality than those obtained assimilating AIRS temperatures.;2008;Joel Susskind;10.1109/IGARSS.2008.4779002;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_24_39;An Increased Potential for the Landsat Data Continuity Mission to Contribute to Water Quality Studies for Inland, Case 2 Waters;The Landsat Data Continuity Mission's (LDCM) Operational Land Imager (OLI) is a new sensor being developed by NASA which is both radiometrically and spatially sufficient for the monitoring of case 2 waters. This study, based on the sensor's design, presents the initial results of an experiment to determine what impact its improved features will have on water quality assessment. Specifically, we investigate how the addition of a deep blue band, 12-bit quantization, and improved signal-to-noise ratios affects our ability to retrieve water constituents. Preliminary tests are performed in the absence of atmospheric effects and indicate that the LDCM sensor can achieve about 5% error in the retrieval process while its predecessor ETM+ produces errors of over 20%. These results illustrate LDCM's potential to be a useful tool for the continuous monitoring of coastal and inland water resources. Ongoing work is focused on atmospherically compensating the data from the OLI instrument to actually achieve the potential demonstrated in this study.;2008;Aaron Gerace;10.1109/IGARSS.2008.4779737;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_24_39;Generating High-Quality Training Data for Automated Land-Cover Mapping;This paper presents two machine learning techniques that greatly reduce the number of person-hours required to generate high-quality training data for land cover classification. The first technique uses active learning to guide the generation of training data by selecting only the most informative examples for labeling. The second technique identifies and mitigates the impact of mislabeled instances. Both techniques are tested on data from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS), which has required thousands of person hours to label. Our results shows that the active learning method requires fewer labeled examples than random sampling to produce a high quality classifier. Our results on class noise mitigation show that if mislabelings occur, we can further improve classifier accuracy, and that weighting instances by their label confidence outperforms an analogous method that discards suspected mislabelings. If combined, these methods have the potential to make training data generation a more efficient and reliable process.;2008;U. Rebbapragada;10.1109/IGARSS.2008.4779779;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_24_39;An Operation-Based Communication of Spatial Data Quality;Improving users' awareness of the imperfections in spatial data has been a research issue explored within Geographic Information Sciences (GIS) for more than 30 years. However, little practical progress has been made toward this objective. Currently, most spatial data producers document information about spatial data quality as part of metadata. However, this information remains largely ignored by GIS users, which leads to the risk of users making poor decisions based on spatial data. As users increasingly make use of various GIS functionalities, GIS still lack the necessary mechanisms to effectively warn the users of the existence of quality issues in the spatial data being used. This becomes even more problematic in a web environment where data and services of unknown qualities can be shared and combined in the same application. In this paper we present an approach which aims at improving the use of quality information by providing it to the users in a more efficient way than existing approaches used for consulting metadata. We use an operation-based approach to link data quality information to the individual operations used in GIS applications. A conceptual framework for associating quality information with GIS operations is presented. Then, a prototype implementing this concept into a GIS software is described and discussed.;2009;Amin Zargar;10.1109/GEOWS.2009.8;Conferences;;978-0-7695-3527-2
ieee_20221205_08_24_39;Methods of Improving the Quality of X-Ray Image Reconstruction with Limited Projection Data;Nowadays X-ray Nondestructive Testing ( NDT ) has played an important role in defect inspection but the X-ray image contains all the information the rays passed by, therefore, it is hard to inspect the inner defect of PCB correctly. The correct inspect is rely on an image reconstruction system. In this study, a X-ray image reconstruction algorism to inspect the defect in a Printed Circuit Board ( PCB ) is developed. The Algebraic Reconstruction Technique ( ART ) is used to reconstruct the vertical section image from the images of several different project angle, the images are from the Planar Computer Tomography ( PCT ) system for PCB. The X-ray is assumed to be parallel beam. The limited angle of projection in PCT reduces the image reconstruction quality. To improve the quality of reconstructed image, a method combines the binary steering mechanism with ART to get the correct convergence and avoiding the local optimum solution of ART is proposed. Furthermore, the computational algorism to increase the reconstructed speed and discussing the influence between the image and the noise of projection data are also discussed.;2008;Dein Shaw;10.1109/IMPACT.2008.4783847;Conferences;2150-5942;978-1-4244-3623-1
ieee_20221205_08_24_39;Effects of data density of echo Fourier domain on quality of high frame rate imaging;Based on the X wave theory, a high-frame rate (HFR) imaging method was developed and extended. In this paper, the effects of spatial data density of echo Fourier domain on the quality of the HFR imaging method is studied experimentally. In the experiment, echo data were acquired with a home-made HFR imaging system from an ATS539 tissue-mimicking phantom. Results show that with a two-fold densification of radiofrequency echo data of a fully sampled phased array transducer, one achieves the best compromise between the quality of reconstructed images and the amount of computations required due to the densification of echo data.;2008;Jian-yu Lu;10.1109/ULTSYM.2008.0235;Conferences;1051-0117;978-1-4244-2480-1
ieee_20221205_08_24_39;SSTL UK-DMC SLIM-6 Data Quality Assessment;"Satellite data from the Surrey Satellite Technology Limited (SSTL) United Kingdom (UK) Disaster Monitoring Constellation (DMC) were assessed for geometric and radiometric quality. The UK-DMC Surrey Linear Imager 6 (SLIM-6) sensor has a 32-m spatial resolution and a ground swath width of 640 km. The UK-DMC SLIM-6 design consists of a three-band imager with green, red, and near-infrared bands that are set to similar bandpass as Landsat bands 2, 3, and 4. The UK-DMC data consisted of imagery registered to Landsat orthorectified imagery produced from the GeoCover program. Relief displacements within the UK-DMC SLIM-6 imagery were accounted for by using global 1-km digital elevation models available through the Global Land One-km Base Elevation (GLOBE) Project. Positional accuracy and relative band-to-band accuracy were measured. Positional accuracy of the UK-DMC SLIM-6 imagery was assessed by measuring the imagery against digital orthophoto quadrangles (DOQs), which are designed to meet national map accuracy standards at 1 : 24 000 scales; this corresponds to a horizontal root-mean-square accuracy of about 6 m. The UK-DMC SLIM-6 images were typically registered to within 1.0-1.5 pixels to the DOQ mosaic images. Several radiometric artifacts like striping, coherent noise, and flat detector were discovered and studied. Indications are that the SSTL UK-DMC SLIM-6 data have few artifacts and calibration challenges, and these can be adjusted or corrected via calibration and processing algorithms. The cross-calibration of the UK-DMC SLIM-6 and Landsat 7 Enhanced Thematic Mapper Plus was performed using image statistics derived from large common areas observed by the two sensors.";2009;Gyanesh Chander;10.1109/TGRS.2009.2013206;Journals;1558-0644;
ieee_20221205_08_24_39;A quality-based birth-and-death queueing model for evaluating the performance of an integrated voice/data CDMA cellular system;CDMA has been proven to be one of the promising mainstream multiaccessing techniques in future cellular mobile communication systems. It is offering attractions such as soft capacity, soft handoff, and antimultipathing. A quality-based birth-and-death queueing model is developed for the purpose of evaluating the performance of a CDMA cellular that supports two-rate transmissions.;1995;Chyi-Nan Wu;10.1109/PIMRC.1995.480909;Conferences;;0-7803-3002-1
ieee_20221205_08_24_39;Quality Data for Data Mining and Data Mining for Quality Data: A Constraint Based Approach in XML;As quality data is important for data mining, reversely data mining is necessary to measure the quality of data. Specifically, in XML, the issue of quality data for mining purposes and also using data mining techniques for quality measures is becoming more necessary as a massive amount of data is being stored and represented over the Web. We propose two important interrelated issues: how quality XML data is useful for data mining in XML and how data mining in XML is used to measure the quality data for XML. When we address both issues, we consider XML constraints because constraints in XML can be used for quality measurement in XML data and also for finding some important patterns and association rules in XML data mining. We note that XML constraints can play an important role for data quality and data mining in XML. We address the theoretical framework rather than solutions. Our research framework is towards the broader task of data mining and data quality for XML data integrations.;2008;Md. Sumon Shahriar;10.1109/FGCNS.2008.74;Conferences;;978-0-7695-3546-3
ieee_20221205_08_24_39;Defining Quality-Measurable Medical Alerts From Incomplete Data Through Fuzzy Linguistic Variables and Modifiers;Alert systems are frequent in the medical field, where they are typically connected to monitoring devices that are able to detect abnormal values. Our system is different in its goals and tools. First of all, it processes data extracted from electronic medical records, which are widely used nowadays, and meteorological databases. Variables that are not continually measured by devices (like the age of patients) can then be taken into account. Next, the alerts it handles are not predefined, but created by users through domain-independent fuzzy linguistic variables whose relationships (the height of an individual is conditioned by its age) are modeled by a weighted oriented graph. Finally, the alerts it triggers are associated with two indicators used for filtering and assessing their relevance to the patients, and their reliability according to the amount of information available. Then, if there is a missing variable in a record, the detection algorithm treats it transparently by automatically decreasing the reliability of the alert. The main qualities of this system are the simplicity—linguistic variables are intuitive—and the ability to measure the informational quality of alerts (applicability and reliability).;2010;Wilmondes Manzi de Arantes;10.1109/TITB.2009.2020063;Journals;1558-0032;
ieee_20221205_08_24_39;The Impact of Design and Code Reviews on Software Quality: An Empirical Study Based on PSP Data;This research investigates the effect of review rate on defect removal effectiveness and the quality of software products, while controlling for a number of potential confounding factors. Two data sets of 371 and 246 programs, respectively, from a personal software process (PSP) approach were analyzed using both regression and mixed models. Review activities in the PSP process are those steps performed by the developer in a traditional inspection process. The results show that the PSP review rate is a significant factor affecting defect removal effectiveness, even after accounting for developer ability and other significant process variables. The recommended review rate of 200 LOC/hour or less was found to be an effective rate for individual reviews, identifying nearly two-thirds of the defects in design reviews and more than half of the defects in code reviews.;2009;Chris F. Kemerer;10.1109/TSE.2009.27;Journals;2326-3881;
ieee_20221205_08_24_39;Quality preserved image data coding in angiography migration from cinefilm;One of the major objectives in angiocardiography is the visualization of anatomical details of a vessel segment of interest. The excellent resolution of the cinefilm can't be reproduced by today's digital systems. A universal compromise between image quality and strong data reduction is difficult to find. Here the authors propose the concept of image content descriptions as related parameters to be stored with(in) the image. The image frame is divided into regions of different medical significance allowing different degrees of data reduction and quality enhancement. This concept is illustrated by several examples: the 3D-reconstruction avoids unnecessary data, the motion compensated loop preserves the film resolution in critical regions, and the predictor-corrector postprocessor, tunes an arbitrary lossy compressor to become partially nonlossy in the most important regions of the image.;1995;M. Rombach;10.1109/CIC.1995.482749;Conferences;;0-7803-3053-6
ieee_20221205_08_24_39;Water quality and Sea Surface Temperature mapping using NOAA AVHRR data;Environmental pollution is coeval with the appearance of humans. Water pollution problem becomes increasingly critical in this present-day, whether in developed or developing countries. Sediment is the primary cause of water pollution. The environmental pollution problem can be measured using ground instruments such as turbidity meters for water measurements. Field measurements cannot provide fine spatial resolution maps with detailed distribution pattern over a large study area. The study was carried out to verify the validity of National Oceanic and Atmospheric Administration Multi-Channel Sea Surface Temperature (SST) (NOAA MCSST) algorithm by NOAA at South China Sea. SST is verified by comparing the SST calculated by algorithm with sea-truth data collected by Research on the Sea and Islands of Malaysia (ROSES). ROSES had travelled and collected data at South China Sea from 26 Jun 2004 to 1 August 2004. In this study the transmittance function for each band was modeled using the MODTRAN code and radiosonde data. The expression of transmittance as a function of zenith view angle was obtained for each channel through regression of the MODTRAN output. The in-situ data (ship collected SST values) were used for verification of results. The derived SST value was compared with the ground truth data collected during Research on the Seas and Islands (ROSES) project and the standard deviation is less than 1 degree Celsius. SST map was created and comparison between the in-situ SST patterns was made in this study. The satellite NOAA AVHRR data used in SST analysis was used for water quality mapping. The DN values were converted into radiance values and later reflectance values - AVHRR Radiometric Correction and Calibration. The reflectance values corresponding to the ground truth sample locations were extracted from all the images. In this study, the multidate data were corrected to minimize the difference in atmospheric effects between the scenes. The reflectance values for window size of 3 by 3 were used because the data set produced higher correlation coefficient and lower RMS value. Finally, an automatic geocoding technique from PCI Geomatica 10.1 - AVHRR Automated Geometric Correction was applied in this study to geocode the SST and TSS maps.;2009;H. S. Lim;10.1109/AERO.2009.4839441;Conferences;1095-323X;978-1-4244-2622-5
ieee_20221205_08_24_39;The data quality estimation for the information web resources;The article is devoted to problems, which are related to the quality evaluation for the information Web resources. Basic tasks and perspective information technology of quality evaluation by the searching robot are selected.;2009;Zoya Dudar;;Conferences;;978-966-2191-05-9
ieee_20221205_08_24_39;Improving information quality of sensory data through asynchronous sampling;In this paper, asynchronous sampling is proposed as a novel approach to improve the information quality of sensory data through shifting the sampling moments of sensors from each other. The exponential correlation model and the entropy model for the sensory data are introduced to quantify their information quality. An asynchronous sampling strategy, EASS, is presented accordingly to assign equal time shifts to sensors, which in turn reduces data correlation and thus improves information quality in terms of increased entropy of sensory data. A lower bound for EASS is derived to evaluate its effectiveness. Simulation results based on both synthetic data and experimental data are satisfactory.;2009;Jing Wang;10.1109/PERCOM.2009.4912837;Conferences;;978-1-4244-3304-9
ieee_20221205_08_24_39;Quality- and energy-aware data compression by aggregation in WSN data streams;Sensor networks consist of autonomous devices that cooperatively monitor an environment. Sensors are equipped with capabilities to store information in memory, process information and communicate with neighbors and with a base station . However, due to the sensors' size, their associated resources are limited. In such a context, the main cause of energy dissipation is the use of the wireless link. Solutions that minimize communication are needed. In this paper a framework to manage efficiently data streams is presented. The proposed approach aims at saving energy by capturing signals and compress them instead of sending them in raw form. The algorithm also guarantees that the compressed representation satisfies quality requirements specified in terms of accuracy, precision, and timeliness.;2009;Cinzia Cappiello;10.1109/PERCOM.2009.4912866;Conferences;;978-1-4244-3304-9
ieee_20221205_08_24_39;A Calculus Approach to Energy-Efficient Data Transmission With Quality-of-Service Constraints;Transmission rate adaptation in wireless devices provides a unique opportunity to trade off data service rate with energy consumption. In this paper, we study optimal rate control to minimize transmission energy expenditure subject to strict deadline or other quality-of-service (QoS) constraints. Specifically, the system consists of a wireless transmitter with controllable transmission rate and with strict QoS constraints on data transmission. The goal is to obtain a rate-control policy that minimizes the total transmission energy expenditure while ensuring that the QoS constraints are met. Using a novel formulation based on cumulative curves methodology, we obtain the optimal transmission policy and show that it has a simple and appealing graphical visualization. Utilizing the optimal ldquoofflinerdquo results, we then develop an online transmission policy for an arbitrary stream of packet arrivals and deadline constraints, and show, via simulations, that it is significantly more energy-efficient than a simple head-of-line drain policy. Finally, we generalize the optimal policy results to the case of time-varying power-rate functions.;2009;Murtaza A. Zafer;10.1109/TNET.2009.2020831;Journals;1558-2566;
ieee_20221205_08_24_39;Complete Video Quality-Preserving Data Hiding;Although many data hiding methods are proposed in the literature, all of them distort the quality of the host content during data embedding. In this paper, we propose a novel data hiding method in the compressed video domain that completely preserves the image quality of the host video while embedding information into it. Information is embedded into a compressed video by simultaneously manipulating Mquant and quantized discrete cosine transform coefficients, which are the significant parts of MPEG and H.26x-based compression standards. To the best of our knowledge, this data hiding method is the first attempt of its kind. When fed into an ordinary video decoder, the modified video completely reconstructs the original video even compared at the bit-to-bit level. Our method is also reversible, where the embedded information could be removed to obtain the original video. A new data representation scheme called reverse zerorun length (RZL) is proposed to exploit the statistics of macroblock for achieving high embedding efficiency while trading off with payload. It is theoretically and experimentally verified that RZL outperforms matrix encoding in terms of payload and embedding efficiency for this particular data hiding method. The problem of video bitstream size increment caused by data embedding is also addressed, and two independent solutions are proposed to suppress this increment. Basic performance of this data hiding method is verified through experiments on various existing MPEG-1 encoded videos. In the best case scenario, an average increase of four bits in the video bitstream size is observed for every message bit embedded.;2009;KokSheik Wong;10.1109/TCSVT.2009.2022781;Journals;1558-2205;
ieee_20221205_08_24_39;An approach to reverse quality assurance with data-oriented program analysis;The paper describes a quality assurance approach for source code to assure source code quality with less effort than conventional method. The paper introduces the data relation tracking method (DRTM) as a practical instance of data oriented program analysis, a key factor of our new quality assurance approach. DRTM helps comprehension of the source code by extracting the internal logic of the source code in declarative notation. An example and evaluation of DRTM for the C language is also described. The example shows that DRTM can deal in data structures and inter-data relations of any control structure. The evaluation shows that DRTM can extract the internal logic of the source code uniquely and that the extracted internal logic is useful for quality assurance of the source code.;1995;Y. Kataoka;10.1109/APSEC.1995.496981;Conferences;;0-8186-7171-8
ieee_20221205_08_24_39;Evaluating process quality in GNOME based on change request data;The lifecycle of defects reports and enhancement requests collected in the Bugzilla database of the GNOME project provides valuable information on the evolution of the change request process and for the assessment of process quality in the GNOME sub projects. We present a quality model for the analysis of quality characteristics that is based on evaluating metrics on the Bugzilla database, and illustrate it with a comparative evaluation for 25 of the largest products within GNOME.;2009;Holger Schackmann;10.1109/MSR.2009.5069485;Conferences;2160-1860;978-1-4244-3493-0
ieee_20221205_08_24_39;Quality Evaluation of Spatial Point-Cloud Data Collected by Vehicle-Borne Laser Scanner;Vehicle-borne laser scanning is used to collect urban street and building data for various modeling and mapping applications. The quality of laser point-cloud data is important for regenerating 3D urban street scene. Quality evaluation includes spatial structure analysis and positioning accuracy analysis. In spatial structure analysis, we evaluate the quality of point-cloud data high or low by analyzing the objectspsila spatial structure projection at each of the three coordinate axis. In positioning accuracy analysis, comparing sign pointspsila 3D coordinates collected by laser scanner with the precise coordinates collected by Total Station, we get the point-cloudspsila positioning accuracy. A new sign point was also designed to overcome the difficulty of extracting sign points coordinates from point cloud data. After the evaluation, we make a suggestion about how to improve the data quality.;2008;Jiaxiang Feng;10.1109/ETTandGRS.2008.97;Conferences;;978-0-7695-3563-0
ieee_20221205_08_24_39;Assuring Image Quality in Spatial Data Sharing Platform for Disaster Management;This paper aims to describe development of a Web-based spatial data sharing platform for disaster management. Alongside functionality, quality of image(IQ) is basic to successful sharing of distributed geo-services.This paper explores IQ provisioning in the context of geo-service sharing for disaster management.The paper presents an IQ model for disaster management and illustrates how user level IQ requirements can be supported in IQ-aware geo-service architecture. In contrast to context-specific IQ assurance approaches, which usually focus on a few variables determined by local needs, this approach provides interactive, multi-granularity and contextsensitive IQ indicators that help experts to build and justify their opinions. The prototype system was designed and validated by enhancing IQ generated by remote sensing through the technologies of visualization.;2008;Ying Su;10.1109/ETTandGRS.2008.275;Conferences;;978-0-7695-3563-0
ieee_20221205_08_24_39;Application of COMERO Data Collecting in Quality Management System;Summary form only given. Exact and real-time collection of quality data from manufacturing process is the most important step in quality management system. It is the foundation of cumulating, analyzing and effectively controlling the processpsilas quality. As an advanced and exact metrical instrument, 3-degree coordinate measuring machining (CMM) has been widely used in machinery manufacturing industry. However, it could only use text file to record the measuring result, which can not be integrated with the quality management system. This paper introduces appropriative interface software, which can automatically read the measurement data from 3-degree CMMpsilas RTF file and can automatically transfer the certain data to database and then let quality management system directly transfer the measurement result in order to analyze the correlative accessorypsilas quality. This analysis is useful to understand the characteristic of random error system, the trend of error, the order of error distributing and the reasons for error and also form the quality report and early warning information for quality control. Meanwhile, informationpsilas automatically transmission can obviously improve working efficiency and effectively avoid the artificial errors. Heterogeneous datapsilas conversion is the key point of system integration. This paper comprehensively analysis the expression mode of every dimension and error in the measurement text files mentioned above, and then applies certain program. In order to understand the meaning of relevant data, key words are needed. Using character string in the measurement files, which match the regular expression and the method of exhaustion, we could get the key words. Then the data, which quality management system needs, is formed by transferring data types with homonymous matching and written into the database. In order to achieve the informationpsilas automatically transition, this paper also studies the data synchronization mechanism and ensure the measurement data can be synchronously updated in the quality management system by setting the trigger rule.;2009;Yue Yan-fang;10.1109/ITNG.2009.167;Conferences;;978-0-7695-3596-8
ieee_20221205_08_24_39;Assessing Quality of Derived Non Atomic Data by Considering Conflict Resolution Function;We present a Data Quality Manager (DQM) prototype providing information regarding the elements of derived non-atomic data values. Users are able to make effective decisions by trusting data according to the description of the conflict resolution function that was utilized for fusing data along with the quality properties of data ancestor. The assessment and ranking of non-atomic data is possible by the specification of quality properties and priorities from users at any level of experience.;2009;Maria del Pilar Angeles;10.1109/DBKDA.2009.10;Conferences;;978-0-7695-3550-0
ieee_20221205_08_24_39;Adaptable Link Quality Estimation for Multi Data Rate Communication Networks;QoS-sensitive applications transmitted over wireless links require precise knowledge of the wireless environment. However, the dynamic nature of wireless channel, together with its different configurations and types, makes so called link quality estimation (LQE) a difficult task. This paper looks into the design of an accurate and fast LQE method for a multi data rate environment. We investigate the impact of various conditions on the LQE accuracy. In result, two different link quality estimation sources, i.e., based on hello packet delivery ratio and signal strength, are measured and their performances are compared. We find that these two methods are not always accurate. As an improvement we propose an adaptive LQE method that chooses different LQE indicators depending on the wireless environment. The performance of the proposed method is verified via an extensive measurement campaign.;2009;Jinglong Zhou;10.1109/VETECS.2009.5073358;Conferences;1550-2252;978-1-4244-2517-4
ieee_20221205_08_24_39;Predicting Quality of Object-Oriented Systems through a Quality Model Based on Design Metrics and Data Mining Techniques;Most of the existing object-oriented design metrics and data mining techniques capture similar dimensions in the data sets, thus reflecting the fact that many of the metrics are based on similar hypotheses, properties, and principles. Accurate quality models can be built to predict the quality of object-oriented systems by using a subset of the existing object-oriented design metrics and data mining techniques. We propose a software quality model, namely QUAMO (QUAlity MOdel) which is based on divide-and-conquer strategy to measure the quality of object-oriented systems through a set of object-oriented design metrics and data mining techniques. The primary objective of the model is to make similar studies on software quality more comparable and repeatable. The proposed model is augmented from five quality models, namely McCall Model, Boehm Model, FURPS/FURPS+ (i.e. functionality, usability, reliability, performance, and supportability), ISO 9126, and Dromey Model. We empirically evaluated the proposed model on several versions of JUnit releases. We also used linear regression to formulate a prediction equation. The technique is useful to help us interpret the results and to facilitate comparisons of results from future similar studies.;2009;Chuan Ho Loh;10.1109/ICIME.2009.78;Conferences;;978-0-7695-3595-1
ieee_20221205_08_24_39;Data acquisition system in a mobile air quality monitoring station;The paper focuses on describing the complex problems concerning air quality monitoring and the necessity and utility of the control and automatic data acquisition system for the interpretation of the results. The paper relates to the experience and long term practice of the authors in monitoring air quality. The build up and control of the station is detailed. The main conclusion indicates that the electronic devices and the program environment is absolute necessary for a real time and correct monitoring result of air quality control.;2009;Ioana Ionel;10.1109/SACI.2009.5136310;Conferences;;978-1-4244-4478-6
ieee_20221205_08_24_39;Wavelet transform based ECG data compression with desired reconstruction signal quality;This paper proposes a new coding strategy by which the desired quality of reproduced signal can be guaranteed with the minimum cost of coding rate. The idea was successfully introduced to the DOWT-based coding system for the ECG compression application.;1994;J. Chen;10.1109/WITS.1994.513911;Conferences;;0-7803-2761-6
ieee_20221205_08_24_39;Quality of archived NDBC data as climate records;The National Data Buoy Center (NDBC) traces its beginning to the formation of the National Data Buoy Development Program in 1967, which consolidated approximately 50 individual programs conducted by a variety of ocean-oriented agencies. Today, NDBC operates three major buoy networks. First, the traditional weather fleet consists of over 100 moored buoys covering the coastal waters of the United States, including the Great Lakes, Hawaii and Alaska. Second, the Deep-ocean Assessment and Reporting of Tsunamis (the National Oceanic and Atmospheric Administration's DART®) Program operates 39 stations that detect and instantly report anomalies in ocean pressure associated with potential tsunami-generating seismic activity. Third, the Tropical-Atmosphere Ocean (TAO) array of climate monitoring platforms covers a wide swath of the equatorial Pacific. In this paper, we assess the traditional weather fleet as a resource for climate monitoring, and we do so in two ways. Both involve scrutinizing weather fleet records exceeding 20 years duration. We assess these according to the ten climate monitoring principles recommended by the U.S. National Research Council. We observe that NDBC has implicitly considered most, if not all, of these principles in the design, maintenance, improvement and expansion of the NDBC moored buoy fleet. Focusing on two stations in the Pacific Ocean, 46035 and 46042, we demonstrate NDBC's adherence to sound network management, careful archiving and description of metadata, steady development of comprehensive automated quality control procedures, giving users ease in data access, addressing issues of complementary data, historical significance and continuity of purpose. One area requiring strengthening remains a need for NDBC to build into its systems long-term climate requirements. Next, we propose a new method for reflecting climatic change over the oceans. The wave energy spectrum, which all NDBC weather buoys routinely report hourly, contain a significant amount of information regarding the origin, intensity and duration of ocean storms. Such measurements are produced from simple accelerometers coming from a mature, stable technology. We show that records of spectral energy density at low frequencies — for wave periods exceeding 20 seconds — suggest climate change signals. This is demonstrated with data collected from station 46042 in Monterey Bay, California. Both assessments clearly indicate that the NDBC network of weather monitoring buoys are a valuable national resource for climatologists, meteorologists and oceanographers interested in marine surface fluctuations on decadal and longer durations. We note areas where small improvements in calibration techniques will likely yield large gains in confident assessment of climate change.;2008;Theodore Mettlach;10.1109/OCEANS.2008.5151872;Conferences;0197-7385;978-1-4244-2620-1
ieee_20221205_08_24_39;Automated data quality assurance for marine observations;"The ocean monitoring community requires high quality data that is Data Management and Communications (DMAC)-compliant for both near real time and climate data records. The authors describe a flexible and cost effective automated data quality assurance (ADQA) system that can be used to assess the quality of marine observations and provide quality controlled data to a wide variety of end users. For example, if a researcher needs data sets from different sources for a modeling project, ADQA provides a means of characterizing the relative quality of these input sets. The ADQA system has been implemented by integrating a set of data quality algorithms based upon National Data Buoy Center (NDBC) Technical Document 03-02 ""Handbook of Automated Data Quality Control Checks and Procedures of the National Data Buoy Center"" into a processing system that was created using the CALIPSO science data processing framework. The CALIPSO framework is actually a library of reusable components that provide the core non-science functionality of any science data processing system. This design separates the science processing components from the basic, reusable system infrastructure and allows for the addition or removal of algorithms with relative ease. The generic infrastructure of the framework includes substantial core functionality that is common to many science data management applications and is easily configurable to work with a wide variety of marine instruments and science data sets. The basic architecture includes the following subsystems: Input, Control, Data Store and Output. The system is a modular design that simplifies the integration of additional data quality assurance (DQA) processing components.";2008;James V. Koziana;10.1109/OCEANS.2008.5151904;Conferences;0197-7385;978-1-4244-2620-1
ieee_20221205_08_24_39;Analysis on the Lake Water Pollution Components Based on Normal Quality Monitoring Data;Take Hongze Lake of Jiangsu Province, China, as a Case, the components of lake water pollution were analyzed, based on the regular water quality monitoring data of 10 sampling points from 1990 to 2002 and the spatial data. By factor analysis and spatio-temporary correlation analysis etc, the spatio- temporary simulations to main lake water pollution components, including eutrophication, were carried out. The lake water pollution components were classified into two types, primary pollution component (PPC) and secondary pollution component (SPC). The living pollution, nitrogen pollution etc, belong to the former, and eutrophication, ion property etc, belong to the later. The equations of the pollution components were constructed, and inversely their seasonal and spatial changes were analyzed. Finally the relation of simulated eutrophication values with N:P was modeled in this paper, which was similar to general rule of shallow lakes. Results showed that a good combination of spatio- temporary study with factor analysis etc was very helpful for the classification, identification and modeling of water pollution components, and was favorable to evaluate water pollution and deeper study on the mechanism of Lake Eutrophication.;2009;Bo Li;10.1109/ICBBE.2009.5162365;Conferences;2151-7622;978-1-4244-2901-1
ieee_20221205_08_24_39;Quality assessment of InSAR-derived DEMs generated with ERS tandem data;Provides a quality assessment of digital elevation models generated by means of SAR interferometry and the use of ERS tandem data. A total of 8 scene pairs have been investigated, acquired at different dates but covering the same site to allow a study of the different parameters such as temporal decorrelation, baseline, and varying incidence angles. The ERS tandem configuration implies some characteristics that are examined, concerning in particular the system parameters. Moreover, peculiarities detected during the interferometric processing are reported. The achieved results are compared to an existing reference DEM. Additionally, the InSAR-derived DEMs are compared to each other to study SAR-specific effects.;1996;M. Schwabisch;10.1109/IGARSS.1996.516480;Conferences;;0-7803-3068-4
ieee_20221205_08_24_39;System for analyses of end-to-end quality of data services in cellular networks;End-to-end quality of service in today's mobile networks is essential to end users since most of the services are non-real-time data services (primarily based on IP) which have time-varying quality over time, end-user position and other-end location. It is possible to have different quality for the same service under same mobile network conditions in the core and wireless part. Therefore, mobile operators need systems for testing and measurements of end-to-end quality of service. In this paper we present our developed system for service quality testing, which is created for testing the user perceived quality of services in mobile cellular networks. The system is consisted of distributed testing stations over the mobile network coverage area and centralized management and processing nodes, which provide collection, storage and statistical processing of data regarding the quality of service parameters for mobile data services. The system provides objective end-to-end quality of service information in real-time, which is essential to provide services with desired quality to the end users.;2009;Toni Janevski;10.1109/EURCON.2009.5167860;Conferences;;978-1-4244-3861-7
ieee_20221205_08_24_39;Data Mining on Source Water Quality (Tianjin, China) for Forecasting Algae Bloom Based on Artificial Neural Network (ANN);Harmful algae in source water become a very serious problem for water plants in China. Artificial neural networks (ANN) have been successfully used to model primary production and predict one-step weekly algae blooms in reservoir. In this study, to avoid selecting inputs randomly during the establishment of feed forward ANN forecasting algae two days later, we use correlation coefficient and index clustering to analyze source water quality parameters totally about 1744 daily measured data from 1997 to 2002 of Tianjin. So twenty-six schemes of input variables are determined and experimented for optimal inputs. Inputs of the final model are chlorophyll-a, turbidity, water temperature, ammonia, pH and alkalinity. The correlation coefficient of output values of the model and real values can reach 0.88 and the prediction accuracy is over 85%.;2009;Jin-Suo Lu;10.1109/CSIE.2009.97;Conferences;;978-0-7695-3507-4
ieee_20221205_08_24_39;Management of Air Quality Monitor Data with Data Warehouse and GIS;Air quality status is an important problem focused by all people. This paper utilizes Oracle 10 g to design and implement a prototype system of air quality data warehouse with the monitor data of 86 main cities from the year of 2000 to 2007 in China. To query and analyze the data in the data warehouse conveniently and effectively, it extends the star model to manage the spatial data with ArcGIS 9.0, and implements a Web spatial OLAP system to improve the ability of spatial analysis and visualization of traditional OLAP systems synchronously. Our work will help to evaluate air quality status, analyze its spatio-temporal characteristic, forecast and provide decision-making support for improvement of air quality.;2009;Yongkang Xiao;10.1109/CSIE.2009.280;Conferences;;978-0-7695-3507-4
ieee_20221205_08_24_39;Genetic Projection Pursuit Interpolation Data Mining Model for Urban Environmental Quality Assessment;In order to solve the incompatibility problem of assessment data indexes and to raise the precision of assessment model for urban environmental quality, a genetic projection pursuit interpolation data mining model (GPPIDMM) is presented for comprehensive assessment of urban environmental quality. In this model the projection pursuit data mining, genetic algorithm, interpolation curve and the assessment standards of urban environmental quality are used. And the indexes values of urban environmental quality can be synthesized to one dimension projection values. The samples can be assessed according to the values of the projection values in one dimension space. 50, 100, 500, 1000 samples in each grade have been adopted to test the stability of parameters in this model. In this new model, 5000 samples are generated from the assessment standards of urban environmental quality, which avoids the low precision in other models with little quantity of samples. The interpolation assessment formula is given with projection values and experiential grades. And the parameters in this model are steady by test. This new model is used to assess Xuanzhou environmental quality with the main indexes of water environment, atmospheric environment and noise environment. GPPIDMM can also be used to design the weights of the index system and deal with data. The results show that the urban environmental quality is still clean in Xuanzhou. GPPIDMM is a new method for evaluation of urban environmental quality and it is more objective in the whole data processing.;2009;Yang Xiaohua;10.1109/CSIE.2009.6;Conferences;;978-0-7695-3507-4
ieee_20221205_08_24_39;Estimation of power factor by the analysis of Power Quality data for voltage unbalance;Power Quality (PQ) has been identified as a complex and diversified problem across the board in electrical power industry. It has not only confused the power utilities but also has attracted the attention of its customers and equipment manufacturers. Although extensive research work is being done in this field but one of the main problem encountered by its stake holders is the voltage unbalance in electrical power system. The problem of voltage unbalance has affected the safety, reliability and economic efficiency at all levels in power industry. In this research Computational Intelligence Techniques have been used for efficiently predicting the power factor of unbalanced load of a power distribution system. The Principal Component Analysis Technique was used to find the optimized number of new dimensions of PQ data. Finally Feed Forward Back Propagation (FFBP) algorithm was used to estimate the power factor of a distribution network by analyzing the real power system parameters. This research highlights the importance of maintaining power factor close to unity for power utilities to achieve sustainable availability of quality supply of electrical power for its customers. The outcomes of the proposed techniques were compared and tested with the field results of a power utility in Victoria, Australia.;2009;Zahir J. Paracha;10.1109/ICEE.2009.5173178;Conferences;;978-1-4244-4361-1
ieee_20221205_08_24_39;Quality assurance for data acquisition in error prone WSNs;This paper proposes a data acquisition scheme which supports probabilistic data quality assurance in an error-prone wireless sensor network (WSN). Given a query and a statistical model of real-world data which is highly correlated, the aim of the scheme is to find a sensor selection scheme which is used to deal with inaccurate data and probabilistic guarantee on the query result. Since most sensor readings are real-valued, we formulate the data acquisition problem as a continuous-state partially observable Markov decision process (POMDP). To solve the continuous-state POMDP, the fitted value iteration (FVI) is applied to find a sensor selection scheme. Numerical results show that FVI can achieve high average long-term reward and provide probabilistic guarantees on the query result more often when compared to other algorithms.;2009;Sunisa Chobsri;10.1109/ICUFN.2009.5174279;Conferences;2165-8536;978-1-4244-4216-4
ieee_20221205_08_24_39;Data-Driven Soft Sensor Approach for Quality Prediction in a Refining Process;In the petrochemical industry, the product quality reflects the commercial and operational performance of a manufacturing process. However, real-time measurement of product quality is generally difficult. Online prediction of quality using readily available, frequent process measurements would be beneficial in terms of operation and quality control. In this paper, a novel soft sensor technology based on partial least squares (PLS) regression is developed and applied to a refining process for quality prediction. The modeling process is described, with emphasis on data preprocessing, multivariate-outlier detection and variables selection. Enhancement of PLS strategy is also discussed for taking into account the dynamics in the process data. The proposed approach is applied to data from a refining process and the performance of the resulting soft sensor is evaluated by comparison with laboratory data and analyzer measurements.;2010;David Wang;10.1109/TII.2009.2025124;Journals;1941-0050;
ieee_20221205_08_24_39;Analysis & design and implementation of quality oriented data management system for cold rollers production;Based on the production processes of cold rollers, this paper deeply analyzes the process and its characteristics. A concept model is designed for quality oriented data management in the cold rollers production processes. Both the systematic architecture and corresponding function modules have been designed. Furthermore, the implementation and its features of the target system are introduced. Application and implementation of the quality management system indicates that not only is it very suitable to quality management in the cold roller production system, but also it can be applied to other industrials with similar production processes.;2009;Gao Bin;10.1109/CCDC.2009.5191823;Conferences;1948-9447;978-1-4244-2723-9
ieee_20221205_08_24_39;A general model for continuous quality improvement by immune system inspired data driven evolution;Today, enterprises are facing more pressure than ever for continuous improvement and adaptation. Therefore, how to capture the flashing by opportunity to improve quality with low cost and high efficiency becomes an imperative task for any enterprise. Inspired by the natural immune system principles and biological evolution mechanism, a data driven system evolution method for continuous quality improvement based on information technology is proposed. First the variation phenomenon in biological system and manufacturing system is compared. Then based on analyse of the similarity and difference between the two systems, the general model for continuous quality improvement is established by utilizing existing data analysis tools and methods. Finally the method is illustrated by a case study which uses association rule mining method as the tool to capture the opportunity to improve system quality. This immune inspired model has the potential to improve the system quality with little additional cost.;2009;Genbao Zhang;10.1109/CCDC.2009.5195183;Conferences;1948-9447;978-1-4244-2723-9
ieee_20221205_08_24_39;Improving data quality and clinical records: Lessons from the UK National Programme about structure, process and utility;Sharing of health data should improve patient safety and improve health services efficiency. These data can also be used for research. The shared data are usually ldquocodedrdquo using a coding system classification or nomenclature. However, ldquocodingrdquo is not a neutral action and is part of the complex social interaction between doctor and patient. To derive meaning from data it is essential to understand the context in which it is recorded and to infer whether data recorded for one purpose is usable in another. Most of the existing models to raise data quality (DQ) are descriptive and don't necessarily inform why lessons from one health system might be applied in another.;2009;Simon de Lusignan;10.1109/ITI.2009.5196042;Conferences;1330-1012;978-953-7138-15-8
ieee_20221205_08_24_39;The use of data-mining to identify indicators of health related quality of life in patients with irritable bowel syndrome;Health-related quality of life can be adversely affected by irritable bowel syndrome (IBS). The aims of this study were to examine the health-related quality of life in a cohort of individuals with IBS and to determine which socio-demographic and IBS symptoms are independently associated with reduced health-related quality of life. Several data-mining models to determine which factors are associated with impaired health-related quality of life are considered in this study and include logistic regression, a classification tree and artificial neural networks. As well as severity of IBS symptoms, results indicate that psychological morbidity and socio-demographic factors such as marital status and employment status also have a major role to play.;2009;Kay I Penny;10.1109/ITI.2009.5196059;Conferences;1330-1012;978-953-7138-15-8
ieee_20221205_08_24_39;Research on Control of the Data Quality in Digital Land Spatial Database Based on GIS;GIS as the platform is being widely used in current construction of geoscience spatial database in China, and the quality of its spatial data directly influences the quality of the spatial database of geoscience. Precision of spatial data, validity and integrality of attribute data, and validity of the spatial database's topological relations are the main factors which influence the quality of the spatial database. Through practical summarize, the article discusses concrete means and methods to control the data quality of the three aspects mentioned above. By the means and methods, the data quality can be guaranteed to meet the needs for setting up spatial database of geoscience and building up a spatial database of geoscience with good quality.;2009;Chen Weigong;10.1109/DBTA.2009.49;Conferences;2167-194X;978-0-7695-3604-0
ieee_20221205_08_24_39;Geospatial Data Quality Self-Assuring Model for Disaster Management System;The objective of this paper is to propose a geospatial data quality (G-DQ) self assuring model using an intelligent multiagent (IMA) technique, which is capable to monitor network status, evaluate G-DQ, generate assuring strategies, and apply it to the disaster management system. To do this, we have employed the IMA algorithm concept based on the advanced artificial intelligence for developing information security models and agent-based evaluation of G-DQ as well as assuring strategy. Our approach differs from others in that, it is able to analyze G-DQ in quantitative manner, it can generate and apply assuring strategies automatically, and it supports a coherent design concept for G-DQ self-assuring system. Simulation results show that the proposed approach increases the efficiency and results in quality information acquisition.;2009;Ying Su;10.1109/GCIS.2009.450;Conferences;2155-6091;978-0-7695-3571-5
ieee_20221205_08_24_39;Quality-based data source selection for web-scale Deep Web data integration;Deep Web has been an important resource on the Web due to its rich and high quality information, leading to emerging a new application area in data mining and information retrieval and integrates. In Web scale deep Web data integration tasks, where there may be hundreds or thousands of data sources providing data of relevance to a particular domain, It must be inefficient to integrate all available deep Web sources. This paper proposes a data source selection approach based on the quality of deep Web source. It is used for automatic finding the highest quality set of deep Web sources related to a particular domain, which is a premise for effective deep Web data integration. The quality of data sources are assessed by evaluating quality dimensions represent the characteristics of deep Web source. Experiments running on real deep Web sources collected from the Internet show that our provides an effective and scalable solution for selecting data sources for deep Web data integration.;2009;Xue-Feng Xian;10.1109/ICMLC.2009.5212537;Conferences;2160-1348;978-1-4244-3703-0
ieee_20221205_08_24_39;Landsat 7 ETM+ on-orbit calibration and data quality assessment;The Enhanced Thematic Mapper Plus (ETM+) multispectral scanner is scheduled for launch aboard the Landsat 7 satellite in 1998. Unlike data from earlier Landsat satellites, the United States (U.S.) Government will distribute data to users in an essentially raw form. The Landsat 7 ground system design therefore includes an image assessment system (IAS) to provide users with the ancillary information needed to generate useful, radiometrically-calibrated, geometrically-corrected ETM+ digital imagery. The IAS will assess ETM+ data quality and calibrate the sensor radiometry and geometry. The information provided by the IAS will allow users to produce digital image data with an absolute radiometric accuracy of 5%, band-to-band registration of 0.3 pixels, and geodetic registration to 250 m (1 sigma) without ground control.;1995;J.R. Irons;10.1109/IGARSS.1995.521813;Conferences;;0-7803-2567-2
ieee_20221205_08_24_39;Extension of the Goal-Decision-Information approach to ensure quality in data warehouse development using software agent;Data warehouses are complex systems that have to deliver highly-aggregated data from heterogeneous sources to decision makers. It is essential that we can assure the quality data warehouse in terms of data as well as the services provided by it. But the requirements and the environment of data warehouse systems is dynamic in nature. To handle these changes efficiently, data warehouses depend largely on the meta databases. In this paper, the proposal is to extend the Goal-Decision-Information approach to model the quality of the data warehouse. In order to fulfill the specific quality goals, dependencies among the various quality factors is exploited in this model.;2009;S. Venkatesan;10.1109/IAMA.2009.5228083;Conferences;;978-1-4244-4710-7
ieee_20221205_08_28_59;Medical diagnostic and data quality;"The spread of electronic use of data in various areas has pushed the importance of data quality to a higher level. Data quality has syntactic and semantic components; the syntactic component is relatively easy to achieve if supported by tools (either off-the-shelf or our own), while the semantic component requires more research. In many cases such data come from different sources, are distributed across enterprises and are at different quality levels. Special attention needs to be paid to data upon which critical decisions are met, such as medical data for example. The starting point for research is in our case the risk of the medical area. We focus on the semantic component of medical data quality.";2002;T. Welzer;10.1109/CBMS.2002.1011361;Conferences;1063-7125;0-7695-1614-9
ieee_20221205_08_28_59;Multilayer perceptron discrimination of software quality in a biomedical data analysis system;Biomedical data analysis typically involves large data sets, a diverse user base and intensive visualization procedures. These features place stringent quality demands upon software development and performance for both the architect and supervisor. An invaluable tool for software supervisors would be the automatic qualitative grading of software objects in terms of their extensibility, reusability, clarity and efficiency. This paper examines the quality assessment of software objects by a multilayer perceptron in relation to a gold standard provided by two independent software architects.;2002;M.D. Alexiuk;10.1109/CCECE.2002.1013039;Conferences;0840-7789;0-7803-7514-9
ieee_20221205_08_28_59;The evolution of power quality data acquisition systems-triggering to capture power quality events;25 years ago, capturing multiple channels of simultaneous data required multiple oscilloscopes or a very specialized, expensive, and customized data acquisition system. In order to capture power quality events, very sophisticated methods were used to invoke simultaneous triggering across all recording channels. Normally, triggering was accomplished using overvoltage or overcurrent conditions and typically, you only had one shot. In addition, there was no easy way to get an oscilloscope to trigger on an undervoltage event (like a sag, for example). Today, all of that has changed. We are now able to record multiple channels of simultaneous data using many types of triggering mechanisms. This paper discusses the evolution of triggering power quality instrumentation and the advanced triggering systems available for portable and fixed panel mounted meters.;2002;D. Carnovale;10.1109/PAPCON.2002.1015146;Conferences;;0-7803-7446-0
ieee_20221205_08_28_59;Using Landsat TM data to aid the assessment of long-term trends in lake water quality in New Hampshire lakes;Assessing long-term trends in lake water quality is an important aspect of lake management. In the state of New Hampshire (NH), volunteer monitors working with the NH Lakes Lay Monitoring Program (LLMP) have measured lake quality parameters in many of the state's lakes for one or two decades. These measurements have served as the data bank for both policy decisions and scientific research on lake health. Because of the promise of improved remote sensing platforms for gathering water quality observations on a consistent basis across the state, we examine the potential relationships between remotely sensed observations and in situ water quality measurements, using Landsat TM imagery at a resolution of 30 m. Strong relationships between water clarity (Secchi disk transparency) and Landsat TM bands 1 and 3 have been determined for lakes in the Upper Midwest region. Using the long-term measurements of water quality data in NH lakes (water clarity, chlorophyll a, dissolved organic color, aquatic plant extent), we examine the usefulness beyond the original region of study of the empirical relationships between Landsat TM bands and Secchi disk transparency developed for Midwestern lakes. TM scenes taken during the period 1993-1999 are compared to water quality measurements for approximately 50 lakes in NH. The ability of 30 m spatial resolution imagery to capture temporal and spatial trends in lake water quality in our region is examined.;2002;A.L. Schloss;10.1109/IGARSS.2002.1026880;Conferences;;0-7803-7536-X
ieee_20221205_08_28_59;Requirement specifications for an enterprise level collaborative, data collection, quality management and manufacturing tool for an EMS provider;Numerous software applications that serve as effective tools in solving localized issues for an electronics manufacturing service (EMS) provider are currently available. However, there are very few products available in the market for an EMS provider that offer an integrated solution to issues ranging from the production floor to supplier quality. Applications with features including that of manufacturing execution systems (MES) and shop floor control systems would be examples of middle layer systems. This paper discusses the requirement specifications developed for a middle layer software tool to support collaborative manufacturing, extend shop floor visibility, monitor product yield and defects, and improve unit level traceability. Also, the software tool would assist in increasing operational efficiency by reducing paperwork of shop floor personnel, helping to locate reference designators and finding part numbers, and reducing debug time. The application interfaces with SMT, tests and inspection equipment for effective product and process monitoring. This paper also discusses the issues that are of concern to quality department personnel and customers such as the non-conformance materials report (NCMR) and tracking of corrective action requests associated with them and providing root cause analysis capability for NCMRs. The requirement specifications detailed here were developed to meet the needs of both high and low volume facilities involved in PCB assembly and subsequent box-build activities. This paper delineates the functionality required in an integrated system at an EMS provider for it to be an effective tool for manufacturing, improving quality and integrating data from facilities worldwide.;2002;S. Bahl;10.1109/IEMT.2002.1032741;Conferences;;0-7803-7301-4
ieee_20221205_08_28_59;Control Data Corporation's Government Systems Group Standard Software Quality Program;The authors describe the necessity of developing the Government Systems Group standard Software Quality Program (SQP), the background in developing the SQP, the advantages of the SQP, the components of the SQP, and the highlights of the SQP. The goal of the standard SQP was to develop common and reusable quality processes. The SQP will produce quality products, while the plan offers the advantages of compliancy, reusability, efficiency, effectiveness, consistency, cost savings, and portability. The components of the SQP include the policy, organization, plan, and handbook. The main elements of the SQP, which reflects government standards DOD-STD-2167A and DOD-STD-2168 for software development projects, are discussed. This standard SQP was developed using the total quality management process methodologies. The influence that the Software Engineering Institute's Capability Assessment had on developing and implementing this standard SQP is also discussed.<>;1990;G. Redig;10.1109/NAECON.1990.112846;Conferences;;
ieee_20221205_08_28_59;Quality Control Procedures in Processing Oceanographic Data;The National Oceanographic Data Center (NODC) receives significant volumes of oceanographic environmental data for processing. These data vary from the conventional ocean station data and expendable bathythermograph data to a wide variety of biological and pollution data. The need for a more objective environmental quality control became evident as the volume of data continued to increase. At the same time, the resources-enriched computer capability and large climatological files of fully processed data-for developing such a system became available. The NODC procedures, prior to the development of environmental models, were based primarily on the initial quality control by data donors and subjective analysis by NODC oceanographers. This study describes the use of ocean models for quality controlling ocean serial and XBT data.;1981;I. Perlroth;10.1109/OCEANS.1981.1151615;Conferences;;
ieee_20221205_08_28_59;Environmental Data Quality Estimates for Ocean Data Buoys;A summary of specific data quality estimates for the major buoy systems now operationally deployed by the NOAA Data Buoy Office (NDBO) is provided. Quality estimates are included for four distinct buoy types: (1) The Prototype Environmental Buoy (PEB) - deep ocean moored, (2) the Engineering Experimental Phase Buoy (EEP) - deep ocean moored, (3) the 5.0m Discus Buoy - shallow ocean moored, and (4) the NOMAD Buoy - shallow or deep ocean moored. The quality estimates presented are a result of in situ and laboratory testing, computer modeling, and analytical evaluations. Total quality estimates, or total system errors, and some examples of total system error breakdowns into component errors, are provided for some general environmental situations. Detailed quality estimates such as these are important since efficient uses of data from measurement platforms such as buoys are dependent on a comprehensive understanding of the accuracy of the data.;1976;G. Withee;10.1109/OCEANS.1976.1154197;Conferences;;
ieee_20221205_08_28_59;Circulatory survey data quality assurance;The National Ocean Survey is developing a data quality assurance program for its circulatory surveys in collaboration with the NOAA Office of Ocean Technology and Engineering Services (OTES). The present emphasis is on East Coast circulatory surveys conducted by the 133-foot NOAA ship FERREL using Grundy Model 9021 current meters. The methodology for estimating total measurement uncertainties is described, including extensive instrument test and evaluation, quality control field checks, error source identification and estimation, and quality assurance analyses. The results of the 1979 Gulf At-Sea Performance Experiment (GASP) on the Louisiana inner shelf are summarized.;1982;H. Frey;10.1109/CCM.1982.1158434;Conferences;;
ieee_20221205_08_28_59;A binary image preprocessor for document quality improvement and data reduction;A new binary image-enhancement operation is proposed to suppress noise in documents, introduced, e.g., by poor-quality copying. As opposed to approaches previously published, the new technique does not erode characters, symbols, drawing elements, etc, and thus improves the intelligibility of documents. In addition, it is shown that the new preprocessing method reduces significantly the data volume necessary to represent documents in compressed form. Finally, a low-cost hardware implementation is proposed, suitable of being integrated, e.g., in future scanning devices.;1986;F. Wahl;10.1109/ICASSP.1986.1169274;Conferences;;
ieee_20221205_08_28_59;Use of lightning location systems data in integrated systems for power quality monitoring;Voltage sags are one of major concerns in power quality, as they cause serious malfunction of a large number of apparatuses. There are a number of reasons for voltage sags in distribution networks: there is some evidence, however, that in electrical systems located in regions with high value of isokeraunic level, lightning can cause the majority of voltage sags. Data from lightning location systems, which provide an estimation of both lightning flash location and return-stroke current amplitude, can then be used to understand whether lightning is indeed the real cause of circuit breaker operation during thunderstorms-which means, in turn, of voltage sags-or not. Due to the complexity of the problem, the information coming from LLS (lighting location and estimate of lighting current amplitude) are, in general, not enough to infer the origin for voltage sags. It is necessary to suitably integrate them with data from system monitoring, e.g. relevant to the intervention of circuit breakers in primary substations, and with simulation results from of accurate models for computing lighting-induced voltages on complex power networks.;2002;C.A. Nucci;10.1109/TDC.2002.1178458;Conferences;;0-7803-7525-4
ieee_20221205_08_28_59;Mind your P's and Q's [power-quality events capture, data acquisition system];This paper reviews the evolution of data-acquisition systems and the use of triggering to capture power-quality events. Triggers characterize the events and provide unique methods of data sorting. They make capturing data easier. They make sorting data easier. They make analysis easier and faster. The drawback is that they may lead to fast/false (oversimplified) conclusions. There is also the danger that, if setup incorrectly, the event may be missed.;2003;D. Carnovale;10.1109/MIA.2003.1180950;Magazines;1558-0598;
ieee_20221205_08_28_59;Data quality and grounding considerations for a medical facility;Medical facilities as well as most other commercial, industrial, and educational facilities depend heavily on telecommunications, data, and computer networks. These often experience problems with signal quality and noise because of the high-speed, low-level signals. In addition, there are serious issues that relate to electrical safety as well as computer grounding and communications. Topics such as the 11 different ground systems, connections to ground, avoiding potential difference, avoiding circulating currents, and wiring methods are discussed. Three areas specific to networks require special attention. These are equipment bonding, isolated earth ground, and isolated power systems.;2002;M.O. Durham;10.1109/MWSCAS.2002.1186828;Conferences;;0-7803-7523-8
ieee_20221205_08_28_59;Development of a real-time data quality monitoring system using embedded intelligence;Rule-based reasoning and case-based reasoning have emerged as two important and complimentary reasoning methodologies in the field of artificial intelligence (AI). This paper describes the development of a real-time data quality monitoring system (CORMS AI) using case-based and rule-based reasoning. CORMS AI was developed to augment an existing decision support system (CORMS Classic) for monitoring the quality of environmental data and information and their respective computer based systems for use in NOAA Ocean Service's oceanographic operational products.;2002;T. Bethem;10.1109/OCEANS.2002.1191909;Conferences;;0-7803-7534-3
ieee_20221205_08_28_59;Quality control of ocean temperature and salinity profile data;The objective of data quality control (QC) is to ensure consistency within a single data set or throughout a historical database. This is a vital step to be taken with any data set before scientific analysis can be proceed. A redesign of data quality procedures for temperature and salinity profile data undertaken by the Naval Oceanographic Office (NAVOCEANO) during the past year has resulted in new data validation standards and procedures. New data QC and editing software has been developed and is being applied to all new and archived shipboard and aircraft temperature and salinity profile data collected or held by NAVOCEANO. The initial QC effort (Level 1 effort) is in progress. Level 1 effort is an assessment of the approximately 8 million profiles held in the NAVOCEANO archives for outlying and invalid profiles by comparison of profiles from relevant environments. The subsequent Level 2 effort entails more detailed investigations and further analyses for editing the previously flagged profiles. The MATLAB-based data edit programs use Master Oceanographic Observation Data Set (MOODS) 2000, a revised internal standard data file format, which will ease the use and exchange of data. The new file structure also stores numerous data quality flags and histories of collections for incorrect or missing metadata, deviations from climatological statistics, and profile data quality.;2002;K.P. Grembowicz;10.1109/OCEANS.2002.1192144;Conferences;;0-7803-7534-3
ieee_20221205_08_28_59;Power quality disturbance data compression, detection, and classification using integrated spline wavelet and S-transform;In this paper, power quality transient data are compressed and stored for analysis and classification purposes. From the compressed data set, original data are reconstructed and then analyzed using a modified wavelet transform known as S-transform. Compression techniques using splines are performed through signal decomposition, thresholding of wavelet transform coefficients, and signal reconstruction. Finally, the authors present compression results using splines and examine the application of splines compression in power quality monitoring to mitigate against data-communication and data-storage problems. Since S-transform has better time frequency and localization property, power quality disturbances are detected and then classified in a superior way than the recently used wavelet transform.;2003;P.K. Dash;10.1109/TPWRD.2002.803824;Journals;1937-4208;
ieee_20221205_08_28_59;Air quality data remediation by means of ANN;We present an application of neural networks to air quality time series remediation. The focus has been set on photochemical pollutants, and particularly on ozone, considering statistical correlations between precursors and secondary pollutants. After a preliminary study of the phenomenon, we tried to adapt a predictive MLP (multi layer perceptron) network to fulfill data gaps. The selected input was, along with ozone series, ozone precursors (NO/sub x/) and meteorological variables (solar radiation, wind velocity and temperature). We then proceeded in selecting the most representative periods for the ozone cycle. We ran all tests for a 80-hours validation set (the most representative gap width in our data base) and an accuracy analysis with respect to gap width as been performed too. In order to maximize the process automation, a software tool has been implemented in the Matlab/spl trade/ environment. The ANN validation showed generally good results but a considerable instability in data prediction has been found out. The re-introduction of predicted data as input of following simulations generates an uncontrolled error propagation scarcely highlighted by the error autocorrelation analysis usually performed.;2002;G. Latini;10.1109/ICONIP.2002.1198163;Conferences;;981-04-7524-1
ieee_20221205_08_28_59;Water quality retrievals from combined Landsat TM data and ERS-2 SAR data in the Gulf of Finland;This paper presents the applicability of combined Landsat Thematic Mapper and European Remote Sensing 2 synthetic aperture radar (SAR) data to turbidity, Secchi disk depth, and suspended sediment concentration retrievals in the Gulf of Finland. The results show that the estimated accuracy of these water quality variables using a neural network is much higher than the accuracy using simple and multivariate regression approaches. The results also demonstrate that SAR is only a marginally helpful to improve the estimation of these three variables for the practical use in the study area. However, the method still needs to be refined in the area under study.;2003;Yuanzhi Zhang;10.1109/TGRS.2003.808906;Journals;1558-0644;
ieee_20221205_08_28_59;Video quality objective metric using data hiding;"In this paper a non-reference objective video quality metric is proposed. The quality metric is obtained by means of a non-conventional use of data hiding technique. Test data are embedded in an MPEG-2 video; the basic assumption is that the data embedded undergo under the same degradation as the host video. To analyze the performance of the system, a comparison between the results obtained using this metric and the perceived mean annoyance values was performed. The annoyance values were obtained through a psychophysical experiment, which measured the threshold and mean annoyance values of compressed videos.";2002;M.C.Q. Farias;10.1109/MMSP.2002.1203346;Conferences;;0-7803-7713-3
ieee_20221205_08_28_59;A variable rate channel quality feedback scheme for 3G wireless packet data systems;An expanded effort is underway to support the evolution of the UMTS and cdma2000-1x standards to meet the rapidly developing needs associated with wireless Internet applications. A number of performance enhancing technologies are proposed to ensure high peak and average packet data rates while supporting circuit-switched voice and packet data on the same spectrum. These techniques include adaptive modulation and coding (AMC), hybrid ARQ (H-ARQ) and fat-pipe scheduling. In order to enable these techniques downlink channel quality feedback (CQF) through explicit uplink signaling is necessary. Frequent CQF results in good estimates of downlink channel quality at the base station, which, in turn, improves downlink system performance. However, this comes at the expense of larger uplink signaling overhead, thereby impacting the overall uplink capacity. Infrequent CQF, on the other hand, reduces this signaling overhead in the uplink at the expense of larger errors in the channel quality estimates available at the base station, thereby leading to system performance degradation. In this paper, we present a variable rate CQF scheme that significantly reduces uplink signaling overhead without affecting downlink system performance. In addition to simulation results demonstrating the performance of the proposed scheme, we also compare the reduction in uplink signaling overhead of the said scheme with other known methods.;2003;A. Das;10.1109/ICC.2003.1204494;Conferences;;0-7803-7802-4
ieee_20221205_08_28_59;The effect of the elevation of GPR antennas on data quality;In many GPR-measurement situations it is possible to measure with the antennas of the GPR-system in contact with medium containing the unknown objects. In this paper, we have tested the performance of a commercial Pulse Ekko 1000 system at various heights above a sand-surface. We show measurements over the same area with the antennas of this near-surface GPR system at four different heights above the surface. We show that the decrease in signal to noise ratio is small for two-way distances between antenna and surface is smaller than one third of the dominant wavelength. For larger distances, the signal quality decreases rapidly for data processing purposes and object detectability.;2003;R. Bloemenkamp;10.1109/AGPR.2003.1207319;Conferences;;90-76928-04-5
ieee_20221205_08_28_59;Survey of medical data from data acquisition to high quality patient treatment;"The European Clinical Database EuCliD/spl reg/ has been developed as a tool for supervising selected quality indicators of about 200 European dialysis centres. EuCliD/spl reg/ is a Lotus Notes/spl reg/ based database currently containing medical data of more than 22,000 dialysis patients from 220 dialysis centres in 10 European countries. Data evaluation is performed with statistical tools like SPSS. EuCliD/spl reg/ is used as a part of the Quality Management System of Fresenius Medical Care (FMC) and its Continuous Quality Improvement (CQI) program. Each participating dialysis centre receives benchmarking reports in regular intervals (currently every half year). The benchmark for all quality parameters is the weighted mean of the corresponding data of all centres. Quality parameters are selected according to the ""Quality Pyramid"" and the European Best Practice Guidelines (EBPG) or other accepted guidelines (for instance DOQI). An obvious impact of our work on the quality of the treatments could be observed. This concerns important outcome predictors like K*t/V and haemoglobin concentration as well as the outcome itself expressed in survival rates.";2003;H. Steil;10.1109/ITAB.2003.1222558;Conferences;;0-7803-7667-6
ieee_20221205_08_28_59;OpenGL volumizer: a toolkit for high quality volume rendering of large data sets;We present the OpenGL Volumizer API for interactive, high-quality, scalable visualization of large volumetric data sets. Volumizer provides a high-level interface to OpenGL hardware to allow application writers and researchers to visualize multiple gigabytes of volumetric data. Use of multiple graphics pipes scales rendering performance and system resources including pixel-fill rate and texture memory size. Volume roaming and multi-resolution volume rendering provide alternatives for interactive visualization of volume data. We combine the concepts of roaming and multi-resolution to introduce 3D Clip-Textures, an efficient technique to visualize arbitrarily large volume data by judiciously organizing and paging them to local graphics resources from storage peripherals. Volumetric shaders provide an interface for high quality volume rendering along with implementing new visualization techniques. This paper gives an overview of the API along with a discussion of large data visualization techniques used by Volumizer.;2002;P. Bhanirantka;10.1109/SWG.2002.1226509;Conferences;;0-7803-7641-2
ieee_20221205_08_28_59;Data quality management improvement;Summary form only given. Nowadays, more and more organizations are realizing the importance of their data, because it can be considered as an important asset present in nearly all business organizational processes. The traditional point of view of quality in information systems has been focused on software quality. The basis of our proposal is to consider information as a result of a data management process, which can be supported by the software running in the information system. Our aim is to optimize the data management process (DMP) in order to assure data quality. For this, we have just drawn up a framework based on maturity levels (such as CMMI) with specific and generic data quality goals, which are achieved by executing the corresponding activities.;2003;I. Caballero;10.1109/AICCSA.2003.1227489;Conferences;;0-7803-7983-7
ieee_20221205_08_28_59;Using repertory grids to test data quality and experts' hunches;The 'theorise-inquire' technique is described, which supports the testing of both experts' hunches and the quality of data sources. This technique is useful for the identification of data sources and data gaps by domain experts. We describe and illustrate the use of group contrasts, an analysis technique that allows an expert to explore and interpret repertory grids interactively to find significant contrasting relationships between attributes and test these against data sources.;2003;S. Stumpf;10.1109/DEXA.2003.1232120;Conferences;1529-4188;0-7695-1993-8
ieee_20221205_08_28_59;A quantitative analysis of the data acquisition requirements for measuring power quality phenomena;Measuring power quality phenomena poses great challenges to instruments designers as well as the users. The many phenomena having different characteristics call for different and possibly contradictory requirements on the data acquisition system. This article reviews the requirements needed for accurate measurement of power quality phenomena. It focuses on the sampling method and the digitization process. The results show that careful selection of data acquisition hardware is paramount to ensuring accurate measurements.;2003;Shiun Chen;10.1109/TPWRD.2003.817521;Journals;1937-4208;
ieee_20221205_08_28_59;High-quality two-level volume rendering of segmented data sets on consumer graphics hardware;One of the most important goals in volume rendering is to be able to visually separate and selectively enable specific objects of interest contained in a single volumetric data set, which can be approached by using explicit segmentation information. We show how segmented data sets can be rendered interactively on current consumer graphics hardware with high image quality and pixel-resolution filtering of object boundaries. In order to enhance object perception, we employ different levels of object distinction. First, each object can be assigned an individual transfer function, multiple of which can be applied in a single rendering pass. Second, different rendering modes such as direct volume rendering, iso-surfacing, and non-photorealistic techniques can be selected for each object. A minimal number of rendering passes is achieved by processing sets of objects that share the same rendering mode in a single pass. Third, local compositing modes such as alpha blending and MIP can be selected for each object in addition to a single global mode, thus enabling high-quality two-level volume rendering on GPUs.;2003;M. Hadwiger;10.1109/VISUAL.2003.1250386;Conferences;;0-7803-8120-3
ieee_20221205_08_28_59;Quality assessment of traversability maps from aerial LIDAR data for an unmanned ground vehicle;In this paper we address the problem of assessing quantitatively the quality of traversability maps computed from data collected by an airborne laser range finder. Such data is used to plan paths for an unmanned ground vehicle (UGV) prior to the execution of long range traverses. Little attention has been devoted to the problem we address in this paper. We use a unique data set of geodetic control points, real robot navigation data, ground LIDAR (light detection and ranging) data and aerial imagery, collected during a week long demonstration to support our work.;2003;N. Vandapel;10.1109/IROS.2003.1250645;Conferences;;0-7803-7860-1
ieee_20221205_08_28_59;High-quality vehicle trajectory generation from video data based on vehicle detection and description;Vehicle trajectories contain rich information on microscopic phenomena such as car following and lane changing. Despite many efforts to retrieve reliable trajectories from video images, previous approaches do not give high enough quality of trajectories that can be used in microscopic analysis. We introduce a new vehicle tracking approach based on a model-based 3-D vehicle detection and description algorithm. The proposed algorithm uses a probabilistic line feature grouping method to detect vehicles with little computation. A dynamic programming algorithm is proposed for fast reasoning. We present the system implementation and the vehicle detection and tracking results.;2003;Zu Whan Kim;10.1109/ITSC.2003.1251944;Conferences;;0-7803-8125-4
ieee_20221205_08_28_59;Enabling data quality notification in cooperative information systems through a Web-service based architecture;"Cooperative information systems (CISs) are often characterized by a high degree of data replication; as an example, in an e-government scenario, the personal data of citizens are stored by almost all administrations. In such scenarios, organizations typically provide the same information with distinct quality levels and this enables providing users with data of the highest available quality. Furthermore, the comparison of data values might be used to enforce a general improvement of data quality in all organizations. In the DaQuinCIS project (Aguilera et al., 1999), we propose an architecture for the management of data quality in CISs; this architecture allows the diffusion of data and related quality and exploits data replication to improve the overall quality of cooperative data. In this paper, we present an overview of a component of the DaQuinCIS architecture, namely the quality notification service (QNS), which is used to inform interested users when changes in quality values occur within the CIS. QNS can be used to control the quality of critical data, e.g. to keep track of quality changes and to be always aware when quality degrades under a certain threshold. The interaction between the QNS and its users follows the publish/subscribe paradigm: a user willing to be notified for quality changes subscribes to the QNS by submitting the features of the events to be notified for, through a specific subscription language. When a change in quality occurs, an event is published by the QNS i.e., all the users which have a consistent subscription receive a notification. However, as shown in the paper by Marchetti et al. (2003), currently available pub/sub infrastructures do not allow to meet all the requirements that a QNS implementation should satisfy, in particular scaling to a large number of users and coping with platform heterogeneity. QNS addresses both these problems through a layered architecture that: (i) encapsulates the technological infrastructure specific of each organization; (ii) adopts the standard Web-service technology to implement inter-organization communications; and (iii) embeds solutions and algorithms (namely, merge subscriptions and diffusion trees) to reduce the use of physical and computational resources. The remainder of this paper is organized as follows: we first introduce some preliminary concepts and the QNS specification; then we motivate and describe the internal architecture of the service. Due to the lack of space, explanations are given at a very high abstraction level. Interested readers can find technical and formal details, as well as running examples, in the paper by Scannapieco et al. (2003).";2003;C. Marchetti;10.1109/WISE.2003.1254505;Conferences;;0-7695-1999-7
ieee_20221205_08_28_59;Use of IEC 61850 object models for power system quality/security data exchange;This paper covers the use of the object models and services of the international standard IEC 61850 for information exchange to support power system quality and security functions. IEC 61850 has many features that make it cost effective, easy to implement and maintain and it may be readily extended to include new devices of interest to the electric utility industry. The object models incorporate basic data types for the common formats used in power system information exchange. Further, the object models are self-defining such that software and/or protocol changes are not required when configuring new and advanced power system applications. Also covered is the status of UCA/sup /spl reg// international, the users group that has assisted with the development of the object models and the related documents.;2003;A. Apostolov;10.1109/QSEPDS.2003.159813;Conferences;;2-85873-015-6
ieee_20221205_08_28_59;IEEE Recommended Practice for the Transfer of Power Quality Data;This recommended practice defines a file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner. The format is designed to represent all power quality phenomena identified in IEEE Std 1159 TM -1995, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to minimize disk space and transmission times. The utilization of globally unique identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.;2004;;10.1109/IEEESTD.2004.94416;Standards;;978-0-7381-3579-3
ieee_20221205_08_28_59;Welding quality monitoring and management system based on data mining technology;"In allusion to automatic arc welding production, this paper focuses on the establishment of on-line quality monitoring system based on data mining (DB) technology. The system is set up with client/server architecture. In the server, Shewhart control chart and decision tree are applied to monitor the production and supply decision-making based on space series DB; whilst Shewhart control chart and multivariate statistical process control (MSPC) are applied to give quality indication of welding process based on time series DB. The system realizes the on-line quality monitoring for each produced parts as well as the quality management for the whole workshop production.";2003;Chun-Hua Zhang;10.1109/ICMLC.2003.1264433;Conferences;;0-7803-8131-9
ieee_20221205_08_28_59;Power quality XML markup language for enhancing the sharing of power quality data;The growing attention on power quality related problems has prompted the need to share power quality information between measurement equipments, and analysis and simulation tools. However, existing equipment designs are still very much proprietary and are rarely catered for the exchange of information between one another. Contrasting data format and communication protocol has often obstructed the sharing of information. This paper describes an attempt to exploit a well-known Internet technology, the XML, to facilitate the sharing of power quality information. Based on the well-known power quality data interchange format (PQDIF), a new power quality XML markup language is proposed. The definition of the of markup language is described in details with illustrative examples language is described in details with illustrative examples of several XML-based power quality documents. Two possible data-exchanging scenarios, including one over the Internet, are also demonstrated in the paper.;2003;S. Chen;10.1109/PES.2003.1267388;Conferences;;0-7803-7989-6
ieee_20221205_08_28_59;Power quality data mining using soft computing and wavelet transform;This paper presents a new approach to power quality data mining using a modified wavelet transform for feature extraction of power disturbance signal data and a fuzzy multilayer perceptron network to generate the rules and classify the patterns. The choice of modified wavelet transform known as multiresolution s-transform is essential for processing very short duration nonstationary time series data from transient disturbances occurring on an electric supply network as they can not be handled by conventional Fourier and other transform methods for extraction of relevant features pertinent for data mining applications. The trained fuzzy neural network infers the output class membership value of an input pattern and a certainty measure is also presented to facilitate rule generation. Using the electric supply network disturbance data obtained from numerical algorithms and MATLAB software, the paper presents transient disturbance pattern classification scores. A knowledge discovery approach is also highlighted in the paper to convert raw power disturbance signal data to knowledge in the form of an answer module to the queries by the end-users. The pattern classification approach used in this paper can also be applied to speech, cardiovascular system and other medical and engineering databases.;2003;P.K. Dash;10.1109/TENCON.2003.1273392;Conferences;;0-7803-8162-9
ieee_20221205_08_28_59;2-D analysis and compression of power-quality event data;This paper introduces a novel two-dimensional (2-D) representation of the power quality event data. 2-D discrete-time wavelet transform is applied to the 2-D representation of real-life event data. The proposed representation and the transform is tested in terms of both event analysis and data compression. The experimental results indicate that the 2-D transform of the event data outperforms the results obtained by conventional one-dimensional (1-D) wavelet transform-based methods.;2004;O.N. Gerek;10.1109/TPWRD.2003.823197;Journals;1937-4208;
ieee_20221205_08_28_59;Factors affecting the signal quality in optical data transmission and estimation method for BER and SNR;Optical communication in fiber and photonic devices is subject to power loss and signal distortion due to linear and nonlinear phenomena. We provide an analysis of factors that affect the signal quality and we describe a method to estimate the signal bit error rate based on eye diagram sampling and statistical estimates.;2004;S.V. Kartalopoulos;10.1109/ITCC.2004.1286721;Conferences;;0-7695-2108-8
ieee_20221205_08_28_59;NEXRAD data quality by spectral processing. Spectral processing on NCAR's S-Pol radar;The WSR-88D radar system (NEXRAD) is developing a phase coded pulsing scheme using spectral processing that will improve data quality by removing overlaid echo contamination. A prototype spectral processor has been implemented on NCAR's S-Pol research radar for validation testing. Spectral processing will simultaneously enhance the anomalous propagation clutter mitigation technique, which uses a fuzzy logic based radar echo classifier, and allow future data quality enhancements.;2003;J. Keeler;10.1109/IGARSS.2003.1294435;Conferences;;0-7803-7929-2
ieee_20221205_08_28_59;Study on the quality of hyperspectral vegetation data observed in the field;A measurement model on spectra quality is presented through a bigram composed of a spectra quality grade and a metadata integrality grade. Quantitative describing datasets and qualitative describing datasets of spectra quality are extracted with spectra enveloping line analysis, spectra line profile analysis, principles of relative parameters matching and spectra prior knowledge. The quality grade is converted from subordinative degree of eigenpoints and eigenvalues from quantitative datasets. The metadata integrality grade is obtained by visiting each node in a multicross tree by which metadata about vegetation spectra is organized. The two grades make up a bigram by which one can evaluate vegetation spectra quality.;2003;Yanmin Shuai;10.1109/IGARSS.2003.1294875;Conferences;;0-7803-7929-2
ieee_20221205_08_28_59;Study on the spectral quality preservation derived from multisensor image fusion techniques between JERS-1 SAR and Landsat TM data;The advantage of multisensor data fusion stems from the fact that the use of multiple types of sensors increases the accuracy with which a quantity can be observed or characterized. The response of radar is more a function of geometry and structure than surface reflection as occurs in the optical wavelengths. A suitable fusion method has to be chosen with respect to the used spectral characteristic of the multispectral bands and the intended application. This paper describes a comparative study of multisensor image fusion techniques in preserving spectral quality of the fused images. Image fusion techniques applied in this study are: wavelet, intensity-hue-saturation (IHS), principal component analysis (PCA), and high pass filtering (HPF). With these image fusion techniques, a higher spatial resolution JERS-1 SAR is fused with Landsat TM data. The merging process is carried out at the pixel level and the comparison of the resulting images is explained based on the measurement in preserving spectral quality of the fused images. Assessment of the spectral quality is performed by graphical and statistical methods between the original TM image and the fused images. The factors computed to qualify the fused images are: mean, standard deviation, coefficient correlation, and entropy. With a visual inspection, wavelet and PCA techniques seem to be better than the other techniques. PCA provided the greatest improvement with an average entropy of about 5.119 bits/pixel.;2003;Rokhmatuloh;10.1109/IGARSS.2003.1295228;Conferences;;0-7803-7929-2
ieee_20221205_08_28_59;CALDEA: a data quality model based on maturity levels;In these days, most organizations have realised, at last, so important as their software is their data, because it can be considered as an important asset present in all business organizational processes. Our aim is to optimise the data management process (DMP) in order to assure data quality. For this, we have just drawn a framework based on maturity levels - as CMMIs ones -with specific and generic data quality goals, which are achieved by executing the corresponding activities.;2003;I. Caballero;10.1109/QSIC.2003.1319125;Conferences;;0-7695-2015-4
ieee_20221205_08_28_59;A data mining approach to objective speech quality measurement;"Existing objective speech quality measurement algorithms still fall short of the measurement accuracy that can be obtained from subjective listening tests. We propose an approach that uses statistical data mining techniques to improve the accuracy of auditory-model based quality measurement algorithms. We present the design of a novel measurement algorithm using the multivariate adaptive regression splines (MARS) method. A large set of speech distortion features is first created. MARS is used to find a small set of features that provide the best estimate (""model"") of speech quality. One appeal of the approach is that the model size can scale with the amount of speech data available for learning. In our simulations, the new algorithm furnishes significant performance improvement over PESQ (perceptual evaluation of speech quality).";2004;Wei Zha;10.1109/ICASSP.2004.1326022;Conferences;1520-6149;0-7803-8484-9
ieee_20221205_08_28_59;Soundness and quality of semantic retrieval in DNA-based memories with abiotic data;Associative memories based on DNA-affinity have been proposed based in L. M. Adleman (1994) and E. Baum (1995). Previously, we have quantified the quality of retrieval of genomic information in simulation as stated in M. Garzon et al. (2003). Here, the ability of two types of DNA-based memories to store abiotic data and retrieve semantic information is evaluated for soundness and compared to state-of-the-art symbolic methods available, such as LSA (latent semantic analysis) of T. K. Landauer et al. Their ability is poor when performed without a proper compaction procedure. However, when the corpus is summarized through a selection protocol based on PCR or a training procedure on J. Chen et al. (2004) to extract useful information, their performance is much closer to that of LSA, according to human expert ratings. These results are expected to improve and scale up when actual DNA molecules are employed in real test tubes, currently a feasible goal.;2004;A. Neel;10.1109/CEC.2004.1331126;Conferences;;0-7803-8515-2
ieee_20221205_08_28_59;High quality isosurface generation from volumetric data and its application to visualization of medical CT data;We propose a method for generating an isosurface from volumetric data sampled with a face-centered cubic lattice. The display quality of the isosurface obtained by our method is greatly enhanced because it generates many good aspect ratio triangle patches. We applied the method to visualization of a colonic wall from medical data. We experimentally compared the resulting surface of our method with those of existing methods, showing the effectiveness of our method.;2004;T. Takahashi;10.1109/ICPR.2004.1334633;Conferences;1051-4651;0-7695-2128-2
ieee_20221205_08_28_59;Data compression technique in recording electric arc furnace voltage and current waveforms for tracking power quality;A data compression technique is used to enhance the storage capability of electric power quality instruments in recording electric arc furnace voltage and currents waveforms, where voltage flicker and harmonic current distortion are major power quality disturbances. The compression approaches by multi-resolution analysis with threshold coding and vector quantization coding are compared. The digital filters are designed by the discrete wavelet transform. The results from spectrum analysis show high compression ratios while keeping low information loss. From the calculation results, the reconstructed voltage waveforms using threshold coding almost keep the same voltage flicker values. The vector quantization coding is better for current waveforms to keep harmonic distortion values. From the simulation results of the measurement data, the memory requirement is reduced to 20.4% while power quality characteristics are almost kept.;2003;Chi-Jui Wu;10.1109/TDC.2003.1335253;Conferences;;0-7803-8110-6
ieee_20221205_08_28_59;Research on the application of data-mining for quality analysis in petroleum refining industry;High product quality is the main target of the petroleum refining industry. It is widely admitted that there are some limitations of traditional product-quality-monitoring methods. Data-mining (DM) is a method to get useful information, which other regular methods cannot find, from enormous data. Data warehouse (DW) is the best way to store and manage massive enterprise data and provide a strong support to data analysis methods. This paper presents a new framework to deal with quality analysis, which combines the soft sensor, DM and DW. It promises to overcome the limitations of soft sensor and apply soft sensor in quality analysis in the petroleum refining industry.;2004;Jiang Chen;10.1109/WCICA.2004.1342326;Conferences;;0-7803-8273-0
ieee_20221205_08_28_59;Water quality classification of lakes using 250-m MODIS data;The traditional method used in the water quality classification of Finnish lakes includes the collection of water samples from lakes and their analysis in laboratory conditions. The classification is based on statistical analysis of water quality parameter values and on expert opinion. It is possible to acquire similar information by using radiance values measured with the Earth Observing System Terra/Aqua Moderate Resolution Imaging Spectroradiometer (MODIS). In this letter, the classification accuracy with MODIS data is about 80%. Only about 0.2% of the 20 391 pixels were misclassified by two or more classes, as a four-class classification system is used.;2004;S. Koponen;10.1109/LGRS.2004.836786;Journals;1558-0571;
ieee_20221205_08_28_59;Data processing development in the field of power quality monitoring;The presented paper deals with the problems of electrical power quality estimation, especially in multi-disturbances conditions. The synergy effect of different kinds of supply voltage disturbances have been pointed out and it have been described in relation to additional temperature rise of induction motor windings. The practical consequences for power quality analysis have been singled out. Proposed solution has been laid and commented on.;2004;P. Gnacinski;10.1109/IMTC.2004.1351394;Conferences;1091-5281;0-7803-8248-X
ieee_20221205_08_28_59;The CMS electromagnetic calorimeter: results on crystal measurements, quality control and data management in the Rome Regional Center;The barrel of the CMS electromagnetic calorimeter is currently under construction and will contain 61200 PbWO/sub 4/ crystals. Half of them are being fully characterized for dimensions, optical properties and light yield in the INFN-ENEA Regional Center near Rome. We describe the setup of an automatic quality control system for the crystal measurements and the present results on their qualification, as well as the REDACLE project, which has been developed to control and ease the production process. As it will not be possible to precalibrate the whole calorimeter, the crystal measurements and quality checks performed of the Regional Center will be crucial to provide a basis for fast in-situ calibration with particles. REDACLE is at the same time a fast database and a data managements system, where the database and the workflow structures are decoupled, in order to obtain the best flexibility.;2004;S. Costantini;10.1109/IMTC.2004.1351443;Conferences;1091-5281;0-7803-8248-X
ieee_20221205_08_28_59;Data quality and grounding considerations in a mixed-use facility;Most industrial, commercial, and educational facilities depend heavily on telecommunications, data, and computer networks. In many petrochemical installations, the functions of all three types of facilities are combined into one location. These facilities often experience problems with signal quality and noise on communication lines because of the proximity of high-speed, low-level signals to higher power lines. Additionally, electrical safety and various code issues must be addressed. Topics such as multiple ground systems, connections to ground, potential difference, circulating currents, and wiring methods are discussed. Eleven different systems must be bonded together. Three areas specific to networks require special attention. These are equipment bonding, isolated earth ground, and isolated power systems. Finally, cathodic protection systems have a unique impact on grounding systems.;2004;M.O. Durham;10.1109/PCICON.2004.1352768;Conferences;0090-3507;0-7803-8698-1
ieee_20221205_08_28_59;A study on the method to reduce data transfer in multi-quality VOD service;In the VOD service, the low resolution video data is expected for content searching, and the high resolution video data is expected for browsing. We have presented the effect of spatial video scalability encoding in cache utilization. In this study, we report comprehensive experiments on cases of various updating procedures, and the effects of differential data transferring. As a result, if a time-cycle of downloading, transcoding and browsing is too short for all clients to download contents within, the scalability encoding method can improve the quality of service, in both of data size and consumed time.;2004;S. Suzuki;10.1109/MWSCAS.2004.1353979;Conferences;;0-7803-8346-X
ieee_20221205_08_28_59;The necessity of assuring quality in software measurement data;Software measurement data is often used to model software quality classification models. Related literature has focussed on developing new classification techniques and schemes with the aim of improving classification accuracy. However, the quality of software measurement data used to build such classification models plays a critical role in their accuracy and usefulness. We present empirical case studies, which demonstrate that despite using a very large number of diverse classification techniques for building software quality classification models, the classification accuracy does not show a dramatic improvement. For example, a simple lines-of-code based classification performs comparatively to some other more advanced classification techniques such as neural networks, decision trees, and case-based reasoning. Case studies of the NASA JM1 and KC2 software measurement datasets (obtained through the NASA Metrics Data Program) are presented. Some possible reasons that affect the quality of a software measurement dataset include presence of data noise, errors due to improper software data collection, exclusion of software metrics that are better representative software quality indicators, and improper recording of software fault data. This study shows, through an empirical study, that instead of searching for a classification technique that perform well for given software measurement dataset, the software quality and development teams should focus on improving the quality of the software measurement dataset.;2004;T.M. Khoshgoftaar;10.1109/METRIC.2004.1357896;Conferences;1530-1435;0-7695-2129-0
ieee_20221205_08_28_59;On data aggregation quality and energy efficiency of wireless sensor network protocols - extended summary;"In-network data gathering and data fusion are essential for the efficient operation of wireless sensor networks. While most existing data gathering routing protocols addressed the issue of energy efficiency, few of them, however, have considered the quality of the implied data aggregation process. In this work, an information model for sensed data is first formulated. A new metric for evaluating data aggregation process, data aggregation quality (DAQ), is formally derived. DAQ does not assume any prior knowledge on values or on statistical distributions of sensing data, and may be applied to most data gathering protocols. Next, two new protocols are proposed: the enhanced LEACH and the clustered PEGASIS, enhanced from two major existing protocols: the cluster-based LEACH and the chain-based PEGASIS. By carefully accounting for listening energy, energy efficiency of all four protocols is evaluated. In addition, DAQ is applied to evaluate their data aggregation process. It is found that, while chain-based protocols are more energy efficient than cluster-based protocols, they however suffer from poor data aggregation quality. DAQ may be readily applied to most of continuous data gathering protocols; it is therefore significant to future development of sensor network protocols.";2004;T. Pham;10.1109/BROADNETS.2004.51;Conferences;;0-7695-2221-1
ieee_20221205_08_28_59;An integrated system for estimating crop quality based on remotely sensed imagery and GIS data;A method for estimating grain quality of winter wheat using Landsat Thematic Mapper data and GIS data is presented, and it gave the development and application of the Geographical Information Application system for estimating wheat grain quality. The system, supported by COMGIS, JModel-Base Management System and other advanced Spatial Information Technologies, can deal with the fusion analysis of remotely sensed data, GIS data and other multisources data, flexible management of models and knowledge;2004;Yuchun Pan;10.1109/IGARSS.2004.1368635;Conferences;;0-7803-8742-2
ieee_20221205_08_28_59;Water quality monitoring in Taihu Lake using MODIS image data;Remote sensing technique has been widely applied in water quality monitoring, since it can provide both spatial and temporal information needed to detect the changes of water quality. However, inland water monitoring using remote sensing technique is still experimental, and its development depends on improved remote sensors with higher spectral and spatial resolution. The purpose of this paper is to apply MODIS image data to inland lake water quality monitoring, and then to provide a MODIS-based procedure for regional inland water quality monitoring. After the correlation analysis between band combinations and water parameters such as chlorophyll-a and suspended sediment, we proposed an empirical algorithm based on certain MODIS bands. The study showed that MODIS image data were useful for water quality monitoring in Taihu Lake.;2004;Lingya Zhu;10.1109/IGARSS.2004.1369749;Conferences;;0-7803-8742-2
ieee_20221205_08_28_59;Studies on methods for quality assessment of crop spectral data;In the process of measurement, a number of factors will affect the quality of data. Therefore, data must be verified and assessed before their applications. This work discussed some methods for quality assessment of crop spectral data, which include methods of analysis of spectral characteristics, statistical test and spectral simulation. The method of spectral analysis compares measured spectra with the reference spectrum, and analyzes the location of wave crest, wave trough and the shape, intensity of spectral reflectance curves. The method of statistical test consists of shape similarity test and intensity test. The shape similarity test analyzes the correlation between measured spectral data and the reference spectral datum over the special wavelength range and assesses quality of the measured data by the correlation coefficients. The intensity test calculates the mean value and standard deviation of spectral data, and forms a spectral zone around the mean value. We consider it as the abnormal one if one spectral curve goes beyond the spectrum zone. The method of spectral simulation mainly compares measured spectra with simulated spectra by combined PROSPECT-SAIL model. Results show these methods of quality assessment are feasible.;2004;Xuehong Zhang;10.1109/IGARSS.2004.1369949;Conferences;;0-7803-8742-2
ieee_20221205_08_28_59;Study on data models of image quality assessment for the Chinese-Brazil Earth Resources Satellite;Analysis on Image Characteristic is a key step to understand and make good use of remote sensing image. Data models were employed to analyze for image quality of the Chinese-Brazil Earth Resources Satellite (CBERS-2) and LANDSAT/TM in Li-jiang region, Yunnan province. They include normalized model, univariate image statistics model, the ratio of inner and mutual variance model and spatial resolution model. The normalized model can be used to convert data into same physical quantity in order to make data be comparability. The other models can be used to open out the classified capability and information of data in supervised classification. With the study on data model, a method to analyze and assess remote sensing data was tried to bring forward;2004;Huang Miao-Fen;10.1109/IGARSS.2004.1369991;Conferences;;0-7803-8742-2
ieee_20221205_08_28_59;Road detection statistics for automated quality control of GIS data;We examine the use of road detection in VHR satellite images to automate the process of quality assessment of digital road network data. An important aspect is the emphasis on accuracy and reliability of the system. Although road detection has been studied for more than a decade, it is often difficult to assess what performance can be expected over datasets other than the images published. We propose a system to train a road detector based on image examples of typical roads. The system calculates the optimal detection parameters and estimates the performance over the dataset in terms of detection rate and degree of fragmentation. The methodology relies on error propagation and image statistics, and is generic in nature. By showing image examples, variations due to shadow, activity on the road, weather conditions etc. can be taken into account when estimating the expected performance.;2004;W. Goernan;10.1109/IGARSS.2004.1370280;Conferences;;0-7803-8742-2
ieee_20221205_08_28_59;A near optimal approach to quality of service data replication scheduling;This paper describes an approach to real-time decision-making for quality of service based scheduling of distributed asynchronous data replication. The proposed approach addresses uncertainty and variability in the quantity of data to replicate over low bandwidth fixed communication links. A dynamic stochastic knapsack is used to model the acceptance policy with dynamic programming optimization employed to perform offline optimization. The obtained optimal values of the input variables are used to build and train a multilayer neural network. The obtained neural network weights and configuration can be used to perform near optimal accept/reject decisions in real-time. Offline processing is used to establish the initial acceptance policy and to verify that the system continues to perform near-optimally. The proposed approach is implemented via simulation enabling the evaluation of a variety of scenarios and refinement of the scheduling portion of the model. The preliminary results are very promising.;2004;K. Adams;10.1109/WSC.2004.1371539;Conferences;;0-7803-8786-4
ieee_20221205_08_28_59;Analysis of data compression methods for power quality events;The use of digital technology for data acquisition and process of electric power quality events results in the growth the increase of available data , which need to be transferred and stored to be post, processed. Therefore much research has recently been carried out in the area of data compression in order to reduce the size of the files that contain the stored data and reduce the communication time. In this paper different data compression methods are reviewed and their application to the power quality area is studied.;2004;F. Lorio;10.1109/PES.2004.1372851;Conferences;;0-7803-8465-2
ieee_20221205_08_28_59;Robust quality management for differentiated imprecise data services;Several applications, such as Web services and e-commerce, are operating in open environments where the workload characteristics, such as the load applied on the system and the worst-case execution times, are inaccurate or even not known in advance. This implies that transactions submitted to a real-time database cannot be subject to exact schedulability analysis given the lack of a priori knowledge of the workload. In this paper we propose an approach, based on feedback control, for managing the quality of service of real-time databases that provide imprecise and differentiated services, given inaccurate workload characteristics. For each service class, the database operator specifies the quality of service requirements by explicitly declaring the precision requirements of the data and the results of the transactions. The performance evaluation shows that our approach provides reliable quality of service even in the face of varying load and inaccurate execution time estimates.;2004;M. Amirijoo;10.1109/REAL.2004.49;Conferences;1052-8725;0-7695-2247-5
ieee_20221205_08_28_59;Statistical quality of service guarantee for temporal consistency of real-time data objects;In this paper, we study the problem of temporal consistency maintenance where a certain degree of temporal inconsistency is tolerable. We propose a suite of statistical more-less (SML) approaches to tradeoff of quality of service (QoS) of temporal consistency against the number of supported transactions. We begin with a base-line algorithm, SML-BA, which provides the requested QoS of temporal consistency. We then propose SML with optimization (SML-OPT) to further improve the QoS by better utilizing the excessive CPU capacity. Finally, we enhance SML-OPT with a slack reclaiming scheme (SML-SR). The reclaimed slacks are used to process jobs whose required computation time is larger than the guaranteed computation time. Simulation experiments are conducted to compare the performance of these schemes (SML-BA, SML-OPT and SML-SR) together with the deterministic more-less and half-half schemes. Our results show that the SML schemes are effective in trading off the schedulability of transactions and the QoS guaranteed. Moreover, SML-SR performs best and offers a significant QoS improvement over SML-BA and SML-OPT.;2004;Kam-Yiu Lam;10.1109/REAL.2004.52;Conferences;1052-8725;0-7695-2247-5
ieee_20221205_08_28_59;A Web Services Application for the Data Quality Management in the B2B Networked Environment;Characteristics of Web services such as interoperability and platform independence make Web service a promising technique to manage data quality effectively in inter-organizational information exchanges. In this paper, we describe an application of Web services for managing data quality in the B2B information exchange that is typically characterized by large volumes of information, widely distributed data sources, and frequent information interchanges. In such environments, it is important that organizations are able to evaluate the quality of information they get from other organizations. We propose a framework for managing data quality in inter-organizational settings using the information product approach. We highlight the requirements for data quality management and the developing Web service standards to show why Web services offer a unique, yet simple platform for managing data quality in inter-organizational settings.;2005;G. Shankaranarayanan;10.1109/HICSS.2005.62;Conferences;1530-1605;0-7695-2268-8
ieee_20221205_08_28_59;Why CRM Efforts Fail? A Study of the Impact of Data Quality and Data Integration;This paper reports the results of a study into the implementation of data-driven customer relationship management (CRM) strategies. Despite its popularity, there is still a significant failure rate of CRM projects. A combination of survey and interviews/case studies research approach was used. It is found that CRM implementers are not investing enough efforts in improving data quality and data integration processes to support their CRM applications.;2005;F. Missi;10.1109/HICSS.2005.695;Conferences;1530-1605;0-7695-2268-8
ieee_20221205_08_28_59;Efficient channel quality feedback schemes for adaptive modulation and coding of packet data;Adaptive modulation and coding (AMC) is a powerful technique to improve throughput especially of wireless packet-oriented channels. In general such link adaptation schemes rely on feedback from the receiver, which allows selecting the appropriate modulation and coding. In this paper we evaluate the performance of different channel quality feedback schemes and propose a general performance metric, that considers data throughput, energy efficiency, channel resource consumption, as well as control overhead. At the example of the high speed downlink packet access (HSDPA) channel for UMTS FDD, we show that channel quality feedback schemes that consider the burstiness of packet data are able to provide notably higher efficiency than the cyclic feedback currently standardized.;2004;M. Dottling;10.1109/VETECF.2004.1400221;Conferences;1090-3038;0-7803-8521-7
ieee_20221205_08_28_59;SIQuA: server-aware image quality adaptation for optimizing server latency and capacity in wireless image data services [mobile radio];This paper investigates the problem of providing guaranteed server-side latency while performing dynamic image adaptation. We introduce a server-aware image quality adaptation (SIQuA) technique, used to optimize the runtime image adaptation process so as to provide guaranteed server-side latency with a maximized quality of image service level. SIQuA utilizes dynamic server conditions (server traffic loads and server resource availability), and optimizes both the quality of image serviced and the server-side latency required for runtime image adaptation, simultaneously, through judicious tuning of image compression parameters. We demonstrate that SIQuA can significantly reduce the server-side latency with a minimal impact on the image quality, thereby enabling efficient wireless image services with greatly reduced overall service costs.;2004;Dong-Gi Lee;10.1109/VETECF.2004.1400529;Conferences;1090-3038;0-7803-8521-7
ieee_20221205_08_28_59;On improving voice quality degraded by packet loss in data networks;"In voice over data networks, packet loss can have a major impact on perceived voice quality. The impact of packet loss on perceived voice quality depends on several factors, including loss pattern, codec type and packet size. Many techniques have been developed to recover lost packets. A newly developed recovery technique, ""switched recovery technique"" (SRT), employs a suitable recovery technique from known techniques to estimate the lost packet content (silence/voiced/unvoiced), depending on the preceding and following packets that were received correctly. The proposed technique was tested with different codecs, such as LD-CELP, LPC-10, and variable rate coders. Two loss models, random model and burst model, have been used to simulate the packet loss";2004;M.E. Nasr;10.1109/AFRICON.2004.1406632;Conferences;;0-7803-8605-1
ieee_20221205_08_28_59;Transduction and typicalness for quality assessment of individual classifications in machine learning and data mining;In the past, machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper, we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier. We perform a series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary TCM-NN method as well as with kernel density estimation. We show that the proposed method significantly outperforms density estimation methods, and how it may be used to improve their performance.;2004;M. Kukar;10.1109/ICDM.2004.10089;Conferences;;0-7695-2142-8
ieee_20221205_08_28_59;Dynamic end-to-end image adaptation for guaranteed quality of service in wireless image data services;The paper investigates total end-to-end service variability issues in providing dynamic image adaptation services to wireless end users. While most existing image adaptation techniques focus on improving wireless transmission latency by adapting content-rich images to the time varying bandwidth availability of wireless networks, we have found dynamic service load variations at the server also significantly affect end-to-end service latency. We introduce a dynamic end-to-end image adaptation technique that optimizes the runtime image adaptation process so as to provide guaranteed total service latency (including both server processing latency and wireless transmission latency) with a maximized image quality. Through judicious tuning of application-layer image compression parameters, the proposed technique dynamically optimizes both the image quality and each component of service latency required for wireless image data services simultaneously depending on service variability issues existing at both server-side and network-side. Experimental results under synthetic service workloads demonstrate that our proposed dynamic end-to-end image adaptation can provide guaranteed service latency with a minimal impact on the image quality loss.;2005;Dong-Gi Lee;10.1109/WCNC.2005.1424909;Conferences;1558-2612;0-7803-8966-2
ieee_20221205_08_28_59;Application of data mining and artificial modeling for coagulant dosage of water treatment plants corresponding to water quality;Shortage of water is gradually accelerated because a high standard of living is required and water resources are rapidly run dry. Therefore, effective water treatment is necessary to retain the required quality and amount of water. The general treatment includes coagulation, flocculation, filtering, and disinfections. Coagulation, flocculation, and disinfections are major parts of the water treatment processes. In this paper, new automatic algorithm is proposed for coagulation that is one of the water treatment processes. The proposed method is showing how to determine the coagulant soil and amounts using data mining techniques.;2004;Hyeon Bae;10.1109/IECON.2004.1432134;Conferences;;0-7803-8730-9
ieee_20221205_08_28_59;Mosaicing of medical video-endoscopic images: data quality improvement and algorithm testing;This conlribution deals with the construction of a cartography of the internal surface of organs based on image sequences acquired during clinical video-endoscopic exams. This paper describes a metrological approach to adapt the images acquisition conditions and the pre-processing tools to the evaluation of our specifically developed registration and mosaicing algorithm. A 3D-micrometric positioning system and an endoscope's tip holding structure was used to acquire sequences of images following a specific protocol. Forty images of a test scene (bladder planar photography) were acquired along a determined displacement looped path. An algorithm for correcting non-linear radial distortion caused by the endoscope was implemented. This algorithm computes projective (camera) and polynomial (distortion) transformations. The optimization process regislers the corrected distorted pattern image with the non-distorted one. Mutual information was used as a measure of similarity and stochastic gradient descent method for optimization. The visual quality of the images was then improved by a shading correction and by removing the optical fibers pattern from the images (specific low-pass filtering). A low-pass filter was used for obtaining the background that is subtracted from every image. The registration algorithm based on mutual inrormation as similarity measure and stochastic gradient descent as optimization method was used to obtain the transformation parameters that were applied to the images for building a mosaic (cartography). Finally, these parameters were applied to a similar sequence of images of a dots pattern (in place of the photography) allowing us to compute significant error parameters for the quality of the mosaicing. The results of the various processes are presented and the evaluation of the performancc of the cartography is discussed.;2004;R. Miranda-Luna;10.1109/ICEEE.2004.1433942;Conferences;;0-7803-8531-4
ieee_20221205_08_28_59;Quality assurance experience data base;An outline is given of a method and a tool developed to ensure permanent software quality evaluation and to create an experience database for future quality assurance tasks. The authors mainly focus on the benefits of using such a tool and its evolution in an industrial environment for global quality assurance of multisite large-scale software projects.<>;1991;M. Defamie;10.1109/ISSRE.1991.145370;Conferences;;0-8186-2143-5
ieee_20221205_08_28_59;Tuning a paper mill SCADA system using quality control data;Various paper qualities are automatically monitored on-line in a paper mill. These qualities are indirectly measured and used by the SCADA System to keep the product in trim. The same qualities are also directly manually measured from paper taken from the finished reels. This data is processed by the quality control section. Both the data from the computer and the quality control data are stored and comparisons made. Although vast amounts of automatically and manually collected data are continuously available it is difficult to confirm that the two sets of data are compatible. The doubt about the validity of the existing data and the proposal to increase the machine speed by one third caused the company to investigate the automatically collected data. This paper describes that investigation and the conclusions drawn.<>;1990;P. Cheek;;Conferences;;0-86341-704-3
ieee_20221205_08_28_59;A study on data quality management maturity model;Previously many studies on data quality have been focused on the realization and evaluation of data value quality and data service quality. But those studies showed poor data value quality and poor data service quality caused by the poor data structure. Meanwhile, in this study we focused on meta data management, that is to say, data structure quality. Especially, data quality management maturity model is introduced as a way of maturity model. And then it is empirically proved to show the improvement of the data quality as the data management matures.;2005;Rye Kyung-seok;10.1109/ICACT.2005.245923;Conferences;;
ieee_20221205_08_28_59;Protection device monitoring using data from power quality monitoring devices;In this paper, a model based expert system method was developed for monitoring the operation of protection devices based on the data obtained from power quality monitoring devices placed at the substations. First, models of physical protection devices were developed, and then actual power quality monitoring data were fed to the models to get an estimate of protection device operation. Test results, based on the actual data, show that, the proposed method can identify which protection device has operated to clear an observed fault, especially when there are multiple protection devices on the system.;2005;M. Baran;10.1109/PES.2005.1489300;Conferences;1932-5517;0-7803-9157-8
ieee_20221205_08_28_59;On improving voice quality degraded by packet loss in data networks;"In voice over data networks, packet loss can have a major impact on perceived voice quality. The impact of packet loss on perceived voice quality depends on several factors, including loss pattern, codec type and packet size. Many techniques were developed to recover lost packets. In this paper a new recovery technique called ""switched recovery technique"", SRT, which is developed by employing the suitable recovery technique from known techniques to an estimate of the lost packet content (silence/voiced/unvoiced) depending on preceding and following packets that received correctly. The proposed technique was tested with different codecs such as LD-CELP, LPC-10, and variable rate coders. Two models of loss have been used in this study, random model and burst model to simulate the packet loss";2005;M.E. Nasr;10.1109/NRSC.2005.194032;Conferences;;977-503183-4
ieee_20221205_08_28_59;An experimental study of the effects of contextual data quality and task complexity on decision performance;The effects of information quality and the importance of information have been reported in the information systems (IS) literature. However, little has been learned about the impact of data quality (DQ) on decision performance. This study explores the effects of contextual DQ and task complexity on decision performance. To examine the effects of contextual DQ and task complexity, a laboratory experiment was conducted. Based on two levels of contextual DQ and two levels of task complexity, this study had a 2/spl times/2 factorial design. The dependent variables were problem-solving accuracy and time. The results demonstrated that the effects of contextual DQ on decision performance were significant. The findings suggest that decision makers can expect to improve their decision performance by enhancing contextual DQ. This research extends a body of research examining the effects of factors that can be tied to human decision-making performance.;2005;W. Jung;10.1109/IRI-05.2005.1506465;Conferences;;0-7803-9093-8
ieee_20221205_08_28_59;Automated quality control of tropical cyclone winds through data mining;The analysis of tropical cyclones (TC) depends heavily on the quality of the incoming data set. With the advances in technology, the sizes of these data sets also increase. There is a great demand for an efficient and effective unsupervised quality control tool. Towards such a demand, data mining algorithms like spatial clustering and specialized distance measures can be applied to perform this task. This paper reports our findings on the studies on utilizing a density-based clustering algorithm with three different distance measures on a series of TC data sets.;2005;N.C. Carrasco;10.1109/IRI-05.2005.1506478;Conferences;;0-7803-9093-8
ieee_20221205_08_28_59;Objective video quality metric based on data hiding;In this paper, a new no-reference (NR) objective metric based on data hiding is proposed. The metric has the advantage of being fast and not requiring knowledge of the original video contents. The proposed method uses a spread-spectrum embedding algorithm to embed a mark (binary image) into video frames. A t the receiver, the mark is extracted and a measure of its degradation is used to estimate the quality of the video. We used data gathered from psychophysical experiments to help in the design of the video quality assessment system. We evaluated the visibility and annoyance of the impairments caused by the embedding algorithm and estimated the 'best' mark strength for a particular video. The performance of the proposed metric is estimated by measuring its ability to predict the total squared error (TSE) of the host video and the mean observer score (MOS) obtained from naive subjects in a psychophysical experiment. Experimental results show that the proposed metric had a good performance and a good correlation with the MOS.;2005;M.C.Q. Farias;10.1109/TCE.2005.1510512;Journals;1558-4127;
ieee_20221205_08_28_59;Quality assessment using data hiding on perceptually important areas;In this paper, we present a no-reference video quality metric that blindly estimates the quality of a video. The proposed approach makes use of a data hiding technique to embed a fragile mark into perceptually important areas of the video frame. To estimate the importance of an area, we take into account three perceptual features that are known to attract visual attention: motion, contrast, and color. At the receiver, the mark is extracted from the perceptually important areas of the decoded video. Then, a quality measure of the video is obtained by computing the degradation of the extracted mark. Simulation results indicate that the proposed video quality metric outperforms standard peak signal to noise ratio (PSNR) in estimating the perceived quality of a video. Additionally, results from a subjective experiment show that the metric output values increase monotonically with the mean annoyance scores gathered from the human observers.;2005;M. Carli;10.1109/ICIP.2005.1530613;Conferences;2381-8549;0-7803-9134-9
ieee_20221205_08_28_59;Prefiltered Gaussian reconstruction for high-quality rendering of volumetric data sampled on a body-centered cubic grid;In this paper a novel high-quality reconstruction scheme is presented. Although our method is mainly proposed to reconstruct volumetric data sampled on an optimal body-centered cubic (BCC) grid, it can be easily adapted lo the conventional regular rectilinear grid as well. The reconstruction process is decomposed into two steps. The first step, which is considered to be a preprocessing, is a discrete Gaussian deconvolution performed only once in the frequency domain. Afterwards, the second step is a spatial-domain convolution with a truncated Gaussian kernel, which is used to interpolate arbitrary samples for ray casting. Since the preprocessing is actually a discrete prefiltering, we call our technique prefiltered Gaussian reconstruction (PGR). It is shown that the impulse response of PGR well approximates the ideal reconstruction kernel. Therefore the quality of PGR is much higher than that of previous reconstruction techniques proposed for optimally sampled data, which are based on linear and cubic box splines adapted to the BCC grid. Concerning the performance, PGR is slower than linear box-spline reconstruction but significantly faster than cubic box-spline reconstruction.;2005;B. Csebfalvi;10.1109/VISUAL.2005.1532810;Conferences;;0-7803-9462-3
ieee_20221205_08_28_59;Evaluating and improving integration quality for heterogeneous data sources using statistical analysis;This paper considers the problem of integrating heterogeneous semi-structured data sources with the purpose of estimating integration quality (IQ). Integration of such data sources leads to results with unpredictable trustworthiness and none of the existing methods is capable of accounting for the uncertainty which is accumulated over all of the integration steps and which affects integration quality. To compute the uncertainties we suggest using a well-established statistical method Latent Class Analysis (LCA). This method allows to analyze the influence of the latent factors associated with the real-world entities on the set of data. We show on examples how the proposed approach can be used for evaluating and improving IQ giving an important tool to the users concerned with the data's trustworthiness.;2005;E. Altareva;10.1109/IDEAS.2005.25;Conferences;1098-8068;0-7695-2404-4
ieee_20221205_08_28_59;An adaptive reverse link data rate control scheme based on channel quality for CDMA system;The mechanism of the reverse link (RL) data rate control in CDMA system is studied, and a new scheme is presented to enhance the efficiency, which is based not only on probabilistic model, but also on a variable signifying the competitive ability of the mobile station (MS). This variable is related to MS' reverse pilot transmit power, which can reflect the reverse channel quality in some sense. With this new mechanism, The MS enjoying better channel quality is easier in raising its data transmission rate while the MS undergoing worse channel quality is more likely to reduce its rate. Since MS' reverse pilot transmit power is a variable known by MS itself, no extra signaling overhead is required from BS. Through dynamic system level simulation for cdma2000 release D where H-ARQ is employed on RL, the results show that the system throughput on RL is improved by the proposed scheme with the rise over thermal (RoT) value kept on a stable level.;2005;Qin Jie;10.1109/WCNM.2005.1544243;Conferences;2161-9654;0-7803-9335-X
ieee_20221205_08_28_59;Quality assessment of gene expression data;With the escalating amount of gene expression data being produced by microarray technology, one of important issues in the analysis of expression data is quality assessment, in which we want to know whether the one chip is artifactually high or low intensity relative to the majority of the chip. We propose a graphical tool implemented in R for visualizing distributions of two gene chips. Moreover, a statistical test based on chi-square test is employed to quantify degrees of array comparability for pairwise comparisons on a large number of arrays.;2005;Chen-An Tsai;10.1109/EITC.2005.1544369;Conferences;;0-7803-9328-7
ieee_20221205_08_28_59;Development of an on-line data quality monitor for the relativistic heavy-ion experiment ALICE;The on-line data monitoring tool developed for the coming ALICE experiment at LHC, CERN is presented. This monitoring tool which is a part of the ALICE-DAQ software framework, written entirely in C++ language, uses standard Linux tools in conjunction with the data display and analysis package ROOT, developed at CERN. It allows checking the consistency and quality of the data and correct functioning of the various sub-detectors either at run time or during off line by playing back the recorded raw data. After discussing the functionality and performance of this package, the experience gained during the test beam periods is also summarized.;2005;O. Cobanoglu;10.1109/RTC.2005.1547409;Conferences;;0-7803-9183-7
ieee_20221205_08_28_59;An Optimization Technique for the Evaluation of Eddy Current Inspection Data to Determine Weld Quality;This paper describes the design, engineering, assembly, testing and implementation of an eddy current inspection system to determine weld quality for a laser welding process. This system performs an in situ circumferential weld depth measurement of nuclear weapons primary components during fabrication. The goal of the inspection is to provide an accurate and repeatable estimate of the weld quality while minimizing contact with the part. Eddy current testing is a non-destructive testing (NDT) method used for interrogating conductive materials for defects and metallurgical characteristics. The technique is commonly used in the construction, aerospace, automotive, chemical processing, and power industries to examine structural components for cracks and voids and to study material properties such as hardness, embrittlement, permeability, and conductivity. This paper presents an analysis technique for the evaluation of eddy current data for the determination of weld quality;2005;K.W. Hench;10.1109/ICSMC.2005.1571365;Conferences;1062-922X;0-7803-9298-1
ieee_20221205_08_28_59;Analyzing software quality with limited fault-proneness defect data;Assuring whether the desired software quality and reliability is met for a project is as important as delivering it within scheduled budget and time. This is especially vital for high-assurance software systems where software failures can have severe consequences. To achieve the desired software quality, practitioners utilize software quality models to identify high-risk program modules: e.g., software quality classification models are built using training data consisting of software measurements and fault-proneness data from previous development experiences similar to the project currently under-development. However, various practical issues can limit availability of fault-proneness data for all modules in the training data, leading to the data consisting of many modules with no fault-proneness data, i.e., unlabeled data. To address this problem, we propose a novel semi-supervised clustering scheme for software quality analysis with limited fault-proneness data. It is a constraint-based semi-supervised clustering scheme based on the k-means algorithm. The proposed approach is investigated with software measurement data of two NASA software projects, JM1 and KC2. Empirical results validate the promise of our semi-supervised clustering technique for software quality modeling and analysis in the presence of limited defect data. Additionally, the approach provides some valuable insight into the characteristics of certain program modules that remain unlabeled subsequent to our semi-supervised clustering analysis.;2005;N. Seliya;10.1109/HASE.2005.4;Conferences;1530-2059;0-7695-2377-3
ieee_20221205_08_28_59;Quality of information for data fusion in net centric publish and subscribe architectures;This paper examines data fusion and target tracking issues involved within net centric publish and subscribe architectures with respect to the quality of information (QOI) provided to the end user. These architectures are commonly selected in shared knowledge environments to enable access to all information by all users. DoD, DOJ, FAA, and other government organizations are pushing net centric concepts to move from stovepiped, closed architectures with limited data accessibility toward open architectures where dissemination of new data sources is intrinsically supported. Although these net centric initiatives (such as DoD's GIG) promise tremendous aggregate bandwidth, sensor-to-user bandwidth, end-user processing power and storage capacity, and other real world issues are expected to impact operations. For example, the soldier of the future may require moving target updates on a bandwidth-limited wireless PDA. This paper examines how the Air Force Research Laboratory's Joint Battlespace Infosphere (JBI), a publish-and-subscribe system in conjunction with the OGC's sensor Web enablement (SWE) initiative, can be used to update clients in a quality-of-information (QOI) paradigm rather than a quality-of-service (QOS) paradigm. We conclude that net centric architectures can support operational flows that improve the quality of information while reducing resource usage over traditional stovepipe systems. We also conclude that net centric tool implementations should specifically address QOI.;2005;M.E. Johnson;10.1109/ICIF.2005.1591976;Conferences;;0-7803-9286-8
ieee_20221205_08_28_59;Feature Selection for Classification with Proteomic Data of Mixed Quality;In this paper we assess experimentally the performance of two state-of-the-art feature selection methods, called RFE and RELIEF, when used for classifying pattern proteomic samples of mixed quality. The data are generated by spiking human sera to artificially create differentiable sample groups, and by handling samples at different storage temperature. We consider two type of classifiers: support vector machines (SVM) and k-nearest neighbour (kNN). Results of leave-one-out cross validation (LOOCV) experiments indicate that RELIEF selects more stable feature subsets than RFE over the runs, where the selected features are mainly spiked ones. However, RFE outperforms RELIEF in terms of (average LOOCV) accuracy, both when combined with SVM and kNN. Perfect LOOCV accuracy is obtained by RFE combined with 1NN. Almost all the samples that are wrongly classified by the algorithms have high storage temperature. The results of experiments on this data indicate that when samples of mixed quality are analyzed computationally, feature selection of only relevant (spiked) features does not necessarily correspond to highest accuracy of classification.;2005;E. Marchiori;10.1109/CIBCB.2005.1594944;Conferences;;0-7803-9387-2
ieee_20221205_08_28_59;Design and Implementation of a Real-Time Control Platform for the Testing of Advanced Control Systems and Data Quality Management in the Wastewater Industry;This paper reports on the design and implementation of a software platform for a wastewater treatment plant (WWTP) for advanced control testing and data quality assessment. The platform can be integrated within an existing control structure, namely the plant SCADA system, with the purpose of providing tools for the implementation and testing of sophisticated control systems and data quality analysis for plant wide operation. The platform is being implemented and tested at a Scottish utility’s wastewater treatment plant, where several control algorithms are under research.;2003;A. Sanchez;10.1109/ICCA.2003.1595058;Conferences;;0-7803-7777-X
ieee_20221205_08_28_59;Test of distributed data quality monitoring of CMS tracker;The complexity of the HEP detectors for LHC (CMS tracker has more than 50 millions of electronic channels) makes the task of monitoring of the detector performance and of the data quality on-line challenging because of the large amount of data and information to be processed. The availability of the grid computing environment opens new possibilities to perform this task, by profiting from the distributed resources in remote sites. As the computing resources may be scarce in the CMS control room it is proposed to send a sizeable amount of tracker raw data to a specialized tier-2 center through an high-bandwidth connection. This center have to be able to analyze the data as soon as they arrive, doing a quasi-online monitoring and sending back to the control room the result of this analysis. The feasibility test performed using a grid farm of INFN (tier-2) in Bari is described.;2005;M. Santa Mennea;10.1109/NSSMIC.2005.1596389;Conferences;1082-3654;0-7803-9221-3
ieee_20221205_08_28_59;Knowledge Discovery in Power Quality Data Using Support Vector Machine and S-Transform;In this paper, we investigate the potential of support vector machines (SVMs) for power quality data mining in electrical power systems. Modified wavelet transform, known as S-transform, has been used to extract unique features of the various power quality disturbances. Feature vectors from S-transform analysis are used to train the SVM classifier. Various multi-class SVM algorithms have been applied on the power quality data under study and the directed acyclic graph (DAGSVM) algorithm is found to be performing well. A comparison between the DAGSVM method and the one based on artificial neural network demonstrates the efficiency of the SVM method in classifying PQ disturbances;2006;K. Vivek;10.1109/ITNG.2006.86;Conferences;;0-7695-2497-4
ieee_20221205_08_28_59;Quality, data, and TR929;To improve the ability to meet customers' needs, it is necessary to improve the quality of the technology and processes used to provide services. To improve quality there is a need to understand current performance and have a way to track change. Data are needed to meet these needs and consistent and common data are needed to make procurement decisions. The Reliability and Quality Measurement System for Telecommunications (RQMS) TR-TSY-000929 provides criteria for standardization of the data and their delivery. One must, however, be sensitive to the need to monitor these requirements and change them as necessary.<>;1991;K.M. Walling;10.1109/ICC.1991.162300;Conferences;;0-7803-0006-8
ieee_20221205_08_28_59;Towards a Quality Model for Effective Data Selection in Collaboratories;Data-driven scientific applications utilize workflow frameworks to execute complex dataflows, resulting in derived data products of unknown quality. We discuss our on-going research on a quality model that provides users with an integrated estimate of the data quality that is tuned to their application needs and is available as a numerical quality score that enables uniform comparison of datasets, providing a way for the community to trust derived data.;2006;Y.L. Simmhan;10.1109/ICDEW.2006.150;Conferences;;0-7695-2571-7
ieee_20221205_08_28_59;Data quality and grounding;Proper application of grounding systems, signal isolation, electrical safety, and power quality are key to the successful installation and operation of any electrical and data system, whether in a building or a plant. This paper necessitates an effective data quality and grounding design that considers the neutral, power grounding, bonding, shielding, and transient protection. With consistent design, installation, and maintenance of the grounding system, noise disturbances can be controlled.;2006;M.O. Durham;10.1109/MIA.2006.1628848;Magazines;1558-0598;
ieee_20221205_08_28_59;GeoExpert A Framework for Data Quality in Spatial Databases;Usage of very large sets of historical spatial data in knowledge discovery process became a common trend and in order to obtain better results from this knowledge discovery process the data should be of high quality. We proposed a framework for data quality assessment and cleansing tool for spatial data that integrates the spatial data visualization and analysis capabilities of the ARCGIS Engine, the reason and inference capability of an expert system. In this paper, we explain the core architecture of the framework and also the functionality of each module in the framework. We will explain the implementation details of the framework.;2005;A. Tadakaluru;10.1109/CIMCA.2005.1631527;Conferences;;0-7695-2504-0
ieee_20221205_08_28_59;Short term forecast of the quality of water in New York coastal zone using multispectral satellite data;One of the possible and probably most effective ways of solving the problem of monitoring the quality of water in ocean coastal areas is to use satellite measurements (remote sensing). Analysis of temporal and spatial structure of the inhomogeneities in coastal waters is an important step for the development of a model of the impurity transport and pollution forecast, but direct contact sensors cannot cover the necessary scales. Therefore, it is very important that WLR (normalized water leaving radiance) measurements made by satellites SeaWiFS, MODIS, and occasionally ASTER be used and analyzed. One of the most informative hydro-optical parameters is known to be the color index defined as the ratio of the WLR values in two spectral ranges, i.e. I(/spl lambda//sub 1/, /spl lambda//sub 2/) = WLR(/spl lambda//sub 1/)/WLR(/spl lambda//sub 2/). When calculating the color index I(/spl lambda//sub 1/, /spl lambda//sub 2/), partial compensation of the multiplicative measurement errors is taking place. For this reason, the influence of such factors as the visual angle, illumination of the ocean surface, sky color, etc. is substantially weakened, which enhances the informativity of the optical data on the upper ocean layer. This property of the color index is the basic prerequisite of its use aimed at determining multiple parameters characterizing the quality of water. Preliminary color index analyses to determine water characteristics in the New York coastal zone have shown that this parameter is quite relevant in this respect. Main mesoscale disturbances of the color index are concentrated near the coastline. Their spectral analysis has revealed two basic scales of such disturbances. The first one corresponds to the dimensions of heterogeneities within 20-30 km, the second one to 9-11 km. The first type of disturbances is identified as mesoscale eddies since their dimension corresponds to 2/spl pi/R/sub b/ (R/sub b/ is the baroclinic deformation radius). The second type of color index disturbances, with dimensions 9-11 km, manifests itself as the structures that move along the Lond Island coast in the western direction. The coherence of such disturbances quickly decreases with the distance from the coastline. These properties of the disturbances are characteristic for the entrapped waves and, as shown by the analysis, their parameters correspond well enough to the dispersion curves of the barotropic shelf waves. The role of such disturbances in the exchange processes remains so far not clear, and it may be a subject for further research. Consecutive digital images of the color index in the New York coastal zone may be a source of information on the development and dynamics of the mesoscale eddies. On this basis, methods of forecast of water quality changes may be developed.;2005;R. Khanbilvardi;10.1109/OCEANS.2005.1639816;Conferences;0197-7385;0-933957-34-3
ieee_20221205_08_28_59;Designing a quality oceanographic data processing environment;Oceanographic data are increasing by data types and volume making present methods of processing and determining quality a cumbersome task. The National Oceanic and Atmospheric Administration's (NOAA) Center for Operational Oceanographic Products and Services (CO-OPS) is developing an end-to-end, state-of-the-art data management system to ingest, quality control, analyze, and disseminate water velocity and related data. The benefits include streamlining preliminary analysis, thus allowing time and resources for more in-depth investigations of the physical phenomena, increasing consistency of results between users, and improving overall data quality. The design of the system architecture follows a planned structured methodology improving the quality of the software developed. Designing a Web-based modular system will allow flexibility so the system can accommodate new analyses, reports and plots as well as allow for future data types. Well-defined algorithms will be implemented determining the quality of both the data and the analyses. This data management system will provide oceanographers the means to study water velocity data using a wide suite of mathematical and graphical tools. This will allow users to focus on the analysis results rather than the process.;2005;C. Paternostro;10.1109/OCEANS.2005.1640149;Conferences;0197-7385;0-933957-34-3
ieee_20221205_08_28_59;Using Data Mining Technology to improve Manufacturing Quality - A Case Study of LCD Driver IC Packaging Industry;In recent year, because of the professional teamwork, to improve the qualification percentage of products, to accelerate the acknowledgement of product defects and to find out the solution, the LCD driver IC packaging factories have to establish an analysis mode for quality problems of product for more effective and quicker acquisition of needed information and to improve the customer’s satisfaction for information system. The past information system used neural network to improve the yield rate of production. In this research employs the star schema of data warehousing as the base of line analysis, and uses decision tree in data mining to establish a quality analysis system for the defects found in the production processes of package factories in order to provide an interface for problem analysis, enabling quick judgment and control over the cause of problem to shorten the time solving the quality problem. The result of research shows that the use of decision tree algorithm reducing the numbers of defected inner leads and chips has been improved, and using decision tree algorithm is more suitable than using neural network in quality problem classification and analysis of the LCD driver IC packaging industry.;2006;Ruey-Shun Chen;10.1109/SNPD-SAWN.2006.75;Conferences;;0-7695-2611-X
ieee_20221205_08_28_59;Accuracy Control in Compressed Multidimensional Data Cubes for Quality of Answer-based OLAP Tools;An innovative technique supporting accuracy control in compressed multidimensional data cubes is presented in this paper. The proposed technique can be efficiently used in QoA-based OLAP tools, where OLAP users/applications and DW servers are allowed to mediate on the accuracy of (approximate) answers, similarly to what happens in QoS-based systems for the quality of services. The compressed data structure KLSA, which implements the technique, is also extensively presented and discussed. We complement our analytical contributions with an experimental evaluation on several kinds of synthetic multidimensional data cubes, demonstrating the superiority of our approach in comparison with other similar techniques;2006;A. Cuzzocrea;10.1109/SSDBM.2006.10;Conferences;1551-6393;0-7695-2590-3
ieee_20221205_08_28_59;Transformer thermal modeling: improving reliability using data quality control;Eventually, all large transformers will be dynamically loaded using models updated regularly from field-measured data. Models obtained from measured data give more accurate results than models based on transformer heat-run tests and can be easily generated using data already routinely monitored. The only significant challenge to use these models is to assess their reliability and improve their reliability as much as possible. In this work, we use data-quality control and data-set screening to show that model reliability can be increased by about 50% while decreasing model prediction error. These results are obtained for a linear model. We expect similar results for the nonlinear models currently being explored.;2006;D.J. Tylavsky;10.1109/TPWRD.2005.864039;Journals;1937-4208;
ieee_20221205_08_28_59;A novel data hiding scheme for keeping high stego-image quality;The LSB-based data hiding scheme is to embed the secret data into the least significant bits of the pixel values in a cover image. When the size of the embedded secret data is bigger, this processing will degrade the quality of the stego-image so significantly as to catch the attention of hostile interceptors. To overcome this drawback, in this paper, we propose a high-capacity, high-quality data hiding scheme. First, the proposed scheme divides the pixel value range into several non-overlapping regions. For each spatial pixel value in the cover image, we embed one secret-bit into it by using the difference expansion technique in single pass, where the difference is calculated between the spatial pixel and the start or end of the region it is situated in. Owing to the use of the difference expansion technique, this processing is also suitable for multiple-layer embedding. Experimental results show significant improvement with respect to the embedding capacity. In addition, the distortion in the stego-image is almost invariable regardless of the increase of the load of embedded data.;2006;Chin-Chen Chang;10.1109/MMMC.2006.1651324;Conferences;1550-5502;1-4244-0028-7
ieee_20221205_08_28_59;On Creating a New Format for Power Quality and Quantity Data Interchange;IEEE 1159, a recommended practice for monitoring power quality has resulted in a consistent set of terms, definitions and guidelines for monitoring power quality. IEEE 1159 does not address the need however, for a consistent mechanism for transporting the output of these instruments to the end user. Each vendor has their own communications, control, and analysis software that is not compatible with any other vendor. In addition to the difficulty with exchangeability of monitored data, there is a similar difficulty in exchanging and comparing the results of computer simulations. Simulation tool vendors also use a variety of file formats and conventions appropriate for their programs. Given that today's power quality engineers need to transport the output of different vendors monitors and simulation programs to various database and analysis programs (also from multiple vendors), a need exists for a standard data interchange format. This paper describes the principal issues that were addressed in the development of the IEEE 1159.3-2003 Recommended Practice for the Transfer of Power Quality Data (also known as the power quality data interchange format - PQDIF);2006;Gunther;10.1109/TDC.2006.1668517;Conferences;2160-8563;0-7803-9194-2
ieee_20221205_08_28_59;An approach to geographical data quality evaluation;We present an approach to the evaluation of the quality of cadastral data that caters for the differing levels of quality required of various parameters in order to meet different goals. The key parameters associated with particular quality classes and their acceptable range of values are obtained through interview with expert users. The approach described is currently being applied in an experimental evaluation of the quality of cadastral map of the Republic of Latvia.;2006;A. Jansone;10.1109/DBIS.2006.1678486;Conferences;;1-4244-0345-6
ieee_20221205_08_28_59;"Monitoring and Improving the Quality of ODC Data using the ""ODC Harmony Matrices"": A Case Study";Orthogonal Defect Classification (ODC) is an advanced software engineering technique to provide in-process feedback to developers and testers using defect data. ODC institutionalization in a large organization involves some challenging roadblocks such as the poor quality of the collected data leading to wrong analysis. In this paper, we have proposed a technique (‘Harmony Matrix’) to improve the data collection process. The ODC Harmony Matrix has useful applications. At the individual defect level, results can be used to raise alerts to practitioners at the point of data collection if a low probability combination is chosen. At the higher level, the ODC Harmony Matrix helps in monitoring the quality of the collected ODC data. The ODC Harmony Matrix complements other approaches to monitor and enhances the ODC data collection process and helps in successful ODC institutionalization, ultimately improving both the product and the process. The paper also describes precautions to take while using this approach.;2006;N. Saraiya;10.1109/SERA.2006.52;Conferences;;0-7695-2656-X
ieee_20221205_08_28_59;Test Data Selection and Quality Estimation Based on the Concept of Essential Branches for Path Testing;A new coverage measure is proposed for efficient and effective software testing. The conventional coverage measure for branch testing has such defects as overestimation of software quality and redundant test data selection because all branches are treated equally. These problems can be avoided by paying attention to only those branches essential for path testing. That is, if one branch is executed whenever another particular branch is executed, the former branch is nonessential for path testing. This is because a path covering the latter branch also covers the former branch. Branches other than such nonessential branches will be referred to as essential branches.;1987;T. Chusho;10.1109/TSE.1987.233196;Journals;2326-3881;
ieee_20221205_08_28_59;Feature Selection for Data Driven Prediction of Protein Model Quality;Features selection to assess the accuracy o f a protein three-dimensional model, when only the protein sequence is known, is a challenging task because it is not clear which features are most important and how they should best be combined. We present the results of an information theory-based approach to select an optimal subset of features for the prediction of protein model quality. The optimal subset of features was calculated by means of a backward selection procedure, starting from a set of structural features belonging to the following three categories: atomic interactions, solvent accessibility, and secondary structure. Three statistical-learning approaches were evaluated to predict the quality of a protein model starting from an optimum subset of features. The performances of a probabilistic classifier modeled by means of a Kernel Probability Density Estimation method (KPDE) were compared with those of a feed-forward Artificial Neural Network (ANN) and a Support Vector Machine (SVM).;2006;A. Montuori;10.1109/IJCNN.2006.247365;Conferences;2161-4407;0-7803-9490-9
ieee_20221205_08_28_59;Fuzzy Clustering of Open-Source Software Quality Data: A Case Study of Mozilla;We present a fuzzy cluster analysis of software quality data extracted from the Mozilla open-source Web browser. This is a new dataset that combines object-oriented software quality metrics with the number of defects per code unit. We undertake a fuzzy cluster analysis of this dataset, which for the first time addresses the use of both hyperspherical and hyperellipsoidal fuzzy clusters (using the Gath-Geva algorithm) in software quality analysis. Using a Pareto analysis based on the fuzzy clusters, we were able to identify groups of modules having higher defect densities than would be found by merely ranking modules based on any single software metric.;2006;S. Dick;10.1109/IJCNN.2006.246954;Conferences;2161-4407;0-7803-9490-9
ieee_20221205_08_28_59;Comparison of I-V, CV, and chemical data for quality control studies of SiO/sub x/N/sub y/ films on Si;Capacitance-voltage (CV) and current-voltage (I-V) measurements for SiO/sub x/N/sub y/ films are compared with chemical data in order to provide some diagnostic capabilities in relating aberrant electrical characteristics with contaminants incorporated in the insulator film structure. In-process monitoring of film quality (utilizing electrical characteristics and chemical data) is especially critical in very large-scale integration (VLSI) processing control where the films are utilized both as an integral part of specific semiconductor device processing steps or as part of the semiconductor device structure.<>;1988;M.W. Huck;10.1109/66.17986;Journals;1558-2345;
ieee_20221205_08_28_59;Using data quality measures in decision-making algorithms;Four decision methods are compared to determine appropriate ways of using data quality measures. Separate studies are directed toward defining and measuring tactical data quality, and calibrating the measures to decision problems. The decision methods compared include Dempster's rule, the linear and logarithmic opinion pools, a fuzzy-logic algorithm and the Mycin certainty factor calculus. It is concluded that none of the four decision algorithms are fully satisfactory. Of the four algorithms, the linear opinion pool is the most likely to succeed in practice because it is the simplest.<>;1992;R.A. Dillard;10.1109/64.180410;Magazines;2374-9407;
ieee_20221205_08_28_59;Reliability and quality data in court;The author presents the types of quality assurance data and information that are frequently used in the courtroom for litigation involving contract disagreements between buyer and seller (as well as personal injuries of the user or bystander versus all of the sellers and manufacturers) or arguments between the manufacturer of an item and a subsequent manufacturer who incorporates the item, over the damage caused by the inoperability or premature failure of the initial product. The author discusses the importance of these types of data, how they are to be recorded, where they are supposed to be recorded, the importance of configuration management (not only of military but of commercial products), cross referencing dates, the use of serial numbers, and the use of model designations with and without modification due to internal changes not affecting form, fit, or function.<>;1988;R.M. Jacobs;10.1109/ARMS.1988.196452;Conferences;;
ieee_20221205_08_28_59;Probabilistic model for quality of service modelling in packet-switched data networks;"A probabilistic model for quality of service available to users for communication via packet-switched public data networks is presented. The concepts of network reliability, availability and congestion are discussed. It is shown that network congestion appears as a loss of availability with regard to data communication services provided to network users, and therefore the quality of service depends not only on equipment reliability and maintenance activity, but also on the congestion probability. After defining the network equivalent availability, a probabilistic model for its evaluation is presented. New concepts are introduced for network equivalent availability evaluation; they consider network component reliability, their maintainability, and the network congestion together.<>";1992;I. Popescu;10.1109/LCN.1992.228128;Conferences;;0-8186-3095-7
ieee_20221205_08_28_59;Drifting buoy data quality and performance assessment at the National Data Buoy Center;A description is given of the first large-scale, centralized, drifting-buoy data quality program, initiated in April 1988 by the National Data Buoy Center (NDBC), an agency within the US National Weather Service (NWS). This program is leading to improved meteorological observations in data-sparse oceanic areas and hence can improve operational numerical analyses and prognoses. All North American drifting buoy reports enter the Global Telecommunications System (GTS) in Washington, DC. The Service Argos US Global Processing Center places the reports in DRIBU format and sends them to the NWS IBM 4341 computer system, where the data are subjected to gross range and time-continuity checks before dissemination of the GTS. More stringent checks are then performed at NDBC by data analysts in near-real time. The close monitoring of drifting buoy sensor data permits analysis of system reliability and performance, which can be used in general evaluation of hardware quality.<>;1988;E.A. Meindl;10.1109/OCEANS.1988.23626;Conferences;;
ieee_20221205_08_28_59;1.5 GHz band wave propagation characteristics and data transmission quality from train to ground;The propagation characteristics, data transmission quality, and service area of a quasi-microwave railway communication system are described. Measurements of propagation path loss taken along a typical railway route and in tunnels are presented.<>;1992;H. Hayashi;10.1109/VETEC.1992.245442;Conferences;1090-3038;0-7803-0673-2
ieee_20221205_08_28_59;Data quality for telecommunications;"The importance of data in large databases to the operation of telecommunications networks has grown considerably. For example, all provisioning, maintenance, and billing operations are critically dependent on data and many new network services are based on real-time access to data. This makes data quality a major issue for the industry. The purpose of the present paper is to outline an approach for addressing at least part of this issue. This approach makes use of process management to focus attention on processes that create data and data tracking, one method to quantify the performance of such processes. The merits of this approach are compared to more traditional methods of ""cleaning"" databases. A joint AT&T/LEC process, by which special services access is ordered and provisioned, is used as an example.<>";1994;T.C. Redman;10.1109/49.272881;Journals;1558-0008;
ieee_20221205_08_28_59;Probing power quality data;A power quality problem can best be described as any variation in the electric power service resulting in misoperation or failure of end-use equipment. As customers seek to increase utilisation and efficiency, utilities strive to better understand power quality events and their effects on these customers. Utilities are creating special programs and organisations to deal with customer power quality needs. A problem shared by both parties is the need for improved methods in the collection, analysis, and reporting of very large amounts of measured power quality data. This article presents one method currently being used to characterise power quality levels on distribution systems throughout the United States. The method utilises a number of software systems, one of which is the data management and analysis program described in the article.<>;1994;W.W. Dabbs;10.1109/67.273780;Magazines;1558-4151;
ieee_20221205_08_28_59;Ultrasound data acquisition system design for collecting high quality RF data from beef carcasses in the slaughterhouse environment;Ultrasound has considerable potential to accurately and precisely grade beef. The ultimate goal is an automated and objective grading process. The current phase of the work does not involve construction of an ultrasound grading device but rather the determination of which of these ultrasound-related and derived quantities best correlate to each grade. In order to make this determination, an ultrasound data acquisition system capable of acquiring large amounts of high-quality RF data from beef carcasses has been constructed. The engineering concepts and construction details of an ultrasound data acquisition device to be used as a research tool for this application are presented.<>;1992;I.A. Hein;10.1109/ULTSYM.1992.276019;Conferences;;0-7803-0562-0
ieee_20221205_08_28_59;A transformation for ordering multispectral data in terms of image quality with implications for noise removal;A transformation known as the maximum noise fraction (MNF) transformation, which always produces new components ordered by image quality, is presented. It can be shown that this transformation is equivalent to principal components transformations when the noise variance is the same in all bands and that it reduces to a multiple linear regression when noise is in one band only. Noise can be effectively removed from multispectral data by transforming to the MNF space, smoothing or rejecting the most noisy components, and then retransforming to the original space. In this way, more intense smoothing can be applied to the MNF components with high noise and low signal content than could be applied to each band of the original data. The MNF transformation requires knowledge of both the signal and noise covariance matrices. Except when the noise is in one band only, the noise covariance matrix needs to be estimated. One procedure for doing this is discussed and examples of cleaned images are presented.<>;1988;A.A. Green;10.1109/36.3001;Journals;1558-0644;
ieee_20221205_08_28_59;An exact solution of the analytic equation of image quality from optimum quantization of microwave imaging data;An exact solution is presented of the analytic equation of image quality in terms of the entropy of the quantized aperture data. Previously published work provides an optimum quantization scheme that obtains the best image quality for a given distribution of complex aperture data and a given number of bits into which each complex data sample is to be quantized. An extension is provided to determine the number of bits required of the quantized signal for the image to achieve a given quality or, conversely, what quality of image can be obtained for a given number of quantized levels. It is shown that the theory imposes substantially no constraints on the distributions of amplitude and phase in the microwave data. It is also shown that the optimum quantization design derived from the theory is nearly scene-independent and may achieve real-time performance.<>;1989;Z. Liang;10.1109/29.31299;Journals;0096-3518;
ieee_20221205_08_28_59;Software quality measurement based on fault-detection data;We develop a methodology to measure the quality levels of a number of releases of a software product in its evolution process. The proposed quality measurement plan is based on the faults detected in field operation of the software. We describe how fault discovery data can be analyzed and reported in a framework very similar to that of the QMP (quality measurement plan) proposed by B. Hoadley (1986). The proposed procedure is especially useful in situations where one has only very little data from the latest release. We present details of implementation of solutions to a class of models on the distribution of fault detection times. The conditions under which the families: exponential, Weibull, or Pareto distributions might be appropriate for fault detection times are discussed. In a variety of typical data sets that we investigated one of these families was found to provide a good fit for the data. The proposed methodology is illustrated with an example involving three releases of a software product, where the fault detection times are exponentially distributed. Another example for a situation where the exponential fit is not good enough is also considered.<>;1994;S. Weerahandi;10.1109/32.317425;Journals;2326-3881;
ieee_20221205_08_28_59;The RASCALS SYSTEM: a tool for ERS-1 data product quality analysis and sensor performance assessment;RASCALS (Radar Altimeter and SCAtterometer Long-term Surveillance) is a software system dedicated to the monitoring and analysis of sensor performance and data product quality for the ERS-1 wind scatterometer and radar altimeter. It is operated by the ERS-1 Product Control Service at ESRIN/Earthnet. The system can access raw data and fast delivery products as well as instrument housekeeping data generated by the AMI wind mode, the AMI wave mode, and the radar altimeter. The functionality covered by the RASCALS system ranges from the graphical visualisation of all parameters provided in the data products up to specialized analysis functions supporting the geophysical validation of measurements. RASCALS provides two modes of operation: In the analysis mode, the operator can interactively display, investigate and process the relevant data. In the routine mode, the system periodically samples the data products available from radar altimeter and wind scatterometer and extracts from the engineering and geophysical data a set of statistical parameters, thus building up a database characterizing the long-term performance. This paper describes the functionality and applications of RASCALS.<>;1993;H. Eichenherr;10.1109/IGARSS.1993.322074;Conferences;;0-7803-1240-6
ieee_20221205_08_28_59;Improved underway ADCP data quality through precise heading measurement;Acoustic Doppler current profilers provide valuable data on ocean currents from ships whilst underway, and they are a valuable tool when used in conjunction with high resolution hydrography. One of the main difficulties in using ADCP data to estimate mean cross-track currents (and hence transport) is the error in the conversion from ship to Earth coordinates brought about by errors in the ship's gyrocompass. Errors in the cross-track velocity of tens of cm s/sup -1/ may occur at moderate passage speeds. A relatively new tool - 3D GPS - provides precise heading measurements that can be used to correct the ADCP data. This paper uses 3D GPS, gyro, and ADCP data gathered during Cruise 198 of RRS Discovery to illustrate gyro errors dependent on latitude, speed and heading, and their effect on ADCP data quality.<>;1993;G. Griffiths;10.1109/OCEANS.1993.325993;Conferences;;0-7803-1385-2
ieee_20221205_08_28_59;Data quality requirements analysis and modeling;A set or premises, terms, and definitions for data quality management are established, and a step-by-step methodology for defining and documenting data quality parameters important to users is developed. These quality parameters are used to determine quality indicators about the data manufacturing process, such as data source creation time, and collection method, that are tagged to data items. Given such tags, and the ability to query over them, users can filter out data having undesirable characteristics. The methodology provides a concrete approach to data quality requirements collection and documentation. It demonstrates that data quality can be an integral part of the database design process. A perspective on the migration towards quality management of data in a database environment is given.<>;1993;R.Y. Wang;10.1109/ICDE.1993.344012;Conferences;;0-8186-3570-3
ieee_20221205_08_28_59;Routine data quality control in a data centre. The example of the TOGA/WOCE Subsurface Centre;"The TOGA/WOCE Subsurface Data Centre was designed to assemble all the available temperature and salinity profiles of the ocean's upper layers in the intertropical area from 1985 according to the ""Tropical Ocean Global Atmosphere"" international program, and from 1990 onwards in the global ocean according to the ""World Ocean Circulation Experiment"". In June 1994, the database was archiving about 252000 profiles. In order to assure a high level of quality and to be in accordance with international requirements, the received data are controlled at the centre which insures time consistency, integrity and elimination of redundancy. The Centre's software must join powerful X/motif interactive displays with a relational database system manager to insure a response time compatible with routine work on large data sets. At present, the centre receives at least 300 profiles per day and can check and disseminate them in a few days.<>";1994;G. Maudire;10.1109/OCEANS.1994.364074;Conferences;;0-7803-2056-5
ieee_20221205_08_28_59;Transmission quality of 10 Mbits/s Ethernet data frames using DPSK in wireless indoor environment;This paper presents the transmission quality of 10 Mbits/s Ethernet data frames in wireless indoor environment by direct modulation of the data frames onto a 1.95 GHz carrier using differential phase shift keying. Rectangular patch antennas with a 3 dB beamwidth of 74/spl deg/ are used in both the transmitter and receiver. Outage probabilities measured in a laboratory of 15.6/spl times/16.3/spl times/2.6 meters using three different transmitter antenna heights and locations are presented. In the first experiment where the antenna height is 1.3 meters, an outage probability of around 13% is reported. This improves to 2% by re-locating the transmitter antenna height and position. A comparison with measured RMS indoor delay spread characteristics at a similar frequency is made.<>;1994;C.L. Law;10.1109/ICC.1994.368819;Conferences;;0-7803-1825-0
ieee_20221205_08_28_59;Administrative databases and the quality of data in a myocardial infarction database;The St. George's post-infarction survey covers approximately 1,000 patients and is used for research into risk factors. Criteria are required for prompt entering of manually entered data. Using dumb terminal emulation, links to the hospital's patient administration system are used to improve data quality. New wiring and central computer staff are not required. Data editing by an events committee or during statistical analysis can exaggerate errors. Purchasing recommendations are made for database software and investigations equipment, which assist database management, make audit checks possible, and avoid manual transcription.<>;1993;J.D. Poloniecki;10.1109/CIC.1993.378375;Conferences;;0-8186-5470-8
ieee_20221205_08_28_59;Improving the quality of heart rate variability spectra by imposing a constraint on the amount of ectopy in each data segment;"Heart rate variability (HRV) is primarily due to instability oscillations of homeostatic feedback loops. Each loop tends to oscillate at a frequency characteristic of the time required to traverse that loop. Discarding all segments with ectopy is impractical in a clinical setting, since the number of ectopy-free segments is low even in patients with moderate ectopy. Investigators are extending spectra into lower frequency ranges, which requires longer data segments, and further reduces the number of ectopy-free segments. A method is proposed to determine the level of ectopy above which data segments should be discarded. By enforcing an effective ectopy cutoff, the mean distance among the spectra making up the average periodogram is reduced, increasing the confidence in these spectra. This technique allows for ""quality control"" of the data upon which HRV spectral analysis is based.<>";1993;J.J. Chungath;10.1109/CIC.1993.378381;Conferences;;0-8186-5470-8
ieee_20221205_08_28_59;Quality-of-service for video and data in very-high-speed DQDB MANs;Simulation is used to investigate the quality of service that a high-speed DQDB metropolitan area network can provide in a mixed-traffic scenario where connection-oriented video services coexist with connectionless data services. The basic DQDB access protocol as well as two of its options, are considered in a network with 1 Gbit/s data rate, and 5 km spall. The performance metrics used for the quality of service assessment are the throughput, the average and variance of the message delay. Results are presented considering the whole network, as well as individual nodes, so that protocol fairness issues can be discussed.<>;1995;M. Ajmone Marsan;10.1109/MASCOT.1995.378678;Conferences;;0-8186-6902-0
ieee_20221205_08_28_59;Data reduction for high quality digital audio storage and transmission;The paper reviews the current state of the art in the field of data reduction for high and medium quality audio applications, with particular emphasis on the methods standardised by the ISO-MPEG as ISO 11172-3. It also summarises the recent ISO-MPEG work which is aimed at defining further enhancements to the original standard, known as MPEG 2. It is acknowledged that the ISO-MPEG approach is not the only one in existence, there being a number of alternative commercial coding systems for high quality audio. These are mentioned briefly.<>;1994;F.J. Rumsey;10.1049/cp:19941125;Conferences;;0-85296-630-X
ieee_20221205_08_28_59;Quality and reliability impact of defect data analysis;In the last decade we have seen a shift towards a broader application of information on IC manufacturing defects. Here an overview is given of the methods used to gather data on the defects with a focus on local defects in the interconnection layers. Next this information is applied to determine a model describing the geometrical aspects of such defects. This model is used to arrive at a definition of hard faults and soft faults and to derive a relationship between the relative number of occurrence for either fault. Because the electrical impact of some of the soft faults will be closely related to the behavior of small open circuits or gate-oxide shorts, this relationship is an indication for the extent of the quality and reliability problems.<>;1995;E. Bruls;10.1109/66.382275;Journals;1558-2345;
ieee_20221205_08_28_59;Fuzzy sensor data fusion for quality monitoring in concrete mixing plant;One of the most important problems to achieve the required quality in concrete manufacturing is the estimation of moisture in aggregates and in concrete mix. The authors present a data fusion and fuzzy logic on line real-time application devoted to optimize the concrete quality. The application is based on smart treatment of existing information coming from plant sensors and on other operators experience. The results obtained at present time with only a process partial mapping shows that an estimation of the aggregate water content can be obtained with a reproducibility better than one order of magnitude more that given by the best actually used systems, for the same recipe and materials. This approach allows control of the most important parameter, the total water amount, to maintain the best quality of the product.<>;1993;A. Boscolo;10.1109/IMTC.1993.382559;Conferences;;0-7803-1229-5
ieee_20221205_08_28_59;Continuous process improvements and the use of quality control methodologies in the data item description process;Over the past few years, the DID process at Air Force Material Command previously Air Force Logistics Command prior to the June 1992 merger with Air Force Systems Command, at Wright-Patterson AFB, Dayton, Ohio has undergone significant process improvements in an effort to reduce lead time and promote quality in the development of DIDs. Statistical Process Control methods were used to evaluate the individual steps in the existing DID process. A number of process changes were made, and a series of training-oriented guides geared toward promoting quality were distributed. These process improvements proved to be quite successful. The lead time to develop a DID went from 938 to 300 h, a 68% reduction. Additional steps are planned to further improve the DID process.<>;1993;B.J. Stanley;10.1109/AUTEST.1993.396316;Conferences;;0-7803-0646-5
ieee_20221205_08_28_59;Image quality, statistical and textural properties of SAREX data from the Tapajos test site;"The image quality of the SAREX data set from the Tapajos test site is examined in the light of the available CCRS calibration data. Noise removal and its effects on the statistical properties of the data is addressed. Mathematical modeling is used to predict HH/VV differences, incidence angle effects and general trends; these are compared to the SAREX date. Model results are close to observations up to near grazing angles of incidence. Observed large HH/VV differences seen at near grazing angles may be due to surface scattering from the top of the forest canopy allied with Brewster angle effects. Textural discrimination of surface cover types is investigated; preliminary results are presented.<>";1994;K.D. Grover;10.1109/IGARSS.1994.399341;Conferences;;0-7803-1497-2
ieee_20221205_08_28_59;X-SAR data and image quality;The first SIR-C/X-SAR mission was flown in April 1994. Part of the acquired SAR data have been downlinked, analyzed and processed to precision images at the German Processing and Archiving Facility (PAF) at DLR. This paper summarizes the first analysis results of raw data and image quality. It is shown that already during the mission using preliminary orbit information the product quality requirements have been surpassed.<>;1994;R. Bamler;10.1109/IGARSS.1994.399376;Conferences;;0-7803-1497-2
ieee_20221205_08_28_59;Measuring Data Abstraction Quality in Multiresolution Visualizations;Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks;2006;Qingguang Cui;10.1109/TVCG.2006.161;Journals;2160-9306;
ieee_20221205_08_28_59;Software Quality Imputation in the Presence of Noisy Data;The detrimental effects of noise in a dependent variable on the accuracy of software quality imputation techniques were studied. The imputation techniques used in this work were Bayesian multiple imputation, mean imputation, instance-based learning, regression imputation, and the REPTree decision tree. These techniques were used to obtain software quality imputations for a large military command, control, and communications system dataset (CCCS). The underlying quality of data was a significant factor affecting the accuracy of the imputation techniques. Multiple imputation and regression imputation were top performers, while mean imputation was ineffective;2006;Taghi M. Khoshgoftaar;10.1109/IRI.2006.252462;Conferences;;0-7803-9788-6
ieee_20221205_08_28_59;Data acquisition equipment for measurement of the parameters for electric energy quality;The paper presents the equipment used to measure some parameters that characterize the electric energy quality. The proposed equipment performs test and acquisition of analogue data (U and I) and numerical data. The sampled data are recorded when preset thresholds are exceeded by the analogical inputs or when the digital inputs states change. The fixed variant is supplementary provided with 2 analogue outputs and 8 numerical outputs. The operation of equipment is simulated and the corresponding software are exemplified for the case of a highly distorting consumer, a set of electric energy quality parameters being determined for this case;2006;G. Vladut;10.1109/AQTR.2006.254612;Conferences;;1-4244-0361-8
ieee_20221205_08_28_59;Data Quality Management using Business Process Modeling;The quality of data contained in the enterprise information systems has significant impact, both from the internal business decision-making perspective and the external regulatory and shareholder obligations. This paper addresses data quality assessment in business processes by proposing a modeling framework to quantify the data quality in an information processing system. We present a business process modeling framework for data quality analysis and develop the mathematical formulation for error propagation. This is overlaid with a business controls framework where the placement and effectiveness of the controls alter the propagation of errors. This framework enables the estimation and management of data quality when faced with changes in various aspects of the business process. It also allows the formulation of optimization problems that trade off the cost of business controls with the level or cost of the resultant data quality. We illustrate the modeling framework and analyses with a revenue management process;2006;Sugato Bagchi;10.1109/SCC.2006.41;Conferences;;0-7695-2670-5
ieee_20221205_08_28_59;Estimating Software Quality with Advanced Data Mining Techniques;Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.;2006;Matej Mertik;10.1109/ICSEA.2006.261275;Conferences;;0-7695-2703-5
ieee_20221205_08_28_59;Application of a Statistical Methodology to Simplify Software Quality Metric Models Constructed Using Incomplete Data Samples;"During the construction of a software metric model, incomplete data often appear in the data sample used for the construction. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable ""retrospectively"" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, the authors have earlier derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a ""goodness-of fit"" criterion. Whilst the methodology has been applied successfully to software effort metric models, it is applied only recently to software quality metric models which usually suffer from far more serious incomplete data. This paper documents the latter application based on a successful case study";2006;Victor K.y. Chan;10.1109/QSIC.2006.13;Conferences;2332-662X;0-7695-2718-3
ieee_20221205_08_28_59;Exploratory Visualization of Multivariate Data with Variable Quality;Real-world data is known to be imperfect, suffering from various forms of defects such as sensor variability, estimation errors, uncertainty, human errors in data entry, and gaps in data gathering. Analysis conducted on variable quality data can lead to inaccurate or incorrect results. An effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content, but also its quality attributes. While some research has been conducted on visualizing uncertainty in spatio-temporal data and univariate data, little work has been reported on extending this capability into multivariate data visualization. In this paper we describe our approach to the problem of visually exploring multivariate data with variable quality. As a foundation, we propose a general approach to defining quality measures for tabular data, in which data may experience quality problems at three granularities: individual data values, complete records, and specific dimensions. We then present two approaches to visual mapping of quality information into display space. In particular, one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions. The other solution is to superimpose the quality information within the data visualizations using additional visual variables. We also report on user studies conducted to assess alternate mappings of quality attributes to visual variables for the second method. In addition, we describe case studies that expose some of the advantages and disadvantages of these two approaches;2006;Zaixian Xie;10.1109/VAST.2006.261424;Conferences;;1-4244-0592-0
ieee_20221205_08_28_59;A framework for analysis of data quality research;Organizational databases are pervaded with data of poor quality. However, there has not been an analysis of the data quality literature that provides an overall understanding of the state-of-art research in this area. Using an analogy between product manufacturing and data manufacturing, this paper develops a framework for analyzing data quality research, and uses it as the basis for organizing the data quality literature. This framework consists of seven elements: management responsibilities, operation and assurance costs, research and development, production, distribution, personnel management, and legal function. The analysis reveals that most research efforts focus on operation and assurance costs, research and development, and production of data products. Unexplored research topics and unresolved issues are identified and directions for future research provided.<>;1995;R.Y. Wang;10.1109/69.404034;Journals;2326-3865;
ieee_20221205_08_28_59;New York City Automated Air-Quality Data Collection Network;The problem of air pollution, particularly in metropolitan areas, is a function of widely distributed sources of contaminants and meteorological conditions which prevail over a large geographical area. To effectively monitor such a large area, and to provide timely, accurate information at a reasonable cost, an automated data network was deemed mandatory. The initial system for monitoring, telemetering, and assembling air-quality data in New York City is composed of ten diverse monitoring locations, each containing instrumentation for the measurement of CO, SO2, particulates, temperature, wind speed, and wind direction. The central, or control point of the network, is at Air Pollution Control Department offices in downtown Manhattan. Display equipment, output devices, controls for the network, and the automatic equipment, including a small-scale integral computer, form the network master station. The system, as now installed, can accommodate up to 30 remote telemetry stations with up to 12 instruments at each location. Plans for extension of the system are now under consideration and include mobile field laboratories using radio telemetry and additional fixed locations.;1970;Robert R. Ryder;10.1109/TGE.1970.271379;Journals;0018-9413;
ieee_20221205_08_28_59;A Web-Based System to Monitor the Quality of Meta-Data in Web Portals;We present a Web-based system to monitor the quality of the meta-data used to describe content in Web portals. The system implements meta-data analysis using statistical, visualization and data mining tools. The Web-based system enables the site's editor to detect and correct problems in the description of contents, thus improving the quality of the Web portal and the satisfaction of its users. We have developed this system and tested it on a Portuguese portal for management executives;2006;Marcos Aurelio Domingues;10.1109/WI-IATW.2006.24;Conferences;;0-7695-2749-3
ieee_20221205_08_28_59;Data-driven Soft Sensor Approach For Quality Prediction in a Refinery Process;In petrochemical industry, the product quality encapsulates the commercial and operational performance of a manufacturing process. Usually, the product quality is measured in the analytical laboratory and it involves resources and considerable time delay. On-line prediction of quality using frequent process measurements would be beneficial in terms of operation and quality control. In this article, a novel soft sensor technology based on partial least squares (PLS) regression between process variables and quality variable is developed and applied to a refinery process for quality prediction. The modeling process is described, with emphasis on data preprocessing, PLS regression, multi-outliers' detection and variables selection in regression. Enhancement of PLS is also discussed to take into account the dynamics in the process data. The proposed approach is applied to data collected from a refinery process and its feasibility and performance are justified by comparison with laboratory data.;2006;D. Wang;10.1109/INDIN.2006.275785;Conferences;2378-363X;0-7803-9700-2
ieee_20221205_08_28_59;Identification of Load Power Quality Characteristics using Data Mining;The rapid increase in computer technology and the availability of large scale power quality monitoring data should now motivate distribution network service providers to attempt to extract information that may otherwise remain hidden within the recorded data. Such information may be critical for identification and diagnoses of power quality disturbance problems, prediction of system abnormalities or failure, and alarming of critical system situations. Data mining tools are an obvious candidate for assisting in such analysis of large scale power quality monitoring data. This paper describes a method of applying unsupervised and supervised learning strategies of data mining in power quality data analysis. Firstly underlying classes in harmonic data from medium and low voltage (MV/LV) distribution systems were identified using clustering. Secondly the link analysis is used to merge the obtained clusters into supergroups. The characteristics of these super-groups are discovered using various algorithms for classification techniques. Finally the a priori algorithm of association rules is used to find the correlation between the harmonic currents and voltages at different sites (substation, residential, commercial and industrial) for the interconnected supergroups;2006;Ali Asheibi;10.1109/CCECE.2006.277720;Conferences;0840-7789;1-4244-0038-4
ieee_20221205_08_28_59;Application of Data Mining Technology Based On BP Neural Network in Yarn Qualities Forecast;In this paper, the sample database for data mining is designed by analyzing historical data of raw cotton properly and spinning yarn quality. Based on the above analytic result, we design the BP neural network model for spinning yarn qualities prediction. And the model was used to forecast and analyze the spinning strength, unevenness value and filoplume. Finally, simulation and test result show that the method has excellent effect.;2006;Wang wen;10.1109/CHICC.2006.280887;Conferences;2161-2927;7-81077-802-1
ieee_20221205_08_28_59;X-SAR radiometric calibration and data quality;In April and October, 1994 the X-SAR was flown as part of the SIR-C/X-SAR space radar laboratory missions (SRL-1/2) on the Space Shuttle. Amongst other activities DLR is responsible for the calibration of all X-SAR data products and is running the German Processing and Archiving Facility (D-PAF). Calibration activities included three major parts. Before the first mission, the authors performed a detailed analysis of the overall system to localize the main error sources and developed algorithms and procedures to correct these errors. During the missions they concentrated their efforts on calibration campaigns at the Oberpfaffenhofen super test site. Post mission activities included the determination of the antenna pattern and the absolute calibration factor as well as detailed performance analyses. This paper describes the overall approach to radiometrically calibrate the X-SAR and provides information on system performance and data quality to users in the different application fields.<>;1995;M. Zink;10.1109/36.406670;Journals;1558-0644;
ieee_20221205_08_28_59;Tracking Spike-Amplitude Changes to Improve the Quality of Multineuronal Data Analysis;During extracellular electrophysiological recording experiments, the waveform of neuronal spikes recorded from a single neuron often changes. These spike-waveform changes make single-neuron identification difficult, particularly when the activities of multiple neurons are simultaneously recorded with a multichannel microelectrode, such as a tetrode or a heptode. We have developed a tracking method of individual neurons despite their changing spike amplitudes. The method is based on a bottom-up hierarchical clustering algorithm that tracks each neuron's spike cluster during temporally overlapping clustering periods. We evaluated this method by comparing spike sorting with and without cluster tracking of an identical series of multineuronal spikes recorded from monkey area-TE neurons responding to a set of visual stimuli. According to Shannon's information theory, errors in spike-amplitude tracking reduce the expected value of the amount of information about a stimulus set that is transferred by the spike train of a cluster. In this study, cluster tracking significantly increased the expected value of the amount of information transferred by a spike train (p<0.01). Additionally, the stability of the stimulus preference and that of the cross-correlation between clusters improved significantly (p<0.000001). We conclude that cluster tracking improves the quality of multineuronal data analysis;2007;Hidekazu Kaneko;10.1109/TBME.2006.886934;Journals;1558-2531;
ieee_20221205_08_28_59;QTOP-K: A novel Algorithm for mining high quality pattern-based clusters in GST Microarray Data;Pattern-based clustering is widely applied in bioinformatics and biomedical Recently, mining high quality pattern-based clusters has become an important research direction. However, the existing methods were neither efficient in large data set nor precise at measuring the quality of clusters. These problems have greatly limited the methods' application in large data set. This paper proposes a new algorithm, which can provide a more accurate measurement for the quality of clusters and sharply cut down the time for mining high quality patterned-based clusters compared with today's methods. Experiments are held on real data set and synthetic data set and the test result suggests that Qtop-k has made notable progress in the aforementioned problems;2006;Shuhui Chen;10.1109/ICCIAS.2006.294252;Conferences;;1-4244-0605-6
ieee_20221205_08_28_59;Contribution to Quality of Life: Cross-National Validation of New Metrics for Mobile Data Service Technology in Korea and Japan;Every technology, especially in ubiquitous computing area, should be geared to improve the quality of their users' life ultimately. In order to achieve this goal, the important pre-requisite is to measure the contribution of the technology to quality of life reliably and validly. This study provides a theoretical as well as an empirical basis for the development of better measures for the contribution of a mobile data service technology to the quality of its users' lives. The reliability and validity of the proposed metrics were verified through online surveys in Korea and Japan. The survey results also indicated that critical mobile data services could be identified based on the degree of contribution to the quality of life of mobile data-service users. Moreover, the critical services were found to be different between Korea and Japan;2006;Jinwoo Kim;10.1109/PICMET.2006.296734;Conferences;2159-5100;1-890843-14-8
ieee_20221205_08_28_59;Compression of Power Quality Disturbance Data Based on Energy Threshold and Adaptive Arithmetic Encoding;Recently, power quality issues have captured more attention. It is necessary to monitor power quality in order to analyze and evaluate it. The monitors will record huge data during disturbance. It is inconvenient for data storage and transmission. Compression of power quality disturbance data will save storage space efficiently and accelerate transmission speed. This paper proposes energy threshold method based on wavelet transform, and then integrates adaptive arithmetic encoding to compress disturbance data. The compression ratio is improved and performance is better. Four typical power quality disturbances including voltage sag, swell, interruption and transient impulse are used to test the proposed method, the validity is verified by simulation results.;2005;Jidong Wang;10.1109/TENCON.2005.300848;Conferences;2159-3450;0-7803-9311-2
ieee_20221205_08_28_59;Learning the Quality of Sensor Data in Distributed Decision Fusion;The problem of decision fusion has been studied for distributed sensor systems in the past two decades. Various techniques have been developed for either binary or multiple hypotheses decision fusion. However, most of them do not address the challenges that come with the changing quality of sensor data. In this paper we investigate adaptive decision fusion rules for multiple hypotheses within the framework of Dempster-Shafer theory. We provide a novel learning algorithm for determining the quality of sensor data in the fusion process. In our approach each sensor actively learns the quality of information from different sensors and updates their reliabilities using the weighted majority technique. Several examples are provided to show the effectiveness of our approach;2006;Bin Yu;10.1109/ICIF.2006.301632;Conferences;;0-9721844-6-5
ieee_20221205_08_28_59;Quality Control of Minerals Management Service - Oil Company ADCP Data at NDBC: A Successful Partnership Implementation;"The Minerals Management Service (MMS) requires that deep water oil drilling and production platforms in the northern Gulf of Mexico collect and provide current profile data to the National Data Buoy Center (NDBC). NDBC processes and displays the resulting currents on the NDBC website. NDBC has recently implemented quality control algorithms agreed upon by industry and the government. The resulting imagery and data, including quality control flags, are available on the publicly available NDBC website. The quality control algorithms and flags are presented and comparisons of the resulting files are described. Oil companies must collect current profile data when drilling wells or operating production platforms in water greater than 400 meters deep. They are required to collect the data at 20 minute intervals and transmit the data via FTP to NDBC. The data are received, decoded, and quality controlled at NDBC. The current profiles are then formatted in TEmperature Salinity and Current (TESAC) messages and transmitted over the Global Telecommunications System (GTS). The data are also viewed over the NDBC website as columnar listings and current vector stick plots. In order to determine the quality control algorithms for the current profiles, a committee of oil company, industry, and government representatives determined an approach that includes both individual bin (depth level) and profile algorithms. The algorithms take advantage of the fact that the Teledyne RDI Acoustic Doppler Current Profiler (ADCP) collects error velocity, percent good statistics for 3 and 4 beams, and correlation matrices and echo amplitudes for each beam. The algorithms described in this presentation were then implemented and flags generated for each quality control test. A total of nine flags are assigned within the NDBC database. The flags indicate good data (3), suspect data (2), or bad (1) data. Only bad data are not reproduced or plotted on the NDBC real-time webpage. Results from the implementation are being reviewed, but a quick look indicates that the algorithms are returning accurate descriptions of the ADCP data. The stick plots of ocean current with depth are much ""cleaner"" following the quality control implementation. The implementation of the quality control algorithms was delayed by Hurricanes Katrina and Rita, which impacted both the NDBC and the oil industry in the Gulf of Mexico. NDBC is now resubmitting past data files through the quality control algorithms to insure that all data at NDCB have been quality controlled. The results of this effort (including the quality control algorithms) are being shared with Integrated Ocean Observing System (IOOS) partners in an effort to standardize quality control of oceanographic data";2006;Richard L. Crout;10.1109/OCEANS.2006.307072;Conferences;0197-7385;1-4244-0115-1
ieee_20221205_08_28_59;Quality-Aware Sampling and Its Applications in Incremental Data Mining;We explore in this paper a novel sampling algorithm, referred to as algorithm PAS (standing for proportion approximation sampling), to generate a high-quality online sample with the desired sample rate. The sampling quality refers to the consistency between the population proportion and the sample proportion of each categorical value in the database. Note that the state-of-the-art sampling algorithm to preserve the sampling quality has to examine the population proportion of each categorical value in a pilot sample a priori and is thus not applicable to incremental mining applications. To remedy this, algorithm PAS adaptively determines the inclusion probability of each incoming tuple in such a way that the sampling quality can be sequential/preserved while also guaranteeing the sample rate close to the user specified one. Importantly, PAS not only guarantees the proportion consistency of each categorical value but also excellently preserves the proportion consistency of multivariate statistics, which will be significantly beneficial to various data mining applications. For better execution efficiency, we further devise an algorithm, called algorithm EQAS (standing for efficient quality-aware sampling), which integrates PAS and random sampling to provide the flexibility of striking a compromise between the sampling quality and the sampling efficiency. As validated in experimental results on real and synthetic data, algorithm PAS can stably provide high-quality samples with corresponding computational overhead, whereas algorithm EQAS can flexibly generate samples with the desired balance between sampling quality and sampling efficiency;2007;Kun-ta Chuang;10.1109/TKDE.2007.1005;Journals;2326-3865;
ieee_20221205_08_28_59;Assessment of the Quality of Teaching and Learning Based on Data Driven Evaluation Methods;"The data-driven decision support tool built around the SAS technology has been developed to support the evaluation and monitoring of the quality of educational process. The tool forms an integrated framework that can be used for managing of teaching and learning processes and for performing comparative studies in the participating institutions. The tool includes: the engine for the comparative statistical analysis of the quality of teaching, the mechanism for dissemination of analytical results and the reporting facility. The main aim of our study is to extract and to compare the information on the quality of courses and teaching obtained from the different sets of databases at the Faculty of Electronic Engineering (FEE) of the Wroclaw, University of Technology (WUT), Poland; the Faculty of Computer Science (FCS) at University of Las Palmas de Grand Canaria (ULPGC), Spain; the Faculty of Engineering, Software, the Polytechnic University of Upper Austria in Hagenberg, Austria; the Electrical and Computer Science Department at the University of Arizona in Tucson, USA and Software Engineering Group at the Faculty of Engineering University of Technology, Sydney (UTS), Australia. In this paper we describe the process involved, the methodology, the tools for the analysis; and we present the results of our study.";2006;Zenon Chaczko;10.1109/ITHET.2006.339723;Conferences;;1-4244-0406-1
ieee_20221205_08_28_59;Data Quality and Query Cost in Wireless Sensor Networks;This research is motivated by emerging, real-world wireless sensor network applications for monitoring and control. We examine the benefits and costs of caching data for such applications. We propose and evaluate several approaches to querying for, and then caching data in a sensor field data server. We show that for some application requirements (i.e., when delay drives data quality), policies that emulate cache hits by computing and returning approximate values for sensor data yield a simultaneous quality improvement and cost savings. This win-win is because when system delay is sufficiently important, the benefit to both query cost and data quality achieved by using approximate values outweighs the negative impact on quality due to the approximation. In contrast, when data accuracy drives quality, a linear trade-off between query cost and data quality emerges. We also identify caching and lookup policies for which the sensor field query rate is bounded when servicing an arbitrary workload of user queries. This upper bound is achieved by having multiple user queries share the cost of a sensor field query. Finally, we demonstrate that our results are robust to the manner in which the environment being monitored changes using two different sensor field models;2007;David Yates;10.1109/PERCOMW.2007.35;Conferences;;0-7695-2788-4
ieee_20221205_08_28_59;Data Quality of 3836 Polarimetric Radar and Its Observation of Melting-Layer;The technical status of the upgraded 3836 C-band dual-linear polarimetric Doppler radar (3836 even radar) is reported first. Then this study analyses its data quality and its detection ability. A model is established in which the bright band can be identified by applying the fuzzy logic method to the four polarimetric radar observation variables. The radar data are used to assess the result from the model and the variety features of different phased hydrometeors in the melting-layer bright band are also discussed;2006;Jun Wu Cao;10.1109/ICR.2006.343182;Conferences;;0-7803-9583-2
ieee_20221205_08_28_59;Quality Metrics for Object-Based Data Mining Applications;A new quality measurement for video sequences utilized in video retrieval systems and visual data mining applications is proposed. First, each frame of the sequence undergoes a segmentation step using extracted texture features from the gray-level cooccurrence matrix (GLCM) (Davis and Johns, 1979). Next, corresponding objects between adjacent frames are matched thus resulting in a 3-dimensional segmentation of the video into objects. Finally, color and texture features are extracted for each object in the sequence and provide the primary input in computing the quality measurement pertaining to the video. A low quality measurement may thus eliminate the possibility of the sequence being stored in a database retrieval system. The algorithm is tested on various types of video segments - pans, zooms, close-ups, and multiple objects' motion - with results included;2007;Mark Smith;10.1109/ITNG.2007.163;Conferences;;0-7695-2776-0
ieee_20221205_08_28_59;P2A-5 Effect of Contrast Microbubble Concentration on Quality of Echo Particle Image Velocimetry (Echo PIV) Data: Initial In Vitro Studies;We have recently developed a novel contrast-based ultrasonic particle image velocimetry (Echo PIV) technique to measure multi-component velocity vectors in opaque flows. In the technique, ultrasound contrast microbubbles are used as flow tracers, and digitally acquired backscattered RF-data are used with custom velocimetry analysis to obtain velocity vectors. The technique has been shown as useful in obtaining 2D flow velocity from various flow patterns, including blood flow. Well control of timely sensitive microbubble concentration in Echo PIV is very important to produce robust data quality. In this paper, effect of microbubble concentration on Echo PIV data quality was examined and a quantitative tool based on feature of Echo PIV image cross-correlation was developed to evaluate real time robustness of Echo PIV data quality induced by bubble concentration and image quality;2006;H. Zheng;10.1109/ULTSYM.2006.396;Conferences;1051-0117;1-4244-0202-6
ieee_20221205_08_28_59;Importance of Data Quality in Virtual Metrology;The purpose of VM is to enable the manufacturers to conjecture the wafer quality and deduce the causes of defects without performing physical metrology. VM requires a large amount of sensor data retrieved from production tools. However, inappropriateness and instability of the data collection system, which leads to incorrectness, fragment and asynchrony of data collected, may lead to inaccurate conjecture results. Hence, not only precision of the VM conjecture module but also quality of the collected data are essential to ensure accurate and stable VM results for improving production yield. In this work, the importance of data quality to VM is investigated. The data quality mechanism is proposed for data characteristic analysis, data anomaly detection, data cleaning, data normalization, and data reduction. Besides, the equipment of semiconductor chemical vapor deposition (CVD) is adopted as a practical example to illustrate the significance of data quality mechanism, and further verify feasibility and effectiveness of VM;2006;Yi-Ting Huang;10.1109/IECON.2006.347318;Conferences;1553-572X;978-1-5090-9155-3
ieee_20221205_08_28_59;Resources Allocation Optimization for Scalable Multimedia Data Subject to Quality of Service Contraints;Scalable multimedia data transmission are subject to specific constraints such as the quality of service (QoS) of sensitivity classes and the transmission rate (yielding a maximum size of each frame to send). Many scalable source decoders are used to discarding data than processing an erroneous stream. This featuring class structure is helpful to define a strategy that determines the maximum number of classes to send and deliver the suitable protection and transmission scheme (coding rates and modulation) to apply in accordance with the transmission constraints. It leads to the possible truncation of frame parts transmitted with an unequal error protection (UEP) scheme for severe channel conditions. In a MPEG-4 speech frames context, we compare our approach to other methods using equal error and existing UEP schemes. It results in a significant improvement of the peak SNR (PSNR) quality in poor channel transmission conditions;2006;Heykel Houas;10.1109/SPAWC.2006.346429;Conferences;1948-3252;0-7803-9711-8
ieee_20221205_08_28_59;Analysis of Power Quality Waveform for Data Transmission Efficiency over IEC 61850 Communication Standard;In power utilities, grid operators monitor power-quality phenomena such as voltage sag at several substations to obtain an overview of the general network state and evolution of power quality. Collections of these huge online data from disturbance recorders involved long and tedious downloading process, usage of Internet protocols consumed bandwidth and causes traffic congestion and collisions with other users of the wide area network. This paper describes an integrated application of power quality monitoring system for a modeled substation over the IEC 61850 standard developed in Java programming language. The application used wavelet compression technique for transmission of voltage sag waveform over IEC 61850 standard and then Internet to a control centre. Analysis done shows that the correct selection of mother wavelet can achieve over 90% data compression ability while preserving all significant power quality features required for power quality waveform analysis required by utilities.;2006;Bahisham Yunus;10.1109/PECON.2006.346639;Conferences;;1-4244-0274-3
ieee_20221205_08_28_59;Workflow Quality of Service Management using Data Mining Techniques;Organizations have been aware of the importance of quality of service (QoS) for competitiveness for some time. It has been widely recognized that workflow systems are a suitable solution for managing the QoS of processes and workflows. The correct management of the QoS of workflows allows for organizations to increase customer satisfaction, reduce internal costs, and increase added value services. In this paper we show a novel method, composed of several phases, describing how organizations can apply data mining algorithms to predict the QoS for their running workflow instances. Our method has been validated using experimentation by applying different data mining algorithms to predict the QoS of workflow;2006;Jorge Cardoso;10.1109/IS.2006.348466;Conferences;1941-1294;1-4244-0195-X
ieee_20221205_08_28_59;LANDSAT-4 MSS And Thematic Mapper Data Quality And Information Content Analysis;Landsat-4 Thematic Mapper and Multispectral Scanner data were analyzed to obtain information on data quality and information content. Geometric evaluations were performed to test band-to-band registration accuracy. Thematic Mapper overall system resolution was evaluated using scene objects which demonstrated sharp high contrast edge responses. Radiometric evaluation included detector relative calibration, effects of resampling, and coherent noise effects. Information content evaluation was carried out using clustering, principal components, transformed divergence separability measure, and numerous supervised classifiers on data from Iowa and Illinois. A detailed spectral class analysis (multispectral classification) was carried out on data from the Des Moines, Iowa area to compare the information content of the MSS and TM for a large number of scene classes.;1984;Paul E. Anuta;10.1109/TGRS.1984.350595;Journals;1558-0644;
ieee_20221205_08_28_59;Improvement in Tile Quality of Vidio Signals by Spatio-Temporal Data-Dependent Filtering;The restoration of video signals corrupted by the additive noise is important for getting the high quality video and the high compression ratio of video signals. In this paper, we propose a novel restoration method for video signals corrupted by the Gaussian noise. Conventional methods use the spatio-temporal filtering after the motion compensation. However, the accuracy of the motion compensation on degraded image is not satisfied. Therefore, in the proposed method, we introduce a spatial filter as the pre-filter for motion compensation. Furthermore, the proposed method makes has four filter choices (i.e., spatio-temporal filter, spatial filter, temporal filter, identity filter) depending on the local information. Thus, the proposed data dependent filter can reduce the additive noise while preserving the signal edge/detail and motion;2006;Shunsuke Saitoh;10.1109/ISPACS.2006.364826;Conferences;;0-7803-9733-9
ieee_20221205_08_28_59;"Comments on ""Water Quality Retrievals From Combined Landsat TM Data and ERS-2 SAR Data in the Gulf of Finland";A paper by Zhang , using a feedforward artificial neural network (ANN) for water quality retrievals from combined Thematic Mapper data and synthetic aperture radar data in the Gulf of Finland, has been published in this journal. This correspondence attempts to discuss and comment on the paper by Zhang The amount of data used in the paper by Zhang is not enough to determine the number of fitting parameters in the networks. Therefore, the models are not mathematically sound or justified. The conclusion is that ANN modeling should be used with care and enough data;2007;W. Sha;10.1109/TGRS.2007.895432;Journals;1558-0644;
ieee_20221205_08_28_59;Toward Assessing Data Quality of Ontology Matching on the Web;Nowadays the semantic Web is a leading Web technology, which enables semantic interoperability between structurally and semantically heterogeneous web sources and web users. Ontologies are a key of semantic interoperability and the main vehicle of the development of the semantic Web. One of the most challenging and important tasks of ontology engineering is integration of ontologies because with the purpose to built a common ontology for all Web sources and consumers in a domain. The present paper describes an approach of assessing data quality of ontology matching that allows evaluating correctness of mapping concepts and relationships from one ontological fragment to another. The purpose of correct mapping is to find identical and synonymous concepts and relationships in ontologies facilitating their integration to a common ontology, which serves as a base for attaining interoperability.;2007;Olga Vorochek;10.1109/CNSR.2007.68;Conferences;;0-7695-2835-X
ieee_20221205_08_28_59;System Requirements of Real Time Continuous Data Acquisition for Power Quality Monitoring;Data acquisition systems for power quality monitoring have complex requirements which also vary with the intended application. Overall, the complexity of an instrumentation system for power system monitoring is most dependent on the number of channels and sites which must be analysed, and the required time stamping accuracy. Three broad categories of system configurations are described with their basic parameters. The emphasis is on a discussion of hardware and software issues which are involved in a system specification.;2006;V. Kuhlmann;10.1109/UPEC.2006.367635;Conferences;;978-186135-342-9
ieee_20221205_08_28_59;Data-quality Guided Load Shedding for Expensive In-Network Data Processing;In situ wireless sensor networks, not only have to route sensed data from sources to destinations, but also have to filter and fuse observations to eliminate potentially irrelevant data. If data arrive faster to such fusion nodes than the speed with which they can consume the inputs, this will result in an overflow of input buffers. In this paper, we develop load shedding mechanisms which take into consideration both data quality and expensive nature of fusion operators. In particular, we present quality assessment models for objects and fusion operators and we highlight that such quality assessments may impose partial orders on objects.;2007;Lina Peng;10.1109/ICDE.2007.369003;Conferences;2375-026X;1-4244-0803-2
ieee_20221205_08_28_59;A Comprehensive Data Quality Methodology for Web and Structured Data;Measuring and improving data quality in an organization or in a group of interacting organizations is a complex task. Several methodologies have been developed in the past providing a basis for the definition of a complete data quality program applying assessment and improvement techniques in order to guarantee high data quality levels. Since the main limitation of existing approaches is their specialization on specific issues or contexts, this paper presents the comprehensive data quality (CDQ) methodology that aims at integrating and enhancing the phases, techniques and tools proposed by previous approaches. CDQ methodology is conceived to be at the same time complete, flexible and simple to apply. Completeness is achieved by considering existing techniques and tools and integrating them in a framework that can work in both intra and inter organizational contexts, and can be applied to all types of data. The methodology is flexible since it supports the user in the selection of the most suitable techniques and tools within each phase and in any context. Finally, CDQ is simple since it is organized in phases and each phase is characterized by a specific goal and techniques to apply. The methodology is explained by means of a running example.;2007;Carlo Batini;10.1109/ICDIM.2007.369236;Conferences;;1-4244-0682-X
ieee_20221205_08_28_59;Research on Air-quality Change with MODIS-derived AOD Data in Corresponding to 2008 Olympic Environment Monitoring Projects;Using aerosol optical depth (AOD) retrieved from temporal MODIS data to monitor region air-quality change. First, AOD images are retrieved from temporal MODIS data by a well-validated method. Based on four different regions in Beijing, AOD changing graphs are obtained from the retrieval images. The yearly graphs of AOD and PM10 in 2004 are compared. Results indicate that their total trends are good in accordance with each other. Meanwhile, the monthly averaged AOD graphs from June to September between 2003 and 2005 are compared at all the four regions. It turns out that monthly mean AOD in 2003 is higher than that of 2005 at most time, which implies air-quality is improved during 2003 and 2005.;2006;L. Liwei;10.1109/IGARSS.2006.388;Conferences;2153-7003;0-7803-9510-7
ieee_20221205_08_28_59;Data Quality Through Business Rules;Good quality data is a valuable asset for any enterprise in this information age. Though volumes of data have grown, their utility for decision making has been meagre. This is a result of poor quality data that enterprises generally possess. Data generation process goes through a number of stages from capture through exchange, integration, migration to storage and data can get corrupted at any stage. Information systems/business applications built with business rules embedded in the code are inadequate to overcome data quality problems. This paper presents a framework that enables enterprise develop business applications that externalize centralized business logic through business rules. Decoupling business rules from the application provides a number of benefits to enterprises including enhanced data quality conforming to defined rules and agility in responding to the ever changing demands of the business environment.;2007;Vivek Chanana;10.1109/ICICT.2007.375390;Conferences;;984-32-3394-8
ieee_20221205_08_28_59;Development of Analog Optohybrid Circuit for the CMS Inner Tracker Data Acquisition System: Project, Quality Assurance, Volume Production, and Final Performance;The tracker system of the compact muon solenoid (CMS) experiment, will employ approximately 40000 analog fiber-optic data and control links. The optical readout system is responsible for converting and transmitting the electrical signals coming out from the front-end to the outside counting room. Concerning the inner part of the Tracker, about 3600 analog optohybrid circuits are involved in this tasks. These circuits have been designed and successfully produced in Italy under the responsibility of INFN Perugia CMS group, completing the volume production phase by February 2005. Environmental features, reliability, and performances of the analog optohybrid circuits have been extensively tested and qualified. This paper reviews the most relevant steps of the manufacturing and quality assurance process: from prototypes to mass-production for the final use in the CMS data acquisition system.;2007;Daniel Ricci;10.1109/TEPM.2007.899101;Journals;1558-0822;
ieee_20221205_08_28_59;Integrating Data and Quality Space Interactions in Exploratory Visualizations;Data quality is an important topic for many fields because real-world data is rarely perfect. Analysis conducted on data of variable quality can lead to inaccurate or incorrect results. To avoid this problem, researchers have introduced visual elements and attributes into traditional visualization displays to represent data quality information in conjunction with the original data. However, little work thus far has focused on creating an interactive interface to enable users to explicitly explore that data quality information. In this paper, we propose a framework for the linkage between data space and quality space for multivariate visualizations. Moreover, we introduce two novel techniques, quality brushing and quality-series animation, to help users with the exploration of this linkage. A visualization technique specifically designed for the quality space, called the quality map, is proposed as a means to help users create and manipulate quality brushes. We present some interesting case studies to show the effectiveness of our approaches.;2007;Zaixian Xie;10.1109/CMV.2007.11;Conferences;;0-7695-2903-8
ieee_20221205_08_28_59;Heat Map Visualizations Allow Comparison of Multiple Clustering Results and Evaluation of Dataset Quality: Application to Microarray Data;Since clustering algorithms are heuristic, multiple clustering algorithms applied to the same dataset will typically not generate the same sets of clusters. This is especially true for complex datasets such as those from microarray time series experiments. Two such microarray datasets describing gene expression activities from regenerating newt forelimbs at various times following limb amputation were used in this study. A cluster stability matrix, which shows the number of times two genes appear in the same cluster, was generated as a heat map. This was used to evaluate the overall variation among the clustering algorithms and to identify similar clusters. A comparison of the cluster stability matrices for two related microarray experiments with different levels of precision was shown to be an effective basis for comparing the quality of the two sets of experiments. A pairwise heat map was generated to show which pairs of clustering algorithms grouped the data into similar clusters.;2007;John Sharko;10.1109/IV.2007.61;Conferences;1550-6037;0-7695-2900-3
ieee_20221205_08_28_59;Study on the Continuous Quality Improvement of Telecommunication Call Centers Based on Data Mining;Based on the study of the processes of telecommunication call centers, the service quality metrics of the call centers are put forward. And the mode of the continuous service quality improvement of the call centers based on data warehouse and data mining is studied. Then the process of the IVR (Interactive Voice Response) is analyzed and a mode for the efficiency improvement of IVR is put forward based on the exchange of the orders of the service items in the IVR. Then a service quality metrics of the agents, the ratio of recall in one hour, is put forward. This metrics can be used in the performance analysis of the agents. Furthermore, the model of the performance analysis and control of the ASA (Average Speed of Answer) based on data mining and SPC (Statistical Process Control) is put forward. At last, a method for forecasting the call arriving in is put forward based the time series analysis using dynamic data mining. The result certified that the efficiency and service quality of the telecommunication call center can be improved obviously using the method in this paper.;2007;He Shu-guang;10.1109/ICSSSM.2007.4280171;Conferences;2161-1904;1-4244-0885-7
ieee_20221205_08_28_59;Data Quality Evaluation Process and Methods of Natural Environment Conceptual Model in Simulation Systems;Currently in almost simulation systems applying environment models, data quality of natural environment is not sufficiently evaluated. In order to address this problem, the process and method of data quality evaluation are proposed for Natural Environment Conceptual Model (NECM). Firstly, a process of 4 steps is presented to evaluate data quality of NECM. Secondly, indexes and factors of data quality evaluation are given according to the characteristics of NECM. Thirdly, in order to meet the requirement of this data evaluation, the analytic hierarchy process (AHP) method is improved and then applied to evaluate the data quality. Finally, the results of data quality evaluation are given and analyzed;2006;Guobing Sun;10.1109/CESA.2006.4281678;Conferences;;7-900718-14-1
ieee_20221205_08_28_59;A Proposal for the Management of Mobile Network's Quality of Service (QoS) using Data Mining Methods;Today, the challenge for the service operators is not only to attract and subscribe new users but to retain already subscribed users. To gain a competitive edge over other service operators, the operating personnel have to measure the services provided to their users and the network performance in terms of Quality of Service (QoS) at regular periods. By analyzing the information in these measurements, they can manage the quality of service, which helps to improve their service and network performance. But due to the heavy increase in the number of users in recent years, they find it difficult to elicit essential information from such a large and complex data to manage the QoS using the existing methods. It is here that the recently developed and more powerful data mining methods come in handy. In this paper we proposed how data mining methods can be used to manage the mobile network QoS. We describe three data mining methods: Rough Set Theory, Classification and Regression Tree (CART), and Self Organizing Map (SOM).;2007;MPS Bhatia;10.1109/WOCN.2007.4284163;Conferences;2151-7703;1-4244-1005-3
ieee_20221205_08_28_59;Data Sharing Strategy for Guaranteeing Quality-of-Service in VoD Application;The phenomenal growth in the distributed multimedia applications has accelerated the popularity of Video-on-Demand (VoD) system. The vital task of multimedia applications is to satisfy diverse client's request for distinct video with confined resources by using assorted Quality-of-Service (QoS) procedures. In this paper a fusion of data sharing techniques like batching and recursive patching is applied in the local server for ensuring Quality-of-Service to the clients and enabling higher throughput. The network resources are apportioned appropriately using batching and the time difference between the requests is minified by recursive patching. The suggested algorithm renders the entire video to the clients using true VoD, near VoD using multicast or broadcast scheme depending on popularity of the video. The experimental results indicate that our approach accomplishes 2% reduction in blocking ratio and throughput is 10% -15% greater than the Poon 's strategy [15], which depicts that not only the resources are efficiently utilized but also a suitable Quality-of-Service is provided to each client.;2006;D. N. Sujatha;10.1109/ICISIP.2006.4286062;Conferences;;1-4244-0612-9
ieee_20221205_08_28_59;Predicting User-Perceived Quality Ratings from Streaming Media Data;Media stream quality is highly dependent on underlying network conditions, but identifying scalable, unambiguous metrics to discern the user-perceived quality of a media stream in the face of network congestion is a challenging problem. User-perceived quality can be approximated through the use of carefully chosen application layer metrics, precluding the need to poll users directly. We discuss the use of data mining prediction techniques to analyze application layer metrics to determine user-perceived quality ratings on media streams. We show that several such prediction techniques are able to assign correct (within a small tolerance) quality ratings to streams with a high degree of accuracy. The time it takes to train and tune the predictors and perform the actual prediction are short enough to make such a strategy feasible to be executed in real time and on real computer networks.;2007;Amy Csizmar Dalal;10.1109/ICC.2007.20;Conferences;1938-1883;1-4244-0353-7
ieee_20221205_08_28_59;Learning Fuzzy Linguistic Models from Low Quality Data by Genetic Algorithms;Incremental rule base learning techniques can be used to learn models and classifiers from interval or fuzzy-valued data. These algorithms are efficient when the observation error is small. This paper is about datasets with medium to high discrepancies between the observed and the actual values of the variables, such as those containing missing values and coarsely discretized data. We will show that the quality of the iterative learning degrades in this kind of problems, and that it does not make full use of all the available information. As an alternative, we propose a new implementation of a mutiobjective Michigan-like algorithm, where each individual in the population codifies one rule and the individuals in the Pareto front form the knowledge base.;2007;Luciano Sanchez;10.1109/FUZZY.2007.4295659;Conferences;1098-7584;1-4244-1210-2
ieee_20221205_08_28_59;An Empirical Study of the Classification Performance of Learners on Imbalanced and Noisy Software Quality Data;In the domain of software quality classification, data mining techniques are used to construct models (learners) for identifying software modules that are most likely to be fault-prone. The performance of these models, however, can be negatively affected by class imbalance and noise. Data sampling techniques have been proposed to alleviate the problem of class imbalance, but the impact of data quality on these techniques has not been adequately addressed. We examine the combined effects of noise and imbalance on classification performance when seven commonly-used sampling techniques are applied to software quality measurement data. Our results show that some sampling techniques are more robust in the presence of noise than others. Further, sampling techniques are affected by noise differently given different levels of imbalance.;2007;Chris Seiffert;10.1109/IRI.2007.4296694;Conferences;;1-4244-1500-4
ieee_20221205_08_28_59;MULS: A General Framework of Providing Multilevel Service Quality in Sequential Data Broadcasting;In recent years, data broadcasting becomes a promising technique to design a mobile information system with power conservation, high scalability and high bandwidth utilization. In many applications, the query issued by a mobile client corresponds to multiple items which should be accessed in a sequential order. In this paper, we study the scheduling approach in such a sequential data broadcasting environment. Explicitly, we propose a general framework referred to as MULS (standing for MUlti-Level Service) for an information system. There are two primary stages in MULS: on-line scheduling and optimization procedure. In the first stage, we propose an On- Line Scheduling algorithm (denoted by OLS) to allocate the data items into multiple channels. As for the second stage, we devise an optimization procedure SCI, standing for Sampling with Controlled Iteration, to enhance the quality of broadcast programs generated by algorithm OLS. Procedure SCI is able to strike a compromise between effectiveness and efficiency by tuning the control parameters. According to the experimental results, we show that algorithm OLS with procedure SCI outperforms the approaches in prior works prominently in both effectiveness (i.e., the average access time of mobile users) and efficiency (i.e., the complexity of the scheduling algorithm). Therefore, by cooperating algorithm OLS with procedure SCI, the proposed MULS framework is able to generate broadcast programs with flexibility of providing different service qualities under different requirements of effectiveness and efficiency: in the dynamic environment in which the access patterns and information contents change rapidly, the parameters used in SCI will perform online scheduling with satisfactory service quality. As for the static environment in which the query profile and the database are updated infrequently, larger values of parameters are helpful to generate an optimized broadcast program, indicating the advantageous feature of MULS.;2007;Hao-Ping Hung;10.1109/TKDE.2007.1072;Journals;2326-3865;
ieee_20221205_08_28_59;Data Compression of Power Quality Events Using the Slantlet Transform;The slantlet transform (SLT) is an orthogonal discrete wavelet transform (DWT) with two zero moments and with improved time localization. It also retains the basic characteristic of the usual filterbank such as octave band characteristic, a scale dilation factor of two and efficient implementation. However, the SLT is based on the principle of designing different filters for different scales unlike iterated filterbank approaches for the DWT. In this paper a novel approach for power quality data compression using the SLT is presented and its performance in terms of compression ratio (CR), percentage of energy retained and mean square error present in the reconstructed signals is assessed. Varieties of power quality events, which include voltage sag, swell, momentary interruption, harmonics, transient oscillation, and voltage flicker are used to test the performance of the new approach. Computer simulation results indicate that the SLT offers superior compression performance compared to the conventional DCT and the DWT based approaches.;2002;G. Panda;10.1109/MPER.2002.4311703;Magazines;1558-1705;
ieee_20221205_08_28_59;Power Quality Disturbance Data Compression, Detection, and Classification Using Integrated Spline Wavelet and S-Transform;In this paper power quality transient data are compressed and stored for analysis and classification purpose. From the compressed data set origtnal data are reconstructed and then analysed using a modified wavelet transform known as transform. Compression techniques using splines are performed through signal decomposition, thresholding of wavelet transform coefficients, and signal reconstruction. Finally we present compression results using splines and examine the application of splines compression in power quality monitoring to mitigate against data communication and data storage problems. Since Stransform has better time frequency and localisation property. power quality disturbances are detected and then classified in a superior way than the recently used wavelet transform.;2002;P. K. Dash;10.1109/MPER.2002.4312423;Magazines;1558-1705;
ieee_20221205_08_28_59;A Data Mining Algorithm for Monitoring PCB Assembly Quality;A pattern clustering algorithm is proposed in this paper as a statistical quality control technique for diagnosing the solder paste variability when a huge number of binary inspection outputs are involved. To accommodate this goal, a latent variable model is first introduced and incorporated into classical logistic regression model so that the interdependencies between measured physical characteristics and their relationship to the final solder defects can be explained. This probabilistic model also allows a maximum-likelihood principal component analysis (MLPCA) method to recognize the dimension of systematic causes contributing to solder paste variability. The correlated measurement variables are then projected onto the reduced latent space, followed by an appropriate clustering approach over the inspected solder pastes for variation interpretation and quality diagnosing. An application to a real stencil printing process demonstrates that this method facilitates in identifying the root causes of solder paste defects and thereby improving PCB assembly yield.;2007;Feng Zhang;10.1109/TEPM.2007.907576;Journals;1558-0822;
ieee_20221205_08_28_59;An EffectiveMulti-Layer Model for Controlling the Quality of Data;"Data mining aims to search for implicit, previously unknown, and potentially useful information that might be embedded in the data. It is well known that ""garbage in, garbage out"". Hence, to get meaningful mining results, a clean set of data is essential. In this paper, we propose an effective model for controlling the quality of data. Specifically, this three-layer model focuses on data validity and data consistency. To elaborate, the internal layer ensures that the observed data are valid and their values fall within reasonable ranges. The temporal layer ensures that data are consistent with their temporal behaviour. The spatial layer ensures that data are consistent with their spatial neighbours. A case study on applying our proposed model to real-life weather data for an agricultural application shows that our model is effective in controlling and improving data quality, and thus leading to better mining results. It is important to note the application of our proposed model is not confined to the weather data for agricultural applications. We also discuss, in this paper, how the proposed three-layer model can be effectively applicable to control the quality of data in some other real-life situations.";2007;Carson Kai-Sang Leung;10.1109/IDEAS.2007.4318086;Conferences;1098-8068;978-0-7695-2947-9
ieee_20221205_08_28_59;A Probabilistic Approach to Web Portal's Data Quality Evaluation;"Advances in technology and the use of the Internet have favoured the emergence of a large number of Web applications, including Web Portals. Web portals provide the means to obtain a large amount of information therefore it is crucial that the information provided is of high quality. In recent years, several research projects have investigated Web Data Quality; however none has focused on data quality within the context of Web Portals. Therefore, the contribution of this research is to provide a framework centred on the point of view of data consumers, and that uses a probabilistic approach for Web portal's data quality evaluation. This paper shows the definition of operational model, based in our previous work.";2007;Angelica Caro;10.1109/QUATIC.2007.10;Conferences;;978-0-7695-2948-6
ieee_20221205_08_28_59;An Empirical Study of China Quality Award on Firm's Market Value - Based on the Data from Chinese Stock Market;This paper empirically investigates the relation between China Quality Award and the market value of the firm by a sample of public listed companies that have won China Quality Award from 2001 to 2005 in the mainland of China. Our results show that the award winners experienced remarkably positive abnormal returns on the day of announcement ranging from 0.55% to 0.77% depending on the model used to generate the abnormal returns. According to Hendricks and Singhal (1996), winning a quality reward conveys the information about the systematic risk of the firm. However, we haven't found a statistically decrease in the equity and asset betas after the quality award announcement. Finally, the factors that affect abnormal returns are investigated, empirical results show that debt ratio of the firms and the award prestige have an significant impact on abnormal returns, however, the firm size doesn't play an important role on abnormal returns.;2007;Xiang-Zhi Bu;10.1109/WICOM.2007.1043;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_28_59;Application of Data Mining in Manufacturing Quality Data;Knowledge on product quality is one of the most important knowledge sources throughout the product lifecycle for the efficiency and effectiveness of product design decisions. To provide quality related knowledge, this paper proposed one data mining based knowledge discovery approach. This approach can extract knowledge on product quality from large volume of quality related manufacturing data. The effectiveness of this approach is illustrated and validated by an example adapted from literature. Finally, some conclusions and future works are discussed.;2007;Keqin Wang;10.1109/WICOM.2007.1318;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_28_59;Study on the Continuous Quality Improvement Systems of LED Packaging Based on Data Mining;LED is one of the most widely used components in electric products. And the LED packaging is a very important process between semiconductor manufacturers and the electric product manufacturers. Based on the analysis of the characteristics of the LED packaging processes, a quality control model based on SPC (statistical process control) and data mining is put forward. The data mining is used as the quality data analysis tool and the quality diagnosis method. Then an infrastructure of the integrated continuous quality improvement systems of the LED packaging is put forward. In this infrastructure, there are three layers of the data collection layer, the data analysis layer and the result viewer layer. Furthermore, the data warehouse of LED packaging is designed with the snowflake schema. A 3 layer yield rate SPC is studied and the decision tree method is used as a quality diagnosis method based on the designed data warehouse. Finally, a prototype of the continuous quality improvement system is developed.;2007;Shu-Guang He;10.1109/WICOM.2007.1378;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_28_59;Evaluating Spatial Data Quality in GIS Database;The quality of spatial data is often limited by the quality of their sources such as paper maps and satellite images. Spatial operations performed on database of geographical information systems (GIS) such as selection, projection, and Cartesian product, do not always work correctly because their accuracy and completeness depends on the quality of spatial data. The present paper suggests a methodology to evaluate two data quality characteristics - accuracy and completeness - of the spatial database. Four quantitative measures are introduced to assess the quality of spatial data. Their explicit forms are derived for a tuple, and four assumptions are presented where the measures can be evaluated efficiently by numerical calculation.;2007;Ying Su;10.1109/WICOM.2007.1463;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_28_59;Study on the Quality Improvement of Injection Molding in LED Packaging Processes Based on DOE and Data Mining;The LED (light emitting diode) packaging is a very important process between semiconductor manufacturers and the electric product manufacturers. And the injection molding process in LED packaging is critical to the quality of the final products. Based on the analysis of the injection molding processes, the main quality problems and their possible causes are studied. Then the RSM (response surface methodology) of DOE (design of experiment) is used for the optimization of the producing parameters. The optimized parameters, the mold temperature, the warm-up temperature, the screw pressure and the screw time, are found with DOE. In the running process, the CTQ (critical to quality) is controlled with SPC (Statistical process control) by different dimensions like product, product catalog, time and devices. Finally, a model of continuous quality improvement based on data mining is put forward. The association analysis is used for the parameter optimization in the running process.;2007;Shu-Guang He;10.1109/WICOM.2007.1626;Conferences;2161-9654;978-1-4244-1312-6
ieee_20221205_08_28_59;Tuning anonymity level for assuring high data quality: an empirical study.;Preserving data privacy is posing new challenges to software engineering researchers. Current technologies can be too cumbersome, pervasive or costly to be successfully applied in dynamic and complex scenarios where data exchange occurs among a large number of applications. Anonymization techniques seem to be a promising candidate, even if preliminary investigations suggest that they could deteriorate the quality of data. An empirical study has been carried out in order to understand the relationship between the anonymization level and the degradation of data quality.;2007;Gerardo Canfora;10.1109/ESEM.2007.23;Conferences;1949-3789;978-0-7695-2886-1
ieee_20221205_08_28_59;Filtering, Robust Filtering, Polishing: Techniques for Addressing Quality in Software Data;Data quality is an important aspect of empirical analysis. This paper compares three noise handling methods to assess the benefit of identifying and either filtering or editing problematic instances. We compare a 'do nothing' strategy with (i) filtering, (ii) robust filtering and (Hi) filtering followed by polishing. A problem is that it is not possible to determine whether an instance contains noise unless it has implausible values. Since we cannot determine the true overall noise level we use implausible val.ues as a proxy measure. In addition to the ability to identify implausible values, we use another proxy measure, the ability to fit a classification tree to the data. The interpretation is low misclassification rates imply low noise levels. We found that all three of our data quality techniques improve upon the 'do nothing' strategy, also that the filtering and polishing was the most effective technique for dealing with noise since we eliminated the fewest data and had the lowest misclassification rates. Unfortunately the polishing process introduces new implausible values. We believe consideration of data quality is an important aspect of empirical software engineering. We have shown that for one large and complex real world data set automated techniques can help isolate noisy instances and potentially polish the values to produce better quality data for the analyst. However this work is at a preliminary stage and it assumes that the proxy measures of lity are appropriate.;2007;Gernot Liebchen;10.1109/ESEM.2007.70;Conferences;1949-3789;978-0-7695-2886-1
ieee_20221205_08_28_59;Data Quality - The Key Success Factor for Data Driven Engineering;As the scale and diversity of data grows in the digital arena, the complexities of data driven engineering grow multifold with it. The last several years have brought forth several new technologies to service this need - semantic Web, grid systems, Web service composition to mention a few. However, a fundamental underpinning of the success of these technologies resides in the quality of data that they can provide. Often the failure of a technology is attributed to its functionality when the real problem lies in the quality of data it uses and subsequently produces. In this paper, we highlight a need to embrace data quality considerations in all aspects of data driven engineering.;2007;Shazia Sadiq;10.1109/NPC.2007.176;Conferences;;978-0-7695-2943-1
ieee_20221205_08_28_59;Experience at Italian National Institute of Health in the quality control in telemedicine: tools for gathering data information and quality assessing;"The authors proposed a set of tools and procedures to perform a Telemedicine Quality Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers. The proposed tools were: the Informative Questionnaire (InQu), the Classification Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The InQu served to acquire the information about the examined TM product/service; the ClFo allowed to classify a TM product/service as belonging to one application area of TM. The TF was intended as a technical dossier of product and forced the TM supplier to furnish the only requested documentation of its product, so to avoid redundant information. The QACL was a checklist of requirements, regarding all the essential aspects of the telemedical applications, that each TM products/services must be met. The final assessment of the TM product/service was carried out via the QACL, by computing the number of agreed requirements: on the basis of this computation, a Quality Level (QL) was assigned to the telemedical application. Seven levels were considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality control of the telemedical applications and should be a guidance to all the TM practitioners, from the manufacturers to the expert evaluators. The quality control process procedures proposed thus could be adopted in future as routine procedures and could be useful in the assessing the TM delivering into the National Health Service versus the traditional face to face healthcare services.";2007;D. Giansanti;10.1109/IEMBS.2007.4352911;Conferences;1558-4615;978-1-4244-0788-0
ieee_20221205_08_28_59;Power Quality of Renewable Energy Systems Can Be Evaluated Using Simulation Data;The paper presents a comparative study of a renewable energy system using simulation data versus the data recorded for its implementation at the Hybrid Power System Test Bed (HPSTB) at the National Wind Technology Center, NREL. The simulation data were obtained from the model realized using RPM-SIM simulator. This study shows that under different conditions the power, voltage, and frequency traces of a simulated system follow closely those recorded. Consequently, it is concluded that the quality of power generated under different conditions can be evaluated using simulation data and that such simulation study can be used to develop the structure and control strategy of renewable energy system to meet power quality requirements.;2007;Jan T. Bialasiewicz;10.1109/ISIE.2007.4375001;Conferences;2163-5145;978-1-4244-0755-2
ieee_20221205_08_28_59;Quality Assessment of Affymetrix GeneChip Data using the EM Algorithm and a Naive Bayes Classifier;Recent research has demonstrated the utility of using supervised classification systems for automatic identification of low quality microarray data. However, this approach requires annotation of a large training set by a qualified expert. In this paper we demonstrate the utility of an unsupervised classification technique based on the Expectation-Maximization (EM) algorithm and naïve Bayes classification. On our test set, this system exhibits performance comparable to that of an analogous supervised learner constructed from the same training data.;2007;Brian E. Howard;10.1109/BIBE.2007.4375557;Conferences;;978-1-4244-1509-0
ieee_20221205_08_28_59;The ALICE-LHC Online Data Quality Monitoring Framework: Present and Future;ALICE is one of the experiments under installation at CERN Large Hadron Collider, dedicated to the study of Heavy-Ion Collisions. The final ALICE Data Acquisition system has been installed and is being used for the testing and commissioning of detectors. The Online Data Quality monitoring is an important part of the DAQ software framework (DATE). In this presentation we overview the implementation and usage experience of the interactive tool MOOD used for the commissioning period of ALICE and we present the architecture of the Automatic Data Quality Monitoring framework, a distributed application aimed to produce, collect, analyze, visualize and store monitoring data in a large, experiment wide scale.;2007;Filimon Roukoutakis;10.1109/RTC.2007.4382730;Conferences;;978-1-4244-0867-2
ieee_20221205_08_28_59;Data Quality Monitoring Framework for the ATLAS Experiment at the LHC;Data quality monitoring (DQM) is an important and integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results while data is being processed. When DQM occurs in the online environment, it provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to only have dependence on software that is common between online and offline (such as ROOT) and therefore is used in the offline framework as well. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.;2007;Corso-Radu;10.1109/RTC.2007.4382779;Conferences;;978-1-4244-0867-2
ieee_20221205_08_28_59;Refinement of a Tool to Assess the Data Quality in Web Portals;The Internet is now firmly established as an environment for the administration, exchange and publication of data. To support this, a great variety of Web applications have appeared, among these web portals. Numerous users worldwide make use of Web portals to obtain information for different purposes. These users, or data consumers, need to ensure that this information is suitable for the use to which they wish to put it. PDQM (portal data quality model) is a model for the assessment of portal data quality. It has been implemented in the PoDQA tool (portal data quality assessment tool), which can be accessed at http://podqa.webportalqualitv.com. In this paper we present the various refinements that it has been necessary to make in order to obtain a tool which is stable and able to make accurate and efficient calculations of the elements needed to assess the quality of the data of a Web portal.;2007;Angelica Caro;10.1109/QSIC.2007.4385501;Conferences;2332-662X;978-0-7695-3035-2
ieee_20221205_08_28_59;Improving grid monitoring with data quality assessment;As Grid emerges as a cyber-infrastructure for the next-generation of e-Science applications, monitoring Grid becomes a very significant task. A typical Grid application is composed of a large number of resources that can fail, including network, hardware and software. Even when monitoring information from all these components is accessible, it is hard to determine whether anomalies and failures during the execution are related to a particular job. However receiving intermediate results and interacting with applications play a key role for users in reality. Considering the complexity of implementation and the large scope the monitoring system covers, there is no doubt we will face incomplete and duplicate data in many applications. Overcoming data heterogeneity is a long standing problem in the Grid research communities. It will be a disaster to handle large amount of inaccurate information where the quality of data is very poor. Fortunately, a wide spectrum of applications exhibit strong dependencies among data samples, the readings of nearby sensors are generally correlated, and the components are connected with interactions. Such relations can be used for promoting the quality of the recorded data. This paper proposes a data cleaning approach oriented Grid monitoring model, which is based on modeling data dependencies based on entity relation graph. We bring effective data quality preprocessing approach into the Grid applications monitoring model, which is critical because many real-world Grid datasets are not perfect, but rather they contain missing, erroneous, duplicate data and other data quality problems.;2007;Wei Liu;10.1109/ISCIT.2007.4392260;Conferences;;978-1-4244-0977-8
ieee_20221205_08_28_59;Representing Data Quality for Streaming and Static Data;In smart item environments, multitude of sensors are applied to capture data about product conditions and usage to guide business decisions as well as production automation processes. A big issue in this application area is posed by the restricted quality of sensor data due to limited sensor precision as well as sensor failures and malfunctions. Decisions derived on incorrect or misleading sensor data are likely to be faulty. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In this paper, we present a flexible model for the efficient transfer and management of data quality for streaming as well as static data. We propose a data stream metamodel to allow for the propagation of data quality from the sensors up to the respective business application without a significant overhead of data. Furthermore, we present the extension of the traditional RDBMS metamodel to permit the persistent storage of data quality information in a relational database. Finally, we demonstrate a data quality metadata mapping to close the gap between the streaming environment and the target database. Our solution maintains a flexible number of DQ dimensions and supports applications directly consuming streaming data or processing data filed in a persistent database.;2007;Anja Klein;10.1109/ICDEW.2007.4400967;Conferences;;978-1-4244-0832-0
ieee_20221205_08_28_59;Scheduling Algorithm of Update Transactions and Quality of Service Management Based on Derived Data in Real-Time and Mobile Database Systems;The demand on mobile and real-time database application and service in distributed environment is becoming increasingly extensive, since the workload cannot be precisely predicted so that they can become overloaded possibly. Low bandwidth in mobile environment will lead to aborting or restarting for transactions because of competing for the limited system resources severely. In this paper, we propose a novel performance metric for example deadline miss ratio of transactions and data freshness based on the relationship of data items in the DAG (directed acyclic graph) to ensure the QoS (quality of service ) in mobile and real-time database systems. Feedback control architecture and MODDFT (modifying on-demand breath-first traversal algorithm) are proposed in our system to decrease miss ratio of transactions and improve data freshness. The MODDFT algorithm can guarantee the QoS performance will not be beyond the reference by database administrator through simulation experiments.;2007;Guohui Li;10.1109/FCST.2007.26;Conferences;2159-631X;978-0-7695-3036-9
ieee_20221205_08_28_59;A Statistical Approach to Volume Data Quality Assessment;Quality assessment plays a crucial role in data analysis. In this paper, we present a reduced-reference approach to volume data quality assessment. Our algorithm extracts important statistical information from the original data in the wavelet domain. Using the extracted information as feature and predefined distance functions, we are able to identify and quantify the quality loss in the reduced or distorted version of data, eliminating the need to access the original data. Our feature representation is naturally organized in the form of multiple scales, which facilitates quality evaluation of data with different resolutions. The feature can be effectively compressed in size. We have experimented with our algorithm on scientific and medical data sets of various sizes and characteristics. Our results show that the size of the feature does not increase in proportion to the size of original data. This ensures the scalability of our algorithm and makes it very applicable for quality assessment of large-scale data sets. Additionally, the feature could be used to repair the reduced or distorted data for quality improvement. Finally, our approach can be treated as a new way to evaluate the uncertainty introduced by different versions of data.;2008;Chaoli Wang;10.1109/TVCG.2007.70628;Journals;2160-9306;
ieee_20221205_08_28_59;Assistance ontology of quality control for enterprise model using data mining;There are many quality domains in which ideas and concepts about quality are represented. The intelligent discovery assistants ontology of data mining (DM) processes was presented to compose and select the large space and non-trivial interaction in quality control for enterprise. We use a prototype to show that quality control for enterprise model is using the virtual enterprise quality ontology. A simple, but typical DM process was presented in the paper, which included preprocessing data, applying a data-mining algorithm, and post processing the mining results. It provides users with systematic enumerations of valid DM processes, in order that important, potentially fruitful options are not overlooked and effective rankings of these valid processes by different criteria, to facilitate the choice of DM processes to execute. Deeply research in the quality and ontology area is realized in protege with the format of OWL. Assistance ontology has the function to help mining workers selecting the algorithm, how to help selecting algorithm, the one prerequisite is that establishes good data mining method ontology. The intelligent discovery assistants search and deduct in the quality ontology. Finally, a study case is given to explain the practical application with the fault diagnosis bases on ontology, and was given encouraging results.;2007;Xuhui Chen;10.1109/IEEM.2007.4419260;Conferences;2157-362X;978-1-4244-1529-8
ieee_20221205_08_28_59;Quality improvement of steel products by using multivariate data analysis;This paper describes quality improvement methods based on multivariate data analysis and their application to an industrial steel process. The PCA-LDA, which combines principal component analysis and linear discriminant analysis, is used for influential factor analysis of the qualitative quality variables. In addition, Data-Driven Quality Improvement (DDQI) is used to determine the optimal operating condition that can achieve the desired product quality under a given objective function and various constraints. The PCA-LDA and the DDQI can provide useful information to improve product quality. The experimental results show the effectiveness of the proposed methods.;2007;Yoshiaki Nakagawa;10.1109/SICE.2007.4421396;Conferences;;978-4-907764-27-2
ieee_20221205_08_28_59;Assessing Data Quality for Geographical Information System Databases;GIS is a tool for spatial-related data processing and decision making. Handling decision making under high quality data is a further extension of GIS functions. Therefore, it is of vital importance to assess data quality in GIS and to decide the fitness of data to user's particular applications. We present a methodology to determine two data quality characteristics - accuracy and completeness - that are of critical importance to decision makers. We examine how the quality metrics of source data affect the quality for information outputs produced using the relational algebra operations Selection, Projection, and Cartesian product. Our methodology can help users deciding the fitness of spatial data to their GIS applications according to the quality levels much efficiently.;2007;Su Ying;10.1109/ICMSE.2007.4421817;Conferences;2155-1855;978-7-88358-080-5
ieee_20221205_08_28_59;Data Quality Guidelines for GEOSS Consideration-The CEOS Working Group on Calibration and Validation (WGCV);The harmonization of operational data products and the creation of higher level information products such as global maps and time series (from different sensor sources) are required to satisfy the operational service requirements of the societal benefit areas as outlined in the GEOSS implementation plan. The CEOS Worldng Group on Calibration and Validation (WGCV) concentrates on defining standards and procedures aimed at allowing for the inter-comparison and ultimate utilization of data from all Earth observing platforms, both current and future. WGCV strives to establish common approaches to validation, calibration and data exchange formats to ensure effective cooperative use of all CEOS member space assets in addressing important global scale problems. This paper reviews the WGCV data assurance strategy, detailed system element requirements to guarantee data quality, and current WGCV activities for the generation and validation of products..;2007;S. Ungar;10.1109/IGARSS.2007.4422791;Conferences;2153-7003;978-1-4244-1212-9
ieee_20221205_08_28_59;DataFed: Mediated web services for distributed air quality data access and processing.;This is a report on the federated data system,DataFed for distributed air quality data. Data sources are federated by applying a universal, multi-dimensional data model. The physical and semantic homogenization is accomplished by wrapper services. Data processing is performed through web services, which themselves can be distributed. Data processing applications are created by the composition of the distributed service components. International Standards and Protocols are used to establish interoperability of the service components. In this work we have adapted the OGC Web services as a standard protocol for air quality data access. This report summarizes our experiences with the OGC W*S based service composition. It includes the results of our participation in the GALEON (geo-interface to atmosphere,land,earth,ocean netCDF) Interoperability experiment.;2007;Rudolf B. Husar;10.1109/IGARSS.2007.4423730;Conferences;2153-7003;978-1-4244-1212-9
ieee_20221205_08_28_59;Data system for the monitoring of power quality in the transmission substations supplying big consumers;During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.;2007;Fanica Vatra;10.1109/EPQU.2007.4424094;Conferences;2150-6655;978-84-690-9441-9
ieee_20221205_08_28_59;The Development of an Objective Metric for the Accessibility Dimension of Data Quality;This paper presents interim results with respect to an objective metric for the accessibility dimension of data quality. It builds on a conceptual framework developed in previous research. This framework examined the dimensions of data quality and in particular the accessibility dimension. A definition of the accessibility dimension was presented based on its objective attributes as outlined by both practitioners and academics. The findings suggest that it is possible to measure this dimension objectively. The attributes are measured in a controlled environment and have been compared with traditional survey methodologies. The results indicate a relationship between the level of data quality and each of the accessibility attributes examined in the controlled environment. The overall measure of data quality can be it is argued improved by measuring these objective attributes and applying the necessary improvements identified. Future research intends to examine all of these attributes, their impact on data quality and possible remedies for overall improved data quality.;2007;Owen Foley;10.1109/IIT.2007.4430434;Conferences;;978-1-4244-1841-1
ieee_20221205_08_28_59;The ALICE-LHC online Data Quality Monitoring framework;ALICE is one of the experiments under installation at CERN Large Hadron Collider, dedicated to the study of heavy- ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Data Quality Monitoring (DQM) is an important aspect of the online procedures for a HEP experiment. In this presentation we overview the architecture, implementation and usage experience of ALICE'S AMORE (Automatic MOnitoRing Environment), a distributed application aimed to collect, analyze, visualize and store monitoring data in a large, experiment wide scale. AMORE is interfaced to the DAQ software framework (DATE) and follows the publish-subscribe paradigm where a large number of batch processes execute detector-specific analysis on raw data samples and publish monitoring results on specialized servers. Clients connected to these servers have the ability to correlate, further analyze and visualize the monitoring data. Provision is taken to archive the most important results so that historic plots can be produced.;2007;Sylvain Chapeland;10.1109/NSSMIC.2007.4436318;Conferences;1082-3654;978-1-4244-0923-5
ieee_20221205_08_28_59;Guidelines for Setting Organizational Policies for Data Quality;From a process perspective, the tasks that individuals carry out within an organization are linked. These linkages are often documented as process flow diagrams that connect the data inputs and outputs of individuals. In such a connected setting, the differences among individuals in preference for data attributes such as timeliness, accuracy, etc., can cause data quality problems. For example, individuals at the head of a process flow may bear all the cost of capturing high quality data but may not receive all of the benefits although the rest of the organization benefits from their diligence. Consequently, these individuals, in absence of any managerial intervention, may not invest enough in data quality. In this research, solutions to this and similar organization data quality problems are proposed. The solutions focus on principles of reengineering, employee empowerment, decentralization of computing, and mechanisms to measure and reward individuals for their data quality efforts.;2008;Rajiv Dewan;10.1109/HICSS.2008.187;Conferences;1530-1605;978-0-7695-3075-8
ieee_20221205_08_28_59;HDQ: A meta-model for the quality improvement of heterogeneous data;In this paper, we outline the HDQ meta-model and exemplify its application to a realistic organizational scenario within a methodology for assessing and improving corporate data quality. The main aim of the HDQ meta-model is to support the modeling of the information resources used within an organization towards the application of comprehensive but yet lean methodologies of data quality improvement. The approach allowed by HDQ meta-model is aimed at addressing the scalable and incremental management of the quality of data represented in either structured, semi- structure or unstructured information sources.;2007;Daniele Barone;10.1109/ICDIM.2007.4444259;Conferences;;978-1-4244-1476-5
ieee_20221205_08_28_59;Technical method for service quality measurement and user’s service usage collection in wireless broadband data service;"In this study, the relationship between network access quality and RF quality parameter in the wireless networks which have different characteristics from wired networks when the dedicated terminals being connected to the high speed wireless data networks (In other words, broadband wireless access service networks) such as CDMA 1X EV-DO, WCDMA(HSDPA), or Mobile WiMax (WiBro) is analyzed. Also, this study proves that the analysis in connection with the users’ service usage patterns is available. This was possible by constructing and operating the broadband wireless access service Monitoring System (BWA Monitoring System) that collects user data in connection with the CM (Connection Manager) program of the respective terminal. And, analysis regarding the users’ network usage patterns was possible which could be utilized as reference in creating new business models; furthermore, the study shows that the SLA (Service Level Agreement) in the wireless network sector can be proved.";2007;Ju Yeoul Park;10.1109/SOFTCOM.2007.4446060;Conferences;;978-953-6114-95-5
ieee_20221205_08_28_59;Data Quality Monitoring Framework for the ATLAS Experiment at the LHC;Data quality monitoring (DQM) is an integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results to the shift personnel while data is being processed. In the online environment, DQM provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing the execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to have dependence only on software that is common between online and offline (such as ROOT) and therefore the same framework can be used in both environments. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.;2008;A. Corso-Radu;10.1109/TNS.2007.912884;Journals;1558-1578;
ieee_20221205_08_28_59;The ALICE-LHC Online Data Quality Monitoring Framework: Present and Future;ALICE is one of the experiments under installation at CERN Large Hadron Collider (LHC), dedicated to the study of heavy-ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Online data quality monitoring is an important part of the DAQ software framework, DATE. In this paper, we overview the implementation and usage experience of the interactive tool MOOD used for the commissioning period of ALICE, and we present the architecture of the automatic data quality monitoring framework, a distributed application aimed at producing, collecting, analyzing, visualizing, and storing monitoring data in a large experiment-wide scale.;2008;Filimon Roukoutakis;10.1109/TNS.2007.913942;Journals;1558-1578;
ieee_20221205_08_28_59;Delivering high quality, secure speech communication through low data rate 802.15.4 WPAN;IEEE 802.15.4 is a Wireless Personal Area Network (WPAN) for low data-rate and duty-cycle applications. How this simple network can also be used for high quality, secure speech communication is described in this paper. The key to achieve this lies in how the various features can be packed into the low bandwidth structure that has been allocated in this protocol. A low bandwidth high quality speech vocoder such as the Advanced Multiband Excitation (AMBE) from DVSI has been found suitable to be used in our evaluation. Guaranteed Time Slot (GTS) has been reserved in the superframe structure to ensure continuous and uninterrupted audio streaming. For high security, the inherently 802.15.4 built in Advanced Encryption Standard (AES128) and Message Integrity Code (MIC) 128 bits algorithms are utilized. At the end of the paper, a methodology to channel speech more efficiently by forming GTS dynamically is proposed.;2007;Lee Yong Hua;10.1109/ICTMICC.2007.4448593;Conferences;;978-1-4244-1094-1
ieee_20221205_08_28_59;Information quality mapping in resource-constrained multi-modal data fusion system over wireless sensor network with losses;One of the approaches to reduce the complexity of application adaptation for a particular sensor network installation is to separate the application completely from the information acquisition level of the sensor network. However, in this case the question arises if the information obtained is good enough for the application. In this paper we describe the possible metrics of information quality (IQ) in the sensor network. We present a framework which addresses the problem of satisfying the IQ in the case of a dynamic system with resource constraints and communication losses. The framework is based on the Dynamic Bayesian Network model. The framework is built on a base of a constraint optimization problem which takes into account all the levels of information processing, from measurement to aggregation to data delivery by the network.;2007;Andrei Tolstikov;10.1109/ICICS.2007.4449835;Conferences;;978-1-4244-0983-9
ieee_20221205_08_28_59;Channel Quality Variation as a Design Consideration for Wireless Data Link Protocols;The data link protocol HDL+, which has been proposed for incorporation into NATO STANAG 4538, employs an innovative combination of Type II Hybrid-ARQ techniques with real-time adaptation of signal constellation and code rate to achieve high throughput performance under a wide variety of channel conditions. The ionospheric HF channels for which HDL+ was designed exhibit important variations in channel quality (SNR, fading and multipath chcracteristics) at a variety of time scales from seconds to minutes. It appears plausible that the design characteristics of the HDL+ protocol couldprove especially valuable in coping with time-varying channel quality over these time scales. In this paper, we • Present and discuss a model of channel quality variation and a way of incorporating it into an ionospheric channel simulator suitable for performance characterization of HF communications waveforms and protocols • Provide overviews of HDL+ and of the STANAG 5066 data link protocol, a widely-used protocol based on conventional (not hybrid) ARQ techniques • Present and discuss comparative performance data for the HDL+ and STANAG 5066 protocols obtained under test conditions including channel quality variation.;2007;William Batts;10.1109/MILCOM.2007.4455231;Conferences;2155-7586;978-1-4244-1513-7
ieee_20221205_08_28_59;Implications of Reduced Measurement Data Sets on overall Microwave Tomographic Image Quality;Microwave tomographic approaches generally acquire measurement data from multi-directional field illuminations and reconstruct the images into dielectric property maps, or images, of the target zone. In general, this inversion process is ill-posed and ill-conditioned. Conventional wisdom suggests that when the amount of data collected can be increased, the image quality will be improved. Of course, there are often real costs involved in acquiring more data - especially with respect to the system hardware design. In fact, previous singular value decomposition studies by our group have indicated the returns from such added costs diminish rapidly with the number of antennas after a certain point. In addition, by implementing our log-magnitude/phase reconstruction algorithm, we have begun examining the actual pieces of measurement data to explore which pieces contain more valuable information. These experiments may have important implications for our reconstruction strategies and also open up new imaging opportunities.;2007;P.M. Meaney;10.1049/ic.2007.1311;Conferences;0537-9989;978-0-86341-842-6
ieee_20221205_08_28_59;Maximising Data Return: Towards a quality control strategy for Managing and Processing TRDI ADCP Data Sets from Moored Instrumentation;In this paper, we evaluate, by means of case studies, methods for quality controlling data obtained from Teledyne RDI ADCP measurements (ensembles and bins or depth cells) such that the data can be checked and acceptance (pass or fail) criteria established. The QA strategy applied utilizes the quality control criteria recorded for each beam and bin by the Teledyne RDI ADCP itself and which are stored within the binary data structure of the recorded file. These stored parameters include the 'Percentage Good Pings', ' Number of 3/4 Beam Solutions' and 'Correlation Magnitude' obtained from the Broadband Signal Processing. 'Error Velocity' values reported from comparison of the two independent vertical velocity measurements from each of the ADCPs beam pairs are also used, either as a percentage of the overall magnitude of current velocity, or as absolute values specific to the instrument deployment. Using a relational database approach to the problem of managing and manipulating current profiler data a quality parameter for each measurement bin may be assigned as the data is imported, and Standard Query Language (SQL) is subsequently used as a means of manipulating the data-set, allowing time-series to be extracted for a particular vertical window or temporal extent, subject to a quality threshold. In this paper we test two such QA strategies the first which weights the error velocity of each measurement bin and a second tiered QA strategy using a series of test data sets to evaluate the performance and ease of implementation of the two approaches.;2008;J. A. Taylor;10.1109/CCM.2008.4480848;Conferences;2160-7176;978-1-4244-1486-4
ieee_20221205_08_28_59;A Computing Model for Marketable Quality and Profitability of Corporations: Model Evaluation Based on Two Different Sources Data;In this paper, we introduce and evaluate a computing model for marketable quality and profitability of corporations. We discuss the model prediction of the turning and transition periods based on data from two different sources. By applying these real data of some leading manufacturing corporations in Japan, we analyze the model accuracy. From the analysis, we conclude that even there are some differences between two sources data, the proposed model give a good approximation and prediction of the turning and transition periods of Japanese economy.;2008;Valbona Barolli;10.1109/AINA.2008.59;Conferences;2332-5658;978-0-7695-3095-6
ieee_20221205_08_28_59;A Rate Control Method for Subjective Video Quality on Mobile Data Terminals;This paper proposes a rate control method providing the maximum subjective video quality using the minimum transmission rate for streaming video services on mobile data terminals. At that time, because user perceptive video quality differs greatly with video contents, we need to consider the control method with reference to them. In this paper, we conduct subjective assessment and analyze the relationship between video content and subjective video quality. The proposed scheme makes it possible to provide high-quality streaming services under the condition of using limited fixed-size monitors.;2008;Yuka Kato;10.1109/WAINA.2008.269;Conferences;;978-0-7695-3096-3
ieee_20221205_08_28_59;Quality-Aware Retrieval of Data Objects from Autonomous Sources for Web-Based Repositories;The goal of this paper is to develop a framework for designing good data repositories for Web applications. The central theme of our approach is to employ statistical methods to predict quality metrics. These prediction quantities can be used to answer important questions such as: How soon should the local repository be synchronized to have a quality of at least 90% precision with certain confidence level? Suppose the local repository was synchronized three days ago, how many objects could have been deleted at the remote source since then?;2008;Houtan Shirani-Mehr;10.1109/ICDE.2008.4497600;Conferences;2375-026X;978-1-4244-1837-4
ieee_20221205_08_28_59;The Scanning Data Collection Strategy for Enhancing the Quality of Electrical Impedance Tomography;This paper addresses a scanning data collection strategy in electrical impedance tomography (EIT) to enhance the quality of an impedance image by expanding the electrode number. This scanning EIT (SEIT) system rotates the electrode pairs at a small angle, and then, the measurement electrodes can scan around the circumference of a phantom tank. The numerical simulations examine the reconstructed result by using a cylindrical model with two conductivity varieties. The experimental results illustrate the reconstruction images with and without the SEIT from 2-D real measurement data. Compared with conventional EIT images, the images reconstructed from the scanning data collection strategy exhibit high resolution and are clearer. This paper provides a feasible configuration to reduce the noise and improve the resolution of the impedance image.;2008;Cheng-Ning Huang;10.1109/TIM.2007.915149;Journals;1557-9662;
ieee_20221205_08_28_59;Online power quality disturbances identification based on data stream technologies;Power quality disturbances identification is the important procedure for improving the power quality, and online application has actual value. An efficient method for power quality disturbances identification is presented in this paper. Wavelet decomposition is used for extracting the features of various disturbances, and decision tree in data mining is used for identifying the disturbances. For online application, sliding window model and one-pass scan algorithms for wavelet decompositions are used. This method has low cost in memory and run time, it can identify different disturbances in high accuracy and less time. Simulation experiment using several typical disturbances, swell, sag, interrupt, harmonic, transient impulse, transient oscillation, show the effectiveness of proposed method.;2007;Yinghui Kong;;Conferences;1947-1270;978-981-05-9423-7
ieee_20221205_08_28_59;A Contemporary Technique to Guarantee Quality of Service (QoS) for Heterogeneous Data Traffic;The upcoming high-speed networks are expected to support a wide variety of  real-time multimedia applications. However, the current Internet architecture offers mainly best-effort service and does not meet the requirements of future integrated services networks that will require guarantee for transferring heterogeneous data. There are many parameters involve in improving the performance of a computer network such as reliability, delay, jitter, bandwidth, etc. These parameters together determine the Quality of Service (QoS). The requirements of the above parameters will vary from one application to another application. Applications like file transfer, remote login, etc., will require high reliability. But, applications like audio, video, etc., will require low reliability, because they can tolerate errors. The objectives of this paper are to propose a technique to store the results of a data transfer in binary based on the above parameters, to compare the expected requirements with the actual requirements, to show performance degradation and to suggest ideas to minimize differences between expected requirements and actual requirements. Ultimately, the outcome of this paper will give better results to improve the performance of the network.;2008;Calduwel P. Newton;10.1109/ISA.2008.14;Conferences;;978-0-7695-3126-7
ieee_20221205_08_28_59;Data Quality and Query Cost in Pervasive Sensing Systems;This research is motivated by large-scale pervasive sensing applications. We examine the benefits and costs of caching data for such applications. We propose and evaluate several approaches to querying for, and then caching data in a sensor field data server. We show that for some application requirements (i.e., when delay drives data quality), policies that emulate cache hits by computing and returning approximate values for sensor data yield a simultaneous quality improvement and cost savings. This win-win is because when system delay is sufficiently important, the benefit to both query cost and data quality achieved by using approximate values outweighs the negative impact on quality due to the approximation. In contrast, when data accuracy drives quality, a linear trade-off between query cost and data quality emerges. We also identify caching and lookup policies for which the sensor field query rate is bounded when servicing an arbitrary workload of user queries. This upper bound is achieved by having multiple user queries share the cost of a single sensor field query. Finally, we demonstrate that our results are robust to the manner in which the environment being monitored changes using models for two different sensing systems.;2008;David Yates;10.1109/PERCOM.2008.117;Conferences;;978-0-7695-3113-7
ieee_20221205_08_28_59;A power-quality event data compression algorithm based on advanced Support Vector Machine;An algorithm using SVM (Support Vector Machine) regression for Power-Quality (PQ) event data compression is presented. the advanced SVM regression can learn dependency from an array of wavelet coefficients which is 2-D representation of PQ event data decomposed by 2-D discrete-time wavelet transform (2-D DWT), using fewer Support Vectors (SV) to represent the original data, thus, the data could be compressed based on this feature. Experiment results show the good performance of proposed algorithm in PQ event data compression, comparing with traditional SVM compression data under the same conditions.;2008;Wei-Yan Zheng;10.1109/DRPT.2008.4523751;Conferences;;978-7-900714-13-8
ieee_20221205_08_28_59;DPSK Data Quality Dependencies in Microring-Based Transmitter and Receiver;Ultra-small DPSK (de)modulators at 10 Gb/s, using silicon-based microrings, are proposed. Data quality of the microring-based DPSK is greatly dependent on operation conditions that can be optimized to improve eye-opening by up-to-7-dB.;2008;Lin Zhang;10.1109/OFC.2008.4528046;Conferences;;978-1-55752-588-1
ieee_20221205_08_28_59;A Methodology for Information Quality Assessment in Data Warehousing;This paper presents a methodology to determine two IQ characteristics-accuracy and comprehensiveness-that are of critical importance to data warehousing. This methodology can examine how the quality metrics of source information affect the quality for information outputs produced using the relational algebra operations selection, projection, and Cubic product. It can be used to determine how quality characteristics associated with diverse data sources affect the quality of the derived data. The study resulted in the development of a model of a data cube and an algebra to support IQ Assessment operations on this cube.;2008;Y. Su;10.1109/ICC.2008.1035;Conferences;1938-1883;978-1-4244-2075-9
ieee_20221205_08_28_59;Complete video quality preserving data hiding with reversible functionality;This paper proposes a novel data hiding method in the MPEG domain where the image quality of the modified video is completely preserved to that of the original (compressed) video. To our best knowledge, there is no data hiding method that completely preserves the video quality after data embedding, and this method is the first attempt of its kind. This method is also reversible where the modifications done during data embedding could be undone to restore the original video. This method is applicable not only to existing MPEG1/2 encoded video streams but also to the encoding process of MPEG video from sequence of raw pictures. The problem of filesize increase as a result of data embedding is addressed, and three independent solutions are presented to suppress the filesize increase while trading off with payload and coding efficiency. Basic performance of this method is verified through experiments on various existing MPEG1 encoded videos.;2008;KokSheik Wong;10.1109/ISCCSP.2008.4537375;Conferences;;978-1-4244-1688-2
ieee_20221205_08_28_59;The variation of power quality indices due to data analysis procedure;Power quality data is often reported using statistical confidence levels. This will exclude the most extreme data for a certain length of time depending on the interval over which the confidence level is applied. There is considerable conjecture as to the effect of applying statistical measures over different time intervals, e.g. several days, weeks or one year. If statistical confidence levels are applied over long intervals, the length of time not included in the statistical confidence interval is long. During such intervals disturbance levels may be continuously high and not be accounted for in the statistical parameter. This study investigates the effect different methods of aggregating data to a specific reporting period will have on the calculated index. Several data processing methods are trialled to evaluate the effect of using different aggregation intervals to produce an index to characterise disturbance levels for the whole year.;2007;Sean Elphick;10.1109/AUPEC.2007.4548030;Conferences;;978-0-646-49499-1
ieee_20221205_08_28_59;Data Quality in Traditional Chinese Medicine;Data quality is a key issue in medical informatics and bioinformatics. Although many researches could be found that discuss data quality in the area of health care and medicine, few literature exist that particularly focuses on data quality in the field of traditional Chinese medicine (TCM). Due to the high domain-specificity of data quality, it is of essential necessity to identify key dimensions of data quality in TCM. In this paper, based on TCM practice in past years, three data quality aspects are highlighted as key dimensions, including representation granularity, representation consistency, and completeness. Moreover, practical methods and techniques to handle data quality problems in these dimensions are also provided, showing how to enhance data quality in TCM field.;2008;Yi Feng;10.1109/BMEI.2008.268;Conferences;1948-2922;978-0-7695-3118-2
ieee_20221205_08_28_59;A High Efficient Quality Control Strategy for Wavelet-Based ECG Data Compression System;Maintaining retrieved signal with desired quality is crucial for ECG data compression. In this paper, a high efficient quality control strategy is proposed for wavelet-based ECG data compression. The strategy is based on a modified non-linear quantization scheme that can obtain a linear distortion behavior with respective to a control variable. The linear distortion characteristic supports the design of a linear control variable prediction algorithm. By using the MIT-BIH arrhythmia database, the experimental results show that the linear control variable prediction method can effectively improve the convergence speed than the previous literatures.;2008;Cheng-Tung Ku;10.1109/BMEI.2008.184;Conferences;1948-2922;978-0-7695-3118-2
ieee_20221205_08_28_59;Information Quality in Healthcare: Coherence of Data Compared between Organization's Electronic Patient Records;In this paper we present a case-based analysis of health care data quality problems in a situation, where data of diabetes patient are combined from different information systems. Nationally uniform integrated health care information systems shall become more important when meeting the demands of patient centered care in the future. During the development of several electronic health records it has become clear that the integration of the data is still challenging. Data collected in various systems can have quality faults, it can for instance be non-coherent or include contradictory information, or the desired data is completely missing, as proved to be in our case as well. The quality of the content of patient information and the process of data production constitute a central part of good patient care, and more attention should be paid to them.;2008;Merja Miettinen;10.1109/CBMS.2008.64;Conferences;1063-7125;978-0-7695-3165-6
ieee_20221205_08_28_59;The application study of ERP data quality assessment and improvement methodology;The problem of ERP data quality is studied and the model of ERP data quality assessment and improvement is established. The problem of ERP data quality can be detected and improved effectively by measuring ERP data quality. Finally, ERP data quality assessment and improvement methodology is verified by case study.;2008;Zhao Xiaosong;10.1109/ICIEA.2008.4582673;Conferences;2158-2297;978-1-4244-1718-6
ieee_20221205_08_28_59;Identifying learners robust to low quality data;Real world datasets commonly contain noise that is distributed in both the independent and dependent variables. Noise, which typically consists of erroneous variable values, has been shown to significantly affect the classification performance of learners. In this study, we identify learners with robust performance in the presence of low quality (noisy) measurement data. Noise was injected into five class imbalanced software engineering measurement datasets, initially relatively free of noise. The experimental factors considered included the learner used, the level of injected noise, the dataset used (each with unique properties), and the percentage of minority instances containing noise. No other related studies were found that have identified learners that are robust in the presence of low quality measurement data. Based on the results of this study, we recommend using the random forest learner for building classification models from noisy data.;2008;Andres Folleco;10.1109/IRI.2008.4583028;Conferences;;978-1-4244-2660-7
ieee_20221205_08_28_59;Signal Quality Measurements for cDNA Microarray Data;Concerns about the reliability of expression data from microarrays inspire ongoing research into measurement error in these experiments. Error arises at both the technical level within the laboratory and the experimental level. In this paper, we will focus on estimating the spot-specific error, as there are few currently available models. This paper outlines two different approaches to quantify the reliability of spot-specific intensity estimates. In both cases, the spatial correlation between pixels and its impact on spot quality is accounted for. The first method is a straightforward parametric estimate of within-spot variance that assumes a Gaussian distribution and accounts for spatial correlation via an overdispersion factor. The second method employs a nonparametric quality estimate referred to throughout as the mean square prediction error (MSPE). The MSPE first smoothes a pixel region and then measures the difference between actual pixel values and the smoother. Both methods herein are compared for real and simulated data to assess numerical characteristics and the ability to describe poor spot quality. We conclude that both approaches capture noise in the microarray platform and highlight situations where one method or the other is superior.;2010;Tracy L. Bergemann;10.1109/TCBB.2008.72;Journals;2374-0043;
ieee_20221205_08_28_59;An Advanced Quality Control System for the CEOP/CAMP In-Situ Data Management;The Coordinated Enhanced Observing Period (CEOP) was proposed in 1997 as an initial step for establishing an integrated observation system for the global water cycle. The Enhanced Observing Period was conducted from October 2002 to December 2004, with satellite data, in-situ data, and model output data collected and available for integrated analysis. Under the framework of CEOP, the CEOP Asia-Australia Monsoon Project (CAMP) was organized and provided the in-situ dataset in the Asian region. CAMP included 13 different reference sites in the Asian monsoon region during Phase 1 (October 2002 to December 2004). These reference sites were operated by individual researchers for their own research objectives. Therefore, the various sites' data had important differences in observational elements, data formats, recording intervals, etc. This usually requires substantial manual data processing to use these data for scientific research which consumes a great deal of researcher time and energy. To reduce the time and effort for data quality checking and format conversion, the CAMP Data Center (CDC) established a Web-based quality control (QC) system. This paper introduces this in-situ data management and quality control system for the Asian region data under the framework of CEOP.;2008;Katsunori Tamagawa;10.1109/JSYST.2008.927710;Journals;2373-7816;
ieee_20221205_08_28_59;Quality measurement for transmitted audio data using distribution of sub-band signals;Recently, the remarkable progress of network technology has increased the requirement for transmission of high quality multimedia data. By the trend, it has been issued to investigate an efficient methodology for quality measurement of transmitted multimedia data. In this paper, we propose a new audio quality measurement technique to substitute for a typical quality measurement tool, RMSE (Root Mean Squared Error). The proposed method modifies the variance of sub-band signals to perform the estimation of audio quality at the transmitter, the receiver is able to estimate the quality distortion of transmitted audio data by calculating the distance between the variance and the reference value representing the characteristics of sub-band signals, so called EVE (Estimated Variance Error). The proposed is as no reference technique, it does not require the original data to measure the audio quality. On the Gaussian noise channel with several standard deviations, we prove that the proposed scheme has good performance, and it is a novel alternative to RMSE.;2008;D.C. Nguyen;10.1109/RIVF.2008.4586350;Conferences;;978-1-4244-2380-4
ieee_20221205_08_28_59;A quality-threshold data summarization algorithm;As database sizes increase, semantic data summarization techniques have been developed, so that data mining algorithms can be run on the summarized set for the sake of efficiency. Clustering algorithms such as K-Means have popularly been used as semantic summarization methods where cluster centers become the summarized set. The goal of semantic summarization is to provide a summarized view of the original dataset such that the summarization ratio is maximized while the error (i.e., information loss) is minimized. This paper presents a new clustering-based data summarization algorithm, in which the quality of the summarized set can be controlled. The algorithm partitions a dataset into a number of clusters until the distortion of each cluster is less than a given threshold, thus guaranteeing the summarized set has less than a fixed amount of information loss. Based on the threshold, the number of clusters is automatically determined. The proposed algorithm, unlike traditional K-Means, adjusts initial centers based on the information about the data space discovered so far, thus significantly alleviating the local optimum effect. Our experiments show that our algorithm generates higher quality clusters than K-Means does and it also guarantees an error bound, an essential criterion for data summarization.;2008;Viet Ha-Thuc;10.1109/RIVF.2008.4586362;Conferences;;978-1-4244-2380-4
ieee_20221205_08_28_59;Evaluating Frequency Quality of Nordic System using PMU data;This paper focuses on analysing frequency quality of Nordic power system using measurements from phasor measurement units (PMU). The PMU data of one year long period is used which has very high time resolution (20 ms per sample) and is able to provide detailed information in evaluating frequency quality and its correlation with time. The results show that the frequency quality of the Nordic power system is not satisfactory according to the suggested requirements. The electricity market operation is found to be one of the major reasons behind. Based on the results, discussion of frequency control for future power system is made where several new technologies of interests are suggested to be investigated.;2008;Zhao Xu;10.1109/PES.2008.4596468;Conferences;1932-5517;978-1-4244-1906-7
ieee_20221205_08_28_59;A Computing Model for Marketable Quality and Profitability of Corporations: A Case Study Evaluation Using a New Sources Data;In this paper, we introduce and evaluate a computing model for marketable quality and profitability of corporations. We discuss the model prediction of the turning and transition period based on a new source data. By applying the real data of some leading manufacturing corporations in Japan we analyze the model accuracy. The analysis results show the proposed model give a good approximation and prediction of the turning and transition period of Japanese economy. By using the proposed model, we can obtain a boundary between sellers' and buyers' market.;2008;Heihachiro Fukuda;10.1109/CISIS.2008.82;Conferences;;978-0-7695-3109-0
ieee_20221205_08_28_59;An efficient data representation scheme for complete video quality preserving data hiding;This paper proposes an efficient data representation scheme to improve the performance of a data hiding method [1] in MPEG compressed domain. Even though [1] completely preserves the quality of the modified video to that of the original (compressed) video and [1] is reversible, [1] suffers from consistent filesize increase caused by data embedding. To suppress filesize increase, reverse zerorun length (RZL) is proposed to efficiently encode the message. RZL utilizes the statistics of the macroblocks with respect to [1], and the distance between two excited macroblocks is considered to encode a message segment. RZL simultaneously achieves high payload and high embedding efficiency, thus RZL is able to suppress the filesize increase caused by data embedding. We theoretically analyzed that RZL outperformsmatrix encoding for both payload and embedding efficiency for this particular data hiding method. Experiments are also carried out to verify the theoretically deduced results, and the observed results agree with the expected outcomes.;2008;KokSheik Wong;10.1109/ICME.2008.4607584;Conferences;1945-788X;978-1-4244-2571-6
ieee_20221205_08_28_59;Applicative solution for testing the quality of data services in mobile networks;Mobile operators nowadays have plenty of data services, since the introduction of GPRS and this number is increasing towards future 3G LTE mobile networks. There are IP-based services which were migrated from wired to mobile networks, such as HTTP, FTP and E-mail. But, also mobile specific services were created such as WAP and MMS. On the other side, behavior of mobile services is influenced from the environment, the terrain, interference, from protocols on different OSI layers, from user behavior, etc. Therefore, mobile operators need to examine their network and be assured that services are performing well. In this paper we describe an applicative solution for service quality testing system (SQTS), which provides efficient way for testing of all data services, including bearer as well as IP-based services in mobile networks from the user point of view, through a defined set of Key Performance Indicators. We define all SQTS functionalities as well as all needed networks nodes for its functioning. The proposed system is robust and scalable regarding definition of new services that will appear in future.;2008;Toni Janevski;10.1109/MELCON.2008.4618437;Conferences;2158-8481;978-1-4244-1633-2
ieee_20221205_08_28_59;A hybrid data mining approach to quality assurance of manufacturing process;Quality assurance (QA) is a process employed to ensure a certain level of quality in a product or service. One of the techniques in QA is to predict the product quality based on the product features. However, traditional QA techniques have faced some drawbacks such as heavily depending on the collection and analysis of data and frequently dealing with uncertainty processing. In order to improve the effectiveness during a QA process, a hybrid approach incorporated with data mining techniques such as rough set theory (RST), fuzzy logic (FL) and genetic algorithm (GA) is proposed in this paper. Based on an empirical case study, the proposed solution approach provides great promise in QA.;2008;Chun-Che Huang;10.1109/FUZZY.2008.4630465;Conferences;1098-7584;978-1-4244-1819-0
ieee_20221205_08_28_59;A comparative study of RBF neural network and SVM classification techniques performed on real data for drinking water quality;The control and monitoring of drinking water is becoming more and more interesting because of its effects on human life. Many techniques were developed in this field in order to ameliorate this process control attending to rigorous follow-ups of the quality of this vital resource. Several methods were implemented to achieve this goal. In this paper, a comparative study of two techniques resulting from the field of the artificial intelligence namely: RBF neural network (RBF-NN) and support vector machine (SVM), is presented. Developed from the statistical learning theory, these methods display optimal training performances and generalization in many fields of application, among others the field of pattern recognition. Applied as classification tools, these techniques should ensure within a multi-sensor monitoring system, a direct and quasi permanent control of water quality. In order to evaluate their performances, a simulation using real data, corresponding to the recognition rate, the training time, and the robustness, is carried out. To validate their functionalities, an application is presented.;2008;Mohamed Bouamar;10.1109/SSD.2008.4632856;Conferences;;978-1-4244-2206-7
ieee_20221205_08_28_59;Evolutive complex scheduling in interaction networks for quality improvment in geographical data base updating;The aim of this paper is to propose a scheduling processes for the mechanism of geographical data bases updating in term of improvement of its quality. Because of the complex interacting network of the components involved in each updating cycle of this mechanism, a traditional analytic solver cannot success in most cases. So we propose to use an evolutionary computation based on a genetic algorithm which is well adapted for the complex interaction network of the involved components. A genetic algorithm may allow to improve the updating quality in increasing the validation rate of a transaction.;2005;Hakima Kadri-Dahmani;10.1109/ICECS.2005.4633616;Conferences;;978-9972-61-100-1
ieee_20221205_08_28_59;TiSeG: A Flexible Software Tool for Time-Series Generation of MODIS Data Utilizing the Quality Assessment Science Data Set;Time series generated from remotely sensed data are important for regional to global monitoring, estimating long-term trends, and analysis of variations due to droughts or other extreme events such as El Nintildeo. Temporal vegetation patterns including phenological states, photosynthetic activity, or biomass estimations are an essential input for climate modeling or the analysis of the carbon cycle. However, long-term analysis requires accurate calibration and error estimation, i.e., the quality of the time series determines its usefulness. Although previous attempts of quality assessment have been made with NOAA-AVHRR data, a first rigorous concept of data quality and validation was introduced with the MODIS sensors. This paper presents the time-series generator (TiSeG), which analyzes the pixel-level quality-assurance science data sets of all gridded MODIS land (MODLand) products suitable for time-series generation. According to user-defined settings, the tool visualizes the spatial and temporal data availability by generating two indices, the number of invalid pixels and the maximum gap length. Quality settings can be modified spatially and temporally to account for regional and seasonal variations of data quality. The user compares several quality settings and masks or interpolates the data gaps. This paper describes the functionality of TiSeG and shows an example of enhanced vegetation index time-series generation with numerous settings for Germany. The example indicates the improvements of time series when the quality information is employed with a critical weighting between data quality and the necessary quantity for meaningful interpolation.;2008;RenÉ R. Colditz;10.1109/TGRS.2008.921412;Journals;1558-0644;
ieee_20221205_08_28_59;Diagnostic quality driven physiological data collection for personal healthcare;We believe that each individual is unique, and that it is necessary for diagnosis purpose to have a distinctive combination of signals and data features that fits the personal health status. It is essential to develop mechanisms for reducing the amount of data that needs to be transferred (to mitigate the troublesome periodically recharging of a device) while maintaining diagnostic accuracy. Thus, the system should not uniformly compress the collected physiological data, but compress data in a personalized fashion that preserves the “important” signal features for each individual such that it is enough to make the diagnosis with a required high confidence level. We present a diagnostic quality driven mechanism for remote ECG monitoring, which enables a notation of priorities encoded into the wave segments. The priority is specified by the diagnosis engine or medical experts and is dynamic and individual dependent. The system pre-processes the collected physiological information according to the assigned priority before delivering to the backend server. We demonstrate that the proposed approach provides accurate inference results while effectively compressing the data.;2008;David Jea;10.1109/IEMBS.2008.4649798;Conferences;1558-4615;978-1-4244-1815-2
ieee_20221205_08_28_59;Analysis of data quality and information quality problems in digital manufacturing;This work focuses on the increasing importance of data quality in organizations, especially in digital manufacturing companies. The paper firstly reviews related works in field of data quality, including definition, dimensions, measurement and assessment, and improvement of data quality. Then, by taking the digital manufacturing as research object, the different information roles, information manufacturing processes, influential factors of information quality, and the transformation levels and paths of the data/information quality in digital manufacturing companies are analyzed. Finally an approach for the diagnosis, control and improvement of data/information quality in digital manufacturing companies, which is the basis for further works, is proposed.;2008;K. Q. Wang;10.1109/ICMIT.2008.4654405;Conferences;;978-1-4244-2330-9
ieee_20221205_08_28_59;Data-Centric Prioritization in a Cognitive Radio Network: A Quality-of-Service Based Design and Integration;"The under-/un-utilized radio spectrum is an area of emphasis and potential in an effort to increase utilization. This paper introduces a new matrix into the frequency evaluation/determination methodology. It will reduce the number of decision and network collisions in a cognitive radio network environment. This is achieved via a new paradigm, data centric prioritization (DCP), the exploitation of the unique relationship between the transmitted application data type and a ""true best fit"" cognitive radio frequency decision in a cognitive radio community cluster. Since application data types during normal wireless device operations vary from user to user and utility to utility, so will the network performance requirements. The network must be able to seamlessly support this diversity and a cognitive radio device is best suited for this task. The concept of associating application-specific design requirements with the network dynamics of the frequency spectrum lends itself to a quality-of-service (QoS) methodology. This paper delves into the usage of a QoS methodology within the cognitive radio cognition cycle.";2008;Urban Wiggins;10.1109/DYSPAN.2008.62;Conferences;;978-1-4244-2016-2
ieee_20221205_08_28_59;A New Field-Data Based EAF Model for Power Quality Studies;A new electric arc furnace (EAF)-specific model based on field measurements of instantaneous furnace voltages and currents has been proposed. This model presents the dynamic behavior of the EAF system including all parts, i.e. the EAF transformer, the secondary circuit, the electrode movements and the arc. It consists of a cascade connected variable-resistance and -inductance combination to represent the time variation at the fundamental frequency, and a current source in parallel with it to inject the harmonics and interharmonics of the EAF current. The model takes into account several typical tap-to-tap periods of the specific EAF operation. This model is especially advantageous for power quality (PQ) analysis, and development of FACTS solutions for PQ problem mitigation of a given busbar supplying single- or multi-EAF installations. The validity of the proposed model has been verified by comparing EMTDC/PSCAD simulations of the model with the field measurements. The results obtained have shown quite satisfactory correlation between the proposed model and the actual EAF operation.;2008;Murat Gol;10.1109/08IAS.2008.335;Conferences;0197-2618;978-1-4244-2279-1
ieee_20221205_08_28_59;A quality-of-information-aware framework for data models in wireless sensor networks;Wireless sensor networks are used to monitor a given environment, such as indoor heating conditions or the micro-climate of glaciers. They offer a low-cost solution that provides a high data density. Usually the user of such a sensor network has a good idea of how, knowing the environment, the sensed values should behave. This idea can be expressed as a data model. Such models can be used to detect anomalies, compress data, or combine data from many inexpensive sensors to increase the quality of the measurements. This paper presents a framework to process arbitrary sensor-network data models. The framework can then be used to distribute the model processing into the wireless sensor network. Quality of information criteria are used to determine the performance of the models. A prototype of the framework is presented together with a comparison of two existing stochastic data model approaches for wireless sensor networks.;2008;Urs Hunkeler;10.1109/MAHSS.2008.4660118;Conferences;2155-6814;978-1-4244-2575-4
ieee_20221205_08_28_59;Data quality driven sensor reporting;Within the field of event driven data reporting from wireless sensor networks, reducing energy consumption is an ongoing problem. Using an applicationpsilas tolerance toward data imprecision (or, data quality) allows energy savings, via fewer messages sent, through the selection of how and when to send sensor readings. This paper is an early work that examines pushing or pulling data depending on which approach is expected to send fewer messages, based upon the recent history of application requests for a sensor and the changes of the sensor values predicted by application specific models. The simulation results indicate that our method is more efficient relative to the push only or pull only methods for situations where application request frequency or data change rate is variable or unknown.;2008;Doug Hakkarinen;10.1109/MAHSS.2008.4660121;Conferences;2155-6814;978-1-4244-2575-4
ieee_20221205_08_28_59;Towards data warehouse business quality through requirements elicitation;Data warehouses are mainly used to support decision-making based on the analysis of highly heterogeneous sources to extract, transform and aggregate data, as well as facilitating ad-hoc queries that retrieve the decisional information. Data warehouse development involves many knowledge-intensive activities, of which requirements elicitation is recognized as being crucial and difficult to model. This paper adapts the data warehouse requirements elicitation process, namely informational scenarios, to incorporate business quality at the requirements engineering level of the DW development. To accomplish this, we look at DW business quality mainly from the context of changing economic factors and environmental concerns.;2008;Anjana Gosain;10.1109/ICADIWT.2008.4664394;Conferences;;978-1-4244-2624-9
ieee_20221205_08_28_59;Achieving data warehouse quality using GDI approach;Data warehouses are complex systems that have to deliver highly-aggregated data from heterogeneous sources to decision makers. It is essential that we can assure the quality data warehouse in terms of data as well as the services provided by it. Therefore, we should use methods, models, techniques and tools to help us in designing and maintaining high quality DWs. In this paper we outline a general methodological framework for the quality of data warehouse systems, adapting the goal-decision-information approach. This model considers the quality of conceptual models to reach at specific decisions in data warehousing environment.;2008;Anjana Gosain;10.1109/ICADIWT.2008.4664399;Conferences;;978-1-4244-2624-9
ieee_20221205_08_28_59;Application of Attribute Recognition Model Based on Coefficient of Data-Driven to Evaluation Water Quality;In order to overcome the weakness of subjectivity in determining weight coefficient of fuzzy comprehensive evaluation method, an attributes recognition method based on coefficient of data-driven is built up, and the weight is determined by the data-mining thinking through comprehensive utilization of classification criteria information and sample information. Utilization of this model to evaluate the water quality of Fuqiao River reservoir verifies its feasibility. Through comparison with traditional fuzzy synthetic evaluation method and attribute recognition model based on coefficient of entropy, the attribute recognition model based on coefficient of data-driven can get more objective evaluation results.;2008;Zhihong Zou;10.1109/FSKD.2008.484;Conferences;;978-0-7695-3305-6
ieee_20221205_08_28_59;Application of Clustering Methods for Analysing of TTCN-3 Test Data Quality;"The use of the standardised testing notation, testing and test control notation (TTCN-3) language has increased continuously over the last years. Many test suites of large sizes covering different domains exist. Therefore, it becomes important to provide the TTCN-3 community with methods and tools to evaluate the quality of tests. This paper presents the idea of evaluating the quality of the test data stimuli by using a data clustering method and measuring the coverage related to data clusters. A cluster contains stimuli which are considered similar for the system under test (SUT) behaviour; that means that each stimuli within a cluster should provide similar results from the test point of view.";2008;Diana Vega;10.1109/ICSEA.2008.44;Conferences;;978-0-7695-3372-8
ieee_20221205_08_28_59;Factor analysis of power quality variation data on a distribution network;Continuous power quality (PQ) surveys of electricity networks generate large amounts of data that must be condensed for the purpose of interpretation and reporting. In this paper, summary indices for continuous PQ disturbances are calculated from distribution network data, and the relationship between these indices and the known physical characteristics of the site is investigated. Results from this survey prompted further analysis into the relationship between voltage harmonic distortion (THDv) levels and variation in load characteristics across the monitored sites. Accepted explanations for variation in THDv are evaluated against the observed levels and alternative explanations are proposed.;2008;G. Nicholson;10.1109/ICHQP.2008.4668769;Conferences;2164-0610;978-1-4244-1770-4
ieee_20221205_08_28_59;An extension of the relational model to intuitionistic fuzzy data quality attribute model;The model we suggest makes the data quality an intrinsic feature of an intuitionistic fuzzy relational database. The quality of the data is no more determined by the level of user complaints or ad hoc sql queries prior to the data load but it is stored explicitly in relational tables and could be monitored and measured regularly. The quality is stored on an attribute level basis in supplementary tables to the base user ones. The quality is measured along preferred quality dimensions and is represented by intuitionistic fuzzy degrees. To consider the preferences of the user with respect to the different quality dimensions and table attributes we create additional tables that contain the weight values. The user base tables are not intuitionistic fuzzy but we have to use an intuitionistic fuzzy RDBMS to represent and manipulate data quality measures.;2008;Diana Boyadzhieva;10.1109/IS.2008.4670520;Conferences;1941-1294;978-1-4244-1740-7
ieee_20221205_08_28_59;The research and development of data acquisition card for on-line monitoring system of power quality based on Windows CE;According to the requirements of stability and real-time in on-line monitoring system of power quality, a data acquisition card is designed based on TMS320C6713 and PCI9030, and use Windows CE embedded system as the software platform. In the process of data acquisition card driver development, we used the Jungopsilas driver development tool WinDriver. It provides user-mode library (API) access to hardware, avoids OS kernel calls and hardware operation, makes users quickly and efficiently develop a data acquisition card driver. The test results show the driver is excellent in operation with stable performance, various work indexes of data acquisition card meet the real-time requirements.;2008;Xiang-wu Yan;10.1109/ISIE.2008.4676916;Conferences;2163-5145;978-1-4244-1666-0
ieee_20221205_08_28_59;Query Quality of Service Management Based on Data Relationship over Real-Time Data Stream Systems;Many real-time applications and data services in distributed environments need to operate on continuous unbounded data streams with the development of large wired and wireless sensor network. Conventional one-time queries cannot be suitable to provide continues results as data and update stream into the system, and continuous queries represent a new paradigm for interacting with dynamically changing data. At the same time, many real-time applications have inherent timing constraints in their tasks, so providing data objects deadline guarantees for continuous queries over dynamic data streams is a challenging problem. A novel performance metric and quality of service management scheme continuous queries over dynamic data streams are proposed to guarantee system performance based on relationship of all updated data items. Experimental simulations demonstrate that the presented algorithm can guarantee the performance and decrease miss ratios of queries for dynamic workload fluctuations especially transient overloads.;2008;Jun Xiang;10.1109/WiCom.2008.1330;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_28_59;Modeling Information Quality Risk in Data Mining;Information quality (IQ) is a critical factor in the success of the data mining (DM). Therefore, it is essential to measure the risk of IQ in a data warehouse to ensure success in implementing DM. This paper presents a methodology to determine two IQ characteristics-accuracy and comprehensiveness-that are of critical importance to decision makers. This methodology can examine how the quality risks of source information affect the quality for information outputs produced using the relational algebra operations selection, projection, and Cubic product. It can be used to determine how quality risks associated with diverse data sources affect the quality of the derived data. The study resulted in the development of a model of a data cube and an algebra to support IQ risk operations on this cube. The model we present is simple and intuitive, and the algebra provides a means to concisely express complex DM queries.;2008;Ying Su;10.1109/WiCom.2008.2424;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_28_59;Improving Uncertain Data-Quality through Effective Use of Knowledge Base;Data quality issues have taken on increasing importance in recent years. In our research, we have discovered that data quality is uncertain because the perceived quality of the data is influenced by the task and that the same data may be viewed through two or more different quality lenses depending on the user and the user' task it is used for. In order to improve uncertain data-quality, we construct a knowledge base to treat with different expectations of data quality. This knowledge-based approach enhances the flexibility and extensibility of the application, and can improve uncertain data-quality according to the user's needs.;2008;Zhimao Huang;10.1109/WiCom.2008.2517;Conferences;2161-9654;978-1-4244-2107-7
ieee_20221205_08_28_59;An Efficient Method of Data Quality using Quality Evaluation Ontology;In SOA (Service Oriented Architecture) and RTE (Real-Time Enterprise) environment, an assurance of data quality is important. Because we do not assure data accuracy among dynamic clustering data set. Traditional methodology for assuring data quality is data profiling and data auditing. However, that is needed lots of time and cost to analysis of metadata and business process for integrating system before evaluating data quality. In this paper, we propose an efficient methodology of assuring data quality with considering dynamic clustering data set. To extract evaluate rules for data quality, we use ontology that has meanings of each word in itself. We gain the relationship among word in ontology, and then make SQL to evaluate data accuracy, especially focused on data meaning.;2008;O-Hoon Choi;10.1109/ICCIT.2008.118;Conferences;;978-0-7695-3407-7
ieee_20221205_08_28_59;A High Quality Histogram Shifting Based Embedding Technique for Reversible Data Hiding;Reversible data hiding is a technique that embeds secret information into a host media without loss of host information. Ni et al.'s histogram shifting technique is a high-quality, reversible method for data embedding. However, their technique still suffers from undesirable distortion at low embedding rates and lack of a mechanism to control the stego-image quality, due to all pixels between the peak point and the minimum point have to be shifted one unit for data embedding. The proposed technique lowers the distortion performance at low embedding rates by scanning pixels from outer region toward the center of the host image, and chooses better locations for shifting histograms to embed data. The experimental results show that the proposed method significantly improves the quality of the stego image of histogram shifting technique, especially at low embedding rates.;2008;Wien Hong;10.1109/ISDA.2008.304;Conferences;2164-7151;978-0-7695-3382-7
ieee_20221205_08_28_59;Predicting Defect Content and Quality Assurance Effectiveness by Combining Expert Judgment and Defect Data - A Case Study;Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from his-torical projects. Due to the fact that many companies do not collect enough data for applying these approaches (es-pecially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method that combines commonly available measurement data and context-specific expert knowledge. To evaluate the method’s applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model pro-vides improved estimation accuracy when compared to applicable models based solely on data. The mean magni-tude of relative error (MMRE) determined by cross-validation is 29.6% compared to 76.5% obtained by the most accurate data-based model.;2008;Michael Kläs;10.1109/ISSRE.2008.43;Conferences;2332-6549;978-0-7695-3405-3
ieee_20221205_08_28_59;Towards a quality of service model for replicated data access;This paper introduces the notion of a quality of service model for read and write access to a replicated database or file system. The goal is to provide clients with a choice of service guarantees and to separate the specification of a client's needs from the mechanisms implemented to meet those needs. An analogy is drawn with the quality of service models being explored for communications protocols.<>;1995;D.B. Terry;10.1109/SDNE.1995.470455;Conferences;;0-8186-7092-4
ieee_20221205_08_28_59;An Efficient Method of Data Quality Evaluation Using Metadata Registry;This paper proposes MDRDP (Metadata Registry based on Data Profiling) to minimize the time and human resource for analyzing and extracting metadata as criteria standard for data profiling. MDRDP is based on MDR (Metadata Registry) which is used for an international standard of standardizing and managing metadata for information sharing in various fields. By MDRDP, we can evaluate data quality with authorize metadata using methodology of data profiling. MDR can guarantee the quality of metadata so that results of quality evaluation would improve.;2008;O-Hoon Choi;10.1109/ASEA.2008.46;Conferences;;978-0-7695-3432-9
ieee_20221205_08_28_59;The Development of Data Acquisition Card in Power Quality Monitoring System Based on Embedded System;In this paper, a data acquisition card in monitoring system of power quality is designed and realized according to system real-time performance and reliability requirements. It systematically introduces the hardware and software platforms, analyses device driver development of data acquisition card and its operation mechanism in embedded system. The test shows the driver is working stability, and work indexes meet the requirements.;2008;Xiang-wu Yan;10.1109/CSSE.2008.608;Conferences;;978-0-7695-3336-0
ieee_20221205_08_28_59;Reinforcing Records for Improving the Quality of Data Integration;In the data integration, the heterogeneity of sources leads to missing value and various expressions of the same value in records, which reduces the quality of data. In this paper, we propose a novel approach to reinforce records for the integrated data. By studying the functional dependency of attribute in schema of data integration, we discover the related attribute that determines the attribute with uncertain value. Then our approach exploits matching algorithms on the value of related attribute to associate different records. And the uncertain value will be reinforced with the consistent value in a certain record. We also propose algorithms for reinforcing dataset of data integration. The experiments based on the data of conference paper demonstrate the effectiveness and performance of our approach on improving the quality of data.;2008;Tiezheng Nie;10.1109/CSSE.2008.626;Conferences;;978-0-7695-3336-0
ieee_20221205_08_28_59;Comparison of Four Performance Metrics for Evaluating Sampling Techniques for Low Quality Class-Imbalanced Data;Erroneous attribute values can significantly impact learning from otherwise valuable data. The learning impact can be exacerbated by the class imbalanced training data. We investigate and compare the overall learning impact of sampling such data by using four distinct performance metrics suitable for models built from binary class imbalanced data. Seven relatively free of noise, class imbalanced software engineering measurement datasets were used. A novel noise injection procedure was applied to these datasets. We injected domain realistic noise into the independent and dependent (class) attributes of randomly selected instances to simulate lower quality measurement data. Seven well known data sampling techniques with the benchmark decision-tree learner C4.5 were used. No other related studies were found that have comprehensively investigated learning by sampling low quality binary class imbalanced data containing both independent and dependent corrupted attributes. Two sampling techniques (random undersampling and Wilson's editing) with better and more robust learning performances were identified. In contrast, all metrics concurred on the identification of the worst performing sampling technique (cluster-based oversampling).;2008;Andres Folleco;10.1109/ICMLA.2008.11;Conferences;;978-0-7695-3495-4
ieee_20221205_08_28_59;Quality control of fan beam scanning data processing with in vitro material;In vivo assessment of human body composition quantities from imaging/scanning systems in clinical research and public health has led to proliferation of indirect methods and techniques that deliver approximating values. The systems are based on morphological or biochemical models and assumptions. Since the development of dual-energy X-ray absorptiometry (DXA) for the bone density, mineral content and the detection of osteoporosis, it assesses lean tissue and fat also. This study conducts a quality control of DXA variables using direct dissection of 14 porcine hind legs as the criterion method. Results show good to excellent correlations between DXA and dissection data (r2 =0.75 to 0.99), but absolute indirect DXA and direct dissection values were significantly different (P<0.05). In addition DXA provides erroneous values for bone density and the data dimensions are morphological, not chemical values as claimed by the manufacturer. Validation and accuracy studies with intact whole bodies are advised.;2008;J.P. Clarys;10.1109/IEEM.2008.4737861;Conferences;2157-362X;978-1-4244-2630-0
ieee_20221205_08_28_59;Analysis of consumers’ requirements for data/information quality by using HOQ;Data/information quality (DQ/IQ*) has great impact on data consumers’ decisions. In order to provide high quality data/information, data consumers’ requirements for DQ/IQ have to be analyzed and identified. Right requirement identification is fundamental to data quality control activities including DQ/IQ measurement, evaluation, improvement, etc. Quality function deployment (QFD) and the house of quality (HOQ) are effective tools to translate consumers’ requirements into specific DQ dimensions for DQ improvement. This work briefly introduces QFD, HOQ and their constitutive elements. Data consumers are also examined. A methodology of applying HOQ in DQ/IQ, which includes five major steps, is described in details. Then an example of product design information quality is presented. By using the methodology, the weak points of DQ in industrial firms can be identified in the form of DQ dimensions in order to take actions to improve data/information quality.;2008;Keqin Wang;10.1109/IEEM.2008.4737862;Conferences;2157-362X;978-1-4244-2630-0
ieee_20221205_08_28_59;A Method for Measuring Data Quality in Data Integration;This paper reports our method on measuring data quality in data integration. Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. Data quality is crucial for operational data integration. We posit that data-integration need to handle the measure of data quality. So, measuring data quality in data integration is one of worthy research topics. This paper focuses on believability, a major aspect of quality. At first, the author analyzes the background and content of this paper, then description of dimensions of believability is given, and we present our approach for computing believability based on metadata, finally the summary and prospect are listed. In this method, we make explicit use of lineage-based measurements and develop a precise approach to measuring data quality.;2008;Lin Mo;10.1109/FITME.2008.146;Conferences;;978-0-7695-3480-0
ieee_20221205_08_28_59;Research on Information Quality Driven Data Cleaning Framework;Considering the limited extensibility and interactivity of the current data cleaning work, this paper proposes a new extensible and interactive data cleaning framework driven by information quality. The framework implements the formation of data quality analysis strategy, data transformation strategy and cleaning result assessment strategy. It also realizes the control of the cleaning process. The framework has the significant features of extensibility and interactivity.;2008;Hao Yan;10.1109/FITME.2008.126;Conferences;;978-0-7695-3480-0
ieee_20221205_08_28_59;Quantization Based Data Hiding Scheme for Efficient Quality Access Control of Images Using DWT via Lifting;This paper proposes a transform domain data-hiding scheme for quality access control of images. The original image is decomposed into tiles by applying n-level lifting-based discrete wavelet transformation (DWT). A binary watermark image (external information) is spatially dispersed using the sequence of number generated by a secret key. The encoded watermark bits are then embedded into all DWT-coefficients of nth-level and only in the high-high (HH) coefficients of the subsequent levels using dither modulation (DM) but without complete self-noise suppression. It is well known that due to insertion of external information, there will be degradation in visual quality of the host image (cover). The degree of deterioration depends on the amount of external data insertion as well as step size used for DM. If this insertion process is reverted, better quality of images can be accessed. To achieve that goal, watermark bits are detected using minimum distance decoder and the remaining self-noise due to information embedding is suppressed to provide better quality of image. The simulation results have shown the validity of this claim.;2008;Amit Phadikar;10.1109/ICVGIP.2008.23;Conferences;;978-0-7695-3476-3
ieee_20221205_08_28_59;Improved reconstructed image quality in a SPECT system with slit-slat collimation by combination of multiplexed and non-multiplexed data;The desirability of multiplexing in SPECT acquisition remains controversial. Allowing multiplexing is a way of optimizing system sensitivity, however multiplexing has been found to deteriorate the image quality due to the ambiguity in the origin of the projections. In this paper we explore the system designed specifically for brain SPECT based on multi slit-slat (MSS) collimator in which the slit spacing is varied to provide a mix of multiplexed (MX) and non-MX data. The degree of multiplexing for individual slits (DM) is fixed and we vary the fractional number of projections that are multiplexed (FMP). The results showed significant increase in Signal to noise ratio (SNR) due to the reduced noise level in the reconstructed images with insignificant effect on the recovery coefficient (RC) and spatial resolution up to high degrees of multiplexing and with less than 30% non-MX projections. We therefore conclude that multiplexing can be used to increase system sensitivity and therefore reduce noise in the image quality provided that mixed MX and non-MX data are acquired.;2008;Shelan Mahmood;10.1109/NSSMIC.2008.4774374;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_28_59;Online data quality monitoring tools of LHCb;LHCb is one of the four major experiments under completion at the Large Hadron Collider (LHC). One crucial aspect of the experiment is the online data quality monitoring, part of which project is the LHCb Histogram Presenter, a flexible tool dedicated to the visualisation of online histograms. Data quality monitoring is important, because it allows the verification of the detector performance: anomalies, such as missing values or unexpected distributions of summary statistical data can be indicators of a malfunctioning detector.;2008;O. Callot;10.1109/NSSMIC.2008.4774777;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_28_59;Commissioning of the ALICE-LHC online Data Quality Monitoring framework;ALICE is one of the experiments installed at CERN Large Hadron Collider, dedicated to the study of heavy-ion collisions. The final ALICE data acquisition system has been installed and is being used for the testing and commissioning of detectors. Data Quality Monitoring (DQM) is an important aspect of the online procedures for a HEP experiment. In this presentation we overview the commissioning and the integration of ALICE’s AMORE (Automatic MOnitoRing Environment), a custom-written distributed application aimed at providing DQM services in a large, experiment-wide scale.;2008;Filimon Roukoutakis;10.1109/NSSMIC.2008.4774928;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_28_59;Data quality monitor of the muon spectrometer tracking detectors of the ATLAS experiment at the Large Hadron Collider: First experience with cosmic rays;The Muon Spectrometer of the ATLAS experiment at the CERN Large Hadron Collider is completely installed and many data have been collected with cosmic rays in different trigger configurations. In the barrel part of the spectrometer, cosmic ray muons are triggered with Resistive Plate Chambers, RPC, and tracks are obtained joining segments reconstructed in three measurement stations equipped with arrays of high-pressure drift tubes, MDT. The data are used to validate the software tools for the data extraction, to assess the quality of the drift tubes response and to test the performance of the tracking programs. We present a first survey of the MDT data quality based on large samples of cosmic ray data selected by the second level processors for the calibration stream. This data stream was set up to provide high statistics needed for the continuous monitor and calibration of the drift tubes response. Track segments in each measurement station are used to define quality criteria and to assess the overall performance of the MDT detectors. Though these data were taken in not optimized conditions, when the gas temperature and pressure was not stabilized, the analysis of track segments shows that the MDT detector system works properly and indicates that the efficiency and space resolution are in line with the results obtained with previous tests with a high energy muon beam.;2008;M. Iodice;10.1109/NSSMIC.2008.4774959;Conferences;1082-3654;978-1-4244-2715-4
ieee_20221205_08_28_59;Improving audio data quality and compression;High data quality at low bit rate is an essential goal that people want to achieve. It is necessary to transfer data at low bit rate so that the bandwidth of the medium can be utilized efficiently. In most of the speech coding techniques the goal of low bit rate transfer is achieved but the data quality is affected badly. The proposed technique is an attempt to improve the data quality at low bit rate as well as fast transmission of data. The proposed technique protects the data quality by applying Linear Predictive Coding-10 and achieves the low bit rate by applying Quadrature Mirror Filter. A comprehensive analysis is on the basis of given parameters as size, compression time, Signal to Noise Ratio, power, energy, power in air, energy in air, mean, standard deviation and intensity.;2008;Qaiser Naeem;10.1109/ICET.2008.4777524;Conferences;;978-1-4244-2211-1
ieee_20221205_08_28_59;Combining NSMS and High-Quality MPV-TD Data for UXO Discrimination;In this paper, a new physics-based approach for estimating a buried object's location and orientation is combined with the normalized surface magnetic source (NSMS) model to analyze high-quality, high-density multiaxis data provided by the Man-Portable Vector (MPV) time domain (TD) sensor. The NSMS is a very simple and robust technique for predicting the EMI responses of various objects. It is applicable to any combination of magnetic or electromagnetic induction data for any arbitrary homogeneous or heterogeneous 3D object or set of objects. The physics-based approach to estimate location assumes that the target exhibits a dipolar response and uses only two global values, the magnetic field vector H and the scalar magnetic potential psi, reconstructed at a set of points in space. To demonstrate the applicability of the NSMS, we first compare its predictions with dynamic MPV-TD measurements and then present the results of a blind-test analysis using multiaxis static MPV-TD data sets.;2008;F. Shubitidze;10.1109/IGARSS.2008.4778916;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_28_59;Data Assimilation Experiments Using Quality Controlled AIRS Version 5 Temperature Soundings;The AIRS Science Team Version 5 retrieval algorithm has been finalized and is now operational at the Goddard DAAC in the processing (and reprocessing) of all AIRS data. The AIRS Science Team Version 5 retrieval algorithm contains two significant improvements over Version 4: 1) Improved physics allows for use of AIRS observations in the entire 4.3 mum CO2 absorption band in the retrieval of temperature profile T(p) during both day and night. Tropospheric sounding 15 mum CO2 observations are now used primarily in the generation of cloud cleared radiances Rcirci. This approach allows for the generation of accurate values of Rcirci and T(p) under most cloud conditions. 2) Another very significant improvement in Version 5 is the ability to generate accurate case-by-case, level-by-level error estimates for the atmospheric temperature profile, as well as for channel-by-channel error estimates for Rcirci. These error estimates are used for quality control of the retrieved products. We have conducted forecast impact experiments assimilating AIRS temperature profiles with different levels of quality control using the NASA GEOS-5 data assimilation system. Assimilation of quality controlled T(p) resulted in significantly improved forecast skill compared to that obtained from analyses obtained when all data used operationally by NCEP, except for AIRS data, is assimilated. We also conducted an experiment assimilating AIRS radiances uncontaminated by clouds, as done operationally by ECMWF and NCEP. Forecast resulting from assimilated AIRS radiances were of poorer quality than those obtained assimilating AIRS temperatures.;2008;Joel Susskind;10.1109/IGARSS.2008.4779002;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_28_59;An Increased Potential for the Landsat Data Continuity Mission to Contribute to Water Quality Studies for Inland, Case 2 Waters;The Landsat Data Continuity Mission's (LDCM) Operational Land Imager (OLI) is a new sensor being developed by NASA which is both radiometrically and spatially sufficient for the monitoring of case 2 waters. This study, based on the sensor's design, presents the initial results of an experiment to determine what impact its improved features will have on water quality assessment. Specifically, we investigate how the addition of a deep blue band, 12-bit quantization, and improved signal-to-noise ratios affects our ability to retrieve water constituents. Preliminary tests are performed in the absence of atmospheric effects and indicate that the LDCM sensor can achieve about 5% error in the retrieval process while its predecessor ETM+ produces errors of over 20%. These results illustrate LDCM's potential to be a useful tool for the continuous monitoring of coastal and inland water resources. Ongoing work is focused on atmospherically compensating the data from the OLI instrument to actually achieve the potential demonstrated in this study.;2008;Aaron Gerace;10.1109/IGARSS.2008.4779737;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_28_59;Generating High-Quality Training Data for Automated Land-Cover Mapping;This paper presents two machine learning techniques that greatly reduce the number of person-hours required to generate high-quality training data for land cover classification. The first technique uses active learning to guide the generation of training data by selecting only the most informative examples for labeling. The second technique identifies and mitigates the impact of mislabeled instances. Both techniques are tested on data from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS), which has required thousands of person hours to label. Our results shows that the active learning method requires fewer labeled examples than random sampling to produce a high quality classifier. Our results on class noise mitigation show that if mislabelings occur, we can further improve classifier accuracy, and that weighting instances by their label confidence outperforms an analogous method that discards suspected mislabelings. If combined, these methods have the potential to make training data generation a more efficient and reliable process.;2008;U. Rebbapragada;10.1109/IGARSS.2008.4779779;Conferences;2153-7003;978-1-4244-2808-3
ieee_20221205_08_28_59;An Operation-Based Communication of Spatial Data Quality;Improving users' awareness of the imperfections in spatial data has been a research issue explored within Geographic Information Sciences (GIS) for more than 30 years. However, little practical progress has been made toward this objective. Currently, most spatial data producers document information about spatial data quality as part of metadata. However, this information remains largely ignored by GIS users, which leads to the risk of users making poor decisions based on spatial data. As users increasingly make use of various GIS functionalities, GIS still lack the necessary mechanisms to effectively warn the users of the existence of quality issues in the spatial data being used. This becomes even more problematic in a web environment where data and services of unknown qualities can be shared and combined in the same application. In this paper we present an approach which aims at improving the use of quality information by providing it to the users in a more efficient way than existing approaches used for consulting metadata. We use an operation-based approach to link data quality information to the individual operations used in GIS applications. A conceptual framework for associating quality information with GIS operations is presented. Then, a prototype implementing this concept into a GIS software is described and discussed.;2009;Amin Zargar;10.1109/GEOWS.2009.8;Conferences;;978-0-7695-3527-2
ieee_20221205_08_28_59;Methods of Improving the Quality of X-Ray Image Reconstruction with Limited Projection Data;Nowadays X-ray Nondestructive Testing ( NDT ) has played an important role in defect inspection but the X-ray image contains all the information the rays passed by, therefore, it is hard to inspect the inner defect of PCB correctly. The correct inspect is rely on an image reconstruction system. In this study, a X-ray image reconstruction algorism to inspect the defect in a Printed Circuit Board ( PCB ) is developed. The Algebraic Reconstruction Technique ( ART ) is used to reconstruct the vertical section image from the images of several different project angle, the images are from the Planar Computer Tomography ( PCT ) system for PCB. The X-ray is assumed to be parallel beam. The limited angle of projection in PCT reduces the image reconstruction quality. To improve the quality of reconstructed image, a method combines the binary steering mechanism with ART to get the correct convergence and avoiding the local optimum solution of ART is proposed. Furthermore, the computational algorism to increase the reconstructed speed and discussing the influence between the image and the noise of projection data are also discussed.;2008;Dein Shaw;10.1109/IMPACT.2008.4783847;Conferences;2150-5942;978-1-4244-3623-1
ieee_20221205_08_28_59;Effects of data density of echo Fourier domain on quality of high frame rate imaging;Based on the X wave theory, a high-frame rate (HFR) imaging method was developed and extended. In this paper, the effects of spatial data density of echo Fourier domain on the quality of the HFR imaging method is studied experimentally. In the experiment, echo data were acquired with a home-made HFR imaging system from an ATS539 tissue-mimicking phantom. Results show that with a two-fold densification of radiofrequency echo data of a fully sampled phased array transducer, one achieves the best compromise between the quality of reconstructed images and the amount of computations required due to the densification of echo data.;2008;Jian-yu Lu;10.1109/ULTSYM.2008.0235;Conferences;1051-0117;978-1-4244-2480-1
ieee_20221205_08_28_59;SSTL UK-DMC SLIM-6 Data Quality Assessment;"Satellite data from the Surrey Satellite Technology Limited (SSTL) United Kingdom (UK) Disaster Monitoring Constellation (DMC) were assessed for geometric and radiometric quality. The UK-DMC Surrey Linear Imager 6 (SLIM-6) sensor has a 32-m spatial resolution and a ground swath width of 640 km. The UK-DMC SLIM-6 design consists of a three-band imager with green, red, and near-infrared bands that are set to similar bandpass as Landsat bands 2, 3, and 4. The UK-DMC data consisted of imagery registered to Landsat orthorectified imagery produced from the GeoCover program. Relief displacements within the UK-DMC SLIM-6 imagery were accounted for by using global 1-km digital elevation models available through the Global Land One-km Base Elevation (GLOBE) Project. Positional accuracy and relative band-to-band accuracy were measured. Positional accuracy of the UK-DMC SLIM-6 imagery was assessed by measuring the imagery against digital orthophoto quadrangles (DOQs), which are designed to meet national map accuracy standards at 1 : 24 000 scales; this corresponds to a horizontal root-mean-square accuracy of about 6 m. The UK-DMC SLIM-6 images were typically registered to within 1.0-1.5 pixels to the DOQ mosaic images. Several radiometric artifacts like striping, coherent noise, and flat detector were discovered and studied. Indications are that the SSTL UK-DMC SLIM-6 data have few artifacts and calibration challenges, and these can be adjusted or corrected via calibration and processing algorithms. The cross-calibration of the UK-DMC SLIM-6 and Landsat 7 Enhanced Thematic Mapper Plus was performed using image statistics derived from large common areas observed by the two sensors.";2009;Gyanesh Chander;10.1109/TGRS.2009.2013206;Journals;1558-0644;
ieee_20221205_08_28_59;A quality-based birth-and-death queueing model for evaluating the performance of an integrated voice/data CDMA cellular system;CDMA has been proven to be one of the promising mainstream multiaccessing techniques in future cellular mobile communication systems. It is offering attractions such as soft capacity, soft handoff, and antimultipathing. A quality-based birth-and-death queueing model is developed for the purpose of evaluating the performance of a CDMA cellular that supports two-rate transmissions.;1995;Chyi-Nan Wu;10.1109/PIMRC.1995.480909;Conferences;;0-7803-3002-1
ieee_20221205_08_28_59;Quality Data for Data Mining and Data Mining for Quality Data: A Constraint Based Approach in XML;As quality data is important for data mining, reversely data mining is necessary to measure the quality of data. Specifically, in XML, the issue of quality data for mining purposes and also using data mining techniques for quality measures is becoming more necessary as a massive amount of data is being stored and represented over the Web. We propose two important interrelated issues: how quality XML data is useful for data mining in XML and how data mining in XML is used to measure the quality data for XML. When we address both issues, we consider XML constraints because constraints in XML can be used for quality measurement in XML data and also for finding some important patterns and association rules in XML data mining. We note that XML constraints can play an important role for data quality and data mining in XML. We address the theoretical framework rather than solutions. Our research framework is towards the broader task of data mining and data quality for XML data integrations.;2008;Md. Sumon Shahriar;10.1109/FGCNS.2008.74;Conferences;;978-0-7695-3546-3
ieee_20221205_08_28_59;Defining Quality-Measurable Medical Alerts From Incomplete Data Through Fuzzy Linguistic Variables and Modifiers;Alert systems are frequent in the medical field, where they are typically connected to monitoring devices that are able to detect abnormal values. Our system is different in its goals and tools. First of all, it processes data extracted from electronic medical records, which are widely used nowadays, and meteorological databases. Variables that are not continually measured by devices (like the age of patients) can then be taken into account. Next, the alerts it handles are not predefined, but created by users through domain-independent fuzzy linguistic variables whose relationships (the height of an individual is conditioned by its age) are modeled by a weighted oriented graph. Finally, the alerts it triggers are associated with two indicators used for filtering and assessing their relevance to the patients, and their reliability according to the amount of information available. Then, if there is a missing variable in a record, the detection algorithm treats it transparently by automatically decreasing the reliability of the alert. The main qualities of this system are the simplicity—linguistic variables are intuitive—and the ability to measure the informational quality of alerts (applicability and reliability).;2010;Wilmondes Manzi de Arantes;10.1109/TITB.2009.2020063;Journals;1558-0032;
ieee_20221205_08_28_59;The Impact of Design and Code Reviews on Software Quality: An Empirical Study Based on PSP Data;This research investigates the effect of review rate on defect removal effectiveness and the quality of software products, while controlling for a number of potential confounding factors. Two data sets of 371 and 246 programs, respectively, from a personal software process (PSP) approach were analyzed using both regression and mixed models. Review activities in the PSP process are those steps performed by the developer in a traditional inspection process. The results show that the PSP review rate is a significant factor affecting defect removal effectiveness, even after accounting for developer ability and other significant process variables. The recommended review rate of 200 LOC/hour or less was found to be an effective rate for individual reviews, identifying nearly two-thirds of the defects in design reviews and more than half of the defects in code reviews.;2009;Chris F. Kemerer;10.1109/TSE.2009.27;Journals;2326-3881;
ieee_20221205_08_28_59;Quality preserved image data coding in angiography migration from cinefilm;One of the major objectives in angiocardiography is the visualization of anatomical details of a vessel segment of interest. The excellent resolution of the cinefilm can't be reproduced by today's digital systems. A universal compromise between image quality and strong data reduction is difficult to find. Here the authors propose the concept of image content descriptions as related parameters to be stored with(in) the image. The image frame is divided into regions of different medical significance allowing different degrees of data reduction and quality enhancement. This concept is illustrated by several examples: the 3D-reconstruction avoids unnecessary data, the motion compensated loop preserves the film resolution in critical regions, and the predictor-corrector postprocessor, tunes an arbitrary lossy compressor to become partially nonlossy in the most important regions of the image.;1995;M. Rombach;10.1109/CIC.1995.482749;Conferences;;0-7803-3053-6
ieee_20221205_08_28_59;Water quality and Sea Surface Temperature mapping using NOAA AVHRR data;Environmental pollution is coeval with the appearance of humans. Water pollution problem becomes increasingly critical in this present-day, whether in developed or developing countries. Sediment is the primary cause of water pollution. The environmental pollution problem can be measured using ground instruments such as turbidity meters for water measurements. Field measurements cannot provide fine spatial resolution maps with detailed distribution pattern over a large study area. The study was carried out to verify the validity of National Oceanic and Atmospheric Administration Multi-Channel Sea Surface Temperature (SST) (NOAA MCSST) algorithm by NOAA at South China Sea. SST is verified by comparing the SST calculated by algorithm with sea-truth data collected by Research on the Sea and Islands of Malaysia (ROSES). ROSES had travelled and collected data at South China Sea from 26 Jun 2004 to 1 August 2004. In this study the transmittance function for each band was modeled using the MODTRAN code and radiosonde data. The expression of transmittance as a function of zenith view angle was obtained for each channel through regression of the MODTRAN output. The in-situ data (ship collected SST values) were used for verification of results. The derived SST value was compared with the ground truth data collected during Research on the Seas and Islands (ROSES) project and the standard deviation is less than 1 degree Celsius. SST map was created and comparison between the in-situ SST patterns was made in this study. The satellite NOAA AVHRR data used in SST analysis was used for water quality mapping. The DN values were converted into radiance values and later reflectance values - AVHRR Radiometric Correction and Calibration. The reflectance values corresponding to the ground truth sample locations were extracted from all the images. In this study, the multidate data were corrected to minimize the difference in atmospheric effects between the scenes. The reflectance values for window size of 3 by 3 were used because the data set produced higher correlation coefficient and lower RMS value. Finally, an automatic geocoding technique from PCI Geomatica 10.1 - AVHRR Automated Geometric Correction was applied in this study to geocode the SST and TSS maps.;2009;H. S. Lim;10.1109/AERO.2009.4839441;Conferences;1095-323X;978-1-4244-2622-5
ieee_20221205_08_28_59;The data quality estimation for the information web resources;The article is devoted to problems, which are related to the quality evaluation for the information Web resources. Basic tasks and perspective information technology of quality evaluation by the searching robot are selected.;2009;Zoya Dudar;;Conferences;;978-966-2191-05-9
ieee_20221205_08_28_59;Improving information quality of sensory data through asynchronous sampling;In this paper, asynchronous sampling is proposed as a novel approach to improve the information quality of sensory data through shifting the sampling moments of sensors from each other. The exponential correlation model and the entropy model for the sensory data are introduced to quantify their information quality. An asynchronous sampling strategy, EASS, is presented accordingly to assign equal time shifts to sensors, which in turn reduces data correlation and thus improves information quality in terms of increased entropy of sensory data. A lower bound for EASS is derived to evaluate its effectiveness. Simulation results based on both synthetic data and experimental data are satisfactory.;2009;Jing Wang;10.1109/PERCOM.2009.4912837;Conferences;;978-1-4244-3304-9
ieee_20221205_08_28_59;Quality- and energy-aware data compression by aggregation in WSN data streams;Sensor networks consist of autonomous devices that cooperatively monitor an environment. Sensors are equipped with capabilities to store information in memory, process information and communicate with neighbors and with a base station . However, due to the sensors' size, their associated resources are limited. In such a context, the main cause of energy dissipation is the use of the wireless link. Solutions that minimize communication are needed. In this paper a framework to manage efficiently data streams is presented. The proposed approach aims at saving energy by capturing signals and compress them instead of sending them in raw form. The algorithm also guarantees that the compressed representation satisfies quality requirements specified in terms of accuracy, precision, and timeliness.;2009;Cinzia Cappiello;10.1109/PERCOM.2009.4912866;Conferences;;978-1-4244-3304-9
ieee_20221205_08_28_59;A Calculus Approach to Energy-Efficient Data Transmission With Quality-of-Service Constraints;Transmission rate adaptation in wireless devices provides a unique opportunity to trade off data service rate with energy consumption. In this paper, we study optimal rate control to minimize transmission energy expenditure subject to strict deadline or other quality-of-service (QoS) constraints. Specifically, the system consists of a wireless transmitter with controllable transmission rate and with strict QoS constraints on data transmission. The goal is to obtain a rate-control policy that minimizes the total transmission energy expenditure while ensuring that the QoS constraints are met. Using a novel formulation based on cumulative curves methodology, we obtain the optimal transmission policy and show that it has a simple and appealing graphical visualization. Utilizing the optimal ldquoofflinerdquo results, we then develop an online transmission policy for an arbitrary stream of packet arrivals and deadline constraints, and show, via simulations, that it is significantly more energy-efficient than a simple head-of-line drain policy. Finally, we generalize the optimal policy results to the case of time-varying power-rate functions.;2009;Murtaza A. Zafer;10.1109/TNET.2009.2020831;Journals;1558-2566;
ieee_20221205_08_28_59;Complete Video Quality-Preserving Data Hiding;Although many data hiding methods are proposed in the literature, all of them distort the quality of the host content during data embedding. In this paper, we propose a novel data hiding method in the compressed video domain that completely preserves the image quality of the host video while embedding information into it. Information is embedded into a compressed video by simultaneously manipulating Mquant and quantized discrete cosine transform coefficients, which are the significant parts of MPEG and H.26x-based compression standards. To the best of our knowledge, this data hiding method is the first attempt of its kind. When fed into an ordinary video decoder, the modified video completely reconstructs the original video even compared at the bit-to-bit level. Our method is also reversible, where the embedded information could be removed to obtain the original video. A new data representation scheme called reverse zerorun length (RZL) is proposed to exploit the statistics of macroblock for achieving high embedding efficiency while trading off with payload. It is theoretically and experimentally verified that RZL outperforms matrix encoding in terms of payload and embedding efficiency for this particular data hiding method. The problem of video bitstream size increment caused by data embedding is also addressed, and two independent solutions are proposed to suppress this increment. Basic performance of this data hiding method is verified through experiments on various existing MPEG-1 encoded videos. In the best case scenario, an average increase of four bits in the video bitstream size is observed for every message bit embedded.;2009;KokSheik Wong;10.1109/TCSVT.2009.2022781;Journals;1558-2205;
ieee_20221205_08_28_59;An approach to reverse quality assurance with data-oriented program analysis;The paper describes a quality assurance approach for source code to assure source code quality with less effort than conventional method. The paper introduces the data relation tracking method (DRTM) as a practical instance of data oriented program analysis, a key factor of our new quality assurance approach. DRTM helps comprehension of the source code by extracting the internal logic of the source code in declarative notation. An example and evaluation of DRTM for the C language is also described. The example shows that DRTM can deal in data structures and inter-data relations of any control structure. The evaluation shows that DRTM can extract the internal logic of the source code uniquely and that the extracted internal logic is useful for quality assurance of the source code.;1995;Y. Kataoka;10.1109/APSEC.1995.496981;Conferences;;0-8186-7171-8
ieee_20221205_08_28_59;Evaluating process quality in GNOME based on change request data;The lifecycle of defects reports and enhancement requests collected in the Bugzilla database of the GNOME project provides valuable information on the evolution of the change request process and for the assessment of process quality in the GNOME sub projects. We present a quality model for the analysis of quality characteristics that is based on evaluating metrics on the Bugzilla database, and illustrate it with a comparative evaluation for 25 of the largest products within GNOME.;2009;Holger Schackmann;10.1109/MSR.2009.5069485;Conferences;2160-1860;978-1-4244-3493-0
ieee_20221205_08_28_59;Quality Evaluation of Spatial Point-Cloud Data Collected by Vehicle-Borne Laser Scanner;Vehicle-borne laser scanning is used to collect urban street and building data for various modeling and mapping applications. The quality of laser point-cloud data is important for regenerating 3D urban street scene. Quality evaluation includes spatial structure analysis and positioning accuracy analysis. In spatial structure analysis, we evaluate the quality of point-cloud data high or low by analyzing the objectspsila spatial structure projection at each of the three coordinate axis. In positioning accuracy analysis, comparing sign pointspsila 3D coordinates collected by laser scanner with the precise coordinates collected by Total Station, we get the point-cloudspsila positioning accuracy. A new sign point was also designed to overcome the difficulty of extracting sign points coordinates from point cloud data. After the evaluation, we make a suggestion about how to improve the data quality.;2008;Jiaxiang Feng;10.1109/ETTandGRS.2008.97;Conferences;;978-0-7695-3563-0
ieee_20221205_08_28_59;Assuring Image Quality in Spatial Data Sharing Platform for Disaster Management;This paper aims to describe development of a Web-based spatial data sharing platform for disaster management. Alongside functionality, quality of image(IQ) is basic to successful sharing of distributed geo-services.This paper explores IQ provisioning in the context of geo-service sharing for disaster management.The paper presents an IQ model for disaster management and illustrates how user level IQ requirements can be supported in IQ-aware geo-service architecture. In contrast to context-specific IQ assurance approaches, which usually focus on a few variables determined by local needs, this approach provides interactive, multi-granularity and contextsensitive IQ indicators that help experts to build and justify their opinions. The prototype system was designed and validated by enhancing IQ generated by remote sensing through the technologies of visualization.;2008;Ying Su;10.1109/ETTandGRS.2008.275;Conferences;;978-0-7695-3563-0
ieee_20221205_08_28_59;Application of COMERO Data Collecting in Quality Management System;Summary form only given. Exact and real-time collection of quality data from manufacturing process is the most important step in quality management system. It is the foundation of cumulating, analyzing and effectively controlling the processpsilas quality. As an advanced and exact metrical instrument, 3-degree coordinate measuring machining (CMM) has been widely used in machinery manufacturing industry. However, it could only use text file to record the measuring result, which can not be integrated with the quality management system. This paper introduces appropriative interface software, which can automatically read the measurement data from 3-degree CMMpsilas RTF file and can automatically transfer the certain data to database and then let quality management system directly transfer the measurement result in order to analyze the correlative accessorypsilas quality. This analysis is useful to understand the characteristic of random error system, the trend of error, the order of error distributing and the reasons for error and also form the quality report and early warning information for quality control. Meanwhile, informationpsilas automatically transmission can obviously improve working efficiency and effectively avoid the artificial errors. Heterogeneous datapsilas conversion is the key point of system integration. This paper comprehensively analysis the expression mode of every dimension and error in the measurement text files mentioned above, and then applies certain program. In order to understand the meaning of relevant data, key words are needed. Using character string in the measurement files, which match the regular expression and the method of exhaustion, we could get the key words. Then the data, which quality management system needs, is formed by transferring data types with homonymous matching and written into the database. In order to achieve the informationpsilas automatically transition, this paper also studies the data synchronization mechanism and ensure the measurement data can be synchronously updated in the quality management system by setting the trigger rule.;2009;Yue Yan-fang;10.1109/ITNG.2009.167;Conferences;;978-0-7695-3596-8
ieee_20221205_08_28_59;Assessing Quality of Derived Non Atomic Data by Considering Conflict Resolution Function;We present a Data Quality Manager (DQM) prototype providing information regarding the elements of derived non-atomic data values. Users are able to make effective decisions by trusting data according to the description of the conflict resolution function that was utilized for fusing data along with the quality properties of data ancestor. The assessment and ranking of non-atomic data is possible by the specification of quality properties and priorities from users at any level of experience.;2009;Maria del Pilar Angeles;10.1109/DBKDA.2009.10;Conferences;;978-0-7695-3550-0
ieee_20221205_08_28_59;Adaptable Link Quality Estimation for Multi Data Rate Communication Networks;QoS-sensitive applications transmitted over wireless links require precise knowledge of the wireless environment. However, the dynamic nature of wireless channel, together with its different configurations and types, makes so called link quality estimation (LQE) a difficult task. This paper looks into the design of an accurate and fast LQE method for a multi data rate environment. We investigate the impact of various conditions on the LQE accuracy. In result, two different link quality estimation sources, i.e., based on hello packet delivery ratio and signal strength, are measured and their performances are compared. We find that these two methods are not always accurate. As an improvement we propose an adaptive LQE method that chooses different LQE indicators depending on the wireless environment. The performance of the proposed method is verified via an extensive measurement campaign.;2009;Jinglong Zhou;10.1109/VETECS.2009.5073358;Conferences;1550-2252;978-1-4244-2517-4
ieee_20221205_08_28_59;Predicting Quality of Object-Oriented Systems through a Quality Model Based on Design Metrics and Data Mining Techniques;Most of the existing object-oriented design metrics and data mining techniques capture similar dimensions in the data sets, thus reflecting the fact that many of the metrics are based on similar hypotheses, properties, and principles. Accurate quality models can be built to predict the quality of object-oriented systems by using a subset of the existing object-oriented design metrics and data mining techniques. We propose a software quality model, namely QUAMO (QUAlity MOdel) which is based on divide-and-conquer strategy to measure the quality of object-oriented systems through a set of object-oriented design metrics and data mining techniques. The primary objective of the model is to make similar studies on software quality more comparable and repeatable. The proposed model is augmented from five quality models, namely McCall Model, Boehm Model, FURPS/FURPS+ (i.e. functionality, usability, reliability, performance, and supportability), ISO 9126, and Dromey Model. We empirically evaluated the proposed model on several versions of JUnit releases. We also used linear regression to formulate a prediction equation. The technique is useful to help us interpret the results and to facilitate comparisons of results from future similar studies.;2009;Chuan Ho Loh;10.1109/ICIME.2009.78;Conferences;;978-0-7695-3595-1
ieee_20221205_08_28_59;Data acquisition system in a mobile air quality monitoring station;The paper focuses on describing the complex problems concerning air quality monitoring and the necessity and utility of the control and automatic data acquisition system for the interpretation of the results. The paper relates to the experience and long term practice of the authors in monitoring air quality. The build up and control of the station is detailed. The main conclusion indicates that the electronic devices and the program environment is absolute necessary for a real time and correct monitoring result of air quality control.;2009;Ioana Ionel;10.1109/SACI.2009.5136310;Conferences;;978-1-4244-4478-6
ieee_20221205_08_28_59;Wavelet transform based ECG data compression with desired reconstruction signal quality;This paper proposes a new coding strategy by which the desired quality of reproduced signal can be guaranteed with the minimum cost of coding rate. The idea was successfully introduced to the DOWT-based coding system for the ECG compression application.;1994;J. Chen;10.1109/WITS.1994.513911;Conferences;;0-7803-2761-6
ieee_20221205_08_28_59;Quality of archived NDBC data as climate records;The National Data Buoy Center (NDBC) traces its beginning to the formation of the National Data Buoy Development Program in 1967, which consolidated approximately 50 individual programs conducted by a variety of ocean-oriented agencies. Today, NDBC operates three major buoy networks. First, the traditional weather fleet consists of over 100 moored buoys covering the coastal waters of the United States, including the Great Lakes, Hawaii and Alaska. Second, the Deep-ocean Assessment and Reporting of Tsunamis (the National Oceanic and Atmospheric Administration's DART®) Program operates 39 stations that detect and instantly report anomalies in ocean pressure associated with potential tsunami-generating seismic activity. Third, the Tropical-Atmosphere Ocean (TAO) array of climate monitoring platforms covers a wide swath of the equatorial Pacific. In this paper, we assess the traditional weather fleet as a resource for climate monitoring, and we do so in two ways. Both involve scrutinizing weather fleet records exceeding 20 years duration. We assess these according to the ten climate monitoring principles recommended by the U.S. National Research Council. We observe that NDBC has implicitly considered most, if not all, of these principles in the design, maintenance, improvement and expansion of the NDBC moored buoy fleet. Focusing on two stations in the Pacific Ocean, 46035 and 46042, we demonstrate NDBC's adherence to sound network management, careful archiving and description of metadata, steady development of comprehensive automated quality control procedures, giving users ease in data access, addressing issues of complementary data, historical significance and continuity of purpose. One area requiring strengthening remains a need for NDBC to build into its systems long-term climate requirements. Next, we propose a new method for reflecting climatic change over the oceans. The wave energy spectrum, which all NDBC weather buoys routinely report hourly, contain a significant amount of information regarding the origin, intensity and duration of ocean storms. Such measurements are produced from simple accelerometers coming from a mature, stable technology. We show that records of spectral energy density at low frequencies — for wave periods exceeding 20 seconds — suggest climate change signals. This is demonstrated with data collected from station 46042 in Monterey Bay, California. Both assessments clearly indicate that the NDBC network of weather monitoring buoys are a valuable national resource for climatologists, meteorologists and oceanographers interested in marine surface fluctuations on decadal and longer durations. We note areas where small improvements in calibration techniques will likely yield large gains in confident assessment of climate change.;2008;Theodore Mettlach;10.1109/OCEANS.2008.5151872;Conferences;0197-7385;978-1-4244-2620-1
ieee_20221205_08_28_59;Automated data quality assurance for marine observations;"The ocean monitoring community requires high quality data that is Data Management and Communications (DMAC)-compliant for both near real time and climate data records. The authors describe a flexible and cost effective automated data quality assurance (ADQA) system that can be used to assess the quality of marine observations and provide quality controlled data to a wide variety of end users. For example, if a researcher needs data sets from different sources for a modeling project, ADQA provides a means of characterizing the relative quality of these input sets. The ADQA system has been implemented by integrating a set of data quality algorithms based upon National Data Buoy Center (NDBC) Technical Document 03-02 ""Handbook of Automated Data Quality Control Checks and Procedures of the National Data Buoy Center"" into a processing system that was created using the CALIPSO science data processing framework. The CALIPSO framework is actually a library of reusable components that provide the core non-science functionality of any science data processing system. This design separates the science processing components from the basic, reusable system infrastructure and allows for the addition or removal of algorithms with relative ease. The generic infrastructure of the framework includes substantial core functionality that is common to many science data management applications and is easily configurable to work with a wide variety of marine instruments and science data sets. The basic architecture includes the following subsystems: Input, Control, Data Store and Output. The system is a modular design that simplifies the integration of additional data quality assurance (DQA) processing components.";2008;James V. Koziana;10.1109/OCEANS.2008.5151904;Conferences;0197-7385;978-1-4244-2620-1
ieee_20221205_08_28_59;Analysis on the Lake Water Pollution Components Based on Normal Quality Monitoring Data;Take Hongze Lake of Jiangsu Province, China, as a Case, the components of lake water pollution were analyzed, based on the regular water quality monitoring data of 10 sampling points from 1990 to 2002 and the spatial data. By factor analysis and spatio-temporary correlation analysis etc, the spatio- temporary simulations to main lake water pollution components, including eutrophication, were carried out. The lake water pollution components were classified into two types, primary pollution component (PPC) and secondary pollution component (SPC). The living pollution, nitrogen pollution etc, belong to the former, and eutrophication, ion property etc, belong to the later. The equations of the pollution components were constructed, and inversely their seasonal and spatial changes were analyzed. Finally the relation of simulated eutrophication values with N:P was modeled in this paper, which was similar to general rule of shallow lakes. Results showed that a good combination of spatio- temporary study with factor analysis etc was very helpful for the classification, identification and modeling of water pollution components, and was favorable to evaluate water pollution and deeper study on the mechanism of Lake Eutrophication.;2009;Bo Li;10.1109/ICBBE.2009.5162365;Conferences;2151-7622;978-1-4244-2901-1
ieee_20221205_08_28_59;Quality assessment of InSAR-derived DEMs generated with ERS tandem data;Provides a quality assessment of digital elevation models generated by means of SAR interferometry and the use of ERS tandem data. A total of 8 scene pairs have been investigated, acquired at different dates but covering the same site to allow a study of the different parameters such as temporal decorrelation, baseline, and varying incidence angles. The ERS tandem configuration implies some characteristics that are examined, concerning in particular the system parameters. Moreover, peculiarities detected during the interferometric processing are reported. The achieved results are compared to an existing reference DEM. Additionally, the InSAR-derived DEMs are compared to each other to study SAR-specific effects.;1996;M. Schwabisch;10.1109/IGARSS.1996.516480;Conferences;;0-7803-3068-4
ieee_20221205_08_28_59;System for analyses of end-to-end quality of data services in cellular networks;End-to-end quality of service in today's mobile networks is essential to end users since most of the services are non-real-time data services (primarily based on IP) which have time-varying quality over time, end-user position and other-end location. It is possible to have different quality for the same service under same mobile network conditions in the core and wireless part. Therefore, mobile operators need systems for testing and measurements of end-to-end quality of service. In this paper we present our developed system for service quality testing, which is created for testing the user perceived quality of services in mobile cellular networks. The system is consisted of distributed testing stations over the mobile network coverage area and centralized management and processing nodes, which provide collection, storage and statistical processing of data regarding the quality of service parameters for mobile data services. The system provides objective end-to-end quality of service information in real-time, which is essential to provide services with desired quality to the end users.;2009;Toni Janevski;10.1109/EURCON.2009.5167860;Conferences;;978-1-4244-3861-7
ieee_20221205_08_28_59;Data Mining on Source Water Quality (Tianjin, China) for Forecasting Algae Bloom Based on Artificial Neural Network (ANN);Harmful algae in source water become a very serious problem for water plants in China. Artificial neural networks (ANN) have been successfully used to model primary production and predict one-step weekly algae blooms in reservoir. In this study, to avoid selecting inputs randomly during the establishment of feed forward ANN forecasting algae two days later, we use correlation coefficient and index clustering to analyze source water quality parameters totally about 1744 daily measured data from 1997 to 2002 of Tianjin. So twenty-six schemes of input variables are determined and experimented for optimal inputs. Inputs of the final model are chlorophyll-a, turbidity, water temperature, ammonia, pH and alkalinity. The correlation coefficient of output values of the model and real values can reach 0.88 and the prediction accuracy is over 85%.;2009;Jin-Suo Lu;10.1109/CSIE.2009.97;Conferences;;978-0-7695-3507-4
ieee_20221205_08_28_59;Management of Air Quality Monitor Data with Data Warehouse and GIS;Air quality status is an important problem focused by all people. This paper utilizes Oracle 10 g to design and implement a prototype system of air quality data warehouse with the monitor data of 86 main cities from the year of 2000 to 2007 in China. To query and analyze the data in the data warehouse conveniently and effectively, it extends the star model to manage the spatial data with ArcGIS 9.0, and implements a Web spatial OLAP system to improve the ability of spatial analysis and visualization of traditional OLAP systems synchronously. Our work will help to evaluate air quality status, analyze its spatio-temporal characteristic, forecast and provide decision-making support for improvement of air quality.;2009;Yongkang Xiao;10.1109/CSIE.2009.280;Conferences;;978-0-7695-3507-4
ieee_20221205_08_28_59;Genetic Projection Pursuit Interpolation Data Mining Model for Urban Environmental Quality Assessment;In order to solve the incompatibility problem of assessment data indexes and to raise the precision of assessment model for urban environmental quality, a genetic projection pursuit interpolation data mining model (GPPIDMM) is presented for comprehensive assessment of urban environmental quality. In this model the projection pursuit data mining, genetic algorithm, interpolation curve and the assessment standards of urban environmental quality are used. And the indexes values of urban environmental quality can be synthesized to one dimension projection values. The samples can be assessed according to the values of the projection values in one dimension space. 50, 100, 500, 1000 samples in each grade have been adopted to test the stability of parameters in this model. In this new model, 5000 samples are generated from the assessment standards of urban environmental quality, which avoids the low precision in other models with little quantity of samples. The interpolation assessment formula is given with projection values and experiential grades. And the parameters in this model are steady by test. This new model is used to assess Xuanzhou environmental quality with the main indexes of water environment, atmospheric environment and noise environment. GPPIDMM can also be used to design the weights of the index system and deal with data. The results show that the urban environmental quality is still clean in Xuanzhou. GPPIDMM is a new method for evaluation of urban environmental quality and it is more objective in the whole data processing.;2009;Yang Xiaohua;10.1109/CSIE.2009.6;Conferences;;978-0-7695-3507-4
ieee_20221205_08_28_59;Estimation of power factor by the analysis of Power Quality data for voltage unbalance;Power Quality (PQ) has been identified as a complex and diversified problem across the board in electrical power industry. It has not only confused the power utilities but also has attracted the attention of its customers and equipment manufacturers. Although extensive research work is being done in this field but one of the main problem encountered by its stake holders is the voltage unbalance in electrical power system. The problem of voltage unbalance has affected the safety, reliability and economic efficiency at all levels in power industry. In this research Computational Intelligence Techniques have been used for efficiently predicting the power factor of unbalanced load of a power distribution system. The Principal Component Analysis Technique was used to find the optimized number of new dimensions of PQ data. Finally Feed Forward Back Propagation (FFBP) algorithm was used to estimate the power factor of a distribution network by analyzing the real power system parameters. This research highlights the importance of maintaining power factor close to unity for power utilities to achieve sustainable availability of quality supply of electrical power for its customers. The outcomes of the proposed techniques were compared and tested with the field results of a power utility in Victoria, Australia.;2009;Zahir J. Paracha;10.1109/ICEE.2009.5173178;Conferences;;978-1-4244-4361-1
ieee_20221205_08_28_59;Quality assurance for data acquisition in error prone WSNs;This paper proposes a data acquisition scheme which supports probabilistic data quality assurance in an error-prone wireless sensor network (WSN). Given a query and a statistical model of real-world data which is highly correlated, the aim of the scheme is to find a sensor selection scheme which is used to deal with inaccurate data and probabilistic guarantee on the query result. Since most sensor readings are real-valued, we formulate the data acquisition problem as a continuous-state partially observable Markov decision process (POMDP). To solve the continuous-state POMDP, the fitted value iteration (FVI) is applied to find a sensor selection scheme. Numerical results show that FVI can achieve high average long-term reward and provide probabilistic guarantees on the query result more often when compared to other algorithms.;2009;Sunisa Chobsri;10.1109/ICUFN.2009.5174279;Conferences;2165-8536;978-1-4244-4216-4
ieee_20221205_08_28_59;Data-Driven Soft Sensor Approach for Quality Prediction in a Refining Process;In the petrochemical industry, the product quality reflects the commercial and operational performance of a manufacturing process. However, real-time measurement of product quality is generally difficult. Online prediction of quality using readily available, frequent process measurements would be beneficial in terms of operation and quality control. In this paper, a novel soft sensor technology based on partial least squares (PLS) regression is developed and applied to a refining process for quality prediction. The modeling process is described, with emphasis on data preprocessing, multivariate-outlier detection and variables selection. Enhancement of PLS strategy is also discussed for taking into account the dynamics in the process data. The proposed approach is applied to data from a refining process and the performance of the resulting soft sensor is evaluated by comparison with laboratory data and analyzer measurements.;2010;David Wang;10.1109/TII.2009.2025124;Journals;1941-0050;
ieee_20221205_08_28_59;Analysis & design and implementation of quality oriented data management system for cold rollers production;Based on the production processes of cold rollers, this paper deeply analyzes the process and its characteristics. A concept model is designed for quality oriented data management in the cold rollers production processes. Both the systematic architecture and corresponding function modules have been designed. Furthermore, the implementation and its features of the target system are introduced. Application and implementation of the quality management system indicates that not only is it very suitable to quality management in the cold roller production system, but also it can be applied to other industrials with similar production processes.;2009;Gao Bin;10.1109/CCDC.2009.5191823;Conferences;1948-9447;978-1-4244-2723-9
ieee_20221205_08_28_59;A general model for continuous quality improvement by immune system inspired data driven evolution;Today, enterprises are facing more pressure than ever for continuous improvement and adaptation. Therefore, how to capture the flashing by opportunity to improve quality with low cost and high efficiency becomes an imperative task for any enterprise. Inspired by the natural immune system principles and biological evolution mechanism, a data driven system evolution method for continuous quality improvement based on information technology is proposed. First the variation phenomenon in biological system and manufacturing system is compared. Then based on analyse of the similarity and difference between the two systems, the general model for continuous quality improvement is established by utilizing existing data analysis tools and methods. Finally the method is illustrated by a case study which uses association rule mining method as the tool to capture the opportunity to improve system quality. This immune inspired model has the potential to improve the system quality with little additional cost.;2009;Genbao Zhang;10.1109/CCDC.2009.5195183;Conferences;1948-9447;978-1-4244-2723-9
ieee_20221205_08_28_59;Improving data quality and clinical records: Lessons from the UK National Programme about structure, process and utility;Sharing of health data should improve patient safety and improve health services efficiency. These data can also be used for research. The shared data are usually ldquocodedrdquo using a coding system classification or nomenclature. However, ldquocodingrdquo is not a neutral action and is part of the complex social interaction between doctor and patient. To derive meaning from data it is essential to understand the context in which it is recorded and to infer whether data recorded for one purpose is usable in another. Most of the existing models to raise data quality (DQ) are descriptive and don't necessarily inform why lessons from one health system might be applied in another.;2009;Simon de Lusignan;10.1109/ITI.2009.5196042;Conferences;1330-1012;978-953-7138-15-8
ieee_20221205_08_28_59;The use of data-mining to identify indicators of health related quality of life in patients with irritable bowel syndrome;Health-related quality of life can be adversely affected by irritable bowel syndrome (IBS). The aims of this study were to examine the health-related quality of life in a cohort of individuals with IBS and to determine which socio-demographic and IBS symptoms are independently associated with reduced health-related quality of life. Several data-mining models to determine which factors are associated with impaired health-related quality of life are considered in this study and include logistic regression, a classification tree and artificial neural networks. As well as severity of IBS symptoms, results indicate that psychological morbidity and socio-demographic factors such as marital status and employment status also have a major role to play.;2009;Kay I Penny;10.1109/ITI.2009.5196059;Conferences;1330-1012;978-953-7138-15-8
ieee_20221205_08_28_59;Research on Control of the Data Quality in Digital Land Spatial Database Based on GIS;GIS as the platform is being widely used in current construction of geoscience spatial database in China, and the quality of its spatial data directly influences the quality of the spatial database of geoscience. Precision of spatial data, validity and integrality of attribute data, and validity of the spatial database's topological relations are the main factors which influence the quality of the spatial database. Through practical summarize, the article discusses concrete means and methods to control the data quality of the three aspects mentioned above. By the means and methods, the data quality can be guaranteed to meet the needs for setting up spatial database of geoscience and building up a spatial database of geoscience with good quality.;2009;Chen Weigong;10.1109/DBTA.2009.49;Conferences;2167-194X;978-0-7695-3604-0
ieee_20221205_08_28_59;Geospatial Data Quality Self-Assuring Model for Disaster Management System;The objective of this paper is to propose a geospatial data quality (G-DQ) self assuring model using an intelligent multiagent (IMA) technique, which is capable to monitor network status, evaluate G-DQ, generate assuring strategies, and apply it to the disaster management system. To do this, we have employed the IMA algorithm concept based on the advanced artificial intelligence for developing information security models and agent-based evaluation of G-DQ as well as assuring strategy. Our approach differs from others in that, it is able to analyze G-DQ in quantitative manner, it can generate and apply assuring strategies automatically, and it supports a coherent design concept for G-DQ self-assuring system. Simulation results show that the proposed approach increases the efficiency and results in quality information acquisition.;2009;Ying Su;10.1109/GCIS.2009.450;Conferences;2155-6091;978-0-7695-3571-5
ieee_20221205_08_28_59;Quality-based data source selection for web-scale Deep Web data integration;Deep Web has been an important resource on the Web due to its rich and high quality information, leading to emerging a new application area in data mining and information retrieval and integrates. In Web scale deep Web data integration tasks, where there may be hundreds or thousands of data sources providing data of relevance to a particular domain, It must be inefficient to integrate all available deep Web sources. This paper proposes a data source selection approach based on the quality of deep Web source. It is used for automatic finding the highest quality set of deep Web sources related to a particular domain, which is a premise for effective deep Web data integration. The quality of data sources are assessed by evaluating quality dimensions represent the characteristics of deep Web source. Experiments running on real deep Web sources collected from the Internet show that our provides an effective and scalable solution for selecting data sources for deep Web data integration.;2009;Xue-Feng Xian;10.1109/ICMLC.2009.5212537;Conferences;2160-1348;978-1-4244-3703-0
ieee_20221205_08_28_59;Landsat 7 ETM+ on-orbit calibration and data quality assessment;The Enhanced Thematic Mapper Plus (ETM+) multispectral scanner is scheduled for launch aboard the Landsat 7 satellite in 1998. Unlike data from earlier Landsat satellites, the United States (U.S.) Government will distribute data to users in an essentially raw form. The Landsat 7 ground system design therefore includes an image assessment system (IAS) to provide users with the ancillary information needed to generate useful, radiometrically-calibrated, geometrically-corrected ETM+ digital imagery. The IAS will assess ETM+ data quality and calibrate the sensor radiometry and geometry. The information provided by the IAS will allow users to produce digital image data with an absolute radiometric accuracy of 5%, band-to-band registration of 0.3 pixels, and geodetic registration to 250 m (1 sigma) without ground control.;1995;J.R. Irons;10.1109/IGARSS.1995.521813;Conferences;;0-7803-2567-2
ieee_20221205_08_28_59;Extension of the Goal-Decision-Information approach to ensure quality in data warehouse development using software agent;Data warehouses are complex systems that have to deliver highly-aggregated data from heterogeneous sources to decision makers. It is essential that we can assure the quality data warehouse in terms of data as well as the services provided by it. But the requirements and the environment of data warehouse systems is dynamic in nature. To handle these changes efficiently, data warehouses depend largely on the meta databases. In this paper, the proposal is to extend the Goal-Decision-Information approach to model the quality of the data warehouse. In order to fulfill the specific quality goals, dependencies among the various quality factors is exploited in this model.;2009;S. Venkatesan;10.1109/IAMA.2009.5228083;Conferences;;978-1-4244-4710-7
ieee_20221205_08_32_33;Data system for the monitoring of power quality in the transmission substations supplying big consumers;During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.;2007;Fanica Vatra;10.1109/EPQU.2007.4424094;Conferences;2150-6655;978-84-690-9441-9
ieee_20221205_08_32_33;Gas Emergence Big Data and neural network filter;The gas supervision is a safety core of the coal mine production, which widespread existent a trouble, gas emergence big data, namely the pulse interference, the cause of the gas signal mistake alarm, is hardly resolved very often. In this paper, the reason and characteristics of gas emergence big data are analyzed to establish a kind of filter based on BP Neural Network. Through great quantities monitor data as training sample to train the network model, and tested by test samples, we get the needed network model. Results show that the model can guarantee alarm occurrence while the gas density is beyond the limit, meanwhile preventing supervision system from mistake alarm caused by pulse interference. Applying this research can promote the robustness of existing coal mine safety supervision system without update any device, which can improve the safety production in both social meaning and economic value.;2008;Li Kun;10.1109/CHICC.2008.4605537;Conferences;2161-2927;978-7-900719-70-6
ieee_20221205_08_32_33;Big Data;This introduction to the special issue on big data discusses the significant scientific opportunities offered by massive amounts of data, along with some directions for future research.;2011;Francis J. Alexander;10.1109/MCSE.2011.99;Magazines;1558-366X;
ieee_20221205_08_32_33;Entertainment in the Age of Big Data;Everybody loves a good story. People all over the world seek out and crave entertainment. This will not change in the coming age of big data and cloud computing. But stories are more than just a distraction from the big problems of the world we are all supposed to be solving. Stories are (computationally) one of the most challenging problems facing our computers. Humans are hardcoded for story. Our computers are not. Or not yet. Through our research we examine a future of entertainment where increasing computational power allows us to be genuinely connected to a truly digital world. We argue that a genuinely digital and connected future opens the door for a new kind of storytelling, and ultimately, a new way of managing and interacting with the massive data sets collected and shared by humans.;2012;Tawny Schlieski;10.1109/JPROC.2012.2189918;Journals;1558-2256;
ieee_20221205_08_32_33;Big data challenges for large radio arrays;"Future large radio astronomy arrays, particularly the Square Kilometre Array (SKA), will be able to generate data at rates far higher than can be analyzed or stored affordably with current practices. This is, by definition, a ""big data"" problem, and requires an end-to-end solution if future radio arrays are to reach their full scientific potential. Similar data processing, transport, storage, and management challenges face next-generation facilities in many other fields. The Jet Propulsion Laboratory is developing technologies to address big data issues, with an emphasis in three areas: 1) Lower-power digital processing architectures to make highvolume data generation operationally affordable, 2) Date-adaptive machine learning algorithms for real-time analysis (or ""data triage"") of large data volumes, and 3) Scalable data archive systems that allow efficient data mining and remote user code to run locally where the data are stored.1";2012;Dayton L. Jones;10.1109/AERO.2012.6187090;Conferences;1095-323X;978-1-4577-0555-7
ieee_20221205_08_32_33;Considerations for big data: Architecture and approach;The amount of data in our industry and the world is exploding. Data is being collected and stored at unprecedented rates. The challenge is not only to store and manage the vast volume of data (“big data”), but also to analyze and extract meaningful value from it. There are several approaches to collecting, storing, processing, and analyzing big data. The main focus of the paper is on unstructured data analysis. Unstructured data refers to information that either does not have a pre-defined data model or does not fit well into relational tables. Unstructured data is the fastest growing type of data, some example could be imagery, sensors, telemetry, video, documents, log files, and email data files. There are several techniques to address this problem space of unstructured analytics. The techniques share a common characteristics of scale-out, elasticity and high availability. MapReduce, in conjunction with the Hadoop Distributed File System (HDFS) and HBase database, as part of the Apache Hadoop project is a modern approach to analyze unstructured data. Hadoop clusters are an effective means of processing massive volumes of data, and can be improved with the right architectural approach.;2012;Kapil Bakshi;10.1109/AERO.2012.6187357;Conferences;1095-323X;978-1-4577-0555-7
ieee_20221205_08_32_33;From Databases to Big Data;"There is a tremendous amount of buzz around the concept of ""big data."" In this article, the author discusses the origins of this trend, the relationship between big data and traditional databases and data processing platforms, and some of the new challenges that big data presents.";2012;Sam Madden;10.1109/MIC.2012.50;Magazines;1941-0131;
ieee_20221205_08_32_33;Big data privacy issues in public social media;Big Data is a new label given to a diverse field of data intensive informatics in which the datasets are so large that they become hard to work with effectively. The term has been mainly used in two contexts, firstly as a technological challenge when dealing with dataintensive domains such as high energy physics, astronomy or internet search, and secondly as a sociological problem when data about us is collected and mined by companies such as Facebook, Google, mobile phone companies, retail chains and governments. In this paper we look at this second issue from a new perspective, namely how can the user gain awareness of the personally relevant part Big Data that is publicly available in the social web. The amount of user-generated media uploaded to the web is expanding rapidly and it is beyond the capabilities of any human to sift through it all to see which media impacts our privacy. Based on an analysis of social media in Flickr, Locr, Facebook and Google+, we discuss privacy implications and potential of the emerging trend of geo-tagged social media. We then present a concept with which users can stay informed about which parts of the social Big Data deluge is relevant to them.;2012;Matthew Smith;10.1109/DEST.2012.6227909;Conferences;2150-4938;978-1-4673-1701-6
ieee_20221205_08_32_33;How Different is Big Data?;One buzzword that has been popular in the last couple of years is Big Data. In simplest terms, Big Data symbolizes the aspiration to build platforms and tools to ingest, store and analyze data that can be voluminous, diverse, and possibly fast changing. In this talk, I will try to reflect on a few of the technical problems presented by the exploration of Big Data. Some of these challenges in data analytics have been addressed by our community in the past in a more traditional relational database context but only with mixed results. I will review these quests and study some of the key lessons learned. At the same time, significant developments such as the emergence of cloud infrastructure and availability of data rich web services hold the potential for transforming our industry. I will discuss the unique opportunities they present for Big Data Analytics.;2012;Surajit Chaudhuri;10.1109/ICDE.2012.153;Conferences;1063-6382;978-1-4673-0042-1
ieee_20221205_08_32_33;Temporal Analytics on Big Data for Web Advertising;"""Big Data"" in map-reduce (M-R) clusters is often fundamentally temporal in nature, as are many analytics tasks over such data. For instance, display advertising uses Behavioral Targeting (BT) to select ads for users based on prior searches, page views, etc. Previous work on BT has focused on techniques that scale well for offline data using M-R. However, this approach has limitations for BT-style applications that deal with temporal data: (1) many queries are temporal and not easily expressible in M-R, and moreover, the set-oriented nature of M-R front-ends such as SCOPE is not suitable for temporal processing, (2) as commercial systems mature, they may need to also directly analyze and react to real-time data feeds since a high turnaround time can result in missed opportunities, but it is difficult for current solutions to naturally also operate over real-time streams. Our contributions are twofold. First, we propose a novel framework called TiMR (pronounced timer), that combines a time-oriented data processing system with a M-R framework. Users write and submit analysis algorithms as temporal queries - these queries are succinct, scale-out-agnostic, and easy to write. They scale well on large-scale offline data using TiMR, and can work unmodified over real-time streams. We also propose new cost-based query fragmentation and temporal partitioning schemes for improving efficiency with TiMR. Second, we show the feasibility of this approach for BT, with new temporal algorithms that exploit new targeting opportunities. Experiments using real data from a commercial ad platform show that TiMR is very efficient and incurs orders-of-magnitude lower development effort. Our BT solution is easy and succinct, and performs up to several times better than current schemes in terms of memory, learning time, and click-through-rate/coverage.";2012;Badrish Chandramouli;10.1109/ICDE.2012.55;Conferences;1063-6382;978-1-4673-0042-1
ieee_20221205_08_32_33;Efficient Skyline Computation on Big Data;Skyline is an important operation in many applications to return a set of interesting points from a potentially huge data space. Given a table, the operation finds all tuples that are not dominated by any other tuples. It is found that the existing algorithms cannot process skyline on big data efficiently. This paper presents a novel skyline algorithm SSPL on big data. SSPL utilizes sorted positional index lists which require low space overhead to reduce I/O cost significantly. The sorted positional index list Lj is constructed for each attribute Aj and is arranged in ascending order of Aj. SSPL consists of two phases. In phase 1, SSPL computes scan depth of the involved sorted positional index lists. During retrieving the lists in a round-robin fashion, SSPL performs pruning on any candidate positional index to discard the candidate whose corresponding tuple is not skyline result. Phase 1 ends when there is a candidate positional index seen in all of the involved lists. In phase 2, SSPL exploits the obtained candidate positional indexes to get skyline results by a selective and sequential scan on the table. The experimental results on synthetic and real data sets show that SSPL has a significant advantage over the existing skyline algorithms.;2013;Xixian Han;10.1109/TKDE.2012.203;Journals;2326-3865;
ieee_20221205_08_32_33;Design Principles for Effective Knowledge Discovery from Big Data;Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.;2012;Edmon Begoli;10.1109/WICSA-ECSA.212.32;Conferences;;978-0-7695-4827-2
ieee_20221205_08_32_33;Mastiff: A MapReduce-based System for Time-Based Big Data Analytics;Existing MapReduce-based warehousing systems are not specially optimized for time-based big data analysis applications. Such applications have two characteristics: 1) data are continuously generated and are required to be stored persistently for a long period of time, 2) applications usually process data in some time period so that typical queries use time-related predicates. Time-based big data analytics requires both high data loading speed and high query execution performance. However, existing systems including current MapReduce-based solutions do not solve this problem well because the two requirements are contradictory. We have implemented a MapReduce-based system, called Mastiff, which provides a solution to achieve both high data loading speed and high query performance. Mastiff exploits a systematic combination of a column group store structure and a lightweight helper structure. Furthermore, Mastiff uses an optimized table scan method and a column-based query execution engine to boost query performance. Based on extensive experiments results with diverse workloads, we will show that Mastiff can significantly outperform existing systems including Hive, HadoopDB, and GridSQL.;2012;Sijie Guo;10.1109/CLUSTER.2012.10;Conferences;2168-9253;978-1-4673-2422-9
ieee_20221205_08_32_33;Asian Information HUB Project: NICT's R&D Vsion and Strategies for Universal Communication Technology in the Big Data Era;The Universal Communications Research Institute (UCRI), NICT conducts research and development on universal communication technologies: multi-lingual machine translation, spoken dialogue, information analysis and ultra-realistic interaction technologies, through which people can truly interconnect, anytime, anywhere, about any topic, and by any method, transcending the boundaries of language, culture, ability and distance. To realizing universal communication, UCRI collects diverse information including huge volumes of web pages focusing on information from Asia. This paper introduces NICT's vision and strategies for Asian information hub as a platform for collecting, storing, analyzing large-scale information and providing advanced communication services in Big Data Era.;2012;Michiaki Iwazume;10.1109/COMPSACW.2012.11;Conferences;;978-0-7695-4758-9
ieee_20221205_08_32_33;A Big-Data Perspective on AI: Newton, Merton, and Analytics Intelligence;"The flood of big data in cyberspace will require immediate actions from the AI and intelligent systems community to address how we manage knowledge. Besides new methods and systems, we need a total knowledge-management approach that willl require a new perspective on AI. We need ""Merton's systems"" in which machine intelligence and human intelligence work in tandem. This should become a normal mode of operation for the next generation of AI and intelligent systems.";2012;Fei-Yue Wang;10.1109/MIS.2012.91;Magazines;1941-1294;
ieee_20221205_08_32_33;The larging-up of big data;The term 'Big Data' has been getting big much exposure in IT circles over the last year or two, on a scale that is bound to cause seasoned industry-watchers to sniff the air for the familiar aroma of industry hyperbole. There is the customary amount of hype, of course, but there is more to it than the covert repackaging and repurposing of existing products.;2012;Martin Courtney;10.1049/et.2012.0814;Magazines;1750-9637;
ieee_20221205_08_32_33;A three-dimensional display for big data sets;Facing with high dimensional information in fields of Science, Technology and Commerce, users need effective visualization tools to find more useful information. For big data sets, it is very difficult to get useful information because the dimension is too large for a practical solution. This paper proposes a 3-D visualization method for big data sets. First of all, we employed the K-means clustering method to get the basic vectors. Then, we use these vectors to construct the reduction mapping. Finally, we get the three dimensional display for a sample point. To verify the feasibility of this method, we perform experiment on some well-known databases such as iris, wine and a large data set: Pendigits. The results are favorable. According to the 3-D display results, we can also get messages like classification, outliers, and classification level when given the level standards.;2012;Cheng-Long Ma;10.1109/ICMLC.2012.6359594;Conferences;2160-133X;978-1-4673-1486-2
ieee_20221205_08_32_33;Bias Correction in a Small Sample from Big Data;"This paper discusses the bias problem when estimating the population size of big data such as online social networks (OSN) using uniform random sampling and simple random walk. Unlike the traditional estimation problem where the sample size is not very small relative to the data size, in big data, a small sample relative to the data size is already very large and costly to obtain. We point out that when small samples are used, there is a bias that is no longer negligible. This paper shows analytically that the relative bias can be approximated by the reciprocal of the number of collisions; thereby, a bias correction estimator is introduced. The result is further supported by both simulation studies and the real Twitter network that contains 41.7 million nodes.";2013;Jianguo Lu;10.1109/TKDE.2012.220;Journals;2326-3865;
ieee_20221205_08_32_33;Big Data Challenges: A Program Optimization Perspective;Big Data is characterized by the increasing volume (of the order of zeta bytes) and velocity of data generation. It is projected that the market size of Big Data shall climb up to $53.7 billion by 2017 from the current market size of $5.1 billion. Big Data in conjunction with emerging applications such as RMS applications and others has sown the seeds of exascale computing. In a similar vein, In [12], Sexton argued that applications from domains such as materials science, energy, environment and life sciences will require exascale computing. Recent studies directed towards challenges in building exascale systems and charting the roadmap of exascale computing conjecture that exascale systems would support 10-to 100-way concurrency per core and hundreds of cores per die. In [15], HPC Advisory Council predicts that the first exaflop system will be built between 2018 -- 2020. In this paper present a program optimization perspective to the challenges posed by Big Data.;2012;Arun Kejariwal;10.1109/CGC.2012.17;Conferences;;978-0-7695-4864-7
ieee_20221205_08_32_33;Ontology-Based Temporal Relation Modeling with MapReduce Latent Dirichlet Allocations for Big EHR Data;In this paper, we propose a model called Temporal & Co reference Topic Modeling (TCTM) to do automatic annotation with respect to the Time Event Ontology (TEO) for the big-size Electronic Health Record (EHR). TCTM, based on Latent Dirichlet Allocations (LDA) and integrated into MapReduce framework, inherently addresses the twin problem of data sparseness and high dimensionality. As a non-parametric Bayesian model, it can flexibly add new attributes or features. Side information associated with corpora, such as section header, timestamp, sentence distance, event distance or disease category in clinical notes makes latent topics more interpretable and more biased toward co referring events. Furthermore, TCTM integrates Hidden Markov Model LDA (HMM-LDA) to obtain the power of both sequential modeling and exchangeability. A MapReduce based variational method is employed to do parameter estimation and inferences, thus enabling TCTM to overcome the bottleneck brought by big data.;2012;Dingcheng Li;10.1109/CGC.2012.112;Conferences;;978-0-7695-4864-7
ieee_20221205_08_32_33;Beyond Simple Integration of RDBMS and MapReduce -- Paving the Way toward a Unified System for Big Data Analytics: Vision and Progress;MapReduce has shown vigorous vitality and penetrated both academia and industry in recent years. MapReduce not only can be used as an ETL tool, it can do even much more. The technique has been applied to SQL summation, OLAP, data mining, machine learning, information retrieval, multimedia data processing, science data processing etc. Basically MapReduce is a general purpose parallel computing framework for large dataset processing. A big data analytics ecosystem built around MapReduce is emerging alongside the traditional one built around RDBMS. The objectives of RDBMS and MapReduce, as well as the ecosystems built around them, overlap much really, in some sense they do the same thing and MapReduce can accomplish more works, such as graph processing, which RDBMS can not handle well. RBDMS enjoys high performance of relational data processing, which MapReduce needs to catch up. The authors envision that the two techniques are fusing into a unified system for big data analytics. With the ongoing endeavor to build up the system, much of the groundwork has been laid while some critical issues are still unresolved, we try to identify some of them. Two of our works as well as experiment results are presented, one is applying a hierarchical encoding to star schema data in Hadoop for high performance of OLAP processing, another is leveraging the natural three copies of HDFS blocks to exploit different data layouts to speed up queries in a OLAP workload, a cost model is used to route user queries to different data layouts.;2012;Xiongpai Qin;10.1109/CGC.2012.39;Conferences;;978-0-7695-4864-7
ieee_20221205_08_32_33;A Big Data Model Supporting Information Recommendation in Social Networks;As information systems are becoming sophisticated and mobile, cloud computing, social networking services are now very popular to people, the amount of data is rapidly increasing every year. Big data is data which should be analyzed by a company or an organization, but has not been tried to be analyzed or could not have been processed by current technology. In this paper, we introduce a big data model for recommender systems using social network data. The model incorporates factors related to social networks and can be applied to information recommendation with respect to various social behaviors that can increase the reliability of the recommended information. The big data model has the flexibility to be expanded to incorporate more sophisticated additional factors if needed. The experimental results using it in information recommendation and using map-reduce to process it show that it is a feasible model to be used for information recommendation.;2012;Xiaoyue Han;10.1109/CGC.2012.125;Conferences;;978-0-7695-4864-7
ieee_20221205_08_32_33;Big Data analytics;In this paper, we explain the concept, characteristics & need of Big Data & different offerings available in the market to explore unstructured large data. This paper covers Big Data adoption trends, entry & exit criteria for the vendor and product selection, best practices, customer success story, benefits of Big Data analytics, summary and conclusion. Our analysis illustrates that the Big Data analytics is a fast-growing, influential practice and a key enabler for the social business. The insights gained from the user generated online contents and collaboration with customers is critical for success in the age of social media.;2012;Sachchidanand Singh;10.1109/ICCICT.2012.6398180;Conferences;;978-1-4577-2076-5
ieee_20221205_08_32_33;Agile visual analytics for banking cyber “big data”;This paper describes the rapid development of a tailored cyber situational awareness and analysis application for the 2012 IEEE VAST Mini-Challenge 1 (MC1) — Cyber Situation Awareness. The novel aspect of this project was in the process of developing the tailored solution for a “big data” application. Aperture is an open, adaptable, and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer-based approach to visualization assembly, and a data mapping API that simplifies the process of transformation of data or analytic results into visual forms and properties.;2012;David Jonker;10.1109/VAST.2012.6400507;Conferences;;978-1-4673-4752-5
ieee_20221205_08_32_33;Big data exploration through visual analytics;SAS® Visual Analytics Explorer is an advanced data visualization and exploratory data analysis application that is a component of the SAS Visual Analytics solution. It excels at handling big data problems like the VAST challenge. With a wide range of visual analytics features and the ability to scale to massive datasets, SAS Visual Analytics Explorer enables analysts to find patt er n s and relationships quickly and easily, no matter the size of their data. In this summary paper, we explain how we used SAS Visual Analytics Explorer to solve the VAST Challenge 2012 minichallenge 1.;2012;Nascif A. Abousalh-Neto;10.1109/VAST.2012.6400514;Conferences;;978-1-4673-4752-5
ieee_20221205_08_32_33;Towards HPC for the digital Humanities, Arts, and Social Sciences: Needs and challenges of adapting academic HPC for big data;This paper examines the needs of emerging applications of High Performance Computing by the Humanities, Arts, and Social Sciences (HASS) disciplines and presents a vision for how the current academic HPC environment could be adapted to better serve this new class of “big data” research.;2012;Kalev H. Leetaru;10.1109/eScience.2012.6404439;Conferences;;978-1-4673-4465-4
ieee_20221205_08_32_33;Distributed Big Advertiser Data Mining;"Advertisers and big data mining experts alike are today are dealing with complex datasets of increasing variety (first and third party data), volume (events, impressions, clicks), and velocity (real time bidding). Creating predictive models to customize advertiser requirements and campaign analytics to show targeted ads to users who are most likely to convert has become increasingly challenging. Advertisers often group customers into a segment defined by a given set of demographic or behavioral attributes. Such segments are often very sparse. ""Look-Alike Modeling"" enables advertisers to enhance the target segment by using predictive models to expand the segment membership by assigning a probability score to users that did not explicitly belong to that segment based on the original segment definition. In this paper accompanied by the demo of a distributed platform, we describe a Look-Alike Modeling framework to expand segment membership using a novel high-dimensional distributed algorithm based on frequent pattern mining. We describe how the distributed algorithm is more efficient than traditional classification techniques that (a) require multiple passes over the dataset and (b) require both positive and negative class labels for training. Our solution is capable of concurrently and continuously processing thousands of segments and includes an efficient grouping operator and a distributed scoring algorithm for predicting multiple segment membership for a given (very large) set of users. This leverages the power of in-database analytics as compared to using standard data mining libraries and is currently deployed on a real-world highly scalable distributed columnar database that powers several hundred campaigns and processes look-alike models for large online display advertisers. The results from the study demonstrate that the proposed algorithm outperforms other comparable techniques for predicting and expanding segments.";2012;Ashish Bindra;10.1109/ICDMW.2012.73;Conferences;2375-9259;978-0-7695-4925-5
ieee_20221205_08_32_33;Driving big data with big compute;Big Data (as embodied by Hadoop clusters) and Big Compute (as embodied by MPI clusters) provide unique capabilities for storing and processing large volumes of data. Hadoop clusters make distributed computing readily accessible to the Java community and MPI clusters provide high parallel efficiency for compute intensive workloads. Bringing the big data and big compute communities together is an active area of research. The LLGrid team has developed and deployed a number of technologies that aim to provide the best of both worlds. LLGrid MapReduce allows the map/reduce parallel programming model to be used quickly and efficiently in any language on any compute cluster. D4M (Dynamic Distributed Dimensional Data Model) provided a high level distributed arrays interface to the Apache Accumulo database. The accessibility of these technologies is assessed by measuring the effort to use these tools and is typically a few lines of code. The performance is assessed by measuring the insert rate into the Accumulo database. Using these tools a database insert rate of 4M inserts/second has been achieved on an 8 node cluster.;2012;Chansup Byun;10.1109/HPEC.2012.6408678;Conferences;;978-1-4673-1575-3
ieee_20221205_08_32_33;ZIP-IO: Architecture for application-specific compression of Big Data;We have entered the “Big Data” age: scaling of networks and sensors has led to exponentially increasing amounts of data. Compression is an effective way to deal with many of these large data sets, and application-specific compression algorithms have become popular in problems with large working sets. Unfortunately, these compression algorithms are often computationally difficult and can result in application-level slow-down when implemented in software. To address this issue, we investigate ZIP-IO, a framework for FPGA-accelerated compression. Using this system we demonstrate that an unmodified industrial software workload can be accelerated 3x while simultaneously achieving more than 1000x compression in its data set.;2012;Sang Woo Jun;10.1109/FPT.2012.6412159;Conferences;;978-1-4673-2844-9
ieee_20221205_08_32_33;An instances placement algorithm based on disk I/O load for big data in private cloud;In generally, large companies or organizations have the demand of big data processing and they do not want to entrust their business processes and data to third parties (Amazon, Google, etc.). The private cloud could meet their needs. In private cloud, tasks run at multiple instances (also known as virtual machines), which could be paced in different physical nodes. Obviously, the instances which be used to process big data need higher CPU and disk performance than other kinds of instances. If the instances of disk resource consuming are placed in the same physical node, clearly, the disk I/O bandwidth would be used up quickly that would affect the performance of the entire node seriously. This paper proposes an instances placement algorithm FFDL that based on disk I/O for private cloud environment to deal with big data that would adopt the disk I/O load balancing strategy and reduce competition for the disk I/O load between instances. We have validated our approach by conducting a performance evaluation study on the open source private cloud platform—Openstack. The results demonstrate that our algorithm has immense potential as it offers significant computation time savings than the Greedy algorithm and demonstrates high potential for the improvement of disk I/O load balancing in the entire private cloud system for the big data.;2012;Jian Guo;10.1109/ICWAMTIP.2012.6413495;Conferences;;978-1-4673-4683-2
ieee_20221205_08_32_33;Finding the Needle in the Big Data Systems Haystack;"With the increasing importance of big data, many new systems have been developed to ""solve"" the big data challenge. At the same time, famous database researchers argue that there is nothing new about these systems and that they're actually a step backward. This article sheds some light on this discussion.";2013;Tim Kraska;10.1109/MIC.2013.10;Magazines;1941-0131;
ieee_20221205_08_32_33;Asynchronous Index Strategy for high performance real-time big data stream storage;Big data insert-intensive applications challenge traditional RDBMS. Key-Value databases achieve the same throughput with much more price/performance ratio, which makes them popular recent years. However, Key-Value databases are not suitable for high performance real-time applications. In this paper we introduce Asynchronous Index Strategy as a high performance solution for insert-intensive time series big data storage. It takes advantage of partial replication and asynchronous indexes, which results in zero overhead for index updates. Furthermore, a general middle-ware for clustering databases based on Asynchronous Index Strategy is implemented. Finally, indexing and inserting performance experiments highlight the efficiency of Asynchronous Index Strategy. As for AIS based on MongoDB, it achieves a throughput that is 17 times of MongoDB sharding cluster.;2012;Xiao Mo;10.1109/ICNIDC.2012.6418750;Conferences;2374-0272;978-1-4673-2203-4
ieee_20221205_08_32_33;Grand Challenge: Applying Regulatory Science and Big Data to Improve Medical Device Innovation;Understanding how proposed medical devices will interface with humans is a major challenge that impacts both the design of innovative new devices and approval and regulation of existing devices. Today, designing and manufacturing medical devices requires extensive and expensive product cycles. Bench tests and other preliminary analyses are used to understand the range of anatomical conditions, and animal and clinical trials are used to understand the impact of design decisions upon actual device success. Unfortunately, some scenarios are impossible to replicate on the bench, and competitive pressures often accelerate initiation of animal trials without sufficient understanding of parameter selections. We believe that these limitations can be overcome through advancements in data-driven and simulation-based medical device design and manufacturing, a research topic that draws upon and combines emerging work in the areas of Regulatory Science and Big Data. We propose a cross-disciplinary grand challenge to develop and holistically apply new thinking and techniques in these areas to medical devices in order to improve and accelerate medical device innovation.;2013;Arthur G. Erdman;10.1109/TBME.2013.2244600;Journals;1558-2531;
ieee_20221205_08_32_33;Puzzling out big data [Information Technology Analytics];"Big data comes in many forms. It comes as customer information and transactions contained in customer-relationship management and enterprise resourceplanning systems and HTML-based web stores. It comes as information generated by machine-to-machine applications collecting data from smart meters, manufacturing sensors, equipment logs, trading systems data and call detail records compiled by fixed and mobile telecommunications companies. Big data can come with big differences. Some say that the `three Vs' of big data should more properly be tagged as the `three HVs': high-volume, high-variety, high-velocity, and high-veracity. Apply those tags to the mountains of information posted on social network and blogging sites, including Facebook, Twitter and VouTube; the deluge of text contained in email and instant messages; not to mention audio and video files. It is evident then that it's not necessarily the 'big-ness' of information that presents big-data applications and services with their greatest challenge, but the variety and the speed at which all that constantly changing information must be ingested, processed, aggregated, filtered, organised and fed back in a meaningful way for businesses to get some value out of it.";2013;Martin Courtney;;Magazines;1750-9637;
ieee_20221205_08_32_33;Addressing Big Data challenges for Scientific Data Infrastructure;This paper discusses the challenges that are imposed by Big Data Science on the modern and future Scientific Data Infrastructure (SDI). The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model.;2012;Yuri Demchenko;10.1109/CloudCom.2012.6427494;Conferences;;978-1-4673-4509-5
ieee_20221205_08_32_33;Big Data Processing in Cloud Computing Environments;With the rapid growth of emerging applications like social network analysis, semantic Web analysis and bioinformatics network analysis, a variety of data to be processed continues to witness a quick increase. Effective management and analysis of large-scale data poses an interesting but critical challenge. Recently, big data has attracted a lot of attention from academia, industry as well as government. This paper introduces several big data processing technics from system and application aspects. First, from the view of cloud data management and big data processing mechanisms, we present the key issues of big data processing, including cloud computing platform, cloud architecture, cloud database and data storage scheme. Following the Map Reduce parallel processing framework, we then introduce Map Reduce optimization strategies and applications reported in the literature. Finally, we discuss the open issues and challenges, and deeply explore the research directions in the future on big data processing in cloud computing environments.;2012;Changqing Ji;10.1109/I-SPAN.2012.9;Conferences;2375-527X;978-0-7695-4930-9
ieee_20221205_08_32_33;Robust Decision Engineering: Collaborative Big Data and its application to international development/aid;"Much of the research that goes into Big Data, and specifically on Collaborative Big Data, is focused upon questions, such as: how to get more of it? (e.g., · participatory mechanisms, social media, geo-coded data from personal electronic devices) and · how to handle it? (e.g., how to ingest, sort, store, and link up disparate data sets). A question that receives far less attention is that of Collaborative analysis of Big Data; how can a multi-disciplinary layered analysis of Big Data be used to support robust decisions, especially in a collaborative setting, and especially under time pressure? The robust Decision Engineering required can be achieved by employing an approach related to Network Science, that we call Relationship Science. In Relationship Science, our methodological framework, karassian netchain analysis (KNA), is utilized to ascertain islands of stability or positive influence dominating sets (PIDS), so that a form of annealed resiliency or latent stability is achieved, thereby mitigating against unintended consequences, elements of instability, and “perfect storm” crises lurking within the network.";2012;Steve Chan;10.4108/icst.collaboratecom.2012.250715;Conferences;;978-1-4673-2740-4
ieee_20221205_08_32_33;T*: A data-centric cooling energy costs reduction approach for Big Data analytics cloud;Explosion in Big Data has led to a surge in extremely large-scale Big Data analytics platforms, resulting in burgeoning energy costs. Big Data compute model mandates strong data-locality for computational performance, and moves computations to data. State-of-the-art cooling energy management techniques rely on thermal-aware computational job placement/migration and are inherently data-placement-agnostic in nature. T* takes a novel, data-centric approach to reduce cooling energy costs and to ensure thermal-reliability of the servers. T* is cognizant of the uneven thermal-profile and differences in thermal-reliability-driven load thresholds of the servers, and the differences in the computational jobs arrival rate, size, and evolution life spans of the Big Data placed in the cluster. Based on this knowledge, and coupled with its predictive file models and insights, T* does proactive, thermal-aware file placement, which implicitly results in thermal-aware job placement in the Big Data analytics compute model. Evaluation results with one-month long real-world Big Data analytics production traces from Yahoo! show up to 42% reduction in the cooling energy costs with T* courtesy of its lower and more uniform thermal-profile and 9x better performance than the state-of-the-art data-agnostic cooling techniques.;2012;Rini T. Kaushik;10.1109/SC.2012.103;Conferences;2167-4329;978-1-4673-0804-5
ieee_20221205_08_32_33;Introduction to Big Data: Scalable Representation and Analytics for Data Science Minitrack;Big data is an emerging phenomenon characterized by the three Vs: volume, velocity, and variety. The volume of data has increased from terabytes to petabytes and is encroaching on exabytes. Some pundits are suggesting that zettabytes (1021) are reachable within the next several years. Velocity is concerned with not only how fast we accumulate data, but also how fast some of the data that we already have is changing. Some systems accumulate data at the rate of multiple petabytes per year, some systems have stored data that changes at the rate of terabytes per year. Changing data usually lags accumulating data by several orders of magnitude. Data accumulating at a multiple petabyte rate requires terabits to petabits of transport capacity. Finally, the variety and modality of data is continually evolving, it may be both structured and unstructured.;2013;Stephen Kaisler;10.1109/HICSS.2013.292;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_32_33;An Agent Model for Incremental Rough Set-Based Rule Induction: A Big Data Analysis in Sales Promotion;Rough set-based rule induction is able to generate decision rules from a database and has mechanisms to handle noise and uncertainty in data. This technique facilitates managerial decision-making and strategy formulation. However, the process for RS-based rule induction is complex and computationally intensive. Moreover, operational databases that are used to run the day-to-day operations, thus large volumes of data are continually updated within a short period of time. The infrastructure required to analyze such large amounts of data must be able to handle extreme data volumes, to allow fast response times, and to automate decisions based on analytical models. This study proposes an Incremental Rough Set-based Rule Induction Agent (IRSRIA). Rule induction is based on creating agents for the main modeling processes. In addition, an incremental architecture is designed, to address large-scale dynamic database problems. A case study of a Home shopping company is used to show the validity and efficiency of this method. The results of experiments show that the IRSRIA can considerably reduce the computation time for inducing decision rules, while maintaining the same quality of rules.;2013;Yu-Neng Fan;10.1109/HICSS.2013.79;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_32_33;Big Data: Issues and Challenges Moving Forward;"Big data refers to data volumes in the range of exabytes (1018) and beyond. Such volumes exceed the capacity of current on-line storage systems and processing systems. Data, information, and knowledge are being created and collected at a rate that is rapidly approaching the exabyte/year range. But, its creation and aggregation are accelerating and will approach the zettabyte/year range within a few years. Volume is only one aspect of big data; other attributes are variety, velocity, value, and complexity. Storage and data transport are technology issues, which seem to be solvable in the near-term, but represent longterm challenges that require research and new paradigms. We analyze the issues and challenges as we begin a collaborative research program into methodologies for big data analysis and design.";2013;Stephen Kaisler;10.1109/HICSS.2013.645;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_32_33;Introduction to Predictive Analytics and Big Data Minitrack;Introduction to Predictive Analytics and Big Data Minitrack.;2013;Dursun Delen;10.1109/HICSS.2013.322;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_32_33;Introduction to Business Analytics, Business Intelligence, and Big Data Minitrack;Introduction to the Business Analytics, Business Intelligence and Big Data Minitrack;2013;Robert Winter;10.1109/HICSS.2013.334;Conferences;1530-1605;978-0-7695-4892-0
ieee_20221205_08_32_33;Danger Theory: A new approach in big data analysis;Danger Theory is a novel computing model inspired by biological immune systems which is some different with the traditional Artificial Immune Systems, especially the self non-self model. The Danger Theory concerns the potential dangers (which presents like the danger signals) rather than non-self pathogens, so the new computing model introduced into information analysis can take a new approach for big data processing on key features and properties choosing. The authors introduce some works on Danger Theory about the definition of danger, and the capture of danger signals, which could be helpful to increase the abilities of self-learning and intelligence in different applications.;2012;Lin Lu;10.1049/cp.2012.1083;Conferences;;978-1-84919-537-9
ieee_20221205_08_32_33;Addressing big data problem using Hadoop and Map Reduce;The size of the databases used in today's enterprises has been growing at exponential rates day by day. Simultaneously, the need to process and analyze the large volumes of data for business decision making has also increased. In several business and scientific applications, there is a need to process terabytes of data in efficient manner on daily bases. This has contributed to the big data problem faced by the industry due to the inability of conventional database systems and software tools to manage or process the big data sets within tolerable time limits. Processing of data can include various operations depending on usage like culling, tagging, highlighting, indexing, searching, faceting, etc operations. It is not possible for single or few machines to store or process this huge amount of data in a finite time period. This paper reports the experimental work on big data problem and its optimal solution using Hadoop cluster, Hadoop Distributed File System (HDFS) for storage and using parallel processing to process large data sets using Map Reduce programming framework. We have done prototype implementation of Hadoop cluster, HDFS storage and Map Reduce framework for processing large data sets by considering prototype of big data application scenarios. The results obtained from various experiments indicate favorable results of above approach to address big data problem.;2012;Aditya B. Patel;10.1109/NUICONE.2012.6493198;Conferences;2375-1282;978-1-4673-1718-4
ieee_20221205_08_32_33;Ethics of Big Data;This column discusses the book Ethics of Big Data by Kord Davis with Doug Patterson.;2013;Richard Mateosian;10.1109/MM.2013.35;Magazines;1937-4143;
ieee_20221205_08_32_33;Ubiquitous Analytics: Interacting with Big Data Anywhere, Anytime;Ubilytics amplifies human cognition by embedding analytical processes into the physical environment to make sense of big data anywhere, anytime.;2013;Niklas Elmqvist;10.1109/MC.2013.147;Magazines;1558-0814;
ieee_20221205_08_32_33;Journey to the centre of big data;In a general context big data is an aggregation of data sets that are so large and complex that it becomes difficult to process using readily available database management tools or traditional data processing applications. This challenge also contains an opportunity for commercial organisations that are equipped to find ways to use it - or elements of it - to inform and enhance revenue drivers (see 'Puzzling out big data', E&T Vol 7 Issue 12). The term 'big data' is partly a recognition of the growing relative weight and importance of unstructured data not amenable to conventional database analytics and reporting tools and techniques. Above all, though, it embodies an ambition to extract value from data, particularly for sales, marketing, and customer relations.;2013;Philip Hunter;10.1049/et.2013.0307;Magazines;1750-9637;
ieee_20221205_08_32_33;Abstract: Networking Research Activities at Fermilab for Big Data Analysis;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center. It must deal with both scaling and wide-area distribution challenges in processing its CMS data. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;P. DeMar;10.1109/SC.Companion.2012.214;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Poster: Big Data Networking at Fermilab;Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R&D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R&D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement.;2012;Phillip J. Demar;10.1109/SC.Companion.2012.215;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Abstract: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.229;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Poster: Cascaded TCP: BIG Throughput for BIG DATA Applications in Distributed HPC;Saturating high capacity and high latency paths is a challenge with vanilla TCP implementations. This is primarily due to congestion-control algorithms which adapt window sizes when acknowledgements are received. With large latencies, the congestion-control algorithms have to wait longer to respond to network conditions (e.g., congestion), and thus result in less aggregate throughput. We argue that throughput can be improved if we reduce the impact of large end-to-end latencies by introducing layer-4 relays along the path. Such relays would enable a cascade of TCP connections, each with lower latency, resulting in better aggregate throughput. This would directly benefit typical applications as well as BIG DATA applications in distributed HPC. We present empirical results supporting our hypothesis.;2012;Umar Kalim;10.1109/SC.Companion.2012.230;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Abstract: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;A. Klimentov;10.1109/SC.Companion.2012.301;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Poster: PanDA: Next Generation Workload Management and Analysis System for Big Data;In real world any big science project implies to use a sophisticated Workload Management System (WMS) that deals with a huge amount of highly distributed data, which is often accessed by large collaborations. The Production and Distributed Analysis System (PanDA) is a high-performance WMS that is aimed to meet production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider data processing scale. PanDA provides execution environments for a wide range of experimental applications, automates centralized data production and processing, enables analysis activity of physics groups, supports custom workflow of individual physicists, provides a unified view of distributed worldwide resources, presents status and history of workflow through an integrated monitoring system, archives and curates all workflow. PanDA is now being generalized and packaged, as a WMS already proven at extreme scales, for the wider use of the Big Data community.;2012;K. De;10.1109/SC.Companion.2012.302;Conferences;;978-1-4673-6218-4
ieee_20221205_08_32_33;Feasibility considerations of multipath TCP in dealing with big data application;In this paper we present an idea to handle big data in a better way. We consider Multipath TCP (MPTCP) to use all available paths simultaneously. MPTCP is a tailored form of TCP which aspire to improve throughput by sharing available resources smartly and fairly. The key concentration of this paper is to analyze the benefit of MPTCP over single path TCP for bandwidth and time sensitive applications as well as big data application. Our simulation shows that MPTCP can higher goodput by bandwidth aggregation, Couple Congestion Control(CCC) provide better throughput without being unfair to other legacy TCP flows and also portray that large receive buffer causes performance enhancement for relatively large RTT link. As a result, MPTCP can be a enormous addition in contrast with single path TCP in dealing with big data application.;2013;Zia Ush Shamszaman;10.1109/ICOIN.2013.6496714;Conferences;2332-5658;978-1-4673-5741-8
ieee_20221205_08_32_33;Customizing Computational Methods for Visual Analytics with Big Data;The volume of available data has been growing exponentially, increasing data problem's complexity and obscurity. In response, visual analytics (VA) has gained attention, yet its solutions haven't scaled well for big data. Computational methods can improve VA's scalability by giving users compact, meaningful information about the input data. However, the significant computation time these methods require hinders real-time interactive visualization of big data. By addressing crucial discrepancies between these methods and VA regarding precision and convergence, researchers have proposed ways to customize them for VA. These approaches, which include low-precision computation and iteration-level interactive visualization, ensure real-time interactive VA for big data.;2013;Jaegul Choo;10.1109/MCG.2013.39;Magazines;1558-1756;
ieee_20221205_08_32_33;Shared disk big data analytics with Apache Hadoop;Big Data is a term applied to data sets whose size is beyond the ability of traditional software technologies to capture, store, manage and process within a tolerable elapsed time. The popular assumption around Big Data analytics is that it requires internet scale scalability: over hundreds of compute nodes with attached storage. In this paper., we debate on the need of a massively scalable distributed computing platform for Big Data analytics in traditional businesses. For organizations which don't need a horizontal., internet order scalability in their analytics platform., Big Data analytics can be built on top of a traditional POSIX Cluster File Systems employing a shared storage model. In this study., we compared a widely used clustered file system: VERITAS Cluster File System (SF-CFS) with Hadoop Distributed File System (HDFS) using popular Map-reduce benchmarks like Terasort., DFS-IO and Gridmix on top of Apache Hadoop. In our experiments VxCFS could not only match the performance of HDFS., but also outperformed in many cases. This way., enterprises can fulfill their Big Data analytics need with a traditional and existing shared storage model without migrating to a different storage model in their data centers. This also includes other benefits like stability & robustness., a rich set of features and compatibility with traditional analytics applications.;2012;Anirban Mukherjee;10.1109/HiPC.2012.6507520;Conferences;;978-1-4673-2370-3
ieee_20221205_08_32_33;Big Data Conferences, Here We Come!;Conferences on big data are starting to appear this year. What can we expect from a big Data conference? As researchers, should we plan to attend, and if yes, what should we submit?;2013;Peter Mika;10.1109/MIC.2013.45;Magazines;1941-0131;
ieee_20221205_08_32_33;Transforming Big Data into Collective Awareness;"Integrating social and sensor networks can transform big data, if treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems. The Web extra at http://youtu.be/xSoAvMNZSL8 is a video in which author Jeremy Pitt expands on his article ""Transforming Big Data into Collective Awareness"" and discusses how integrating social and sensor networks can transform big data, if it's treated as a knowledge commons, into a higher form of collective awareness that can motivate users to self-organize and create innovative solutions to various socioeconomic problems.";2013;Jeremy Pitt;10.1109/MC.2013.153;Magazines;1558-0814;
ieee_20221205_08_32_33;A big data implementation based on Grid computing;Big Data is a term defining data that has three main characteristics. First, it involves a great volume of data. Second, the data cannot be structured into regular database tables and third, the data is produced with great velocity and must be captured and processed rapidly. Oracle adds a fourth characteristic for this kind of data and that is low value density, meaning that sometimes there is a very big volume of data to process before finding valuable needed information. Big Data is a relatively new term that came from the need of big companies like Yahoo, Google, Facebook to analyze big amounts of unstructured data, but this need could be identified in a number of other big enterprises as well in the research and development field. The framework for processing Big Data consists of a number of software tools that will be presented in the paper, and briefly listed here. There is Hadoop, an open source platform that consists of the Hadoop kernel, Hadoop Distributed File System (HDFS), MapReduce and several related instruments. Two of the main problems that occur when studying Big Data are the storage capacity and the processing power. That is the area where using Grid Technologies can provide help. Grid Computing refers to a special kind of distributed computing. A Grid computing system must contain a Computing Element (CE), and a number of Storage Elements (SE) and Worker Nodes (WN). The CE provides the connection with other GRID networks and uses a Workload Management System to dispatch jobs on the Worker Nodes. The Storage Element is in charge with the storage of the input and the output of the data needed for the job execution. The main purpose of this article is to present a way of processing Big Data using Grid Technologies. For that, the framework for managing Big Data will be presented along with the way to implement it around a grid architecture.;2013;Dan Garlasu;10.1109/RoEduNet.2013.6511732;Conferences;2068-1038;978-1-4673-6114-9
ieee_20221205_08_32_33;Big Data in Neonatal Intensive Care;"The effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions. The Web extra at http://youtu.be/OIQBCboQs0g is a video in which author Carolyn McGregor expands on her article ""Big Data in Neonatal Intensive Care"" and discusses how the effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions.";2013;Carolyn McGregor;10.1109/MC.2013.157;Magazines;1558-0814;
ieee_20221205_08_32_33;Governing Big Data: Principles and practices;As data-intensive decision making is being increasingly adopted by businesses, governments, and other agencies around the world, most organizations encountering a very large amount and variety of data are still contemplating and assessing their readiness to embrace “Big Data.” While these organizations devise various ways to deal with the challenges such data brings, the impact and importance of Big Data to information quality and governance programs should not be underestimated. Drawing upon implementation experiences of early adopters of Big Data technologies across multiple industries, this paper explores the issues and challenges involved in the management of Big Data, highlighting the principles and best practices for effective Big Data governance.;2013;P. Malik;10.1147/JRD.2013.2241359;Journals;0018-8646;
ieee_20221205_08_32_33;IBM Streams Processing Language: Analyzing Big Data in motion;The IBM Streams Processing Language (SPL) is the programming language for IBM InfoSphere® Streams, a platform for analyzing Big Data in motion. By “Big Data in motion,” we mean continuous data streams at high data-transfer rates. InfoSphere Streams processes such data with both high throughput and short response times. To meet these performance demands, it deploys each application on a cluster of commodity servers. SPL abstracts away the complexity of the distributed system, instead exposing a simple graph-of-operators view to the user. SPL has several innovations relative to prior streaming languages. For performance and code reuse, SPL provides a code-generation interface to C++ and Java®. To facilitate writing well-structured and concise applications, SPL provides higher-order composite operators that modularize stream sub-graphs. Finally, to enable static checking while exposing optimization opportunities, SPL provides a strong type system and user-defined operator models. This paper provides a language overview, describes the implementation including optimizations such as fusion, and explains the rationale behind the language design.;2013;M. Hirzel;10.1147/JRD.2013.2243535;Journals;0018-8646;
ieee_20221205_08_32_33;Big Data text-oriented benchmark creation for Hadoop;Massive-scale Big Data analytics is representative of a new class of workloads that justifies a rethinking of how computing systems should be optimized. This paper addresses the need for a set of benchmarks that system designers can use to measure the quality of their designs and that customers can use to evaluate competing systems offerings with respect to commonly performed text-oriented workflows in Hadoop™. Additions are needed to existing benchmarks such as HiBench in terms of both scale and relevance. We describe a methodology for creating a petascale data-size text-oriented benchmark that includes representative Big Data workflows and can be used to test total system performance, with demands balanced across storage, network, and computation. Creating such a benchmark requires meeting unique challenges associated with the data size and its often unstructured nature. To be useful, the benchmark also needs to be sufficiently generic to be accepted by the community at large. Here, we focus on a text-oriented Hadoop workflow that consists of three common tasks: categorizing text documents, identifying significant documents within each category, and analyzing significant documents for new topic creation.;2013;A. Gattiker;10.1147/JRD.2013.2240732;Journals;0018-8646;
ieee_20221205_08_32_33;GPFS-SNC: An enterprise cluster file system for Big Data;A new class of data-intensive applications commonly referred to as Big Data applications (e.g., customer sentiment analysis based on click-stream logs) involves processing massive amounts of data with a focus on semantically transforming the data. This class of applications is massively parallel and well suited for the MapReduce programming framework that allows users to perform large-scale data analyses such that the application execution layer handles the system architecture, data partitioning, and task scheduling. In this paper, we introduce GPFS-SNC (General Parallel File System for Shared Nothing Clusters), a scalable file system that operates over a cluster of commodity machines and direct-attached storage and meets the requirements of analytics and traditional applications that are typically used together in analytics solutions. The architecture extends an existing enterprise cluster file system to support these emerging classes of workloads by applying five innovative optimizations: 1) locality awareness to allow compute jobs to be scheduled on nodes where the data resides, 2) metablocks that allow large and small block sizes to co-exist in the same file system to meet the needs of different types of applications, 3) write affinity that allows applications to dictate the layout of files on different nodes in order to maximize both write and read bandwidth, 4) pipelined replication to maximize use of network bandwidth for data replication, and 5) distributed recovery to minimize the effect of failures on ongoing computation.;2013;R. Jain;10.1147/JRD.2013.2243531;Journals;0018-8646;
ieee_20221205_08_32_33;Understanding system design for Big Data workloads;This paper explores the design and optimization implications for systems targeted at Big Data workloads. We confirm that these workloads differ from workloads typically run on more traditional transactional and data-warehousing systems in fundamental ways, and, therefore, a system optimized for Big Data can be expected to differ from these other systems. Rather than only studying the performance of representative computational kernels, and focusing on central-processing-unit performance, this paper studies the system as a whole. We identify three major phases in a typical Big Data workload, and we propose that each of these phases should be represented in a Big Data systems benchmark. We implemented our ideas on two distinct IBM POWER7® processor-based systems that target different market sectors, and we analyze their performance on a sort benchmark. In particular, this paper includes an evaluation of POWER7 processor-based systems using MapReduce TeraSort, which is a workload that can be a “stress test” for multiple dimensions of system performance. We combine this work with a broader perspective on Big Data workloads and suggest a direction for a future benchmark definition effort. A number of methods to further improve system performance are proposed.;2013;H. P. Hofstee;10.1147/JRD.2013.2242674;Journals;0018-8646;
ieee_20221205_08_32_33;Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost;"Finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The first Web extra at http://youtu.be/B2RlkoNjrzA is a video in which author Paul Tallon expands on his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and discusses how finding data governance practices that maintain a balance between value creation and risk exposure is the new organizational imperative for unlocking competitive advantage and maximizing value from the application of big data. The second Web extra at http://youtu.be/g0RFa4swaf4 is a video in which author Paul Tallon discusses the supplementary material to his article ""Corporate Governance of Big Data: Perspectives on Value, Risk, and Cost"" and how projection models can help individuals responsible for data handling plan for and understand big data storage issues.";2013;Paul P. Tallon;10.1109/MC.2013.155;Magazines;1558-0814;
ieee_20221205_08_32_33;Bootstrapping smart cities through a self-sustainable model based on big data flows;"We have a clear idea today about the necessity and usefulness of making cities smarter, the potential market size, and trials and tests. However, it seems that business around Smart Cities is having difficulties taking off and is thus running short of projected potentials. This article looks into why this is the case and proposes a procedure to make smart cities happen based on big data exploitation through the API stores concept. To this end, we first review involved stakeholders and the ecosystem at large. We then propose a viable approach to scale business within that ecosystem. We also describe the available ICT technologies and finally exemplify all findings by means of a sustainable smart city application. Over the course of the article, we draw two major observations, which are seen to facilitate sustainable smart city development. First, independent smart city departments (or the equivalent) need to emerge, much like today's well accepted IT departments, which clearly decouple the political element of the improved city servicing from the underlying technologies. Second, a coherent three-phase smart city rollout is vital, where in phase 1 utility and revenues are generated; in phase 2 only-utility service is also supported; and in phase 3, in addition, a fun/leisure dimension is permitted.";2013;Ignasi Vilajosana;10.1109/MCOM.2013.6525605;Magazines;1558-1896;
ieee_20221205_08_32_33;Big Data's Big Unintended Consequences;"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article ""Big Data's Big Unintended Consequences"" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.";2013;Marcus R. Wigan;10.1109/MC.2013.195;Magazines;1558-0814;
ieee_20221205_08_32_33;Big Data: New Opportunities and New Challenges [Guest editors' introduction];"We can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldn't blind ourselves to the possible irreversibility of changes-whether good or bad-to society. The first Web extra at http://youtu.be/24czULRCI9c is an audio recording in which Katina Michael at the University of Wollongong discusses the June 2013 Computer magazine special issue on ""Big Data: New Opportunities and New Challenges,"" introducing the special issue, the guest editors, the authors, the articles, and the IEEE Society on Social Implications of Technology (SSIT). The second Web extra at http://youtu.be/9zpFqEDydDA is an audio recording in which Katina Michael at the University of Wollongong talks about the IEEE Society on the Social Implications of Technology (SSIT), IEEE Technology and Society (T&S) magazine, and the International Symposium on Technology and Society (ISTAS). The third Web extra at http://youtu.be/mn_9YHV2RGQis an audio recording in which Katina Michael at the University of Wollongong discusses how we can live with many of the uncertainties of big data for now, with the hope that its benefits will outweigh its harms, but we shouldnít blind ourselves to the possible irreversibility of changes-whether good or bad-to society.";2013;Katina Michael;10.1109/MC.2013.196;Magazines;1558-0814;
ieee_20221205_08_32_33;Efficient and secure Cloud storage for handling big data;The term “Cloud” has been used historically as a metaphor for the internet. It is one of the most active application for enterprise. It has been more and more accepted by enterprises which can take advantage of low cost, fast deployment and elastic scaling. Due to demand of large volume of data processing in enterprises, huge amount of data are generated and dispersed on internet around the globe. Currently, storing the data safely and efficiently on Cloud is one of the biggest challenge in Cloud computing. There is no guarantee that data stored on Cloud is securely protected. We propose a method to build a trusted computing environment by providing a secure platform in a Cloud computing system. The proposed method allows users to store data safely and efficiently in the Cloud. It solves the problem of handling big data and security issues using encryption and compression technique while uploading data to the Cloud storage.;2012;Arjun Kumar;;Conferences;;978-1-4673-0876-2
ieee_20221205_08_32_33;Scalable single linkage hierarchical clustering for big data;"Personal computing technologies are everywhere; hence, there are an abundance of staggeringly large data sets-the Library of Congress has stored over 160 terabytes of web data and it is estimated that Facebook alone logs nearly a petabyte of data per day. Thus, there is a pertinent need for systems by which one can elucidate the similarity and dissimilarity among and between groups in these big data sets. Clustering is one way to find these groups. In this paper, we extend the scalable Visual Assessment of Tendency (sVAT) algorithm to return single-linkage partitions of big data sets. The sVAT algorithm is designed to provide visual evidence of the number of clusters in unloadable (big) data sets. The extension we describe for sVAT enables it to also then efficiently return the data partition as indicated by the visual evidence. The computational complexity and storage requirements of sVAT are (usually) significantly less than the O(n2) requirement of the classic single-linkage hierarchical algorithm. We show that sVAT is a scalable instantiation of single-linkage clustering for data sets that contain c compact-separated clusters, where c ≪ n; n is the number of objects. For data sets that do not contain compact-separated clusters, we show that sVAT produces a good approximation of single-linkage partitions. Experimental results are presented for both synthetic and real data sets.";2013;Timothy C. Havens;10.1109/ISSNIP.2013.6529823;Conferences;;978-1-4673-5500-1
ieee_20221205_08_32_33;Data stream mining to address big data problems;Today, the IT world is trying to cope with “big data” problems (data volume, velocity, variety, veracity) on the path to obtaining useful information. In this paper, we present implementation details and performance results of realizing “online” Association Rule Mining (ARM) over big data streams for the first time in the literature. Specifically, we added Apriori and FP-Growth algorithms for stream mining inside an event processing engine, called Esper. Using the system, these two algorithms were compared over LastFM social music site data and by using tumbling windows. The better-performing FP-Growth was selected and used in creation of a real-time rule-based recommendation engine. Our most important findings show that online association rule mining can generate (1) more rules, (2) much faster and more efficiently, and (3) much sooner than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as “George Harrison⇒Beatles”. We hope that our findings can shed light on the design and implementation of other big data analytics systems in the future.;2013;Erdi Ölmezoğulları;10.1109/SIU.2013.6531483;Conferences;;978-1-4673-5561-2
ieee_20221205_08_32_33;Panel: Big data for the public;"Summary form only given. While data are now being produced and collected on unprecedented scales, most of the ""big data"" remain inaccessible or difficult to use by the public.";2013;;10.1109/ICDE.2013.6544803;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_32_33;Machine learning on Big Data;Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.;2013;Tyson Condie;10.1109/ICDE.2013.6544913;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_32_33;Big data integration;The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.;2013;Xin Luna Dong;10.1109/ICDE.2013.6544914;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_32_33;Workload management for Big Data analytics;Parallel database systems and MapReduce systems are essential components of today's infrastructure for Big Data analytics. These systems process multiple concurrent workloads consisting of complex user requests, where each request is associated with an (explicit or implicit) service level objective.;2013;Ashraf Aboulnaga;10.1109/ICDE.2013.6544915;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_32_33;Very fast estimation for result and accuracy of big data analytics: The EARL system;Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets (a.k.a. `big data') can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in big data systems (e.g., Hadoop). Therefore, we propose a nonparametric accuracy estimation method and system to speedup big data analytics. Our framework is called EARL (Early Accurate Result Library) and it works by predicting the learning curve and choosing the appropriate sample size for achieving the desired error bound specified by the user. The error estimates are based on a technique called bootstrapping that has been widely used and validated by statisticians, and can be applied to arbitrary functions and data distributions. Therefore, this demo will elucidate (a) the functionality of EARL and its intuitive GUI interface whereby first-time users can appreciate the accuracy obtainable from increasing sample sizes by simply viewing the learning curve displayed by EARL, (b) the usability of EARL, whereby conference participants can interact with the system to quickly estimate the sample sizes needed to obtain the desired accuracies or response times, and then compare them against the accuracies and response times obtained in the actual computations.;2013;Nikolay Laptev;10.1109/ICDE.2013.6544928;Conferences;1063-6382;978-1-4673-4908-6
ieee_20221205_08_32_33;Towards an Optimized Big Data Processing System;Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.;2013;Bogdan Ghit;10.1109/CCGrid.2013.53;Conferences;;978-1-4673-6465-2
ieee_20221205_08_32_33;A Holistic Architecture for the Internet of Things, Sensing Services and Big Data;Wireless Sensor Networks (WSNs) increasingly enable the interaction of the physical world with services, which may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). This potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which provides a set of abstractions for the different types of sensors and services, enabling them to take advantage of Big Data and cloud technologies and which is termed holistic as it caters for the data flow from sensors through to services. The architecture has been designed for implementation on a resource constrained node and to be extensible to server environments, shown in this paper where we present a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase data store.;2013;David Tracey;10.1109/CCGrid.2013.100;Conferences;;978-1-4673-6465-2
ieee_20221205_08_32_33;Getting an Intuition for Big Data;"IEEE Software Editor-in-Chief Forrest Shull discusses the importance of building reliable systems to interpret big data. In addition, he discusses the IBM Impact 2013 Unconference; the Software Engineering Institute's SATURN 2013 conference in which the IEEE Software Architecture in Practice Award went to Simon Brown of Coding the Architecture, for his presentation titled ""The Conflict between Agile and Architecture: Myth or Reality"" and the IEEE Software New Directions Award went to Darryl Nelson of Raytheon for his presentation titled, ""Next-Gen Web Architecture for the Cloud Era."" He also welcomes Professor Rafael Prikladnicki of the Computer Science School at PUCRS, Brazil, and Chief Software Economist Walker Royce of IBM's Software Group to the IEEE Software Advisory Board. The first Web extra at http://youtu.be/JrQorWS5m6w is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Paul Zikopoulos, Director--IBM Information Management Technical Professionals, Competitive Database, and Big Data at IBM, about the potentials of mining big data. Zikopoulos will deliver a keynote at Software Experts Summit 2013 on 17 July in Redmond, Washington. The second Web extra at http://youtu.be/NHHThAeONv8 is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Catherine Plaisant and Megan Monroe of the University of Maryland Human-Computer Interaction Laboratory about big data information visualization and its applications to software development. The third Web extra at http://youtu.be/NqXE0ewoTKA is a video overview of the IBM Impact 2013 Unconference, sponsored by IEEE Software magazine, an event specifically designed for developers that featured Grady Booch and Tim O'Reilly as keynote speakers.";2013;Forrest Shull;10.1109/MS.2013.76;Magazines;1937-4194;
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.84;Magazines;1937-4194;
ieee_20221205_08_32_33;Data mining with big data;Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.;2014;Xindong Wu;10.1109/TKDE.2013.109;Journals;2326-3865;
ieee_20221205_08_32_33;Big Data Analytics: Perspective Shifting from Transactions to Ecosystems;Understanding the flow and interrelated nature of institutions and business entities' processes and exchanges helps researchers develop and apply big data analytics techniques more effectively from an ecosystem-based perspective (rather than individual transactions and components).;2013;Daniel Zeng;10.1109/MIS.2013.40;Magazines;1941-1294;
ieee_20221205_08_32_33;Artificial Intelligence and Big Data;AI Innovation in Industry is a new department for IEEE Intelligent Systems, and this paper examines some of the basic concerns and uses of AI for big data (AI has been used in several different ways to facilitate capturing and structuring big data, and it has been used to analyze big data for key insights).;2013;;10.1109/MIS.2013.39;Magazines;1941-1294;
ieee_20221205_08_32_33;Big data analysis of irregular operations: Aborted approaches and their underlying factors;Procedures such as Missed Approaches and Holding Patterns are designed into Air Traffic Control procedures to provide a safe manner for flights to temporarily exit the airspace or the traffic flow when irregular operations occur. These procedures serve as “pressure release valves” and in this way are symptoms of the occurrence of infrequent phenomena that impact efficiency and safety margins. The occurrence of these procedures is not currently tracked by airlines or Air Navigation Service Providers (ANSP) due to the inability to identify these situations using the existing time-stamped event data (i.e. OOOI data) that is the basis for NAS performance analysis today. This paper describes a Big Data analysis of surveillance track data to establish the frequency of occurrence of Aborted Approaches, and an analysis of voluntary pilot/air traffic controller reports to establish factors leading to Aborted Approaches. Aborted Approaches include a Go Around for a Missed Approach as well as a turn off the final approach segment prior to the Missed Approach Point (MAP). Analysis of 21 days of surveillance track data for approaches at ORD identified a 7.4 in 1000 frequency of approaches resulting in an Aborted Approach. Daily Aborted Approaches ranged from 0 per day to 21 per 1000 approaches per day. Eighty percent of the Aborted Approaches involved a turn off the final approach segment prior to the MAP. An analysis of 467 voluntary pilot/air traffic controller reports from all U.S. airports identified factors leading to aborted approaches: (1) 48% airplane issues (e.g. onboard failure, unstable approach), (2) 27% traffic separation issues, (3) 16% weather (e.g. ceiling, visibility, crosswind), (4) 5% runway issues, and (5) 4% flightcrew-ATC interaction issues. These results suggest mitigation strategies to reduce the high variance in daily occurrences through procedure modification, training and equipment design.;2013;Lance Sherry;10.1109/ICNSurv.2013.6548548;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_32_33;Cross-platform aviation analytics using big-data methods;This paper identifies key aviation data sets for operational analytics, presents a methodology for application of big-data analysis methods to operational problems, and offers examples of analytical solutions using an integrated aviation data warehouse. Big-data analysis methods have revolutionized how both government and commercial researchers can analyze massive aviation databases that were previously too cumbersome, inconsistent or irregular to drive high-quality output. Traditional data-mining methods are effective on uniform data sets such as flight tracking data or weather. Integrating heterogeneous data sets introduces complexity in data standardization, normalization, and scalability. The variability of underlying data warehouse can be leveraged using virtualized cloud infrastructure for scalability to identify trends and create actionable information. The applications for big-data analysis in airspace system performance and safety optimization have high potential because of the availability and diversity of airspace related data. Analytical applications to quantitatively review airspace performance, operational efficiency and aviation safety require a broad data set. Individual information sets such as radar tracking data or weather reports provide slices of relevant data, but do not provide the required context, perspective and detail on their own to create actionable knowledge. These data sets are published by diverse sources and do not have the standardization, uniformity or defect controls required for simple integration and analysis. At a minimum, aviation big-data research requires the fusion of airline, aircraft, flight, radar, crew, and weather data in a uniform taxonomy, organized so that queries can be automated by flight, by fleet, or across the airspace system.;2013;Tulinda Larsen;10.1109/ICNSurv.2013.6548579;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_32_33;Predictive analytics with aviation big data;"Current archive is 50 billion records and growing - Approximately 34 million elements per day - \textasciitilde 1GB/day; Sheer volume of raw surveillance data makes analytics process very difficult; The raw data runs through a series of processes before it can be used for analytics; Next Steps - Continue application of predictive and prescriptive analytics - Big data visualization.";2013;Paul Comitz;10.1109/ICNSurv.2013.6548645;Conferences;2155-4943;978-1-4673-6252-8
ieee_20221205_08_32_33;On interference-aware provisioning for cloud-based big data processing;Recent advances in cloud-based big data analysis offers a convenient mean for providing an elastic and cost-efficient exploration of voluminous data sets. Following such a trend, industry leaders as Amazon, Google and IBM deploy various of big data systems on their cloud platforms, aiming to occupy the huge market around the globe. While these cloud systems greatly facilitate the implementation of big data analysis, their real-world applicability remains largely unclear. In this paper, we take the first steps towards a better understanding of the big data system on the cloud platforms. Using the typical MapReduce framework as a case study, we find that its pipeline-based design intergrades the computational-intensive operations (such as mapping/reducing) together with the I/O-intensive operations (such as shuffling). Such computational-intensive and I/O-intensive operations will seriously affect the performance of each other and largely reduces the system efficiency especially on the low-end virtual machines (VMs). To make the matter worse, our measurement also indicates that more than 90 % of the task-lifetime is in the shadow of such interference. This unavoidably reduces the applicability of cloud-based big data processing and makes the overall performance hard to predict. To address this problem, we re-model the resource provisioning problem in the cloud-based big data systems and present an interference-aware solution that smartly allocates the MapReduce jobs to different VMs. Our evaluation result shows that our new model can accurately predict the job completion time across different configurations and significantly improve the user experience for this new generation of data processing service.;2013;Yi Yuan;10.1109/IWQoS.2013.6550282;Conferences;1548-615X;978-1-4799-0588-1
ieee_20221205_08_32_33;Case of small-data analysis for ion implanters in the era of big-data FDC;This paper presents a case study of constructing process models based on physical mechanisms of semiconductor manufacturing tools in attempts to predict behaviours of process conditions. Actual measurements from the processing tools are always corrupted with noises and crunching huge volumes of temporal traces of status variables very often fail to pinpoint the accurate fault conditions, not to mention any of their efficient classifications, should abnormal conditions really exist. The current fashion of moving into massive big data computing is yet to distill concrete correlations among tool conditions and impacts on process results of semiconductor devices. As an alternative before the foolproof maturity of big data cracking, and in contrast to the conventional black-box approach of statistical regressions, we take a fundamental view in constructing physical model of the ion implantation process for a flywheel implanter, first to calculate the motion trajectories and subsequently, the implantation dosage on the wafer. We summarize the underlying solution techniques in principles and leave the specific details of parameter calibrations to individual field practitioners.;2013;Keung Hui;10.1109/ASMC.2013.6552752;Conferences;2376-6697;978-1-4673-5006-8
ieee_20221205_08_32_33;Heading towards big data building a better data warehouse for more data, more speed, and more users;As a new company, GLOBALFOUNDRIES is aggressively agile and looking at ways to not just mimic existing semiconductor manufacturing data management but to leverage new technologies and advances in data management without sacrificing performance or scalability. Being a global technology company that relies on the understanding of data, it is important to centralize the visibility and control of this information, bringing it to the engineers and customers as they need it. Currently, the factories are employing the best practices and data architectures combined with business intelligence analysis and reporting tools. However, the expected growth in data over the next several years and the need to deliver more complex data integration for analysis will easily stress the traditional tools beyond the limits of the traditional data infrastructure. The manufacturing systems vendors need to offer new solutions based on Big Data concepts to reach the new level of information processing that work well with other vendor offerings. In this paper, we will show where we are and where we are heading to manage the increasing needs for handling larger amounts of data with faster as well as secure access for more users.;2013;Raymond Gardiner Goss;10.1109/ASMC.2013.6552808;Conferences;2376-6697;978-1-4673-5006-8
ieee_20221205_08_32_33;A comparative study of enterprise and open source big data analytical tools;In this paper, we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same. The Transaction Processing Council (TPC) has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment, amount of data that can be processed, decision making capabilities, ease of use, energy and time consumed, and the pricing.;2013;Udaigiri Chandrasekhar;10.1109/CICT.2013.6558123;Conferences;;978-1-4673-5757-9
ieee_20221205_08_32_33;Leveraging Big Data Analytics to Reduce Healthcare Costs;The healthcare sector deals with large volumes of electronic data related to patient services. This article describes two novel applications that leverage big data to detect fraud, abuse, waste, and errors in health insurance claims, thus reducing recurrent losses and facilitating enhanced patient care. The results indicate that claim anomalies detected using these applications help private health insurance funds recover hidden cost overruns that aren't detectable using transaction processing systems. This article is part of a special issue on leveraging big data and business analytics.;2013;Uma Srinivasan;10.1109/MITP.2013.55;Magazines;1941-045X;
ieee_20221205_08_32_33;Big Data and Transformational Government;The big data phenomenon is growing throughout private and public sector domains. Profit motives make it urgent for companies in the private sector to learn how to leverage big data. However, in the public sector, government services could also be greatly improved through the use of big data. Here, the authors describe some drivers, barriers, and best practices affecting the use of big data and associated analytics in the government domain. They present a model that illustrates how big data can result in transformational government through increased efficiency and effectiveness in the delivery of services. Their empirical basis for this model uses a case vignette from the US Department of Veterans Affairs, while the theoretical basis is a balanced view of big data that takes into account the continuous growth and use of such data. This article is part of a special issue on big data and business analytics.;2013;Rhoda C. Joseph;10.1109/MITP.2013.61;Magazines;1941-045X;
ieee_20221205_08_32_33;Business Process Analytics Using a Big Data Approach;"Continuous improvement of business processes is a challenging task that requires complex and robust supporting systems. Using advanced analytics methods and emerging technologies--such as business intelligence systems, business activity monitoring, predictive analytics, behavioral pattern recognition, and ""type simulations""--can help business users continuously improve their processes. However, the high volumes of event data produced by the execution of processes during the business lifetime prevent business users from efficiently accessing timely analytics data. This article presents a technological solution using a big data approach to provide business analysts with visibility on distributed process and business performance. The proposed architecture lets users analyze business performance in highly distributed environments with a short time response. This article is part of a special issue on leveraging big data and business analytics.";2013;Alejandro Vera-Baquero;10.1109/MITP.2013.60;Magazines;1941-045X;
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MCG.2013.62;Magazines;1558-1756;
ieee_20221205_08_32_33;Moving big data to the cloud;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geodispersed data into the cloud, for processing using a MapReducelike framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1/l+λ κ/λ with a lookahead window of l, where κ and λ are system parameters of similar magnitude.;2013;Linquan Zhang;10.1109/INFCOM.2013.6566804;Conferences;0743-166X;978-1-4673-5945-0
ieee_20221205_08_32_33;Comparative performance analysis of a Big Data NORA problem on a variety of architectures;Non Obvious Relationship Analysis (NORA) is one of the most stressing classes of Big Data Analytics problems. This paper proposes a reference NORA problem that is representative of real problems, and can rationally scale to very large sizes. It then develops a highly concurrent implementation that can run on large systems. Each step of this implementation is sized in terms of how much of four different resources (CPU, memory, disk, and network) might be used. From this, a parameterized model projecting both execution time and utilizations is used to identify the “tall poles” in performance. The parameters are then modified to represent several different target systems, from a large cluster typical of today to variations in an advanced architecture where processing has been moved into memory. A “thought experiment” then uses this model to discover the parameters of a system that would provide both a near 100X speedup, but with a balanced design where no resource is badly over or under utilized.;2013;Peter M. Kogge;10.1109/CTS.2013.6567199;Conferences;;978-1-4673-6402-7
ieee_20221205_08_32_33;Big data, deep data, and the effect of system architectures on performance;"Summary form only given. “Big Data” traditionally refers to some combination of high volume of data, high velocity of change, and/or wide variety and complexity of the underlying data. Solving such problems has evolved into using paradigms like MapReduce on large clusters of compute nodes. More recently, a growing number of “Deep Data” problems have arisen where it is the relationships between objects, and not necessarily the collections of objects, that are important, and for which the traditional implementation techniques are unsatisfactory. This talk addresses a study of a class of such “challenge problems” first formulated by David Bayliss of LexisNexis, and what are their execution characteristics on both current and future architectures. The goal is to discover, to at least a first order approximation, what are the tall poles preventing a speedup of their solution. A variety or architectures are considered, ranging from standard server blades in large scale configurations, to emerging variations that leverage simpler and more energy efficient chip sets, through systems built on 3D chip stacks, and on to new architectures that were designed from the ground up to “follow the links.” Such architectures are considered for two variants of such problems: a traditional partitioned data approach where data is “pre-boiled” to provide fast response, and one that uses very large graphs in very large shared memories. The results are not necessarily intuitive; the bottlenecks in such problems are not where current systems have the bulk of their capabilities or costs, nor where obvious near term upgrades will have major effects. Instead, it appears that only highly scalable memory-intensive architectures offer the potential for truly major gains in application performance.";2013;Peter M. Kogge;10.1109/CTS.2013.6567201;Conferences;;978-1-4673-6402-7
ieee_20221205_08_32_33;Big data: A review;Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.;2013;Seref Sagiroglu;10.1109/CTS.2013.6567202;Conferences;;978-1-4673-6402-7
ieee_20221205_08_32_33;Addressing big data issues in Scientific Data Infrastructure;Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.;2013;Yuri Demchenko;10.1109/CTS.2013.6567203;Conferences;;978-1-4673-6402-7
ieee_20221205_08_32_33;A disk based stream oriented approach for storing big data;This paper proposes an extension to the generally accepted definition of Big Data and from this extended definition proposes a specialized database design for storing high throughput data from low-latency sources. It discusses the challenges a financial company faces with regards to processing and storing data and how existing database technologies are unsuitable for this niche task. A prototype database called CakeDB is built using a stream oriented, disk based storage design and insert throughput tests are conducted to demonstrate how effectively such a design would handle high throughput data as per the use case.;2013;Peter Membrey;10.1109/CTS.2013.6567204;Conferences;;978-1-4673-6402-7
ieee_20221205_08_32_33;An application-aware approach to systems support for big data [Keynote address];Summary form only given. Everyday 2.5 quintillion (2.5×1018, or 2.5 million trillion) bytes of data are created by people. This data comes from everywhere: from traditional scientific computing and on-line transactions, to popular social network and mobile applications. Data produced in the last two years alone amounts to 90% of the data in the world today! This phenomenal growth and ubiquity of data has ushered in an era of “Big Data”, which brings with it new challenges as well as opportunities. In this talk, I will first discuss big data challenges facing computer and storage systems research, brought on by the huge volume, high velocity, great variety and veracity with which digital data are being produced in the world. I will first introduce some new and ongoing programs at NSF that are relevant to Big Data and to ASAP. I will then present research being conducted in my research group that seeks a scalable systems and application-aware approach to addressing some of the challenges, from the many core and storage architectures to the systems and up to the applications.;2013;Hong Jiang;10.1109/ASAP.2013.6567537;Conferences;1063-6862;978-1-4799-0494-5
ieee_20221205_08_32_33;From green computing to big-data learning: A kernel learning perspective [Keynote address];Summary form only given. The SVM learning model has been successfully applied to an enormously broad spectrum of application domains and has become a main stream of the modern machine learning technologies. Unfortunately, along with its success and popularity, there also raises a grave concern on it suitability for big data learning applications. For example, in some biomedical applications, the sizes may be hundreds of thousands. In social media application, the sizes could be easily in the order of millions. This curse of dimensionality represents a new challenge calling for new learning paradigm as well as application-specific parallel and distributed hardware and software. This talk will explore cost-effective design on kernel-based machine learning and classification for big data learning applications. It will present a recursive tensor based classification algorithm, especially amenable to systolic/wavefront array processors, which may potentially expedite realtime prediction speed by orders of magnitude. For time-series analysis, with nonstationary environment, it is vital to develop time-adaptive learning algorithms so as to allow incremental and active learning. The talk will tackle the active learning problems from two kernel-induced perspectives, one in intrinsic space and another in empirical space. The talk will show, if time permits, an algorithmic example highlighting the application of Map-Reduce technologies to supervised kernel (Slackmin) learning under a parallel and distributed processing framework.;2013;Sun-Yuan Kung;10.1109/ASAP.2013.6567539;Conferences;1063-6862;978-1-4799-0494-5
ieee_20221205_08_32_33;Big Data in 10 Years;Summary form only given, as follows. The complete panel presentation was not made available for publication as part of the conference proceedings. There is a lot of excitement about “Big Data” which is at the intersection of the ongoing explosion in data (volumes, variety, and velocity at which it arrives and must be acted upon), the dramatic increase in cost-effective memory capacities, and the maturation of scale-out processing technologies. Huge investments are being made, and there are great expectations for the gains to be had and the range of applications that will be transformed by new data-driven approaches. What does the future hold? Will the changes indeed be transformative, and if so, what will some of the main changes be? What domains are likely to benefit the most? Or is this just a case of unrealistic expectations waiting to be debunked by reality? We will ask panelists drawn from diverse backgrounds to offer their opinions and, hopefully, to get into violent arguments!;2013;Raghu Ramakrishnan;10.1109/IPDPS.2013.124;Conferences;1530-2075;978-0-7695-4971-2
ieee_20221205_08_32_33;Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that address the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more. This Special Issue therefore aims to publish high quality, original, unpublished research papers in the broad area of Cloud Computing and Big Data, and thus presents a platform for scientists and scholars to share their observations and research results in the field.;2013;;10.1109/TST.2013.6574681;Journals;1007-0214;
ieee_20221205_08_32_33;Authorized Public Auditing of Dynamic Big Data Storage on Cloud with Efficient Verifiable Fine-Grained Updates;"Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the verification is done by a trusted third party, this verification process is also called data auditing, and this third party is called an auditor. However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof of integrity of certain file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads. In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of frequent small updates, such as applications in social media and business transactions.";2014;Chang Liu;10.1109/TPDS.2013.191;Journals;2161-9883;
ieee_20221205_08_32_33;Modeling of system of systems via data analytics — Case for “Big Data” in SoS;Large data has been accumulating in all aspects of our lives for quite some time. Advances in sensor technology, the Internet, wireless communication, and inexpensive memory have all contributed to an explosion of “Big Data”. System of Systems (SoS) integrate independently operating, non-homogeneous systems to achieve a higher goal than the sum of the parts. Today's SoS are also contributing to the existence of unmanageable “Big Data”. Recent efforts have developed a promising approach, called “Data Analytics”, which uses statistical and computational intelligence (CI) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro-computing, evolutionary computation, Bayesian networks, etc. to reduce the size of “Big Data” to a manageable size and apply these tools to a) extract information, b) build a knowledge base using the derived data, and c) eventually develop a non-parametric model for the “Big Data”. This paper attempts to construct a bridge between SoS and Data Analytics to develop reliable models for such systems. A photovoltaic energy forecasting problem of a micro grid SoS will be offered here for a case study of this modeling relation.;2013;Barnabas K. Tannahill;10.1109/SYSoSE.2013.6575263;Conferences;;978-1-4673-5595-7
ieee_20221205_08_32_33;Scaling challenges of packaging in the Era of Big Data;The exascale computing is required in the Era of Big Data. In order to achieve this demand, new technology innovation must be required and packaging scaling including 3D-IC with TSV (Through Silicon Vias) is one of most promising technology. To increase the total bandwidth, the fine pitch die to die interconnection is necessary. Micro-bumping, thermally enhanced underfill and advanced interposer technologies are one of the key technologies. Material selection for reliable fine-pitch interconnection has become a critical challenge in 3D chip stacking. Underfill material between die to die is also very important to reduce the total packaging stress and to enhance the vertical thermal conductivity. Low CTE high density organic substrate is emerging technology for 2.5D structure.;2013;Yasumitsu Orii;;Conferences;0743-1562;978-1-4673-5226-0
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.245;Magazines;1558-0814;
ieee_20221205_08_32_33;Detecting unfolding crises with Visual Analytics and Conceptual Maps Emerging phenomena and big data;Detecting the emergence of a political crisis is a key goal of security informatics. Big data provides us with valuable information on the many socio-economic indicators of crisis dynamics, ranging from unemployment to the trustworthiness of political institutions. However, it is currently challenging to link information on these factors in order for analysts to assess the possible directions of a conflict. At present, while some solutions offer theoretical frameworks for understanding those indicators in the abstract, these frameworks cannot easily be operationalised to the level needed for automatic processing of big data streams. Alternative solutions do automatically code political events, but only offer a high level picture that cannot support the analysis of deeper conflict processes. In this paper, we combine Visual Analytics with Concept Maps to support analysts in monitoring conflicts. Visual Analytics allows the interactive visual exploration of data, while Concept Maps keep this exploration focused by linking data patterns (e.g., occurrence and frequency of keywords) to underlying dynamics (e.g., coordination of activism, salience of violence). We illustrate the potential of our approach through a discussion of how it could be used to study the on-going Syrian crisis. While this approach still requires validation with analysts, we fully specify the technical structure of our approach and exemplify its use to detect shifts in political stability.;2013;Simon F. Pratt;10.1109/ISI.2013.6578819;Conferences;;978-1-4673-6212-2
ieee_20221205_08_32_33;Big Data Security Hardening Methodology Using Attributes Relationship;"Recently developments in network, mining and data store technology have heightened the need for big data and big data security. In this paper, we focus on the big data's characteristic which takes seriously the analysis of value than the data itself. We express the relationship between attributes using nodes and edges. Through this, we propose a big data security hardening methodology by selecting ""protect attributes"" from attributes relationship graph.";2013;Sung-Hwan Kim;10.1109/ICISA.2013.6579427;Conferences;2162-9048;978-1-4799-0602-4
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.287;Magazines;1558-0814;
ieee_20221205_08_32_33;Storage Challenge: Where Will All That Big Data Go?;Big data creates numerous exciting possibilities for organizations, but first they must figure out where they're going to store all that information.;2013;Neal Leavitt;10.1109/MC.2013.326;Magazines;1558-0814;
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.324;Magazines;1558-0814;
ieee_20221205_08_32_33;High productivity processing - Engaging in big data around distributed computing;The steadily increasing amounts of scientific data and the analysis of `big data' is a fundamental characteristic in the context of computational simulations that are based on numerical methods or known physical laws. This represents both an opportunity and challenge on different levels for traditional distributed computing approaches, architectures, and infrastructures. On the lowest level data-intensive computing is a challenge since CPU speed has surpassed IO capabilities of HPC resources and on the higher levels complex cross-disciplinary data sharing is envisioned via data infrastructures in order to engage in the fragmented answers to societal challenges. This paper highlights how these levels share the demand for `high productivity processing' of `big data' including the sharing and analysis of `large-scale science data-sets'. The paper will describe approaches such as the high-level European data infrastructure EUDAT as well as low-level requirements arising from HPC simulations used in distributed computing. The paper aims to address the fact that big data analysis methods such as computational steering and visualization, map-reduce, R, and others are around, but a lot of research and evaluations still need to be done to achieve scientific insights with them in the context of traditional distributed computing infrastructures.;2013;Morris Riedel;;Conferences;;978-953-233-076-2
ieee_20221205_08_32_33;Social-Network-Sourced Big Data Analytics;Very large datasets, also known as big data, originate from many domains. Deriving knowledge is more difficult than ever when we must do it by intricately processing this big data. Leveraging the social network paradigm could enable a level of collaboration to help solve big data processing challenges. Here, the authors explore using personal ad hoc clouds comprising individuals in social networks to address such challenges.;2013;Wei Tan;10.1109/MIC.2013.100;Magazines;1941-0131;
ieee_20221205_08_32_33;A Discussion of Privacy Challenges in User Profiling with Big Data Techniques: The EEXCESS Use Case;User profiling is the process of collecting information about a user in order to construct their profile. The information in a user profile may include various attributes of a user such as geographical location, academic and professional background, membership in groups, interests, preferences, opinions, etc. Big data techniques enable collecting accurate and rich information for user profiles, in particular due to their ability to process unstructured as well as structured information in high volumes from multiple sources. Accurate and rich user profiles are important for applications such as recommender systems, which try to predict elements that a user has not yet considered but may find useful. The information contained in user profiles is personal and thus there are privacy issues related to user profiling. In this position paper, we discuss user profiling with big data techniques and the associated privacy challenges. We also discuss the ongoing EU-funded EEXCESS project as a concrete example of constructing user profiles with big data techniques and the approaches being considered for preserving user privacy.;2013;Omar Hasan;10.1109/BigData.Congress.2013.13;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Engineering Privacy for Big Data Apps with the Unified Modeling Language;This paper describes proposed privacy extensions to UML to help software engineers to quickly visualize privacy requirements, and design privacy into big data applications. To adhere to legal requirements and/or best practices, big data applications will need to apply Privacy by Design principles and use privacy services, such as, and not limited to, anonymization, pseudonymization, security, notice on usage, and consent for usage. We extend UML with ribbon icons representing needed big data privacy services. We further illustrate how privacy services can be usefully embedded in use case diagrams using containers. These extensions to UML help software engineers to visually and quickly model privacy requirements in the analysis phase, this phase is the longest in any software development effort. As proof of concept, a prototype based on our privacy extensions to Microsoft Visio's UML is created and the utility of our UML privacy extensions to the Use Case Diagram artifact is illustrated employing an IBM Watson-like commercial use case on big data in a health sector application.;2013;Dawn N. Jutla;10.1109/BigData.Congress.2013.15;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Consistent Process Mining over Big Data Triple Stores;'Big Data' techniques are often adopted in cross-organization scenarios for integrating multiple data sources to extract statistics or other latent information. Even if these techniques do not require the support of a schema for processing data, a common conceptual model is typically defined to address name resolution. This implies that each local source is tasked of applying a semantic lifting procedure for expressing the local data in term of the common model. Semantic heterogeneity is then potentially introduced in data. In this paper we illustrate a methodology designed to the implementation of consistent process mining algorithms in a `Big Data' context. In particular, we exploit two different procedures. The first one is aimed at computing the mismatch among the data sources to be integrated. The second uses mismatch values to extend data to be processed with a traditional map reduce algorithm.;2013;Antonia Azzini;10.1109/BigData.Congress.2013.17;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud;Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.;2013;Farhana Zulkernine;10.1109/BigData.Congress.2013.18;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Towards a Quality-centric Big Data Architecture for Federated Sensor Services;As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.;2013;Lakshmish Ramaswamy;10.1109/BigData.Congress.2013.21;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Approximate Incremental Big-Data Harmonization;The needs of `big data analytics' increasingly require IT organizations to ingest, process, and extract business insights from ever larger volumes of data that arrive far more rapidly than before, as well as from new sources such as social media, mobile devices, and sensors. However, in order to extract insights from diverse information feeds from multiple, often unrelated sources, these first need to be correlated or harmonized to a common level of granularity. We formally define this commonly arising data harmonization problem. We show how to correlate disparate data sources using map-reduce, but in an approximate and/or incremental manner as often required in practice. We motivate our techniques through a real-life enterprise data-harmonization case study for which we describe our performance results on big-data technologies, namely, Map Reduce, Hadoop and PIG.;2013;Puneet Agarwal;10.1109/BigData.Congress.2013.24;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Countering the Concept-Drift Problem in Big Data Using iOVFDT;How to efficiently uncover the knowledge hidden within massive and big data remains an open problem. One of the challenges is the issue of 'concept drift' in streaming data flows. Concept drift is a well-known problem in data analytics, in which the statistical properties of the attributes and their target classes shift over time, making the trained model less accurate. Many methods have been proposed for data mining in batch mode. Stream mining represents a new generation of data mining techniques, in which the model is updated in one pass whenever new data arrive. This one-pass mechanism is inherently adaptive and hence potentially more robust than its predecessors in handling concept drift in data streams. In this paper, we evaluate the performance of a family of decision-tree-based data stream mining algorithms. The advantage of incremental decision tree learning is the set of rules that can be extracted from the induced model. The extracted rules, in the form of predicate logics, can be used subsequently in many decision-support applications. However, the induced decision tree must be both accurate and compact, even in the presence of concept drift. We compare the performance of three typical incremental decision tree algorithms (VFDT [2], ADWIN [3], iOVFDT [4]) in dealing with concept-drift data. Both synthetic and real-world drift data are used in the experiment. iOVFDT is found to produce superior results.;2013;Hang Yang;10.1109/BigData.Congress.2013.25;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Challenges of Privacy Protection in Big Data Analytics;The big data paradigm implies that almost every type of information eventually can be derived from sufficiently large datasets. However, in such terms, linkage of personal data of individuals poses a severe threat to privacy and civil rights. In this position paper, we propose a set of challenges that have to be addressed in order to perform big data analytics in a privacy-compliant way.;2013;Meiko Jensen;10.1109/BigData.Congress.2013.39;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Techniques for Graph Analytics on Big Data;Graphs enjoy profound importance because of their versatility and expressivity. They can be effectively used to represent social networks, web search engines and genome sequencing. The field of graph pattern matching has been of significant importance and has wide-spread applications. Conceptually, we want to find subgraphs that match a pattern in a given graph. Much work has been done in this field with solutions like Subgraph Isomorphism and Regular Expression matching. With Big Data, scientists are frequently running into massive graphs that have amplified the challenge that this area poses. We study the speedup and communication behavior of three distributed algorithms for inexact graph pattern matching. We also study the impact of different graph partitionings on runtime and network I/O. Our extensive results show that the algorithms exhibit excellent scalable behavior and min-cut partitioning can lead to improved performance under some circumstances, and can drastically reduce the network traffic as well.;2013;M. Usman Nisar;10.1109/BigData.Congress.2013.78;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Service-Generated Big Data and Big Data-as-a-Service: An Overview;With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.;2013;Zibin Zheng;10.1109/BigData.Congress.2013.60;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Big Data Infrastructure for Active Situation Awareness on Social Network Services;Awareness computing aims at our final goal in computer science to simulate human's awareness and cognition. Awareness of social network knowledge in everyday life is actively enabled by big data society. In this paper, we investigate infrastructure for big data analytics for social network services, and propose TF-IDF calculation on big data infrastructure to be aware of social relations on social networks.;2013;Incheon Paik;10.1109/BigData.Congress.2013.61;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Storage Mining: Where IT Management Meets Big Data Analytics;The emerging paradigm shift to cloud based data center infrastructures imposes remarkable challenges to IT management operations, e.g., due to virtualization techniques and more stringent requirements for cost and efficiency. On one hand, the voluminous data generated by daily IT operations such as logs and performance measurements contain abundant information and insights which can be leveraged to assist the IT management. On the other hand, traditional IT management solutions cannot consume and exploit the rich information contained in the data due to the daunting volume, velocity, variety, as well as the lack of scalable data mining and machine learning frameworks to extract insights from such raw data. In this paper, we present our on-going research thrust of designing novel IT management solutions by leveraging big data analytics frameworks. As an example, we introduce our project of Storage Mining, which exploits big data analytics techniques to facilitate storage cloud management. The challenges are discussed and our proof-of-concept big data analytics framework is presented.;2013;Yang Song;10.1109/BigData.Congress.2013.66;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;Distributed Stochastic Aware Random Forests -- Efficient Data Mining for Big Data;Some top data mining algorithms, as ensemble classifiers, may be inefficient to very large data set. This paper makes an initial proposal of a distributed ensemble classifier algorithm based on the popular Random Forests for Big Data. The proposed algorithm aims to improve the efficiency of the algorithm by a distributed processing model called MapReduce. At the same time, our proposed algorithm aims to reduce the randomness impact by following an algorithm called Stochastic Aware Random Forests - SARF.;2013;Joaquim Assunção;10.1109/BigData.Congress.2013.68;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;The Knowledge Service Project in the Era of Big Data;The integration of industrialization and IT application is going to be one of the Chinese new economy development strategies in the future. Therefore, how to make Big Data useful in generating significant productivity improvement in industries has already become one of the most important issues. This paper outlines the platform of knowledge service based on big data processing techniques, which have guided the implementation of the integration of industrialization and IT application in Shenyang. And some challenges we met during implementation of the project were also discussed.;2013;Dongfeng Cai;10.1109/BigData.Congress.2013.70;Conferences;2379-7703;978-0-7695-5006-0
ieee_20221205_08_32_33;A Novel Use of Big Data Analytics for Service Innovation Harvesting;Service innovation has assumed considerable significance with the growth of the services sectors of economies globally, yet progress has been slow in devising carefully formulated, systematic techniques to under pin service innovation. This paper argues that a novel approach to big data analytics offers interesting solutions in this space. The paper argues that the use of big data analytics for generating enterprise service insights is often ignored (while the extraction of insights about customers, the market and the enterprise context has received considerable attention). The paper offers a set of techniques (collectively referred to as innovation harvesting) which leverage big data in various forms, including object state sensor data, behaviour logs as well large-scale sources of open data such as the web to mine service innovation insights. The paper also outlines how systematic search might help overcome the limitations of big data analytics in this space.;2013;Aditya K. Ghose;10.1109/ICSSI.2013.45;Conferences;;978-0-7695-4985-9
ieee_20221205_08_32_33;Innovation as the strategic driver of sustainability: big data knowledge for profit and survival;Innovation has long been a central strategic focus of firms, and sustainability has recently become such a focus. We posit that innovation-across the value chain, in strategy, and in business models-is the central element of any truly sustainable business. Linking the theoretical models of Market Orientation (MO) and the Resource Based View of the Firm (RBV), purposive search directed through a Knowledge Based View (KBV) offers a schematic outline for how and where applications of big data analytics can facilitate innovation for long-term sustainability of the firm-for survival, profit, and dynamic fit with the changing environment.;2013;Mariann Jelinek;10.1109/EMR.2013.2259978;Journals;1937-4178;
ieee_20221205_08_32_33;Efficient global portfolios: Big data and investment universes;In this analysis of the risk and return of stocks in the United States and global markets, we apply several portfolio construction and optimization techniques to U.S. and global stock universes. We find that (1) mean-variance techniques continue to produce portfolios capable of generating excess returns above transaction costs and statistically significant asset selection, (2) optimization techniques minimizing expected tail loss are statistically significant in portfolio construction, and (3) global markets offer the potential for greater returns relative to risk than domestic markets. In this experiment, mean-variance, enhanced-index-tracking techniques, and mean-expected tail-loss methodologies are examined. Global equity data and the vast quantity (and quality) of the data relative to U.S. equity modeling have been discussed in the literature. We estimate expected return models in the U.S. and global equity markets using a given stock-selection model and generate statistically significant active returns from various portfolio construction techniques.;2013;J. B. Guerard;10.1147/JRD.2013.2272483;Journals;0018-8646;
ieee_20221205_08_32_33;Storm System Database: A Big Data Approach to Moving Object Databases;Rainfall data is often collected by measuring the amount of precipitation collected in a physical container at a site. Such methods provide precise data for those sites, but are limited in granularity to the number and placement of collection devices. We use radar images of storm systems that are publicly available and provide rainfall estimates for large regions of the globe, but at the cost of loss of precision. We present a moving object database called Storm DB that stores decibel measurements of rain clouds as moving regions, i.e., we store a single rain cloud as a region that changes shape and position over time. Storm DB is a prototype system that answers rain amount queries over a user defined time duration for any point in the continental United States. In other words, a user can ask the database for the amount of rainfall that fell at any point in the US over a specified time window. Although this single query seems straightforward, it is complicated due to the expected size of the dataset: storm clouds are numerous, radar images are available in high resolution, and our system will collect data over a large timeframe, thus, we expect the number and size of moving regions representing storm clouds to be large. To implement our proposed query, we bring together the following concepts: (i) image processing to retrieve storm clouds from radar images, (ii) interpolation mechanisms to construct moving regions with infinite temporal resolution from region snapshots, (iii) transformations to compute exact point in moving polygon queries using 2-dimensional rather than 3-dimensional algorithms, (iv) GPU algorithms for massively parallel computation of the duration that a point lies inside a moving polygon, and (v) map/reduce algorithms to provide scalability. The resulting prototype lays the groundwork for building big data solutions for moving object databases.;2013;Brian Olsen;10.1109/COMGEO.2013.30;Conferences;;978-0-7695-5012-1
ieee_20221205_08_32_33;Efficient Online Sharing of Geospatial Big Data Using NoSQL XML Databases;Summary form only given: Today a huge amount of geospatial data is being created, collected and used more than ever before. The ever increasing observations and measurements of geo-sensor networks, satellite imageries, point clouds from laser scanning, geospatial data of Location Based Services (LBS) and location-based social networks has become a serious challenge for data management and analysis systems. Traditionally, Relational Database Management Systems (RDBMS) were used to manage and to some extent analyze the geospatial data. Nowadays these systems can be used in many scenarios but there are some situations when using these systems may not provide the required efficiency and effectiveness. More specifically when the geospatial data has high volume, high frequency of change (in both data content and data structure) and variety of structures, the conventional data storage systems cannot provide needed efficiency in online systems in terms of performance and scalability. In these situations, NoSQL solutions can provide the efficiency necessary for applications using geospatial data. This paper provides an overview of the characteristics of geospatial big data, possible solutions for managing and processing them. Then the paper provides an overview of the major types of NoSQL solutions, their advantages and disadvantages and the challenges they present in managing geospatial big data. Then the paper elaborates on serving geospatial data using standard geospatial web services with a NoSQL XML database as a backend.;2013;Pouria Amirian;10.1109/COMGEO.2013.34;Conferences;;978-0-7695-5012-1
ieee_20221205_08_32_33;Big Data: Unleashing information;Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.;2013;James M. Tien;10.1109/ICSSSM.2013.6602615;Conferences;2161-1904;978-1-4673-4434-0
ieee_20221205_08_32_33;Store, schedule and switch - A new data delivery model in the big data era;The big data era is posing unprecedented challenges on the existing network infrastructure. In today's networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service level requirements. The so called elephant data, which may be less sensitive to transfer delay, compete precious network resources with mice data, in most cases from interactive and delay sensitive applications. Consequently, the Quality of Service (QoS) of interactive applications is hard to provision, and the utility of network is low. We propose a new data transfer model to complement the existing per-packet forwarding paradigm. In the new data transfer model, a service level requirement is assigned (by the data source) to each big data transfer request. Instead of transferring these data on per-packet bases immediately upon entering the network, the network stores the data until it find necessary, or enough network resource is available for that transfer. The scheduled data delivery is realized through the use of dynamic circuit switching. We also present some preliminary simulation results of SSS networks.;2013;Weiqiang Sun;10.1109/ICTON.2013.6602860;Conferences;2162-7339;978-1-4799-0683-3
ieee_20221205_08_32_33;Study on Big Data Center Traffic Management Based on the Separation of Large-Scale Data Stream;The network of traditional data center has been usually designed and constructed for the provision of user's equal access of data centre's resource or data. Therefore, network administrators have a strong tendency to manage user traffic from the viewpoint that the traffic has a similar size and characteristics. But, the emersion of big data begins to make data centers have to deal with 1015 byte-data transfer at once. Such a big data transfer can cause problems in network traffic management in the existed data center. And, the tiered network architecture of the legacy data center magnifies the magnitude of the problems. One of the well-known big data in science is from large hadron collider such as LHC in Swiss CERN. CERN LHC generates multi-peta byte data per year. From our experience of CERN data service, this paper showed the impact of network traffic affected by large-scale data stream using NS2 simulation, and then, suggested the evolution direction based on separating of large-scale data stream for the big data center's network architecture.;2013;Hyoung Woo Park;10.1109/IMIS.2013.104;Conferences;;978-0-7695-4974-3
ieee_20221205_08_32_33;Split File Model for Big Data in Low Throughput Storage;The demand for low-cost, large-scale storage is increasing. Recently, several low-throughput storage services such as the Pogo plug Cloud have been developed. These services are based on Amazon Glacier. They have low throughput, but low cost and large capacity. Therefore, these services are suitable for backups or archiving big data and can be used instead of offline storage tiers. To utilize such low throughput storage efficiently, we need tools for effective deduplication and resumable transfers, amongst others. We propose a split file model that can represent big data efficiently in low throughput storage. In the split file model, a large file is divided into many small parts, which are stored in a directory. We have developed tool commands to support the use of split files in a transparent way. Using these commands, replicated data is naturally excluded and effective shallow copying is supported. In this paper, we describe the split file model in detail and evaluate an implementation thereof.;2013;Minoru Uehara;10.1109/CISIS.2013.48;Conferences;;978-0-7695-4992-7
ieee_20221205_08_32_33;Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds;Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.;2013;Weiyi Shang;10.1109/ICSE.2013.6606586;Conferences;1558-1225;978-1-4673-3073-2
ieee_20221205_08_32_33;How deep data becomes big data;We present some problems and solutions for situations when compound and semantically rich nature of data records, such as scientific articles, creates challenges typical for big data processing. Using a case study of named entity matching in SONCA system we show how big data problems emerge and how they are solved by bringing together methods from database management and computational intelligence.;2013;Marcin Szczuka;10.1109/IFSA-NAFIPS.2013.6608465;Conferences;;978-1-4799-0348-1
ieee_20221205_08_32_33;Big-data integration methodologies for effective management and data mining of petroleum digital ecosystems;"Petroleum industries' big data characterize heterogeneity and they are often multidimensional in nature. In the recent past, explorers narrate petroleum system, as an ecosystem, in which elements and processes are constantly interacted and communicated each other. Exploration is one of the key super-type data dimensions of petroleum ecosystem, (including seismic dimension), exhibiting high degree of heterogeneity, sequence identity and structural similarity; this is especially the case for, elements and processes that are unique to petroleum systems of South East Asia. Existing approaches of petroleum data organizations have limitations in capturing and integrating petroleum systems data. An alternative method uses ontologies and does not rely on keywords or similarity metrics. The conceptual framework of petroleum ontology (PO) is to promote reuse of concepts and a set of algebraic operators for querying petroleum ontology instances. This ontology-based fine-grained multidimensional data structuring adapts to warehouse metadata modeling. The data integration process facilitates to metadata models, which are deduced for Indonesian sedimentary basins, and is useful for data mining and subsequent data interpretation including geological knowledge mapping.";2013;Shastri L Nimmagadda;10.1109/DEST.2013.6611345;Conferences;2150-4946;978-1-4799-0784-7
ieee_20221205_08_32_33;Data migration ecosystem for big data invited paper;Data Migration is the process of moving data from a system or systems to a new environment. Often, it is a sub-activity of a business application deployment. Big data is defined as data that is huge, has heterogeneous data dictionaries and involves complex manipulation. Due to nature of the process complexity and its resources hungry approach in migrating Big Data, special attention is required to have a proven methodology and ecosystem to govern the process. The Data Migration Ecosystem for Big Data is the productive set of interacting processes, practices and environments, to collect data from one location, storage medium, or hardware/software system, to cleanse, transform and transfer it to another. The processes and practices are governed by rules and disciplines, with the goal of ensuring information is complete, of high accuracy and consistent. This paper is based on our experience in migrating data for a Malaysia government agency, which involves approximately 1 billion rows of data from 31 heterogeneous sources / systems. Some of the data migrated was created in the seventies (1970), for which the business logic has since been enhanced or changed. The challenge is further complicated by available data being from proprietary databases that are non-RDMS compliance and includes data that is manually maintained in Microsoft Excel spreadsheets.;2013;Koong Wah Yan;10.1109/DEST.2013.6611352;Conferences;2150-4946;978-1-4799-0784-7
ieee_20221205_08_32_33;Big data: Issues, challenges, tools and Good practices;Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.;2013;Avita Katal;10.1109/IC3.2013.6612229;Conferences;;978-1-4799-0190-6
ieee_20221205_08_32_33;Wearable monitors on babies: Big data saving little people;"Today 8% of Canadian babies are born premature and internationally the average is 10% These early births, are responsible for three quarters of all infant deaths in Canada. Premature infants together with ill term infants are cared for in Neonatal Intensive Care U nits (NICUs) internationally contain state of th e art medical equipment to monitor and provide life support, resulting in a significant Big Data environment. In addition, graduates of neonatal intensive care may be discharged with medical devices to support continued monitoring as ambulatory patients in and outside the ho me setting. In both NICU and ambulatory contexts wearable patient monitoring has many social implications. This research presents an assessment of the social implications of Big Data solutions for criti cal care within the context of the Artemis project that is enabling Big Data solutions for: 1) Real-ti me processing of complex intensive care physiological signals for new and earlier condition onset detection; 2) new approaches to physiological data analysis to support clinical research; and 3) cloud computing/services computing to provide rural and remote communities with greater options for a dvanced critical care within their own community healthcare facilities.";2013;Carolyn McGregor;10.1109/ISTAS.2013.6613120;Conferences;2158-3404;978-1-4799-1242-1
ieee_20221205_08_32_33;HireSome-II: Towards Privacy-Aware Cross-Cloud Service Composition for Big Data Applications;Cloud computing promises a scalable infrastructure for processing big data applications such as medical data analysis. Cross-cloud service composition provides a concrete approach capable for large-scale big data processing. However, the complexity of potential compositions of cloud services calls for new composition and aggregation methods, especially when some private clouds refuse to disclose all details of their service transaction records due to business privacy concerns in cross-cloud scenarios. Moreover, the credibility of cross-clouds and on-line service compositions will become suspicional, if a cloud fails to deliver its services according to its “promised” quality. In view of these challenges, we propose a privacy-aware cross-cloud service composition method, named HireSome-II (History record-based Service optimization method) based on its previous basic version HireSome-I. In our method, to enhance the credibility of a composition plan, the evaluation of a service is promoted by some of its QoS history records, rather than its advertised QoS values. Besides, the k-means algorithm is introduced into our method as a data filtering tool to select representative history records. As a result, HireSome-II can protect cloud privacy, as a cloud is not required to unveil all its transaction records. Furthermore, it significantly reduces the time complexity of developing a cross-cloud service composition plan as only representative ones are recruited, which is demanded for big data processing. Simulation and analytical results demonstrate the validity of our method compared to a benchmark.;2015;Wanchun Dou;10.1109/TPDS.2013.246;Journals;2161-9883;
ieee_20221205_08_32_33;Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data;This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that addresses the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more.;2013;;10.1109/TST.2013.6616527;Journals;1007-0214;
ieee_20221205_08_32_33;Inconsistencies in big data;We are faced with a torrent of data generated and captured in digital form as a result of the advancement of sciences, engineering and technologies, and various social, economical and human activities. This big data phenomenon ushers in a new era where human endeavors and scientific pursuits will be aided by not only human capital, and physical and financial assets, but also data assets. Research issues in big data and big data analysis are embedded in multi-dimensional scientific and technological spaces. In this paper, we first take a close look at the dimensions in big data and big data analysis, and then focus our attention on the issue of inconsistencies in big data and the impact of inconsistencies in big data analysis. We offer classifications of four types of inconsistencies in big data and point out the utility of inconsistency-induced learning as a tool for big data analysis.;2013;Du Zhang;10.1109/ICCI-CC.2013.6622226;Conferences;;978-1-4799-0781-6
ieee_20221205_08_32_33;Is privacy still an issue in the era of big data? — Location disclosure in spatial footprints;Geospatial data that were once difficult to obtain are now readily available to the public with the development of geospatial technologies. The ubiquitous use of social networking and location-based services enables easy sharing of personal stories among many people. With or without awareness of location disclosure, some users reveal a considerable amount of geopersonal information to the general public. Privacy issues have been raised again in the GIScience community. This study shows that home and work places may be inferred from georeferenced tweets of heavy Twitter users. Is privacy still an issue in the era of big data when people freely share blogs, photos, videos, and spatial footprints over the Internet? Or do people willingly decide to give out location information in order to gain benefits from the disclosure? After a discussion on possible reasons for people to share spatial footprints, this paper invites more research on perception of geoprivacy.;2013;Linna Li;10.1109/Geoinformatics.2013.6626191;Conferences;2161-0258;978-1-4673-6227-6
ieee_20221205_08_32_33;BigLS - The 1st International Workshop on Big Data in Life Sciences;With the ever-increasing volume, velocity and variety biological and biomedical data collections continue to pose new challenges and increasing demands on computing and data management. The inherent complexity of this big data has forced us to rethink how we collect, store, combine and analyze it.;2013;Ananth Kalyanaraman;10.1109/ICCABS.2013.6629242;Conferences;;978-1-4799-0716-8
ieee_20221205_08_32_33;Workflow-driven programming paradigms for distributed analysis of biological big data;Scientific workflows have been used as a programming model to automate scientific tasks ranging from short pipelines to complex workflows that span across heterogeneous data and computing resources. While utilization of scientific workflow technologies varies slightly across different scientific disciplines, all informatics and computational science disciplines provide a common set of attributes to facilitate and accelerate workflow-driven research. Scientific workflows provide assembly of complex processing easily in local or distributed environments via rich and expressive programming models. Scientific workflows enable transparent access to diverse resources ranging from local clusters and traditional supercomputers to elastic and heterogeneous Cloud resources. Scientific workflows support incorporation of multiple software tools including domain specific tools for standard processing to custom generalized workflows and middleware tools that can be reused in various contexts. Scientific workflows often collect provenance information on workflow entities, e.g., workflow definitions, their executions and run time parameters, and, in turn, assure a level of reproducibility while enabling referencing and replicating results. While doing all these, scientific workflows often foster an open-source, open-access and standards-driven community development model based on sharing and collaborations. Cyberinfrastructure platforms and gateways commonly employ scientific workflows to bridge the gap between the infrastructure and users needs. While capturing and communicating the scientific process formally, workflows ensure flexibility, synergy between users, provide optimized usage of resources, increase reuse and ensure compliance with system specific data models and community-driven standards. Currently, scientific workflows are used widely in life sciences at different stages of end-to-end data lifecycle from generation to analysis and publication of biological data. The data handled by such workflows can be produced by sequencers, sensor networks, medical imaging instruments and other heterogeneous resources at significant rates at decreasing costs making the analysis and archival of such data a 'big data' challenge. Additionally, these new biological data resources are making new and exciting research in areas including metagenomics and personalized medicine possible. However, the analysis of big biological data is still very costly requiring new scalable computational models and programming paradigms to be applied to biological analysis. Although, some new paradigms exists for analysis of big data, application of these best practices to life sciences is still in its infancy. Scientific workflows can act as a scaffold and help speed this process up via combination of existing programming models and computational models with the challenges of biological problems as reusable blocks. In this talk, I will talk about such an approach that builds upon distributed data parallel patterns, e.g., MapReduce, and underlying execution engines, e.g., Hadoop, and matches the computational requirements of bioinformatics tools with such patterns and engines. The results of the presented approach is developed as a part of the bioKepler (bioKepler.org) module and can be downloaded to work within the release 2.4 of the Kepler scientific workflow system (kepler-project.org).;2013;Ilkay Altintas;10.1109/ICCABS.2013.6629243;Conferences;;978-1-4799-0716-8
ieee_20221205_08_32_33;Breaking the boundary for whole-system performance optimization of big data;MapReduce plays an critical role in finding insights in Big Data. The performance optimization of MapReduce programs is challenging because it requires a comprehensive understanding of the whole system including both hardware layers (processors, storages, networks and etc), and software stacks (operating systems, JVM, runtime, applications and etc). However, most of the existing performance tuning and optimization are based on empirical and heuristic attempts. It remains a blank on how to build a systematical framework which breaks the boundary of multiple layers for performance optimization. In this paper, we propose a performance evaluation framework by correlating performance metrics from different layers, which provides insights to efficiently pinpoint the performance issue. This framework is composed of a series of predefined patterns. Each pattern indicates one or more potential issues. The behavior of a MapReduce program is mapped to the corresponding resource utilization. The framework provides a holistic approach which allows users at different levels of experience to conduct MapReduce program performance optimization. We use Terasort benchmark running on a 10-node Power7R2 cluster as a real case to show how this framework improves the performance. By this framework, we finally get the Terasort result improved from 47 mins to less than 8 mins. In addition to the best practice on performance tuning, several key findings are summarized as valuable workload analysis for JVM, MapReduce runtime and application design.;2013;Yan Li;10.1109/ISLPED.2013.6629278;Conferences;;978-1-4799-1234-6
ieee_20221205_08_32_33;The role of big data in improving power system operation and protection;This paper focuses on the use of extremely large data sets in power system operation, control, and protection, which are difficult to process with traditional database tools and often termed big data. We will discuss three aspects of using such data sets: feature extraction, systematic integration for power system applications, and examples of typical applications in the utility industry. The following analytics tasks based on big data methodology are elaborated upon: corrective, predictive, distributed and adaptive. The paper also outlines several research topics related to asset management, operation planning, realtime monitoring and fault detection/protection that present new opportunities but require further investigation.;2013;Mladen Kezunovic;10.1109/IREP.2013.6629368;Conferences;;978-1-4799-0199-9
ieee_20221205_08_32_33;Big Data's Risks and Opportunities for ICT Agriculture;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address these issues and to create a common background to be applied in the new NEDO project in which Sojo University is an active research member.";2013;Dennis A. Ludena R;10.1109/IIAI-AAI.2013.60;Conferences;;978-1-4799-2134-8
ieee_20221205_08_32_33;Virtual Dataspace -- A Service Oriented Model for Scientific Big Data;The massive, distributed, heterogeneous and diverse features of big data have raised challenges to traditional data management systems. As the development and innovation of Data Space, virtual data space (VDS) model is proposed for big data management. Local ontologies are created from data sources. Then the local ontologies are mapped and formed a global ontology. Based on this, access log and user feedback are considered for data evolution. At last, a material scientists-oriented service (materials scholar assistant) is introduced as the application case of VDS.;2013;Wei Lin;10.1109/EIDWT.2013.5;Conferences;;978-1-4799-2140-9
ieee_20221205_08_32_33;On use of big data for enhancing network coverage analysis;Proliferation of data services has made it mandatory for operators to be able identify geographical regions with 3G connectivity discontinuity in a scalable and cost-efficient manner. The currently used methods for such analysis are either costly — such as in drive tests, partly unreliable — such as in network simulation approaches, or are not precise enough — such as in base station key performance indicators (KPI) based approaches. In this paper, towards addressing these inadequacies, we propose a 3G coverage analysis method that makes use of “big data” processing schemes and the vast amounts of network data logged in mobile operators. In the proposed scheme, the BSSAP mobility and radio resource management messages between the BSS and MSC nodes of the operator network are processed to identify inter-technology handovers from 3G (WCDMA) access to 2G (EDGE, GPRS, GSM). Demonstrative examples show that the proposed mechanism produces accurate and precise results, outperforming the base station KPI-based approach.;2013;Ömer Faruk Çelebi;10.1109/ICTEL.2013.6632155;Conferences;;978-1-4673-6425-6
ieee_20221205_08_32_33;Survey of Research on Big Data Storage;With the development of cloud computing and mobile Internet, the issues related to big data have been concerned by both academe and industry. Based on the analysis of the existed work, the research progress of how to use distributed file system to meet the challenge in storing big data is expatiated, including four key techniques: storage of small files, load balancing, copy consistency, and de-duplication. We also indicate some key issues need to concern in the future work.;2013;Xiaoxue Zhang;10.1109/DCABES.2013.21;Conferences;;978-0-7695-5060-2
ieee_20221205_08_32_33;Big data on small screens;Data is one of the integral assets of the organization. There are many companies which have the huge amount of data but the challenge lies in how to make it more usable. In this 21st century the world around us is rapidly changing and smartphones are tightly coupled to the users. Customers mainly concentrate on using smartphone devices to access telephonic services. Whereas now it's the time for companies to make use of this available big data from the customers. This big data can be used in one or the other form in understanding the behavior of the customers. In this paper author would present the usage of the big data on smart phones to provide sophisticated services to the customers in real time. Also author would explore how this big data on small screens can make a difference in major events like US Presidential Elections.;2013;Aditya R Desai;10.1109/ICACCI.2013.6637287;Conferences;;978-1-4799-2659-6
ieee_20221205_08_32_33;HPCS 2013 Keynotes: Tuesday keynote: Big process for big data;"These keynotes discuss the following: Big Process for Big Data; Killer-Mobiles: The Way Towards Energy Efficient High Performance Computers?; The UberCloud HPC Experiment - Paving the Way to HPC as a Service; and High Performance Fault Tolerance / Resilience at Extreme Scale.";2013;Ian Foster;10.1109/HPCSim.2013.6641378;Conferences;;978-1-4799-0837-0
ieee_20221205_08_32_33;Big data analytics as a service: Exploring reuse opportunities;As data scientists, we live in interesting times. Data has been the No. 1 fast growing phenomenon on the Internet for the last decade. Big data analytics have the potential to reveal deep insights hidden by big data that exceeds the processing capacity of existing systems, such as peer influence among customers, revealed by analyzing shoppers' transactions, social and geographical data. In the past 40 years, data was primarily used to record and report business activities and scientific events, and in the next 40 years data will be used also to derive new insights, to influence business decisions and to accelerate scientific discovery. The key challenge is to provide the right platforms and tools to make reasoning of big data easy and simple. In this keynote talk, I will explore reuse opportunities and challenges from multiple dimensions towards delivering big data analytics as a service. I will illustrate by example the importance and challenges of utilizing programmable algorithm abstractions for many seemingly domain-dependent data analytics tasks. Another reuse opportunity is to exploit unconventional data structures and big data processing constructs to simplify and speed up the big data processing.;2013;Ling Liu;10.1109/IRI.2013.6642438;Conferences;;978-1-4799-1050-2
ieee_20221205_08_32_33;Implementation of the Big Data concept in organizations - possibilities, impediments and challenges;This paper is devoted to the analysis of the Big Data phenomenon. It is composed of seven parts. In the first, the growing role of data and information and their rapid increase in the new socio-economical reality, are discussed. Next, the notion of Big Data is defined and the main sources of growth of data are characterized. In the following part of the paper the most significant possibilities linked with Big Data are presented and discussed. The next part is devoted to the characterization of tools, techniques and the most useful data in the context of Big Data initiatives. In the following part of the paper the success factors of Big Data initiatives are analyzed, followed by an analysis of the most important problems and challenges connected with Big Data. In the final part of the paper, the most significant conclusions and suggestions are offered.;2013;Janusz Wielki;;Conferences;;978-83-60810-52-1
ieee_20221205_08_32_33;Applying big data and linked data concepts in supply chains management;"One of the contemporary problems, and at the same time a big opportunity, in business networks of supply chains are the issues associated with the vast amounts of data arising there. The data may be utilized by the decision support systems in logistics; nevertheless, often there is an information integration problem. The problems with information interchange are related to issues with exchange between independently designed data systems. The networked supply chains will need appropriate IT architectures to support the cooperating business units utilizing structured and unstructured big data and the mechanisms to integrate data in heterogeneous supply chains. In this paper we analyze the capabilities of the big data technology architectures with cloud computing under usage of Linked Data in business process management in supply chains to cope with unstructured near-time data and data silos problems. We present our approach on a 4PL (Fourth-party Logistics) integrator business process example.";2013;Silva Robak;;Conferences;;978-83-60810-52-1
ieee_20221205_08_32_33;Using Big Data and predictive machine learning in aerospace test environments;"It is estimated that in 2012 most mid-size companies in the USA generate the equivalent data of the US Library of Congress in 1 year. As a company, Wal-Mart creates the equivalent of 50 million filing cabinets worth of data every hour. While these numbers seem incredible, the trend for most companies is an increasing volume of data generation and storage. Test Data generated by Automatic Test Equipment (ATE) in R&D, manufacturing and Repair environments is no exception to this increased volume of data. The challenge of this enormous amount of Test Data is how to provide people with effective ways to make decisions from it. Data visualization through charts, graphs and reports has been, historically, one of the more effective ways to provide actionable intelligence because humans can readily make decisions based on patterns and comparisons. But as data volume goes up, even this method is reaching its limits. When one starts to combine large datasets like Manufacturing Test Data and Repair Data together, data visualization becomes problematic. More sophisticated algorithmic, machine learning and predictive approaches become critical. In this paper, we will explore the experiences of using predictive algorithms on ""Big Data"" from both Manufacturing Test and Repair Test environments in the complex mission critical aerospace industry. By effectively using datasets from different functional areas, we will be looking at applying SPC techniques to answer new questions about the correlation of Repair test data and manufacturing data with the end goal to predict number of returns in the future and minimize product escapes.";2013;Tom Armes;10.1109/AUTEST.2013.6645085;Conferences;1558-4550;978-1-4673-5681-7
ieee_20221205_08_32_33;Embedded Analytics and Statistics for Big Data;Embedded analytics and statistics for big data have emerged as an important topic across industries. As the volumes of data have increased, software engineers are called to support data analysis and applying some kind of statistics to them. This article provides an overview of tools and libraries for embedded data analytics and statistics, both stand-alone software packages and programming languages with statistical capabilities.;2013;Panos Louridas;10.1109/MS.2013.125;Magazines;1937-4194;
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MS.2013.130;Magazines;1937-4194;
ieee_20221205_08_32_33;Location: A Feature for Service Selection in the Era of Big Data;This paper introduces a service selection model with the service location considered. The location of a service represents its position in the network, which determines the transmission cost of calling this service in the composite service. The more concentrated the invoking services are, the less transmission time the composite service costs. On the other hand, the more and more popular big data processing services, which need to transfer mass data as input, make the effect much more obvious than ever before. Therefore, it is necessary to introduce service location as a basic feature in service selection. The definition and membership functions of service location are presented in this paper. After that, the optimal service selection problem is represented as an optimization problem under some reasonable assumptions. A shortest-path based algorithm is proposed to solve this optimization problem. At last, the case of railway detection is studied for better understanding of our model.;2013;Luo Zhiling;10.1109/ICWS.2013.75;Conferences;;978-0-7695-5025-1
ieee_20221205_08_32_33;pLSM: A Highly Efficient LSM-Tree Index Supporting Real-Time Big Data Analysis;Big Data boosts the development of data management and analysis in database systems but it also poses a challenge to traditional database. NoSQL databases are provided to deal with the new challenges brought by Big Data because of its high performance, storage, scalability and availability. In NoSQL databases, it is an essential requirement to provide scalable and efficient index services for real-time data analysis. Most existing index solutions focus on improving write throughput, but at the cost of poor read performance. We designed a new plug-in system PuntStore with pLSM (Punt Log Structured Merge Tree) index engine. To improve read performance, Cache Oblivious Look-ahead Array (COLA) is adopted in our design. We also presented a novel compact algorithm in bulk deletion to support migration of data from temporary storage to data warehouse for further analysis.;2013;Jin Wang;10.1109/COMPSAC.2013.40;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_32_33;Big Data -- Opportunities and Challenges Panel Position Paper;This paper summarizes opportunities and challenges of big data. It identifies important research directions and includes a number of questions that have been debated by the panel.;2013;Elisa Bertino;10.1109/COMPSAC.2013.143;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_32_33;Analytics over Big Data: Exploring the Convergence of DataWarehousing, OLAP and Data-Intensive Cloud Infrastructures;This paper explores the convergence of Data Warehousing, OLAP and data-intensive Cloud Infrastructures in the context of so-called analytics over Big Data. The paper briefly reviews some state-of-the-art proposals, highlights open research issues and, finally, it draws possible research directions in this scientific field.;2013;Alfredo Cuzzocrea;10.1109/COMPSAC.2013.152;Conferences;0730-3157;978-0-7695-4986-6
ieee_20221205_08_32_33;Rock Stars of Big Data Advertisement;Rock Stars of Big Data Advertisement;2013;;10.1109/MC.2013.370;Magazines;1558-0814;
ieee_20221205_08_32_33;Data Evolution Analysis of Virtual DataSpace for Managing the Big Data Lifecycle;New challenge about the constantly changing of associated data in big data management has arisen, which leads to the issue of data evolution. In this paper, a data evolution model of Virtual Data Space (VDS) is proposed for managing the big data lifecycle. Firstly, the concept of data evolution cycle is defined, and the lifecycle process of big data management is described. Based on these, the data evolution lifecycle is analyzed from the data relationship, the user requirements, and the operation behavior. Secondly, the classification and key concepts about the data evolution process are described in detail. According to this, the data evolution model is constructed by defining the related concepts and analyzing the data association in VDS, for the capture and tracking of dynamic data in the data evolution cycle. Then we discuss the cost problem about data dissemination and change. Finally, as the application case, the service process of dynamic data in the field of materials science is described and analyzed. We verify the validity of data evolution modeling in VDS by the comparison of traditional database, data space, and VDS. It shows that this analysis method is efficient for the data evolution processing, and very suitable for the data-intensive application and the real-time dynamic service.;2013;Xin Cheng;10.1109/IPDPSW.2013.57;Conferences;;978-0-7695-4979-8
ieee_20221205_08_32_33;A Workflow Framework for Big Data Analytics: Event Recognition in a Building;This paper studies event recognition in a building based on the patterns of power consumption. It is a big challenge to identify what kinds of events happened in a building without additional devices such as camera and motion sensors, etc. Instead, we learn when and how the events happened from the historical record of power consumption and apply the lesson into the design of an event recognition system (ERS). The ERS will find out abnormal power usage to avoid wasting power, which leads to the energy savings in a building. The ERS involves big data analytics with a large size of dataset collected in a real time. Such a data intensive system is usually viewed as a workflow. A workflow management is a significant task of the system requiring data analysis in terms of the system scalability to maintain high throughput or fast speed analysis. We propose a workflow framework that allows users to perform remote and parallel workflow execution, whose tasks are efficiently scheduled and distributed in cloud computing environment. We run the ERS as a target system for the proposed framework with power consumption data (whose size is approximately 20GB or more) collected from each of over 240 rooms in a building at Dept. of Engineering, Tokyo University in 2011. We show that the proposed framework accelerates the speed of data analysis by providing scaling infrastructure and parallel processing feature utilizing cloud computing technologies. We also share our experience and results on the big data analytics and discuss how the studies contribute to achieve Green Campus.;2013;Chonho Lee;10.1109/SERVICES.2013.29;Conferences;2378-3818;978-0-7695-5024-4
ieee_20221205_08_32_33;Social Issues of Big Data and Cloud: Privacy, Confidentiality, and Public Utility;"Business people and academia are now excited about Big Data and Cloud Computing as the new and most innovative means for enhancing productivity and customer satisfaction. Simultaneously, there are strong concerns about privacy not only among privacy advocates but among consumers in general, and how to strike a right balance is the main theme in every field of science. However, it is quite strange that very little attention has been paid to the concept of confidentiality, which must be the core element of privacy. This paper first tries to analyze the following two dichotomies as a basis for possible policy considerations: (1) privacy approach in the United States versus confidentiality approach in the United Kingdom, though they share the same common law tradition, and (2) clear demarcation between Information Service and Telecommunications in the United States, dating back to the Computer Inquiry in the 1970s. This paper also analyzes the features of the Cloud and discusses the possibility of treating it as a new type of Public Utility, namely Information Utility. This hypothesis should be rejected, because there are crucial differences in market structures, regardless of clear similarities in service features. Instead, this paper emphasizes the necessity of protecting confidentiality as an industrial norm. Taking into account the long tradition of free market for computing industries, self-regulation is basically preferable to government regulation. But from a different viewpoint of ""nudge"", a hybrid combination of libertarianism and paternalism, this paper concludes by proposing five short recommendations including fair contract terms as well as unbundling confidentiality from privacy.";2013;Koichiro Hayashi;10.1109/ARES.2013.66;Conferences;;978-0-7695-5008-4
ieee_20221205_08_32_33;Towards an efficient routing web processing service through capturing real-time road conditions from big data;The rapidly growing number of crowdsourcing platforms generates huge volumes of volunteered geographic information (VGI), which requires analysis to reveal their potential. The huge volumes of data appear as an opportunity to improve various applications, including routing and navigation services. How existing techniques for dealing with Big Data could be useful for the analysis of VGI remains an open question, since VGI differs from traditional data. In this paper, we focus on examining the latest developments and issues associated with big data from the perspective of the analysis of VGI. This paper notably presents our new architecture for exploiting Big VGI in event service processing in support to optimization of routing service. In addition, our study highlights the opportunities that are created by the emergence of Big VGI and crowdsourced data on improving routing and navigation services, as well as the challenges that remain to be addressed to make this a reality. Finally, avenues for future research on the next generation of collaborative routing and navigation services are presented.;2013;Mohamed Bakillah;10.1109/CEEC.2013.6659463;Conferences;;978-1-4799-0383-2
ieee_20221205_08_32_33;A big data file transfer tool for tablet-class machines;A big data file transport (BDFT) protocol is presented that minimize overheads associated with packet streaming in Java. The BDFT protocol relies on block 1-data transfers, and the elimination of unnecessary data copying between the application layer and the send socket in the standard Java IO model. The implementation uses the Java New IO (NIO) and the Zerocopy libraries. Several experiments are described and results compared against the standard Java IO - a stream-based file transport protocol. The motivation for this study is the development of a client/server big data file transport protocol for tablet-class client machines that rely on the Java Remote Method Invocation (RMI) package for distributed computing.;2013;Tevaganthan Veluppillai;10.1109/IDAACS.2013.6663011;Conferences;;978-1-4799-1427-2
ieee_20221205_08_32_33;A Big Data application framework for consumer behavior analysis;More than ever before, the amount of data about consumers, suppliers and products has been exploding in today consumer world referred as “Big Data”. In addition, more data is available to the consumer world from multiple sources including social network platforms. In order to deal with such amount of data, a new emerging technology “Big Data Analytics” is explored and employed for analyzing consumer behaviors and searching their information needs. Specifically, this paper proposes a Big Data application framework for analyzing consumer behaviors by using topological data structure, co-occurrence methodology and Markov chain theory. First, the consumer related data is translated into a topological data structure. Second, using topological relationships, a co-occurrence matrix is formed to deduce Markov chain model for consumer behavior analysis. Finally, some simulation results are shown to confirm the effectiveness of the proposed framework.;2013;Thi Thi Zin;10.1109/GCCE.2013.6664813;Conferences;2378-8143;978-1-4799-0890-5
ieee_20221205_08_32_33;A successful application of big data storage techniques implemented to criminal investigation for telecom;With the emerging of digital convergence, lots of communication services are generated, and the quantity of data grows rapidly. We face the scalability issue to deal with call data records (CDRs) and so are the other telecom companies. This research uses police CDR query as an example with an intension to increase system execution efficiency and scalability and to reduce total cost by applying cloud service. The implementation applying distributed parallel database (Hive), distributed computing (Hadoop MapReduce), and distributed file system (Hadoop HDFS) will be introduced by a simulation to evaluate the execution efficiency of a query from CDRs. The factors influencing query efficiency, such as the settings of data block size and partition size in HDFS, will also be explored. The experimental results show that applying the big data processing technologies to execute queries from a huge amount of CDRs could improve the system execution efficiency significantly and reduce cost.;2013;Ju-Chi Tseng;;Conferences;;978-4-8855-2279-6
ieee_20221205_08_32_33;Beyond Big Data?;What can we expect to find beyond Big Data? And how can we exploit Big Data to get there?;2013;Judith Bayard Cushing;10.1109/MCSE.2013.102;Magazines;1558-366X;
ieee_20221205_08_32_33;Leveraging Big Data and Business Analytics [Guest editors' introduction];"Big data projects and programs provide tremendous opportunity for organizations looking to transform their operations, innovate in their markets, and better serve their customers; however, these initiatives must be based on sound approaches and principles and not fads or empty vendor claims. This special issue aims to promote a better understanding of big data to foster wider deployment of big data approaches and a new era of business analytics capabilities.";2013;Sunil Mithas;10.1109/MITP.2013.95;Magazines;1941-045X;
ieee_20221205_08_32_33;Proper orthogonal decomposition based parallel compression for visualizing big data on the K computer;The development of supercomputers has greatly help us to carry on large-scale computing for dealing with various problems through simulating and analyzing them. Visualization is an indispensable tool to understand the properties of the data from supercomputers. Especially, interactive visualization can help us to analyze data from various viewpoints and even to find out some local small but important features. However, it is still difficult to interactively visualize such kind of big data directly due to the slow file I/O problem and the limitation of memory size. For resolving these problems, we proposed a parallel compression method to reduce the data size with low computational cost. Furthermore, the fast linear decompression process is another merit for interactive visualization. Our method uses proper orthogonal decomposition (POD) to compress data because it can effectively extract important features from the data and the resulting compressed data can also be linearly decompressed. Our implementation achieves high parallel efficiency with a binary load-distributed approach, which is similar to the binary-swap image composition used in parallel volume rendering [2]. This approach allows us to effectively utilize all the processing nodes and reduce the interprocessor communication cost throughout the parallel compression calculations. Our test results on the K computer demonstrate superior performance of our design and implementation.;2013;Chongke Bi;10.1109/LDAV.2013.6675169;Conferences;;978-1-4799-1659-7
ieee_20221205_08_32_33;RUBA: Real-time unstructured big data analysis framework;We are greeting “Big Data Generation”. As ICT technology is developing, the volume of data is incredibly growing and many works to deal with a big data are underway. In this paper, we proposed a novel framework for real-time unstructured big data analysis, such as a movie, sound, text and image data. Our proposed framework provides functions of a real-time analysis and dynamic modification for unstructured big data analysis. We have implemented the object monitoring system as a test system which is applied our framework, and we have confirmed each functions and the availability of our framework.;2013;Jaein Kim;10.1109/ICTC.2013.6675410;Conferences;2162-1241;978-1-4799-0698-7
ieee_20221205_08_32_33;Efficient and Customizable Data Partitioning Framework for Distributed Big RDF Data Processing in the Cloud;Big data business can leverage and benefit from the Clouds, the most optimized, shared, automated, and virtualized computing infrastructures. One of the important challenges in processing big data in the Clouds is how to effectively partition the big data to ensure efficient distributed processing of the data. In this paper we present a Scalable and yet customizable data PArtitioning framework, called SPA, for distributed processing of big RDF graph data. We choose big RDF datasets as our focus of the investigation for two reasons. First, the Linking Open Data cloud has put forwards a good number of big RDF datasets with tens of billions of triples and hundreds of millions of links. Second, such huge RDF graphs can easily overwhelm any single server due to the limited memory and CPU capacity and exceed the processing capacity of many conventional data processing software systems. Our data partitioning framework has two unique features. First, we introduce a suite of vertexcentric data partitioning building blocks to allow efficient and yet customizable partitioning of large heterogeneous RDF graph data. By efficient, we mean that the SPA data partitions can support fast processing of big data of different sizes and complexity. By customizable, we mean that the SPA partitions are adaptive to different query types. Second, we propose a selection of scalable techniques to distribute the building block partitions across a cluster of compute nodes in a manner that minimizes inter-node communication cost by localizing most of the queries on distributed partitions. We evaluate our data partitioning framework and algorithms through extensive experiments using both benchmark and real datasets. Our experimental results show that the SPA data partitioning framework is not only efficient for partitioning and distributing big RDF datasets of diverse sizes and structures but also effective for processing big data queries of different types and complexity.;2013;Kisung Lee;10.1109/CLOUD.2013.63;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_32_33;Toward an Ecosystem for Precision Sharing of Segmented Big Data;As the amount of data created and stored by organizations continues to increase, attention is turning to extracting knowledge from that raw data, including making some data available outside of the organization to enable crowd analytics. The adoption of the MapReduce paradigm has made processing Big Data more accessible, but is still limited to data that is currently available, often only within an organization. Fine-grained control over what information is shared outside an organization is difficult to achieve with Big Data, particularly in the MapReduce model. We introduce a novel approach to sharing that enables fine-grained control over what data is shared. Users submit analytics tasks that run on infrastructure near the actual data, reducing network bottlenecks. Organizations allow access to a logical version of their data created at runtime by filtering and transforming the actual data without creating storage-intensive stale copies, and resellers can further segment or augment this data to provide added value to analytics tasks. A loosely-coupled ecosystem driven by web services allows for discovery and sharing with a flexible, secure environment that limits the knowledge those running analytics need to have about the actual provider of the data. We describe a proof-of-concept implementation of the various components required to realize this ecosystem, and present a set of experiments to demonstrate feasibility, showing advantageous performance versus storage trade-offs.;2013;Mark Shtern;10.1109/CLOUD.2013.131;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_32_33;Tape Cloud: Scalable and Cost Efficient Big Data Infrastructure for Cloud Computing;Magnetic tapes have been a primary medium of backup storage for a long time in many organizations. In this paper, the possibility of establishing an inter-network accessible, centralized, tape based data backup facility is evaluated. Our motive is to develop a cloud storage service that organizations can use for long term storage of big data which is typically Write-Once-Read-Many. This Infrastructure-as-a-Service (IaaS) cloud can provide the much needed cost effectiveness in storing huge amounts of data exempting client organizations from high infrastructure investments. We make an attempt to understand some of the limitations induced by the usage of tapes by studying the latency of tape libraries in scenarios most likely faced in the backing up process in comparison to its hard disk counterpart. The result of this study is an outline of methods to overcome these limitations by adopting novel tape storage architectures, filesystem, schedulers to manage data transaction requests from various clients and develop faster ways to retrieve requested data to extend the applications beyond backup. We use commercially available tapes and a tape library to perform latency tests and understand the basic operations of tape. With the optimistic backing of statistics that suggests the extensive usage of tapes to this day and in future, we propose an architecture to provide data backup to a large and diverse client base.;2013;Varun S. Prakash;10.1109/CLOUD.2013.129;Conferences;2159-6182;978-0-7695-5028-2
ieee_20221205_08_32_33;Moving Big Data to The Cloud: An Online Cost-Minimizing Approach;Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible or secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geo-dispersed data into the cloud, for processing using a MapReduce-like framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms: an online lazy migration (OLM) algorithm and a randomized fixed horizon control (RFHC) algorithm , for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. Careful comparisons among these online and offline algorithms in realistic settings are conducted through extensive experiments, which demonstrate close-to-offline-optimum performance of the online algorithms.;2013;Linquan Zhang;10.1109/JSAC.2013.131211;Journals;1558-0008;
ieee_20221205_08_32_33;Social Genome: Putting Big Data to Work for Population Informatics;Data-intensive research using distributed, federated, person-level datasets in near real time has the potential to transform social, behavioral, economic, and health sciences--but issues around privacy, confidentiality, access, and data integration have slowed progress in this area. When technology is properly used to manage both privacy concerns and uncertainty, big data will help move the growing field of population informatics forward.;2014;Hye-Chung Kum;10.1109/MC.2013.405;Magazines;1558-0814;
ieee_20221205_08_32_33;Influence maximization for Big Data through entropy ranking and min-cut;As Big Data becomes prevalent, the traditional models from Data Mining or Data Analysis, although very efficient, lack the speed necessary to process problems with data sets in the range of million samples. Therefore, the need for designing more efficient and faster algorithms for these new types of problems. Specifically, from the field of social network analysis, we have the influence maximization problem. This is a problem with many possible applications in advertising, marketing, social studies, etc, where we have representations of influences by large scale graphs. Even though, the optimal solution of this problem, the minimum set of graph nodes which can influence a maximum set of nodes, is a NP-Hard problem, it is possible to devise an approximated solution to the problem. In this paper, we have proposed a novel algorithm for influence maximization analysis. This algorithm consist in two phases: the first one is an entropy based node ranking where entropy ranking is used to determine node importance in a directed weighted influence graph. The second phase computes the minimum cut using a novel metric. To test the propose algorithm, experiments were performed in several popular data sets to evaluate performance and the seed quality over the influences.;2013;Agustin Sancen-Plaza;10.4108/icst.collaboratecom.2013.254119;Conferences;;978-1-936968-92-3
ieee_20221205_08_32_33;Real-time collaborative planning with big data: Technical challenges and in-place computing (invited paper);There is increasing collaboration in new generation supply chain planning applications, where participants across a supply chain analyze and plan on a big volume of sales data over the internet together. To achieve real-time collaborative planning over big data, we have developed an unconventional technology, BigObject, based on an in-place computing approach in two ways. First, instead of moving (big) data around, move (small) code to where data resides for execution. Second, organize the complexity by determining the basic functional units (objects) for computing in the same sense that macromolecules are determined for living cells. The term ”in-place” indicates that data is in residence in memory space and ready for computing. BigObject is an in-place computing system, designed for storing and computing multidimensional data. Our experiment shows that in-place computing approach outperforms traditional computing approach in two orders of magnitude.;2013;Wenwey Hseush;10.4108/icst.collaboratecom.2013.254100;Conferences;;978-1-936968-92-3
ieee_20221205_08_32_33;A Sketch of Big Data Technologies;This paper outlines the recent developed information technologies in big data. The basic principles and theories, concepts and terminologies, methods and implementations, and the status of research and development in big data are depicted. The paper also highlights the technical challenges and major difficulties. The number of key technologies required to handle big data are deliberated. They include big data acquisition, pre/post-processing, data storage and distribution, networks, and analysis and mining, etc. At last, the development trend in big data technologies is addressed for discussion.;2013;Zaiying Liu;10.1109/ICICSE.2013.13;Conferences;2330-9857;978-0-7695-5118-0
ieee_20221205_08_32_33;An Iterative Hierarchical Key Exchange Scheme for Secure Scheduling of Big Data Applications in Cloud Computing;As the new-generation distributed computing platform, cloud computing environments offer high efficiency and low cost for data-intensive computation in big data applications. Cloud resources and services are available in pay-as-you-go mode, which brings extraordinary flexibility and cost-effectiveness as well as zero investment in their own computing infrastructure. However, these advantages come at a price-people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated Key Exchange (AKE) is essential to a security system that is based on high efficiency symmetric-key encryption. With virtualization technology being applied, existing key exchange schemes such as Internet Key Exchange (IKE) becomes time-consuming when directly deployed into cloud computing environment. In this paper we propose a novel hierarchical key exchange scheme, namely Cloud Background Hierarchical Key Exchange (CBHKE). Based on our previous work, CBHKE aims at providing secure and efficient scheduling for cloud computing environment. In our new scheme, we design a two-phase layer-by-layer iterative key exchange strategy to achieve more efficient AKE without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud computing environment, efficiency of the proposed scheme is dramatically superior to its predecessors CCBKE and IKE schemes.;2013;Chang Liu;10.1109/TrustCom.2013.65;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_32_33;Combining Top-Down and Bottom-Up: Scalable Sub-tree Anonymization over Big Data Using MapReduce on Cloud;In big data applications, data privacy is one of the most concerned issues because processing large-scale privacy-sensitive data sets often requires computation power provided by public cloud services. Sub-tree data anonymization, achieving a good trade-off between data utility and distortion, is a widely adopted scheme to anonymize data sets for privacy preservation. Top-Down Specialization (TDS) and Bottom-Up Generalization (BUG) are two ways to fulfill sub-tree anonymization. However, existing approaches for sub-tree anonymization fall short of parallelization capability, thereby lacking scalability in handling big data on cloud. Still, both TDS and BUG suffer from poor performance for certain value of k-anonymity parameter if they are utilized individually. In this paper, we propose a hybrid approach that combines TDS and BUG together for efficient sub-tree anonymization over big data. Further, we design MapReduce based algorithms for two components (TDS and BUG) to gain high scalability by exploiting powerful computation capability of cloud. Experiment evaluations demonstrate that the hybrid approach significantly improves the scalability and efficiency of sub-tree anonymization scheme over existing approaches.;2013;Xuyun Zhang;10.1109/TrustCom.2013.235;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_32_33;Big Data Real-Time Processing Based on Storm;As the growth of Internet, Cloud Computing, Mobile Network and Internet of Things is increasing rapidly, Big Data is becoming a hot-spot in recent years. Big Data Processing is involved in our daily life such as mobile devices, RFID and wireless sensors, which aims at dealing with billions of users' interactive data. At the same time, real-time processing is eagerly needed in integrated system. In this paper, several technologies associated with real-time big data processing are introduced, among which the core technology called Storm is emphasized. An entire system is built based on Storm, associated with RabbitMQ, NoSQL and JSP. To ensure the practical applicability and high efficiency, a simulation system is established and shows acceptable performance in various expressions using data sheet and Ganglia. It is proved that the big data real-time processing based on Storm can be widely used in various computing environment.;2013;Wenjie Yang;10.1109/TrustCom.2013.247;Conferences;2324-9013;978-0-7695-5022-0
ieee_20221205_08_32_33;A Universal Storage Architecture for Big Data in Cloud Environment;With the rapid development of the Internet of Things and Electronic Commerce, we have entered the era of big data. The characteristics, such as great amount and heterogeneousity, of big data bring the challenge to the storage and analytics. The paper presented a universal storage architecture for big data in cloud environment. We use clustering analysis to divide the cloud nodes into multiple clusters according to the communication cost between different nodes. The cluster with the strongest computing power is selected to provide the universal storage and query interface for users. Each of other clusters is responsible for storing the data of a particular model, such as relational data, key-value data, and document data and so on. Experiments show that our architecture can store all kinds of heterogeneous big data and provide users with unified storage and query interface for big data easily and quickly.;2013;Qingchen Zhang;10.1109/GreenCom-iThings-CPSCom.2013.96;Conferences;;978-0-7695-5046-6
ieee_20221205_08_32_33;IOT-StatisticDB: A General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things;In large scale Internet of Things (IoT) systems, statistical analysis is a crucial technique for transforming data into knowledge and for obtaining overall information about the physical world. However, most existing statistical analysis methods for sensor sampling data are implemented outside the database kernel and focus on specialized analytics, making them unsuited for the IoT environment where both the data types and the statistical queries are diverse. To solve this problem, we propose a General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things (IOT-StatisticDB) in this paper. In IOT-StatisticDB, statistical functions are performed through statistical operators inside the DBMS kernel, so that complicated statistical queries can be expressed in the standard SQL format. Besides, statistical analysis is executed in a distributed and parallel manner over multiple servers so that the performance can be greatly improved, which is confirmed by the experiments.;2013;Zhiming Ding;10.1109/GreenCom-iThings-CPSCom.2013.104;Conferences;;978-0-7695-5046-6
ieee_20221205_08_32_33;Big Data Analytics for Security;"Big data is changing the landscape of security tools for network monitoring, security information and event management, and forensics; however, in the eternal arms race of attack and defense, security researchers must keep exploring novel ways to mitigate and contain sophisticated attackers.";2013;Alvaro A. Cárdenas;10.1109/MSP.2013.138;Magazines;1558-4046;
ieee_20221205_08_32_33;Rapid Scanning of Spectrograms for Efficient Identification of Bioacoustic Events in Big Data;Acoustic sensing is a promising approach to scaling faunal biodiversity monitoring. Scaling the analysis of audio collected by acoustic sensors is a big data problem. Standard approaches for dealing with big acoustic data include automated recognition and crowd based analysis. Automatic methods are fast at processing but hard to rigorously design, whilst manual methods are accurate but slow at processing. In particular, manual methods of acoustic data analysis are constrained by a 1:1 time relationship between the data and its analysts. This constraint is the inherent need to listen to the audio data. This paper demonstrates how the efficiency of crowd sourced sound analysis can be increased by an order of magnitude through the visual inspection of audio visualized as spectrograms. Experimental data suggests that an analysis speedup of 12× is obtainable for suitable types of acoustic analysis, given that only spectrograms are shown.;2013;Anthony Truskinger;10.1109/eScience.2013.25;Conferences;;978-0-7695-5083-1
ieee_20221205_08_32_33;Catching the wave: Big data in the classroom;Many diverse domains-in the sciences, engineering, healthcare, and homeland security-have been grappling with the analysis of “Big Data,” which has become shorthand to represent extremely large amounts of diverse types of data. A recent Gartner report predicts that around 4.4 million IT jobs globally will be created by 2015 to support Big Data, with 1.9 million of those jobs in the United States. Therefore, understanding approaches and techniques for handling and analyzing Big Data from diverse domains has become crucial for not only in computing but also engineering students. The mini-workshop will make use of active and collaborative learning exercises to introduce faculty in computer science, software engineering, and other disciplines to concepts and techniques involved in managing and analyzing Big Data. Approaches for incorporating Big Data into the engineering and computing curricula will also be presented.;2013;Carol J. Romanowski;10.1109/FIE.2013.6684855;Conferences;2377-634X;978-1-4673-5261-1
ieee_20221205_08_32_33;The Convergence of Big Data and Mobile Computing;In recent years, we have witnessed the influx of data driven by the exponential use of digital video, music, and mobile applications. The combination of the internet, mobile technologies and social media applications has been the most influential factor in the emergence of Big Data paradigm. Given that online activities of mobile users constitute the major contributors to the Big Data analytics, which significantly benefit various business enterprises, it is also interesting to look at how the information derived may also be useful for mobile users, and how it can be delivered in the most effective, and efficient manner. These will be the key points of discussion in this paper.;2013;Agustinus Borgy Waluyo;10.1109/NBiS.2013.15;Conferences;2157-0426;978-1-4799-2509-4
ieee_20221205_08_32_33;A Big Data Approach for a New ICT Agriculture Application Development;"Big Data is becoming a common term among researchers, who are looking for a tool to broaden their research and to improve their results because the ""probable"" relation between different scientific areas. But, although the term Big Data is not new, its recent application and methodologies are changing some well establish paradigms in the research area as well in the several industry applications where Big Data methodologies are used. Because of their rapid development, Big Data is also raising specific issues related to some of its core concepts. It is the aim of this paper to address the impact of these issues in the novel nutrition-based vegetable production and distribution system project in which Sojo University is an active member.";2013;R. Dennis A. Ludena;10.1109/CyberC.2013.30;Conferences;;978-0-7695-5106-7
ieee_20221205_08_32_33;Big Data a Sure Thing for Telecommunications: Telecom's Future in Big Data;Big Data is made for telecommunications. No other industry has access to the wealth of information about their customers the way communications service providers (CSPs) do. Big data can be seen as the CSPs' most valuable asset. It puts them in a key position to win the battle for customers and generate new revenue streams - provided they can get their acts together.;2013;Rob Van Den Dam;10.1109/CyberC.2013.32;Conferences;;978-0-7695-5106-7
ieee_20221205_08_32_33;Big Data Analytics in the Public Sector: Improving the Strategic Planning in World Class Universities;"This paper presents an application in Information Fusion related to Analytics and fusion of Big Data. The approach involves the analysis of specific aspects of the management of a Brazilian university and the proposition of a new framework that may be useful mainly for universities focused on improving their strategic planning while having excellence in operational execution. The framework intends to be used for the development of new decision support systems that integrate ""past data"" with ""real time data"". The research methodology emphasizes induction with the collection of qualitative data in order to move from observed facts to theory. The main result is the framework itself while future work may involve the development and the validation of the new system.";2013;Joni A. Amorim;10.1109/CyberC.2013.33;Conferences;;978-0-7695-5106-7
ieee_20221205_08_32_33;A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud;The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.;2013;Xuyun Zhang;10.1109/CGC.2013.24;Conferences;;978-0-7695-5114-2
ieee_20221205_08_32_33;Visual Analytics for Big Data Using R;The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.;2013;Aziz Nasridinov;10.1109/CGC.2013.96;Conferences;;978-0-7695-5114-2
ieee_20221205_08_32_33;A distribute parallel approach for big data scale optimal power flow with security constraints;This paper presents a mathematical optimization framework for security-constrained optimal power flow (SCOPF) computations. The SCOPF problem determines the optimal control of power systems under constraints arising from a set of postulated contingencies. This problem is challenging due to the significantly large problem size, the stringent real-time requirement and the variety of numerous post-contingency states. In order to solve the resultant big data scale optimization problem with manageable complexity, the alternating direction method of multipliers (ADMM) is utilized. The SCOPF is decomposed into independent subproblems correspond to each individual pre-contingency and post-contingency case. Those subproblems are solved in parallel on distributed nodes and coordinated through dual (prices) variables. As a result, the algorithm is implemented in a distributive and parallel fashion. Numerical tests validate the effectiveness of the proposed algorithm.;2013;Lanchao Liu;10.1109/SmartGridComm.2013.6688053;Conferences;;978-1-4799-1526-2
ieee_20221205_08_32_33;Towards Service-Oriented Enterprise Architectures for Big Data Applications in the Cloud;Applications with Service-oriented Enterprise Architectures in the Cloud are emerging and will shape future trends in technology and communication. The development of such applications integrates Enterprise Architecture and Management with Architectures for Services & Cloud Computing, Web Services, Semantics and Knowledge-based Systems, Big Data Management, among other Architecture Frameworks and Software Engineering Methods. In the present work in progress research, we explore Service-oriented Enterprise Architectures and application systems in the context of Big Data applications in cloud settings. Using a Big Data scenario, we investigate the integration of Services and Cloud Computing architectures with new capabilities of Enterprise Architectures and Management. The underlying architecture reference model can be used to support semantic analysis and program comprehension of service-oriented Big Data Applications. Enterprise Services Computing is the current trend for powerful large-scale information systems, which increasingly converge with Cloud Computing environments. In this paper we combine architectures for services with cloud computing. We propose a new integration model for service-oriented Enterprise Architectures on basis of ESARC - Enterprise Services Architecture Reference Cube, which is our previous developed service-oriented enterprise architecture classification framework, with MFESA - Method Framework for Engineering System Architectures - for the design of service-oriented enterprise architectures, and the systematic development, diagnostics and optimization of architecture artifacts of service-oriented cloud-based enterprise systems for Big Data applications.;2013;Alfred Zimmermann;10.1109/EDOCW.2013.21;Conferences;2325-6605;978-1-4799-3048-7
ieee_20221205_08_32_33;Strategic Alignment of Cloud-Based Architectures for Big Data;Big Data is an increasingly significant topic for management and IT departments. In the beginning, Big Data applications were large on premise installations. Today, cloud services are used increasingly to implement Big Data applications. This can be done on different ways supporting different strategic enterprise goals. Therefore, we develop a framework that enumerates the alternatives for implementing Big Data applications using cloud-services and identify the strategic goals supported by these Alternatives. The created framework clarifies the options for Big Data initiatives using cloud-computing and thus improves the strategic alignment of Big Data applications.;2013;Rainer Schmidt;10.1109/EDOCW.2013.22;Conferences;2325-6605;978-1-4799-3048-7
ieee_20221205_08_32_33;Security — A big question for big data;Summary form only given. Big data implies performing computation and database operations for massive amounts of data, remotely from the data owner's enterprise. Since a key value proposition of big data is access to data from multiple and diverse domains, security and privacy will play a very important role in big data research and technology. The limitations of standard IT security practices are well-known, making the ability of attackers to use software subversion to insert malicious software into applications and operating systems a serious and growing threat whose adverse impact is intensified by big data. So, a big question is what security and privacy technology is adequate for controlled assured sharing for efficient direct access to big data. Making effective use of big data requires access from any domain to data in that domain, or any other domain it is authorized to access. Several decades of trusted systems developments have produced a rich set of proven concepts for verifiable protection to substantially cope with determined adversaries, but this technology has largely been marginalized as “overkill” and vendors do not widely offer it. This talk will discuss pivotal choices for big data to leverage this mature security and privacy technology, while identifying remaining research challenges.;2013;Roger Schell;10.1109/BigData.2013.6691547;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Communication efficient algorithms for fundamental big data problems;Big Data applications often store or obtain their data distributed over many computers connected by a network. Since the network is usually slower than the local memory of the machines, it is crucial to process the data in such a way that not too much communication takes place. Indeed, only communication volume sublinear in the input size may be affordable. We believe that this direction of research deserves more intensive study. We give examples for several fundamental algorithmic problems where nontrivial algorithms with sublinear communication volume are possible. Our main technical contribution are several related results on distributed Bloom filter replacements, duplicate detection, and data base join. As an example of a very different family of techniques, we discuss linear programming in low dimensions.;2013;Peter Sanders;10.1109/BigData.2013.6691549;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;P-DOT: A model of computation for big data;"In response to the high demand of big data analytics, several programming models on large and distributed cluster systems have been proposed and implemented, such as MapRe-duce, Dryad and Pregel. However, compared with high performance computing areas, the basis and principles of computation and communication behavior of big data analytics is not well studied. In this paper, we review the current big data computational model DOT and DOTA, and propose a more general and practical model p-DOT (p-phases DOT). p-DOT is not a simple extension, but with profound significance: for general aspects, any big data analytics job execution expressed in DOT model or BSP model can be represented by it; for practical aspects, it considers I/O behavior to evaluate performance overhead. Moreover, we provide a cost function implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, and demonstrate the effectiveness of the function through several experiments.";2013;Tao Luo;10.1109/BigData.2013.6691551;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Elastic algorithms for guaranteeing quality monotonicity in big data mining;When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.;2013;Rui Han;10.1109/BigData.2013.6691553;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Building a generic platform for big sensor data application;The drive toward smart cities alongside the rising adoption of personal sensors is leading to a torrent of sensor data. While systems exist for storing and managing sensor data, the real value of such data is the insight which can be generated from it. However there is currently no platform which enables sensor data to be taken from collection, through use in models to produce useful data products. The architecture of such a platform is a current research question in the field of Big Data and Smart Cities. In this paper we explore five key challenges in this field and provide a response through a sensor data platform “Concinnity” which can take sensor data from collection to final product via a data repository and workflow system. This will enable rapid development of applications built on sensor data using data fusion and the integration and composition of models to form novel workflows. We summarize the key features of our approach, exploring how it enables value to be derived from sensor data efficiently.;2013;Chun-Hsiang Lee;10.1109/BigData.2013.6691559;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;clusiVAT: A mixed visual/numerical clustering algorithm for big data;Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.;2013;Dheeraj Kumar;10.1109/BigData.2013.6691561;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Algebraic dataflows for big data analysis;Analyzing big data requires the support of dataflows with many activities to extract and explore relevant information from the data. Recent approaches such as Pig Latin propose a high-level language to model such dataflows. However, the dataflow execution is typically delegated to a MapRe-duce implementation such as Hadoop, which does not follow an algebraic approach, thus it cannot take advantage of the optimization opportunities of PigLatin algebra. In this paper, we propose an approach for big data analysis based on algebraic workflows, which yields optimization and parallel execution of activities and supports user steering using provenance queries. We illustrate how a big data processing dataflow can be modeled using the algebra. Through an experimental evaluation using real datasets and the execution of the dataflow with Chiron, an engine that supports our algebra, we show that our approach yields performance gains of up to 19.6% using algebraic optimizations in the dataflow and up to 39.1% of time saved on a user steering scenario.;2013;Jonas Dias;10.1109/BigData.2013.6691567;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Robot: An efficient model for big data storage systems based on erasure coding;It is well-known that with the explosive growth of data, the age of big data has arrived. How to save huge amounts of data is of great importance to both industry and academia. This paper puts forward a solution based on coding technologies in big data system that store a lot of cold data. By studying existing coding technologies and big data systems, we can not only maintain the system's reliability, but also improve the security and the utilization of storage systems. Due to the remarkable reliability and space saving rate of coding technologies, importing coding schema in to big data systems becomes prerequisite. In our presented schema, the storage node is divided into several virtual nodes to keep load balancing. By setting up different virtual node storage groups for different codec server, we can ensure system availability. And by utilizing the parallel decoding computing of the node and the block of data, we can also reduce the system recovery time when data is corrupted. Additionally, different users set different coding parameters can improve the robustness of big data storage systems. We configure various data block m and calibration block k to improve the utilization rate in the quantitative experiments. The results shows that parallel decoding speed can rise up two times than the past serial decoding speed. The encoding efficiency with ICRS coding is 34.2% higher than using CRS and 56.5% more than using RS coding equally. The decoding rate by using ICRS is 18.1% higher than using CRS and 31.1% higher than using RS averagely.;2013;Chao Yin;10.1109/BigData.2013.6691569;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Multilevel Active Storage for big data applications in high performance computing;Given the growing importance of supporting dataintensive sciences and big data applications, an effective HPC I/O solution has become a key issue and has attracted intensive attention in recent years. Active storage has been shown effective in reducing data movement and network traffic as a potential new I/O solution. Existing prototypes and systems, however, are primarily designed for read-intensive applications. In addition, they generally assume that offloaded processing kernels have small computational demands, which makes this solution a poor fit for data-intensive operations that have significant computational demands, including write-intensive operations. In this research, we propose a new Multilevel Active Storage (MAS) solution. The new MAS design can support and handle both read- and write-intensive operations, as well as complex operations that have considerable computational demands. Experimental tests have been carried out and confirmed that the MAS approach is feasible and outperformed existing approaches. The new multilevel active storage design has a potential to deliver a high performance I/O solution for big data applications in HPC.;2013;Chao Chen;10.1109/BigData.2013.6691570;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;GPU accelerated item-based collaborative filtering for big-data applications;"Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.";2013;Chandima Hewa Nadungodage;10.1109/BigData.2013.6691571;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Using pattern-models to guide SSD deployment for Big Data applications in HPC systems;"Flash-memory based Solid State Drives (SSDs) embrace higher performance and lower power consumption compared to traditional storage devices (HDDs). These benefits are needed in HPC systems, especially with the growing demand of supporting Big Data applications. In this paper, we study placement and deployment strategies of SSDs in HPC systems to maximize the performance improvement, given a practical fixed hardware budget constraint. We propose a pattern-model approach to guide SSD deployment for HPC systems through two steps; characterizing workload and mapping deployment strategy. The first step is responsible for characterizing the access patterns of the workload and the second step contributes the actual deployment recommendation for Parallel File System (PFS) configuration combining with an analytical model. We have carried out initial experimental tests and the results confirmed that the proposed approach can guide placement of SSDs in HPC systems for accelerating data accesses. Our research will be helpful in guiding designs and developments for Big Data applications in current and projected HPC systems including exascale systems.";2013;Junjie Chen;10.1109/BigData.2013.6691592;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Efficient large graph pattern mining for big data in the cloud;Mining big graph data is an important problem in the graph mining research area. Although cloud computing is effective at solving traditional algorithm problems, mining frequent patterns of a massive graph with cloud computing still faces the three challenges: 1) the graph partition problem, 2) asymmetry of information, and 3) pattern-preservation merging. Therefore, this paper presents a new approach, the cloud-based SpiderMine (c-SpiderMine), which exploits cloud computing to process the mining of large patterns on big graph data. The proposed method addresses the above issues for implementing a big graph data mining algorithm in the cloud. We conduct the experiments with three real data sets, and the experimental results demonstrate that c-SpiderMine can significantly reduce execution time with high scalability in dealing with big data in the cloud.;2013;Chun-Chieh Chen;10.1109/BigData.2013.6691618;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A Higher-order data flow model for heterogeneous Big Data;We introduce a data flow model that supports highly parallelisable design patterns and also has useful properties for analysing data serially over extended time periods without requiring traditional Big Data computing facilities. The model ranges over a class of higher-order relations which are sufficiently expressive to represent a wide variety of unstructured, semi-structured and structured data. Using JSONMatch, our web service implementation of the model, we show that the combination of this model and higher-order representation provides a powerful and extensible framework that is particularly well suited to analysing Big Variety data in a web application context.;2013;Simon Price;10.1109/BigData.2013.6691624;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Breaking the Arc: Risk control for Big Data;"The use of Big Data technologies and analytics have the potential to revolutionise the world. The mass instrumentation of the planet and society is providing intelligence that is not only enhancing our personal lives, but also opening up new opportunities for addressing some of key environmental, social and economic challenges of the 21st century. Unfortunately, as with all technology, there is the potential for misuse; in the case of personal data the ability to gather, enrich and mine at extreme pace and volume could result in societal-scale privacy intrusions. We apply a model for identity across cyber and physical spaces to the question of risk control for personal-data in the context of big data analytics. Using a graphical model for identity we reflect on the response options we have and how such risk controls may or may not be effective.";2013;Duncan Hodges;10.1109/BigData.2013.6691630;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation;"The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10-6) and very poor (102) selectivity, and short (seconds) to long (hours) job duration.";2013;Tim Hegeman;10.1109/BigData.2013.6691631;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Big data analytics on high Velocity streams: A case study;"Big data management is often characterized by three Vs: Volume, Velocity and Variety. While traditional batch-oriented systems such as MapReduce are able to scale-out and process very large volumes of data in parallel, they also introduce some significant latency. In this paper, we focus on the second V (Velocity) of the Big Data triad; We present a case-study where we use a popular open-source stream processing engine (Storm) to perform real-time integration and trend detection on Twitter and Bitly streams. We describe our trend detection solution below and experimentally demonstrate that our architecture can effectively process data in real-time - even for high-velocity streams.";2013;Thibaud Chardonnens;10.1109/BigData.2013.6691653;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Visualization and rhetoric: Key concerns for utilizing big data in humanities research: A case study of vaccination discourses: 1918-1919;Visualization of data mining results is the linchpin of successful research in the humanities that uses computational techniques. This paper describes efforts to utilize “big data” in a case study of news reporting on vaccination before, during, and after the 1918 influenza pandemic, focusing primarily on the conventions underlying methods of data extraction, data visualization practices, and the rhetorical impact of visualization design choices on researchers' observations and interpretive decisions. Purposeful attention to visualization and the methodological conventions that are embedded in particular visualization practices will allow humanists to have more confidence in their interpretations of big data, a key element in the acceptance of data mining as a valuable method for humanities research.;2013;Kathleen Kerr;10.1109/BigData.2013.6691666;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Humanities ‘big data’: Myths, challenges, and lessons;This paper argues that there have always been `big data' in the humanities, and challenges commonly held myths in this regard. It does so by discussing the case of transnational research on dispersed communities. Concluding, it examines the lessons humanities and sciences can learn from each other.;2013;Amalia S. Levi;10.1109/BigData.2013.6691667;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Bibliographic records as humanities big data;Most discussion hitherto of big data in the humanities has assumed that it is characterized by its heterogeneous nature. This paper examines the extent to which bibliographic records generated by libraries represent a more homogenous form of humanities big data, more closely related to the observational big data generated by scientific data. It is suggested from an examination of the British Library catalogue that, while superficially bibliographic records appear to be created according to consistent standards and form a more homogenous dataset, close examination reveals that bibliographical records often go through a marked process of historical development. However, the critical methods require to disaggregate such data are perhaps analogous to those used in some scientific disciplines.;2013;Andrew Prescott;10.1109/BigData.2013.6691670;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A concept of Generic Workspace for Big Data Processing in Humanities;Big Data challenges often require application of new data processing paradigms (like MapReduce), and corresponding software solutions (e. g. Hadoop). This trend causes a pressure on both cyber-infrastructure providers (to quickly integrate new services) and infrastructure users (to quickly learn to use new tools). In this paper we present the concept of DARIAH Generic Workspace for Big Data Processing in eHumanities which alleviates the aforementioned problems. It establishes a common integration layer, thus enables a quick integration of new services, and by providing unified interfaces, allows the users to start using new tools without learning their internal details. We describe the overall architecture and implementation details of the working prototype. The presented concept is generic enough to be applied in other emerging cyber-infrastructures for humanities.;2013;Jedrzej Rybicki;10.1109/BigData.2013.6691672;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A case study on entity Resolution for Distant Processing of big Humanities data;At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.;2013;Weijia Xu;10.1109/BigData.2013.6691678;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Business model canvas perspective on big data applications;Large and complex data that becomes difficult to be handled by traditional data processing applications triggers the development of big data applications which have become more pervasive than ever before. In the era of big data, data exploration and analysis turned into a difficult problem in many sectors such as the smart routing and health care sectors. Companies which can adapt their businesses well to leverage big data have significant advantages over those that lag this capability. The need for exploring new approaches to address the challenges of big data forces companies to shape their business models accordingly. In this paper, we summarize and share our findings regarding the business models deployed in big data applications in different sectors. We analyze existing big data applications by taking into consideration the core elements of a business (via business model canvas) and present how these applications provide value to their customers by making profit out of using big data.;2013;F. Canan Pembe Muhtaroğlu;10.1109/BigData.2013.6691684;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Understanding the value of (big) data;This paper acts as a primer on an economic outlook at the value and pricing of big data. We introduce a simple taxonomy, discuss rights to access and analyze the case of big data as a common pool resource.;2013;Koutroumpis Pantelis;10.1109/BigData.2013.6691691;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Memory system characterization of big data workloads;Two recent trends that have emerged include (1) Rapid growth in big data technologies with new types of computing models to handle unstructured data, such as map-reduce and noSQL (2) A growing focus on the memory subsystem for performance and power optimizations, particularly with emerging memory technologies offering different characteristics from conventional DRAM (bandwidths, read/write asymmetries). This paper examines how these trends may intersect by characterizing the memory access patterns of various Hadoop and noSQL big data workloads. Using memory DIMM traces collected using special hardware, we analyze the spatial and temporal reference patterns to bring out several insights related to memory and platform usages, such as memory footprints, read-write ratios, bandwidths, latencies, etc. We develop an analysis methodology to understand how conventional optimizations such as caching, prediction, and prefetching may apply to these workloads, and discuss the implications on software and system design.;2013;Martin Dimitrov;10.1109/BigData.2013.6691693;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;An ensemble MIC-based approach for performance diagnosis in big data platform;The era of big data has began. Although applications based on big data bring considerable benefit to IT industries, governments and social organizations, they bring more challenges to the management of big data platforms which are the fundamental infrastructures due to the complexity, variety, velocity and volume of big data. To offer a healthy platform for big data applications, we propose a novel signature-based performance diagnosis approach employing MIC invariants between performance metrics. We formalize the performance diagnosis as a pattern recognition problem. The normal state of a big data application is used to train a set of MIC (Maximum Information Criterion) invariants. One performance problem occurred in the big data application is identified by a unique binary tuple consisted by a set violations of MIC invariants. All the signatures of performance problems form a diagnosis knowledge database. If the KPI (Key Performance Indicator) of the big data application deviates its normal region, our approach can identify the real culprits through looking for similar signatures in the signature database. To detect the deviation of the KPI, we propose a new metric named unpredictability based on ARIMA model. And considering the variety of big data applications, we build an ensemble performance diagnosis approach which means a unique ARIMA model and a unique set of MIC invariants are built for a specific kind of application. Through experiment evaluation in a controlled environment running a state of the art big data benchmark, we find our approach can pinpoint the real culprits of performance problems in an average 83% precision and 87% recall which is better than a correlation based and single model based performance diagnosis.;2013;Pengfei Chen;10.1109/BigData.2013.6691701;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;The implications from benchmarking three big data systems;Along with today's data explosion and application diversification, a variety of hardware platforms for data centers are emerging and are attracting interests from both industry and academia. The existing hardware platforms represent a wide range of implementation approaches, and different hardware have different strengths. In this paper, we conduct comprehensive evaluations on three representative data center systems based on BigDataBench, which is a benchmark suite for benchmarking and ranking systems running big data applications. Then we explore the relative performance of the three implementation approaches with different big data applications, and provide strong guidance for the data center system construction. Through our experiments, we has inferred that a data center system based on specific hardware has different performance in the context of different applications and data volumes. When we construct a system, we can take into account not only the performance or energy consumption of the pure hardwares, but also the application-level characteristics. Data scale, application type and complexity should be considered comprehensively when researchers or architects plan to choose fundamental components for their data center system.;2013;Jing Quan;10.1109/BigData.2013.6691706;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A characterization of big data benchmarks;Recently, big data has been evolved into a buzzword from academia to industry all over the world. Benchmarks are important tools for evaluating an IT system. However, benchmarking big data systems is much more challenging than ever before. First, big data systems are still in their infant stage and consequently they are not well understood. Second, big data systems are more complicated compared to previous systems such as a single node computing platform. While some researchers started to design benchmarks for big data systems, they do not consider the redundancy between their benchmarks. Moreover, they use artificial input data sets rather than real world data for their benchmarks. It is therefore unclear whether these benchmarks can be used to precisely evaluate the performance of big data systems. In this paper, we first analyze the redundancy among benchmarks from ICTBench, HiBench and typical workloads from real world applications: spatio-temporal data analysis for Shenzhen transportation system. Subsequently, we present an initial idea of a big data benchmark suite for spatio-temporal data. There are three findings in this work: (1) redundancy exists in these pioneering benchmark suites and some of them can be removed safely. (2) The workload behavior of trajectory data analysis applications is dramatically affected by their input data sets. (3) The benchmarks created for academic research cannot represent the cases of real world applications.;2013;Wen Xiong;10.1109/BigData.2013.6691707;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A big data analytics framework for scientific data management;The Ophidia project is a research effort addressing big data analytics requirements, issues, and challenges for eScience. We present here the Ophidia analytics framework, which is responsible for atomically processing, transforming and manipulating array-based data. This framework provides a common way to run on large clusters analytics tasks applied to big datasets. The paper highlights the design principles, algorithm, and most relevant implementation aspects of the Ophidia analytics framework. Some experimental results, related to a couple of data analytics operators in a real cluster environment, are also presented.;2013;Sandro Fiore;10.1109/BigData.2013.6691720;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Scalable sentiment classification for Big Data analysis using Naïve Bayes Classifier;A typical method to obtain valuable information is to extract the sentiment or opinion from a message. Machine learning technologies are widely used in sentiment classification because of their ability to “learn” from the training dataset to predict or support decision making with relatively high accuracy. However, when the dataset is large, some algorithms might not scale up well. In this paper, we aim to evaluate the scalability of Naïve Bayes classifier (NBC) in large datasets. Instead of using a standard library (e.g., Mahout), we implemented NBC to achieve fine-grain control of the analysis procedure. A Big Data analyzing system is also design for this study. The result is encouraging in that the accuracy of NBC is improved and approaches 82% when the dataset size increases. We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput.;2013;Bingwei Liu;10.1109/BigData.2013.6691740;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Frequent Itemset Mining for Big Data;Frequent Itemset Mining (FIM) is one of the most well known techniques to extract knowledge from data. The combinatorial explosion of FIM methods become even more problematic when they are applied to Big Data. Fortunately, recent improvements in the field of parallel programming already provide good tools to tackle this problem. However, these tools come with their own technical challenges, e.g. balanced data distribution and inter-communication costs. In this paper, we investigate the applicability of FIM techniques on the MapReduce platform. We introduce two new methods for mining large datasets: Dist-Eclat focuses on speed while BigFIM is optimized to run on really large datasets. In our experiments we show the scalability of our methods.;2013;Sandy Moens;10.1109/BigData.2013.6691742;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;A look at challenges and opportunities of Big Data analytics in healthcare;Big Data analytics can revolutionize the healthcare industry. It can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. This paper provides an overview of Big Data, applicability of it in healthcare, some of the work in progress and a future outlook on how Big Data analytics can improve overall quality in healthcare systems.;2013;Raghunath Nambiar;10.1109/BigData.2013.6691753;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;BIG DATA infrastructures for pharmaceutical research;Big Data is an emerging paradigm covering production, collection, processing, analysis, access and presentation of huge data-sets: Big Data infrastructures represent an opportunity to approach this paradigm on an organizational level. We describe challenges and opportunities of big data infrastructures for the pharmaceutical industry. Pharmaceutical research and product development are huge investments and require intense exploration and analysis of data. Future trends show that pharmaceutical companies need to develop new methods of data and information processing to quicker and more precisely respond to changing markets and the need of patients and providers. The individual case and patients will become more influential in healthcare delivery and in consequence for the rationale behind these decisions. Our approach of a platform for semantic exploitation of BIG DATA supports a knowledge based infrastructure for deep analysis of clinical information from structured sources and clinical narratives. This infrastructure is able to pave the way towards the necessary big data-management possibilities. Example applications for cancer research will be given.;2013;Christian Seebode;10.1109/BigData.2013.6691759;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Big data solutions for predicting risk-of-readmission for congestive heart failure patients;Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.;2013;Kiyana Zolfaghar;10.1109/BigData.2013.6691760;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Big spatial data mining;In this paper, spatial data mining is discussed in the context of big data. Firstly, we elaborate the fact that spatial data plays a primary role in big data, attracting academic community, business industry and governments. Secondly, the adverse of spatial data mining is discussed, such as much garbage, heavy pollution and its difficulties in utilization. Finally, we dissect the value in spatial big data, expound the techniques to discover knowledge from spatial big data, and investigate the transformation from knowledge into data intelligences.;2013;Wang Shuliang;10.1109/BigData.2013.6691764;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;IntegrityMR: Integrity assurance framework for big data analytics and management applications;Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).;2013;Yongzhi Wang;10.1109/BigData.2013.6691780;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Exploring big data in small forms: A multi-layered knowledge extraction of social networks;Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.;2013;Yun Wei Zhao;10.1109/BigData.2013.6691784;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Tile based visual analytics for Twitter big data exploratory analysis;New tools for raw data exploration and characterization of “big data” sets are required to suggest initial hypotheses for testing. The widespread use and adoption of web-based geo maps have provided a familiar set of interactions for exploring extremely large geo data spaces and can be applied to similarly large abstract data spaces. Building on these techniques, a tile based visual analytics system (TBVA) was developed that demonstrates interactive visualization for a one billion point Twitter dataset. TBVA enables John Tukey-inspired exploratory data analysis to be performed on massive data sets of effectively unlimited size.;2013;Daniel Cheng;10.1109/BigData.2013.6691787;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Risk adjustment of patient expenditures: A big data analytics approach;For healthcare applications, voluminous patient data contain rich and meaningful insights that can be revealed using advanced machine learning algorithms. However, the volume and velocity of such high dimensional data requires new big data analytics framework where traditional machine learning tools cannot be applied directly. In this paper, we introduce our proof-of-concept big data analytics framework for developing risk adjustment model of patient expenditures, which uses the “divide and conquer” strategy to exploit the big-yet-rich data to improve the model accuracy. We leverage the distributed computing platform, e.g., MapReduce, to implement advanced machine learning algorithms on our data set. In specific, random forest regression algorithm, which is suitable for high dimensional healthcare data, is applied to improve the accuracy of our predictive model. Our proof-of-concept framework demonstrates the effectiveness of predictive analytics using random forest algorithm as well as the efficiency of the distributed computing platform.;2013;Lin Li;10.1109/BigData.2013.6691790;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Big data for business managers — Bridging the gap between potential and value;Given the surge of interest in research, publication and application on Big Data over the last few years, the potential of Big Data seems to be well-established now across businesses. However, in most of the business implementations Big Data still seem to be struggling to deliver the promised value (ROI). Such results despite using the market leading Big Data solutions and talented deployment team are forcing the business managers to think what needs to be done differently. This paper lays down the framework for business managers to understand Big Data processes. Besides providing a business overview of Big Data core components, the paper presents several questions that the managers must ask to assess the effectiveness of their Big Data processes. This paper is based on the analysis of several Big Data projects that never delivered and comparison against successful ones. The hypothesis is developed based on public information and is proposed as the first step for business managers keen on effectively leveraging Big Data.;2013;Anmol Rajpurohit;10.1109/BigData.2013.6691794;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Access control for big data using data content;Conventional database access control models have difficulties in dealing with big data, especially for the features of volume, variety and velocity. To address the problem, we introduce the Content-based Access Control (CBAC) model for content-centric information sharing. As a complement to conventional models, CBAC makes access control decisions based on the content similarity between user credentials and data content dynamically. We present an enforcement mechanism for CBAC exploiting Oracle's Virtual Private Database (VPD). Experimental results show that CBAC makes reasonable access control decision with a small overhead.;2013;Wenrong Zeng;10.1109/BigData.2013.6691798;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Knowledge cubes — A proposal for scalable and semantically-guided management of Big Data;A Knowledge Cube, or cube for short, is an intelligent and adaptive database instance capable of storing, analyzing, and searching data. Each cube is established based on semantic aspects, e.g., (1) Topical, (2) Contextual, (3) Spatial, or (4) Temporal. A cube specializes in handling data that is only relevant to the cube's semantics. Knowledge cubes are inspired by two prime architectures: (1) Dataspaces that provides an abstraction for data management where heterogeneous data sources can co-exist and it requires no prespecified unifying schema, and (2) Linked Data that provides best practices for publishing and interlinking structured data on the web. A knowledge cube uses Linked Data as its main building block for its data layer and encompasses some of the data integration abstractions defined by Dataspaces. In this paper, knowledge cubes are proposed as a semantically-guided data management architecture, where data management is influenced by the data semantics rather than by a predefined scheme. Knowledge cubes support the five pillars of Big Data also known as the five V's, namely Volume, Velocity, Veracity, Variety, and Value. Interesting opportunities can be leveraged when learning the semantics of the data. This paper highlights these opportunities and proposes a strawman design for knowledge cubes along with the research challenges that arise when realizing them.;2013;Amgad Madkour;10.1109/BigData.2013.6691800;Conferences;;978-1-4799-1293-3
ieee_20221205_08_32_33;Towards Network Reduction on Big Data;"The increasing ease of data collection experience and the increasing availability of large data storage space lead to the existence of very large datasets that are commonly referred as ""Big Data"". Such data not only take over large amount of database storage, but also increase the difficulties for data analysis due to data diversity, which, also makes the datasets seemingly isolated with each other. In this paper, we present a solution to the problem that is to build up connections among the diverse datasets, based upon their similarities. Particularly, a concept of similarity graph along with a similarity graph generation algorithm were introduced. We then proposed a similarity graph reduction algorithm that reduces vertices of the graph for the purpose of graph simplification.";2013;Xing Fang;10.1109/SocialCom.2013.103;Conferences;;978-0-7695-5137-1
ieee_20221205_08_32_33;Big Data and Policy Design for Data Sovereignty: A Case Study on Copyright and CCL in South Korea;The purpose of this paper is as follows. First, I am trying to conceptualize big data as a social problem. Second, I would like to explain the difference between big data and conventional mega information. Third, I would like to recommend the role of the government for utilization of big data as a policy tools. Fourth, while referring to copyright and CCL(Creative Commons License) cases, I would like to explain the regulation for big data on data sovereignty. Finally, I would like to suggest a direction of policy design for big data. As for the result of this study, policy design for big data should be distinguished from policy design for mega information to solve data sovereignty issues. From a law system perspective, big data is generated autonomously. It has been accessed openly and shared without any intention. In market perspective, big data is created without any intention. Big data can be changed automatically in case of openness with reference feature such as Linked of Data. Some policy issues such as responsibility and authenticity should be raised. Big data is generated in a distributed and diverse way without any concrete form in technology perspective. So, we need a different approach.;2013;Hyejung Moon;10.1109/SocialCom.2013.165;Conferences;;978-0-7695-5137-1
ieee_20221205_08_32_33;Next Big Thing in Big Data: The Security of the ICT Supply Chain;In contemporary society, with supply chains becoming more and more complex, the data in supply chains increases by means of volume, variety and velocity. Big data rise in response to the proper time and conditions to offer advantages for the nodes in supply chains to solve prewiously difficult problems. For any big data project to succeed, it must first depend on high-quality data but not merely on quantity. Further, it will become increasingly important in many big data projects to add external data to the mix and companies will eventually turn from only looking inward to also looking outward into the market, which means the use of big data must be broadened considerably. Hence the data supply chains, both internally and externally, become of prime importance. ICT (Information and Telecommunication) supply chain management is especially important as supply chain link the world closely and ICT supply chain is the base of all supply chains in today's world. Though many initiatives to supply chain security have been developed and taken into practice, most of them are emphasized in physical supply chain which is addressed in transporting cargos. The research on ICT supply chain security is still in preliminary stage. The use of big data can promote the normal operation of ICT supply chain as it greatly improve the data collecting and processing capacity and in turn, ICT supply chain is a necessary carrier of big data as it produces all the software, hardware and infrastructures for big data's collection, storage and application. The close relationship between big data and ICT supply chain make it an effective way to do research on big data security through analysis on ICT supply chain security. This paper first analyzes the security problems that the ICT supply chain is facing in information management, system integrity and cyberspace, and then introduces several famous international models both on physical supply chain and ICT supply chain. After that the authors describe a case of communication equipment with big data in ICT supply chain and propose a series of recommendations conducive to developing secure big data supply chain from five dimensions.;2013;Tianbo Lu;10.1109/SocialCom.2013.172;Conferences;;978-0-7695-5137-1
ieee_20221205_08_32_33;Analysis of Big Data Technologies and Method - Query Large Web Public RDF Datasets on Amazon Cloud Using Hadoop and Open Source Parsers;Extremely large datasets found in Big Data projects are difficult to work with using conventional databases, statistical software, and visualization tools. Massively parallel software, such as Hadoop, running on tens, hundreds, or even thousands of servers is more suitable for Big Data challenges. Additionally, in order to achieve the highest performance when querying large datasets, it is necessary to work these datasets at rest without preprocessing or moving them into a repository. Therefore, this work will analyze tools and techniques to overcome working with large datasets at rest. Parsing and querying will be done on the raw dataset - the untouched Web Data Commons RDF files. Web Data Commons comprises five billion pages of web pages crawled from the Internet. This work will analyze available tools and appropriate methods to assist the Big Data developer in working with these extremely large, semantic RDF datasets. Hadoop, open source parsers, and Amazon Cloud services will be used to data mine these files. In order to assist in further discovery, recommendations for future research will be included.;2013;Ted Garcia;10.1109/ICSC.2013.49;Conferences;;978-0-7695-5119-7
ieee_20221205_08_32_33;An improved BP algorithm over out-of-order streams for big data;Due to the difficulty of getting the association rules over out-of-order streams for big data, a new improved BP algorithm based on dynamic adjustment is proposed. We firstly use a dynamic adaptive structural adjustment mechanism to change the network training structure according to the environmental requirements, which can automatically remove invalid training node, and optimize the iterative training process. Secondly, we adjust three factors (i.e. learning index, momentum factor and scaling factor) during the learning process to speed up the learning response, and to enhance the stability of the network. Simulation results show that compared with traditional BP algorithm, this algorithm can get more convergence times,the convergence rate can be improved effectively, and finally obtain the association rules over out-of-order data streams.;2013;Kun Wang;10.1109/ChinaCom.2013.6694712;Conferences;;978-1-4799-1406-7
ieee_20221205_08_32_33;Advanced analytics for harnessing the power of smart meter big data;Smart meters or advanced metering infrastructure (AMI) are being deployed in many countries around the world. Smart meters are the basic building block of the smart grid and governments have invested vast amounts in smart meter deployment targeting wide economic, social and environmental benefits. The key functionality of the smart meter is the capture and transfer of data relating to the consumption (electricity, gas) and events such as power quality and meter status. Such capability has also resulted in the generation of an unprecedented data volume, speed of collection and complexity, which has resulted in the so called big data challenge. To realize the hidden value and power in such data, it is important to use the appropriate tools and technology which are currently being called advanced analytics. In this paper we define a smart metering landscape and discuss different technologies available for harnessing the smart meter captured data. Main limitations and challenges with existing techniques with big data are also highlighted and several future directions in smart metering are presented.;2013;Damminda Alahakoon;10.1109/IWIES.2013.6698559;Conferences;;978-1-4799-1135-6
ieee_20221205_08_32_33;The Oklahoma PetaStore: Big data on a small budget;In the era of Big Data, research productivity can be highly sensitive to the availability of large scale, long term storage. Unfortunately, most mass storage systems are prohibitively expensive at scales appropriate for individual institutions rather than national centers. Furthermore, many researchers won't adopt any centralized technology that they perceive as more expensive than what their research teams could do on their own. This poster examines a combination of business model and technology that addresses these concerns in a comprehensive way, distributing the costs among a funding agency, the institution and the research teams, thereby reducing the challenges faced by each.;2013;Patrick Calhoun;10.1109/CLUSTER.2013.6702629;Conferences;2168-9253;978-1-4799-0898-1
ieee_20221205_08_32_33;HcBench: Methodology, development, and characterization of a customer usage representative big data/Hadoop benchmark;"Big Data analytics using Map-Reduce over Hadoop has become a leading edge paradigm for distributed programming over large server clusters. The Hadoop platform is used extensively for interactive and batch analytics in ecommerce, telecom, media, retail, social networking, and being actively evaluated for use in other areas. However, to date no industry standard or customer representative benchmarks exist to measure and evaluate the true performance of a Hadoop cluster. Current Hadoop micro-benchmarks such as HiBench-2, GridMix-3, Terasort, etc. are narrow functional slices of applications that customers run to evaluate their Hadoop clusters. However, these benchmarks fail to capture the real usages and performance in a datacenter environment. Given that typical datacenter deployments of Hadoop process a wide variety of analytic interactive and query jobs in addition to batch transform jobs under strict Service Level Agreement (SLA) requirements, performance benchmarks used to evaluate clusters must capture the effects of concurrently running such diverse job types in production environments. In this paper, we present the methodology and the development of a customer datacenter usage representative Hadoop benchmark ""HcBench"" which includes a mix of large number of customer representative interactive, query, machine learning, and transform jobs, a variety of data sizes, and includes compute, storage 110, and network intensive jobs, with inter-job arrival times as in a typical datacenter environment. We present the details of this benchmark and discuss application level, server and cluster level performance characterization collected on an Intel Sandy Bridge Xeon Processor Hadoop cluster.";2013;Vikram A. Saletore;10.1109/IISWC.2013.6704672;Conferences;;978-1-4799-0553-9
ieee_20221205_08_32_33;Characterizing the efficiency of data deduplication for big data storage management;The demand for data storage and processing is increasing at a rapid speed in the big data era. Such a tremendous amount of data pushes the limit on storage capacity and on the storage network. A significant portion of the dataset in big data workloads is redundant. As a result, deduplication technology, which removes replicas, becomes an attractive solution to save disk space and traffic in a big data environment. However, the overhead of extra CPU computation (hash indexing) and IO latency introduced by deduplication should be considered. Therefore, the net effect of using deduplication for big data workloads needs to be examined. To this end, we characterize the redundancy of typical big data workloads to justify the need for deduplication. We analyze and characterize the performance and energy impact brought by deduplication under various big data environments. In our experiments, we identify three sources of redundancy in big data workloads: 1) deploying more nodes, 2) expanding the dataset, and 3) using replication mechanisms. We elaborate on the advantages and disadvantages of different deduplication layers, locations, and granularities. In addition, we uncover the relation between energy overhead and the degree of redundancy. Furthermore, we investigate the deduplication efficiency in an SSD environment for big data workloads.;2013;Ruijin Zhou;10.1109/IISWC.2013.6704674;Conferences;;978-1-4799-0553-9
ieee_20221205_08_32_33;Big data and humanitarian supply networks: Can Big Data give voice to the voiceless?;"Billions of US dollars are spent each year in emergency aid to save lives and alleviate the suffering of those affected by disaster. This aid flows through a humanitarian system that consists of governments, different United Nations agencies, the Red Cross movement and myriad non-governmental organizations (NGOs). As scarcer resources, financial crisis and economic inter-dependencies continue to constrain humanitarian relief there is an increasing focus from donors and governments to assess the impact of humanitarian supply networks. Using commercial (`for-profit') supply networks as a benchmark; this paper exposes the counter-intuitive competition dynamic of humanitarian supply networks, which results in an open-loop system unable to calibrate supply with actual need and impact. In that light, the phenomenon of Big Data in the humanitarian field is discussed and an agenda for the `datafication' of the supply network set out as a means of closing the loop between supply, need and impact.";2013;Asmat Monaghan;10.1109/GHTC.2013.6713725;Conferences;;978-1-4799-2401-1
ieee_20221205_08_32_33;A predictive tool for nonattendance at a specialty clinic: An application of multivariate probabilistic big data analytics;The field of data analytics has recently garnered significant attention as a means of improving service, outcome and cost in healthcare. Health informatics tools to date are largely rules-based, static in knowledge-gathering capability, and are not well incorporated within standard clinical processes. We introduce a multivariate analysis tool, built at the user-client interface, directly from data extracted from thousands of EMRs, for automated decision making. The predictive algorithms created to date have been implemented in a variety of clinical scenarios, including ER triage of chest pain, selection of optimal diagnostic testing, and management of chronic illnesses. This technology has now been applied to prediction of patient compliance with scheduled appointments in a medical specialty clinic. Approximately 16% of scheduled VA patients do not show for their scheduled appointments for Gastroenterology clinic. This results in misallocation of provider resources and inefficiencies in health care delivery. The improvement in quality of care, patient outcomes and cost is potentially immense.;2013;Victor Levy;10.1109/CEWIT.2013.6713760;Conferences;;978-1-4799-2546-9
ieee_20221205_08_32_33;KASR: A Keyword-Aware Service Recommendation Method on MapReduce for Big Data Applications;Service recommender systems have been shown as valuable tools for providing appropriate recommendations to users. In the last decade, the amount of customers, services and online information has grown rapidly, yielding the big data analysis problem for service recommender systems. Consequently, traditional service recommender systems often suffer from scalability and inefficiency problems when processing or analysing such large-scale data. Moreover, most of existing service recommender systems present the same ratings and rankings of services to different users without considering diverse users' preferences, and therefore fails to meet users' personalized requirements. In this paper, we propose a Keyword-Aware Service Recommendation method, named KASR, to address the above challenges. It aims at presenting a personalized service recommendation list and recommending the most appropriate services to the users effectively. Specifically, keywords are used to indicate users' preferences, and a user-based Collaborative Filtering algorithm is adopted to generate appropriate recommendations. To improve its scalability and efficiency in big data environment, KASR is implemented on Hadoop, a widely-adopted distributed computing platform using the MapReduce parallel processing paradigm. Finally, extensive experiments are conducted on real-world data sets, and results demonstrate that KASR significantly improves the accuracy and scalability of service recommender systems over existing approaches.;2014;Shunmei Meng;10.1109/TPDS.2013.2297117;Journals;2161-9883;
ieee_20221205_08_32_33;A Time Efficient Approach for Detecting Errors in Big Sensor Data on Cloud;Big sensor data is prevalent in both industry and scientific research applications where the data is generated with high volume and velocity it is difficult to process using on-hand database management tools or traditional data processing applications. Cloud computing provides a promising platform to support the addressing of this challenge as it provides a flexible stack of massive computing, storage, and software services in a scalable manner at low cost. Some techniques have been developed in recent years for processing sensor data on cloud, such as sensor-cloud. However, these techniques do not provide efficient support on fast detection and locating of errors in big sensor data sets. For fast data error detection in big sensor data sets, in this paper, we develop a novel data error detection approach which exploits the full computation potential of cloud platform and the network feature of WSN. Firstly, a set of sensor data error types are classified and defined. Based on that classification, the network feature of a clustered WSN is introduced and analyzed to support fast error detection and location. Specifically, in our proposed approach, the error detection is based on the scale-free network topology and most of detection operations can be conducted in limited temporal or spatial data blocks instead of a whole big data set. Hence the detection and location process can be dramatically accelerated. Furthermore, the detection and location tasks can be distributed to cloud platform to fully exploit the computation power and massive storage. Through the experiment on our cloud computing platform of U-Cloud, it is demonstrated that our proposed approach can significantly reduce the time for error detection and location in big data sets generated by large scale sensor network systems with acceptable error detecting accuracy.;2015;Chi Yang;10.1109/TPDS.2013.2295810;Journals;2161-9883;
ieee_20221205_08_32_33;Attribute Relationship Evaluation Methodology for Big Data Security;There has been an increasing interest in big data and big data security with the development of network technology and cloud computing. However, big data is not an entirely new technology but an extension of data mining. In this paper, we describe the background of big data, data mining and big data features, and propose attribute selection methodology for protecting the value of big data. Extracting valuable information is the main goal of analyzing big data which need to be protected. Therefore, relevance between attributes of a dataset is a very important element for big data analysis. We focus on two things. Firstly, attribute relevance in big data is a key element for extracting information. In this perspective, we studied on how to secure a big data through protecting valuable information inside. Secondly, it is impossible to protect all big data and its attributes. We consider big data as a single object which has its own attributes. We assume that a attribute which have a higher relevance is more important than other attributes.;2013;Sung-Hwan Kim;10.1109/ICITCS.2013.6717808;Conferences;;978-1-4799-2845-3
ieee_20221205_08_32_33;Secured e-health data retrieval in DaaS and Big Data;Big Data is one of rising IT trends such as cloud computing, social networking or ubiquitous computing. Big Data can offer beneficial scenarios in the e-health arena. However, one of the scenarios can be that Big Data needs to be kept secured for a long time in order to gain its benefits such as finding cures for infectious diseases and keeping patients' privacy. From this connection, it is beneficial to analyze Big Data to make meaningful information while the data are stored in a secure manner. Thus, the analysis of various database encryption techniques is essential. In this study, we simulated 3 types of technical environments such as Plain-text, Microsoft Built-in Encryption, and custom Advanced Encryption Standard using Bucket Index in Data-as-a-Service. The results showed that custom AES-DaaS has faster range query response time than MS built-in encryption. In addition, while carrying out the scalability test, we acknowledged there are performance thresholds according to physical IT resources. Therefore, for the purpose of efficient Big Data management in e-health, it is noteworthy to examine its scalability limits as well even if it is under cloud computing environment. Furthermore, when designing an e-health database, both patients' privacy and system performance needs to be dealt as top priorities.;2013;David Shin;10.1109/HealthCom.2013.6720677;Conferences;;978-1-4673-5800-2
ieee_20221205_08_32_33;Applying Sensor Web strategies to Big Data earth observations;Although the focus of the Sensor Web has been somewhat limited to a single architectural view in the form of web services and service oriented architectures our experience has shown in a number of projects that this is not always the most effective solution, especially when deal with Big Data. The correct approach is then to hold to the overall vision of the Sensor Web as a way of gaining access to and organising sensors and sensor data and to use the appropriate architectural patterns and strategies in overcoming the challenges presented. Using these other approaches is not necessarily conflict with an open systems view nor is it non web centric.;2013;Terence L. van Zyl;10.1109/IGARSS.2013.6721278;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_32_33;Big Data and the bright future of simulation — The case of agent-based modeling;Big Data may be an ugly word describing a diverse reality, but it also points to a bright future for simulation in general, and Agent-Based Modeling (ABM) in particular. As companies are struggling to make sense of the staggering amounts of data they have been amassing, data-driven simulation will be the backbone of how value is discovered and captured from data. Drawing from successful applications of ABM, I will make it explicit that what used to be an afterthought of simulation modeling (calibration) is now the cornerstone of the Big Data edifice.;2013;Eric Bonabeau;10.1109/WSC.2013.6721399;Conferences;1558-4305;978-1-4799-2077-8
ieee_20221205_08_32_33;Big Data Framework;We are constantly being told that we live in the Information Era - the Age of BIG data. It is clearly apparent that organizations need to employ data-driven decision making to gain competitive advantage. Processing, integrating and interacting with more data should make it better data, providing both more panoramic and more granular views to aid strategic decision making. This is made possible via Big Data exploiting affordable and usable Computational and Storage Resources. Many offerings are based on the Map-Reduce and Hadoop paradigms and most focus solely on the analytical side. Nonetheless, in many respects it remains unclear what Big Data actually is, current offerings appear as isolated silos that are difficult to integrate and/or make it difficult to better utilize existing data and systems. Paper addresses this lacunae by characterising the facets of Big Data and proposing a framework in which Big Data applications can be developed. The framework consists of three Stages and seven Layers to divide Big Data application into modular blocks. The aim is to enable organizations to better manage and architect a very large Big Data application to gain competitive advantage by allowing management to have a better handle on data processing.;2013;Firat Tekiner;10.1109/SMC.2013.258;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_32_33;MapReduce Based Method for Big Data Semantic Clustering;Big data analysis is very hot in cloud computing environments. How to automatically map heterogeneous data with the same semantics is one of the key problems in big data analysis. A big data clustering method based on the MapReduce framework is proposed in this paper. Big data are decomposed into many data chunks for parallel clustering, which is implemented by Ant Colony. Data elements are moved and clustered by ants according to the presented criterion. The proposed method is compared with the MapReduce framework based k-means clustering algorithm on a great amount of practical data. Experimental results show that the proposal is much effective for big data clustering.;2013;Jie Yang;10.1109/SMC.2013.480;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_32_33;A Reduction Algorithm for the Big Data in 3D Surface Reconstruction;As big data acquisition and storage becomes increasingly affordable, especially in the modern range sensing technology for the scans of complex objects, it is a challenge to reconstruct the surface of 3D geometric model effectively and precisely. In this paper, we describe a reduction method for the big data with noises in the 3D surface reconstruction based on partition of unity, Hermite radial basis functions, and sparse regularization. The proposed method not only provides an approach for pruning some redundant data according to the sparsity, but also contains a good robustness to the noises. This approach can be regarded as one of effective methods for processing big data. Experimental results are also provided.;2013;Jianwei Zhao;10.1109/SMC.2013.824;Conferences;1062-922X;978-1-4799-0652-9
ieee_20221205_08_32_33;Road traffic big data collision analysis processing framework;With the advancement of sensor technologies, big data processing becomes a new paradigm for large scale information processing. Big data comes from different sources, such as from traffic information, social sites, mobile phone GPS signals and so on. In this paper, we propose a new architecture for distributed processing that enables big data processing on the road traffic data and its related information analysis. We applied Hadoop and HBase that can store and analyze real-time collision data in a distributed processing framework. This framework is designed as flexible and scalable framework using distributed CEP that process massive real-time traffic data and ESB that integrates other services. We tested the proposed framework on road traffic data on a 45-mile section of I-880N freeway CA, USA. By integrating freeway traffic big data and collision data over a ten-year period (1TB Size), we obtained the collision probability.;2013;Duckwon Chung;10.1109/ICAICT.2013.6722733;Conferences;;978-1-4673-6419-5
ieee_20221205_08_32_33;VegaIndexer: A Distributed composite index scheme for big spatio-temporal sensor data on cloud;With the prevalence of data-intensive geospatial applications, massive spatio-temporal sensor data are obtained and the big data have posed grand challenges on existing index methods based on spatial databases due to their intrinsic poor scalability and retrieval efficiency. Motivated by the deficiencies, in this paper, we propose a distributed composite spatio-temporal index scheme called VegaIndexer for efficiently answering queries from large collections of space-time sensor data. Firstly, we present a distributed spatio-temporal indexing architecture based on cloud platform which consists of global index and local index. Moreover, we propose Multi-version Distributed enhanced R+ (MDR+) tree algorithm for accelerating data retrieval and spatio-temporal query efficiency. Furthermore, we design a MapReduce-based parallel processing approach of batch constructing indices for big spatiotemporal sensor data. In addition, we implement VegaIndexer middleware on top of the leading cloud platform, i.e., Hadoop and associated NoSQL database. The experimental experiments show that VegaIndexer outperforms the index methods of typical spatial databases.;2013;Yunqin Zhong;10.1109/IGARSS.2013.6723126;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_32_33;Game theory applied to big data analytics in geosciences and remote sensing;This paper introduces the basic concepts of game theory and outlines the mechanisms for applying game theory models to big data analytics and decision making in the field of geosciences and remote sensing. The author proposes the use of strategic, competitive game theory models for the purpose of spectral band grouping when exploiting hyperspectral imagery. The proposed system uses conflict data filtering based on mutual entropy and a strategy interaction process of multiple band groups in a conflict environment, the goal of which is to maximize the payoff benefit of multiple groups of the whole system. The proposed system uses the Nash equilibrium as the means to find a steady state solution to the band grouping problem, and implements the model under the assumption that all players are rational. The author uses the proposed band grouping as a component in a multi-classifier decision fusion (MCDF) system for automated ground cover classification with hyperspectral imagery. The paper provides experimental results demonstrating that the proposed game theoretic approach significantly outperforms the comparison methods.;2013;Lori Mann Bruce;10.1109/IGARSS.2013.6723733;Conferences;2153-7003;978-1-4799-1114-1
ieee_20221205_08_32_33;A cloud-based architecture for Big-Data analytics in smart grid: A proposal;A Smart Grid is an enhanced version of electric grid in which the demand and supply are balanced to meet the customers need. The paper deals with the formation of a cloud-based Smart Grid for analyzing the Bid-Data and taking decisions to balance the demand of customer needs. The proposed formation of smart grid will deal with Big Data set which will contain the data regarding the power usage patterns of customers, historic weather data of the location, the current demand and supply details. The grid will operate on the data being fetched from the cloud storage. The paper also focuses on smart grid being framed with the renewable energy sources.;2013;M. Mayilvaganan;10.1109/ICCIC.2013.6724168;Conferences;;978-1-4799-1595-8
ieee_20221205_08_32_33;Forecasting consumer behavior with innovative value proposition for organizations using big data analytics;The term `Big Data' is used to represent collection of such a huge amount of data that it becomes impossible to manage and process data using conventional database management tools. Big Data is defined by three important parameters `Volume' - Size of Data, `Velocity' - Speed of increase of data and `Variety' - Type of Data. Big data analytics is the process of analyzing this ever growing Big Data. The goal of every organization is to maximize its value for its stake holders. The paper aims to demonstrate that Big data analytics can be used as a catalyst for generating and increasing value for organizations by improving various business parameters. Furthermore, by utilizing case studies the paper also aims to establish that big data analytics supports creation, enhancement and improvement of various business services to significantly improve customer experience as well as value creation for organizations.;2013;Ankur Balar;10.1109/ICCIC.2013.6724280;Conferences;;978-1-4799-1595-8
ieee_20221205_08_32_33;Security Analytics: Big Data Analytics for cybersecurity: A review of trends, techniques and tools;The rapid growth of the Internet has brought with it an exponential increase in the type and frequency of cyber attacks. Many well-known cybersecurity solutions are in place to counteract these attacks. However, the generation of Big Data over computer networks is rapidly rendering these traditional solutions obsolete. To cater for this problem, corporate research is now focusing on Security Analytics, i.e., the application of Big Data Analytics techniques to cybersecurity. Analytics can assist network managers particularly in the monitoring and surveillance of real-time network streams and real-time detection of both malicious and suspicious (outlying) patterns. Such a behavior is envisioned to encompass and enhance all traditional security techniques. This paper presents a comprehensive survey on the state of the art of Security Analytics, i.e., its description, technology, trends, and tools. It hence aims to convince the reader of the imminent application of analytics as an unparalleled cybersecurity solution in the near future.;2013;Tariq Mahmood;10.1109/NCIA.2013.6725337;Conferences;;978-1-4799-1287-2
ieee_20221205_08_32_33;Big data platform development with a domain specific language for telecom industries;This paper introduces a system that offer a special big data analysis platform with Domain Specific Language for telecom industries. This platform has three main parts that suggests a new kind of domain specific system for processing and visualization of large data files for telecom organizations. These parts are Domain Specific Language (DSL), Parallel Processing/Analyzing Platform for Big Data and an Integrated Result Viewer. In addition to these main parts, Distributed File Descriptor (DFD) is designed for passing information between these modules and organizing communication. To find out benefits of this domain specific solution, standard framework of big data concept is examined carefully. Big data concept has special infrastructure and tools to perform for data storing, processing, analyzing operations. This infrastructure can be grouped as four different parts, these are infrastructure, programming models, high performance schema free databases, and processing-analyzing. Although there are lots of advantages of Big Data concept, it is still very difficult to manage these systems for many enterprises. Therefore, this study suggest a new higher level language, called as DSL which helps enterprises to process big data without writing any complex low level traditional parallel processing codes, a new kind of result viewer and this paper also presents a Big Data solution system that is called Petaminer.;2013;Cüneyt Şenbalcı;10.1109/HONET.2013.6729768;Conferences;1949-4106;978-1-4799-2568-1
