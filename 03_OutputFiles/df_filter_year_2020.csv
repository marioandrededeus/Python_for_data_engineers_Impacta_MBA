abstract;author;categories;citable_docs._(3years);cites_/_doc._(2years);country;coverage;doi;eigenfactor_score;h_index;isbn;issn;issn1;issn2;issn3;jcr_value;keywords;publisher_x;publisher_y;ref._/_doc.;region;scimago_value;sjr_best_quartile;sourceid;title_csv;title_bib;total_cites;total_cites_(3years);total_docs._(2020);total_docs._(3years);total_refs.;type;type_publication;year
The burgeoning demands for quality, safety, and value in cardiothoracic surgery, in combination with the advancement and acceleration of digital health solutions and information technology, provide a unique opportunity to improve efficiency and effectiveness simultaneously in cardiothoracic surgery. This primer on digital health explores and reviews data integration, data processing, complex modeling, telehealth with remote monitoring, and cybersecurity as they shape the future of cardiothoracic surgery.;Ronald D. Baxter and James I. Fann and J. Michael DiMaio and Kevin Lobdell;"Cardiology and Cardiovascular Medicine (Q1); Pulmonary and Respiratory Medicine (Q1); Surgery (Q1)";2269.0;213.0;United States;1965-2020;10.1016/j.athoracsur.2020.02.072;0.0351;197.0;;00034975;15526259;00034975;15526259;4.33;;Elsevier USA;;1370.0;Northern America;1130.0;Q1;20489.0;Annals of thoracic surgery;Digital health primer for cardiothoracic surgeons;41620.0;6373.0;1111.0;2981.0;15218.0;journal;article;2020
"Objectives
To understand how health status preceding traumatic brain injury (TBI) affects relative functional gain after inpatient rehabilitation using a data mining approach.
Design
Population-based, sex-stratified, retrospective cohort study using health administrative data from Ontario, Canada (39% of the Canadian population).
Setting
Inpatient rehabilitation.
Participants
Patients 14 years or older (N=5802; 63.4% male) admitted to inpatient rehabilitation within 1 year of a TBI-related acute care discharge between April 1, 2008, and March 31, 2015.
Interventions
Not applicable.
Main Outcome Measures
Relative functional gain (RFG) in percentage, calculated as ([discharge FIM−admission FIM]/[126−admission FIM]×100). Health status prior to TBI was identified and internally validated using a data mining approach that categorized all International Classification of Diseases, 10th revision, codes for each patient.
Results
The average RFG was 52.8%±27.6% among male patients and 51.6%±27.1% among female patients. Sex-specific Bonferroni adjusted multivariable linear regressions identified 10 factors of preinjury health status related to neurology, emergency medicine, cardiology, psychiatry, geriatrics, and gastroenterology that were significantly associated with reduced RFG in FIM for male patients. Only 1 preinjury health status category, geriatrics, was significantly associated with RFG in female patients.
Conclusions
Comorbid health conditions present up to 5 years preceding the TBI event were significantly associated with RFG. These findings should be considered when planning and executing interventions to maximize functional gain and to support an interdisciplinary approach. Best practices guidelines and clinical interventions for older male and female patients with TBI should be developed given the increasingly aging population with TBI.";Vincy Chan and Mitchell Sutton and Tatyana Mollayeva and Michael D. Escobar and Mackenzie Hurst and Angela Colantonio;"Physical Therapy, Sports Therapy and Rehabilitation (Q1); Rehabilitation (Q1); Sports Science (Q1)";829.0;329.0;United Kingdom;1945-2020;10.1016/j.apmr.2020.05.017;0.01786;188.0;;00039993;1532821X;00039993;1532821X;3.966;Brain injuries, Comorbidity, Data mining, International Classification of Diseases, Rehabilitation;W.B. Saunders Ltd;;3866.0;Western Europe;1305.0;Q1;16270.0;Archives of physical medicine and rehabilitation;Data mining to understand how health status preceding traumatic brain injury affects functional outcome: a population-based sex-stratified study;30690.0;3351.0;289.0;917.0;11172.0;journal;article;2020
The rigor and reproducibility of science methods depends heavily on the appropriate use of statistical methods to answer research questions and make meaningful and accurate inferences based on data. The increasing analytic complexity and valuation of novel statistical and methodological approaches to data place greater emphasis on statistical review. We will outline the controversies within statistical sciences that threaten rigor and reproducibility of research published in the behavioral sciences and discuss ongoing approaches to generate reliable and valid inferences from data. We outline nine major areas to consider for generally evaluating the rigor and reproducibility of published articles and apply this framework to the 116 Behaviour Research and Therapy (BRAT) articles published in 2018. The results of our analysis highlight a pattern of missing rigor and reproducibility elements, especially pre-registration of study hypotheses, links to statistical code/output, and explicit archiving or sharing data used in analyses. We recommend reviewers consider these elements in their peer review and that journals consider publishing results of these rigor and reproducibility ratings with manuscripts to incentivize authors to publish these elements with their manuscript.;Tom Hildebrandt and Jason M. Prenoveau;"Clinical Psychology (Q1); Developmental and Educational Psychology (Q1); Experimental and Cognitive Psychology (Q1); Psychiatry and Mental Health (Q1)";413.0;413.0;United Kingdom;1963-2020;10.1016/j.brat.2020.103552;0.01417;185.0;;00057967;00057967;1873622X;00057967;4.473;Statistics, Big data, Reproducibility, Reliability, -hacking;Elsevier Ltd.;;6426.0;Western Europe;2506.0;Q1;12127.0;Behaviour research and therapy;Rigor and reproducibility for data analysis and design in the behavioral sciences;22841.0;2304.0;143.0;423.0;9189.0;journal;article;2020
"Complete blood count (CBC) analysis is one of the most commonly ordered laboratory tests and is a critical first step in patients' clinical evaluation. However, CBC analyzers are limited in their ability to positively identify several types of white blood cells (WBC), and cells with substantial clinical significance, such as immature granulocytes or blasts, are merely marked as flags. Also, CBC analyzers fall short of recognizing informative red blood cell (RBC) morphology, such as schistocytes, and often provide inaccurate platelets count. Flags and clinically non-sufficient CBC-derived data reflex to generation of blood smear (BS), and BS review comprises a substantial portion of the workload in routine hematology laboratories. For accurate identification and classification of WBC, BS analysis (BSA) requires detailed observation of cells with high-magnification objective (60-100X), which provides a relatively narrow Field of View (FOV). This physical limitation restricts current BSA to either low resolution/wide FOV or to high resolution/narrow FOV data generation (Fig. 1A). Hence, key issues of BSA such as the effects of the smearing process on the distribution of blood components, the effects of cells distribution on their morphology and further classification, as well as many other attributes, are addressed only qualitatively or empirically, leaving the real topology of the BS obscure. The computational imaging microscopy system presented herein uses a low resolution and wide FOV objective, and records a plurality of images under different illumination conditions, of the same sample area (Fig. 1B). An algorithm reconstructs a high resolution and aberration free image of whole specimens, as can be observed in the attached link (https://tinyurl.com/Scopio-Labs-X100-ASH-2020). High resolution images are critical not only for manual BSA, but also for artificial intelligence (AI)-derived BSA, since data quality is of prime importance for deep-learning processes, and to a large extent determine their outcome. Thus, the combination of high resolution/wide FOV turns each BS into a big data analytic field, rendering the measurement of yet undetermined cell characteristics. In order to elucidate the basic topology, 60 normal BS (28 females, 32 males) were subjected to analysis utilizing this novel computational imaging microscopy. For convenience of analysis and comparison with current BSA methodology, BS were segmented into strips according to RBC density (Fig. 1C, D). The average length of smear from females (F) was higher by nearly 28% compared with smear from males (M), and the presence of acute inflammation (A) resulted in a significant 33% increase in overall smear length compared to normal (N) average (Fig. 1E). As expected, RBC density formed a linear gradient (Fig. 1C) along the axis of sample smearing, however, RBC morphology was affected by location within the BS. For example, strips 4-5 contained RBC with the appearance of spherocytes (Fig. 1F; arrows), while in strips with increased RBC density, cells aggregated resembling rouleaux formation (Fig. 1F; arrowheads). Platelets distribution was non-linear, with only a few of them reaching the feathered edge of the smear (Fig. 1G). Since the variance of both RBC/FOV and platelets/FOV concentrations drops starting with strip 4, BS-derived platelets number estimates should not be performed in strips 1-3. On average, a normal BS contains 890+399 WBC in the scanned area (strips 1-8). Similar to RBC, the location of individual WBC throughout the BS may affect their morphology, and hence their classification. WBC in the feathered edge (strips 1-3) are generally more stretched, and often squeezed between RBC, rendering their classification by AI-based tools challenging (Fig. 1H). In strips 4-7, WBC morphology is optimal for a classification task, enabling favorable outcomes for either manual or AI cell analysis (Fig. 1H). These data indicate that BSA can be taken to a sensitivity level of at least 10-3 of WBC analysis, provided that a large portion of the BS is scanned. Our system provides a novel combination of computational imaging microscopy and AI-based classification tools to unravel the complex topology of blood smears, and upgrade the data obtained in BSA. This approach enables the establishment of quantitative rules to scientifically direct the objective analysis of cellular blood components both manually, and by AI-tools. Figure
Disclosures
Katz: Scopio Labs: Consultancy.";Ben Zion Katz and Irit Avivi and Dan Benisty and Shahar Karni and Hadar Shimoni and Omri Grooper and Olga Pozdnyakova;"Biochemistry (Q1); Cell Biology (Q1); Hematology (Q1); Immunology (Q1)";2041.0;741.0;United States;1946-2020;10.1182/blood-2020-134903;0.18725;465.0;;00064971;15280020;00064971;15280020;22.113;;American Society of Hematology;;3106.0;Northern America;5515.0;Q1;25454.0;Blood;A novel approach to blood smear analysis based on specimen topology: implications for human and artificial intelligence decision making;200027.0;22558.0;853.0;2755.0;26498.0;journal;article;2020
Celiac disease (CD) has been on the rise in the world and a large part of it remains undiagnosed. Novel methods are required to address the gaps in prompt detection and management. Artificial intelligence (AI) has seen an exponential surge in the last decade worldwide. With the advent of big data and powerful computational ability, we now have self-driving cars and smart devices in our daily lives. Huge databases in the form of electronic medical records and images have rendered healthcare a lucrative sector where AI can prove revolutionary. It is being used extensively to overcome the barriers in clinical workflows. From the perspective of a disease, it can be deployed in multiple steps i.e. screening tools, diagnosis, developing novel therapeutic agents, proposing management plans, and defining prognostic indicators, etc. We review the areas where it may augment physicians in the delivery of better healthcare by summarizing current literature on the use of AI in healthcare using CD as a model. We further outline major barriers to its large-scale implementations and prospects from the healthcare point of view.;Muhammad Khawar Sana and Zeshan M. Hussain and Pir Ahmad Shah and Muhammad Haisum Maqsood;"Computer Science Applications (Q1); Health Informatics (Q2)";923.0;559.0;United Kingdom;1970-2020;10.1016/j.compbiomed.2020.103996;0.01186;94.0;;00104825;00104825;18790534;00104825;4.589;Artificial intelligence, Machine learning, Deep learning, Celiac disease, Systematic review;Elsevier Ltd.;;5216.0;Western Europe;884.0;Q1;17957.0;Computers in biology and medicine;Artificial intelligence in celiac disease;9751.0;5223.0;383.0;936.0;19977.0;journal;article;2020
"Coseismic slip distributions of repeated earthquakes are important data used to forecast magnitude and recurrence characteristics of future earthquakes. Many studies infer coseismic slip of past individual paleoearthquakes from frequency histograms of geomorphic offsets. Such histograms, often expressed as a COPD (cumulative offset probability distribution), commonly show multiple peaks with similar displacements, which are interpreted to be incremental slip associated with successive paleoearthquakes. However, assumptions in linking COPD peaks to individual earthquakes have not been fully explored. Here, we combine statistical modeling with global historical coseismic surface rupture data to evaluate the conditions required for the approach. The results show that only coseismic slip with low variability allow more than one event identification; for example, even low CoVs (coefficient of offset variation) of 0.25 and 0.35 permit identification of only 2 to 3 events. In contrast, the mean CoV value for all historical ruptures is 0.58 to 0.66. Furthermore, interpreting the first COPD peak as a single-event slip distribution could result in artificially low variation and flatter distribution of offset than observed in historical earthquakes. This study cautions that robust COPD interpretation on large strike-slip faults requires low offset variability, a sufficient number of offset measurements, and additional constraints like historical coseismic slip data or paleoseismic event sequence data.";Zhou Lin and Jing Liu-Zeng and Ray J. Weldon and Jing Tian and Chao Ding and Yu Du;"Earth and Planetary Sciences (miscellaneous) (Q1); Geochemistry and Petrology (Q1); Geophysics (Q1); Space and Planetary Science (Q1)";1673.0;522.0;Netherlands;1966-2020;10.1016/j.epsl.2020.116313;0.06549;248.0;;0012821X;0012821X;0012821X;0012821X;5.255;geomorphic offset,  (coefficient of offset variation), COPD (cumulative offset probability distribution), global historical coseismic surface ruptures, strike-slip fault, earthquake recurrence models;Elsevier BV;;5457.0;Western Europe;2829.0;Q1;22566.0;Earth and planetary science letters;Modeling repeated coseismic slip to identify and characterize individual earthquakes from geomorphic offsets on strike-slip faults;74499.0;9373.0;543.0;1692.0;29632.0;journal;article;2020
"Introduction
The extent of the biological impact of passive smoke exposure is unclear. We sought to investigate the association between passive smoke exposure and DNA methylation, which could serve as a biomarker of health risk.
Materials and methods
We derived passive smoke exposure from self-reported questionnaire data among smoking and non-smoking partners of participants enrolled in the UK Household Longitudinal Study ‘Understanding Society’ (n=769). We performed an epigenome-wide association study (EWAS) of passive smoke exposure with DNA methylation in peripheral blood measured using the Illumina Infinium Methylation EPIC array.
Results
No CpG sites surpassed the epigenome-wide significance threshold of p<5.97 × 10−8 in relation to partner smoking, compared with 10 CpG sites identified in relation to own smoking. However, 10 CpG sites surpassed a less stringent threshold of p<1 × 10−5 in a model of partner smoking adjusted for own smoking (model 1), 7 CpG sites in a model of partner smoking restricted to non-smokers (model 2) and 16 CpGs in a model restricted to regular smokers (model 3). In addition, there was evidence for an interaction between own smoking status and partners’ smoking status on DNA methylation levels at the majority of CpG sites identified in models 2 and 3. There was a clear lack of enrichment for previously identified smoking signals in the EWAS of passive smoke exposure compared with the EWAS of own smoking.
Conclusion
The DNA methylation signature associated with passive smoke exposure is much less pronounced than that of own smoking, with no positive findings for ‘expected’ signals. It is unlikely that changes to DNA methylation serve as an important mechanism underlying the health risks of passive smoke exposure.";Paige M. Hulls and Frank {de Vocht} and Yanchun Bao and Caroline L. Relton and Richard M. Martin and Rebecca C. Richmond;"Biochemistry (Q1); Environmental Science (miscellaneous) (Q1)";1650.0;628.0;United States;1967-2020;10.1016/j.envres.2020.109971;0.03426;136.0;;00139351;10960953;00139351;10960953;6.498;DNA methylation, EWAS, Understanding society, Passive smoke exposure;Academic Press Inc.;;5969.0;Northern America;1460.0;Q1;21524.0;Environmental research;Dna methylation signature of passive smoke exposure is less pronounced than active smoking: the understanding society study;28576.0;11076.0;1355.0;1688.0;80881.0;journal;article;2020
Studies have indicated that detection of circulating tumor DNA (ctDNA) prior to treatment is a negative prognostic marker in non-small cell lung cancer (NSCLC). ctDNA is currently identified by detection of tumor mutations. Commercial next-generation sequencing (NGS) assays for mutation analysis of ctDNA for routine practice usually include small gene panels and are not suitable for general mutation analysis. In this study, we investigated whether mutation analysis of cfDNA could be performed using a commercially available comprehensive NGS gene panel and bioinformatics workflow. Tumor DNA, plasma DNA and peripheral blood leukocyte DNA from 30 NSCLC patients were sequenced. In two patients (7%), tumor mutations in cfDNA were immediately called by the bioinformatic workflow. In 13 patients (43%), tumor mutations were not called, but were present in ctDNA and were identified based on the known tumor mutation profile. In the remaining 15 patients (50%), no concordant mutations were detected. In conclusion, we were able to identify tumor mutations in ctDNA from 57% of NSCLC patients using a comprehensive gene panel. We demonstrated that sequencing paired tumor DNA was helpful to interpret data and confirm ctDNA, and thus increased the ratio of patients with detectable ctDNA. This approach might be feasible for mutation analysis of ctDNA in routine diagnostic practice, especially in case of suboptimal plasma quality and quantity.;Anine Larsen Ottestad and Sissel G.F. Wahl and Bjørn Henning Grønberg and Frank Skorpen and Hong Yan Dai;"Clinical Biochemistry (Q2); Pathology and Forensic Medicine (Q2); Molecular Biology (Q3)";327.0;306.0;United States;1962-1995, 1997-2020;10.1016/j.yexmp.2019.104347;0.00369;68.0;;00144800;10960945;00144800;10960945;3.362;Non-small cell lung cancer (NSCLC), Next-generation sequencing (NGS), Circulating tumor DNA (ctDNA);Academic Press Inc.;;4805.0;Northern America;791.0;Q2;13247.0;Experimental and molecular pathology;The relevance of tumor mutation profiling in interpretation of ngs data from cell-free dna in non-small cell lung cancer patients;4792.0;986.0;170.0;329.0;8168.0;journal;article;2020
The pedotransfer function is a mathematical model used to convert direct soil measurements into known and unknown soil properties. It provides information for modelling and simulation in soil research, hydrology, environmental science and climate change impacts, including investigating the carbon cycle and the exchange of carbon between soils and the atmosphere to support carbon farming. In particular, the pedotransfer function can provide input parameters for landscape design, soil quality assessment and economic optimisation. The objective of the study was to investigate the feasibility of using a generalised pedotransfer function derived with a machine learning method to predict soil electrical conductivity (EC) and soil organic carbon content (OC) for different regional locations in the state of Victoria, Australia. This strategy supports a unified approach to the interpolation and population of a single regional soils database, in contrast to a range of pedotransfer functions derived from local databases with measurement sets that may have limited transferability. The pedotransfer function generation was based on a machine learning algorithm incorporating the Generalized Linear Mixed Model with interactions and nested terms, with Residual Maximum Likelihood estimation, and a predictor-frequency ranking system with step-wise reduction of predictors to evaluate the predictive errors in reduced models. The source of the data was the Victorian Soil Information System (VSIS), which is a database administered for soil information and mapping purposes. The database contains soil measurements and information from locations across Victoria and is a repository of historical data, including monitoring studies. In total, data from 93 projects were available for inputs to modelling and analysis, with 5158 samples used to derive predictors for EC and 1954 samples used to derive predictors for OC. Over 500 models were tested by systematically reducing the number of predictors from the full model. Five-fold cross-validation was used for estimation of model mean-squared prediction error (MSPE) and mean-absolute percentage error (MAPE). The results were statistically significant with only a gradual reduction in error for the top-ranked 50 models. The prediction errors (MSPE and MAPE) of the top ranked model for EC are 0.686 and 0.635, and 0.413 and 0.474 for OC respectively. The four most frequently occurring predictors both for EC and OC prediction across the full set of models were found to be soil depth, pH, particle size distribution and geomorphological mapping unit. The possible advantages and disadvantages of this approach were discussed with respect to other machine learning approaches.;K.K. Benke and S. Norng and N.J. Robinson and K. Chia and D.B. Rees and J. Hopley;Soil Science (Q1);1460.0;613.0;Netherlands;1967-2021;10.1016/j.geoderma.2020.114210;0.02441;165.0;;00167061;18726259;00167061;18726259;6.114;Artificial intelligence, Big data, Biostatistics, Carbon farming, Climate change, Predictive analytics, Regional analysis, Simulation;Elsevier;;6319.0;Western Europe;1846.0;Q1;39112.0;Geoderma;Development of pedotransfer functions by machine learning for prediction of soil electrical conductivity and organic carbon content;33708.0;9006.0;543.0;1472.0;34310.0;journal;article;2020
"The process of cleaning motion capture data of aberrant points has been described as “the bane of motion capture operators”. Yet, managing the high volume kinematic data generated through in-home neurogames requires data quality control that, executed insufficiently, jeopardizes accuracy of outcomes. To begin to address this issue at the intersection of biomechanics and “big data”, we performed a secondary analysis of a neurogame, evaluating gesture count as well as shoulder and elbow joint angle outcomes calculated from kinematic data in which valid gestures were identified through 3 methods: visual review of regions of interest by an expert (BP); manufacturer-recommended data smoothing (MS); and automated methods (AI). We hypothesized that upper extremity kinematic outcomes from BP would be matched by AI but not MS methods. From one person with post-stroke hemiparesis, upper-extremity kinematic data were collected for 6 days over 2 weeks using a Microsoft Kinect™-based neurogame. We calculated gesture count, shoulder angle, and elbow angle outcomes from data managed using BP, MS, and AI methods. BP identified 1929 valid gestures total over 6 days which was different than the other two methods (p = 0.0015). In contrast, the AI algorithm with best precision identified 4372 and MS identified 4459 valid gestures. Furthermore, angle outcomes calculated from AI and MS methods resulted in different values than BP (p < 0.001 for 5 of 6 variables). More research is needed to automate treatment of high volume, low quality motion data to support investigation of motion associated with in-home rehabilitation neurogames.";Lise C. Worthen-Chaudhari and Michael P. McNally and Akshay Deshpande and Vivek Bakaraju;"Rehabilitation (Q1); Biomedical Engineering (Q2); Biophysics (Q2); Orthopedics and Sports Medicine (Q2); Sports Science (Q2)";1361.0;266.0;United Kingdom;1968-2020;10.1016/j.jbiomech.2020.109726;0.01862;199.0;;00219290;18732380;00219290;18732380;2.712;;Elsevier Ltd.;;3796.0;Western Europe;826.0;Q1;15846.0;Journal of biomechanics;In-home neurogaming: demonstrating the impact of valid gesture recognition method on high volume kinematic outcomes;35320.0;3948.0;510.0;1383.0;19358.0;journal;article;2020
Single-cell RNA sequencing (scRNA-seq), a method of transcriptome sequencing at the single-cell level, has recently emerged as a revolutionary technology in the field of biomedical research. Compared to conventional gene expression profiling in bulk, scRNA-seq resolves biological differences among individual cells and enables the identification of rare cell populations that are easily overlooked. This review introduces the method of scRNA-seq, summarizes its applications in the field of cardiovascular disease research, and discusses existing limitations and prospects for future applications.;Chen Yifan and Yang Fan and Pu Jun;"Cardiology and Cardiovascular Medicine (Q1); Molecular Biology (Q2)";568.0;427.0;United States;1970-2020;10.1016/j.yjmcc.2020.03.005;0.01569;159.0;;00222828;00222828;10958584;00222828;5.0;Atherosclerosis, Vulnerable plaques, Macrophage polarization, Melatonin, Single-cell sequencing analysis, Multiomics analysis;Academic Press Inc.;;5705.0;Northern America;1645.0;Q1;23882.0;Journal of molecular and cellular cardiology;Visualization of cardiovascular development, physiology and disease at the single-cell level: opportunities and future challenges;17399.0;2631.0;177.0;589.0;10098.0;journal;article;2020
"Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.";Kim S. Betts and Steve Kisely and Rosa Alati;"Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)";755.0;441.0;United Kingdom;1961-1982, 1984-2020;10.1016/j.jpsychires.2020.07.002;0.02003;136.0;;00223956;18791379;00223956;18791379;4.791;Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning;Elsevier Ltd.;;5173.0;Western Europe;1875.0;Q1;16812.0;Journal of psychiatric research;Predicting postpartum psychiatric admission using a machine learning approach;20371.0;3624.0;357.0;776.0;18469.0;journal;article;2020
As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.;Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo;Surgery (Q2);1614.0;196.0;United States;1961-2021;10.1016/j.jss.2019.09.053;0.01875;108.0;;00224804;10958673;00224804;10958673;2.192;Electronic health record, Big data, Drug safety, Health care database, Cancer risk;Academic Press Inc.;;2850.0;Northern America;780.0;Q2;21728.0;Journal of surgical research;What can we learn about drug safety and other effects in the era of electronic health records and big data that we would not be able to learn from classic epidemiology?;17062.0;3583.0;688.0;1666.0;19607.0;journal;article;2020
"Shales are a class of multiscale, multiphase, hybrid inorganic-organic composite materials exhibiting both frictional and cohesive behavior, and it is very challenging to characterize and interpret their complex mechanical properties. A statistical nanoindentation approach with pertinent viable data analytics was developed to probe the mechanical properties of shales across different length scales. Grid nanoindentation experiments with continuous stiffness measurement performed on shales to relatively large depths of 6–8 µm obtained massive data, which were processed by the new data analytics: segmentation at selected depths of a great number (e.g., >500) of continuous Young's modulus versus indentation depth curves obtained from unknown constituent phases yielded multiple discretized sub-datasets that were processed to extract individual phases’ elastic moduli at respective segmentation depths via probability density function (PDF)-based deconvolution; these depth-dependent Young's moduli of each phase were then fitted by a newly proposed surround effect model, leading to determination of the properties of both individual phases at the nano/micro-scales (i.e., virtually infinitesimal depths) and the bulk rock at the macroscale (i.e., ~10–100 µm depths). A significant advantage of this massive data-based indentation approach is that the mechanical properties of composite materials such as shales can be probed across different scales by a single measurement technique. In addition, a new criterion, termed Bin Size Index, was formulated for selecting depth-dependent, rational, optimized bin sizes for PDF construction. For the studied shales, results show that five mechanically-distinct phases are discerned, including a virtual interface phase between hard and soft constituents accounting for a majority of indents. Coincidently, the Young's modulus of the bulk rock is nearly the same as that of the interface phase, suggesting that the macroscopic properties of similar composites may be estimated from measurements on the interface of two phases with contrasting mechanical properties. Finally, this approach can guide the selection of appropriate indentation depths to probe the mechanical properties of both highly heterogeneous bulk materials at the macroscale and their individual constituent phases at the nano/micro-scale.";Shengmin Luo and Yunhu Lu and Yongkang Wu and Jinliang Song and Don J. DeGroot and Yan Jin and Guoping Zhang;"Condensed Matter Physics (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1)";725.0;616.0;United Kingdom;1952-2020;10.1016/j.jmps.2020.103945;0.01754;173.0;;00225096;00225096;00225096;00225096;5.471;Data analytics, Nanoindentation, Shale, Surround effect, Young's modulus;Elsevier Ltd.;;5399.0;Western Europe;1857.0;Q1;14428.0;Journal of the mechanics and physics of solids;Cross-scale characterization of the elasticity of shales: statistical nanoindentation and data analytics;24119.0;4551.0;315.0;727.0;17008.0;journal;article;2020
With the wide adoption of functional magnetic resonance imaging (fMRI) by cognitive neuroscience researchers, large volumes of brain imaging data have been accumulated in recent years. Aggregating these data to derive scientific insights often faces the challenge that fMRI data are high-dimensional, heterogeneous across people, and noisy. These challenges demand the development of computational tools that are tailored both for the neuroscience questions and for the properties of the data. We review a few recently developed algorithms in various domains of fMRI research: fMRI in naturalistic tasks, analyzing full-brain functional connectivity, pattern classification, inferring representational similarity and modeling structured residuals. These algorithms all tackle the challenges in fMRI similarly: they start by making clear statements of assumptions about neural data and existing domain knowledge, incorporate those assumptions and domain knowledge into probabilistic graphical models, and use those models to estimate properties of interest or latent structures in the data. Such approaches can avoid erroneous findings, reduce the impact of noise, better utilize known properties of the data, and better aggregate data across groups of subjects. With these successful cases, we advocate wider adoption of explicit model construction in cognitive neuroscience. Although we focus on fMRI, the principle illustrated here is generally applicable to brain data of other modalities.;Ming Bo Cai and Michael Shvartsman and Anqi Wu and Hejia Zhang and Xia Zhu;"Behavioral Neuroscience (Q1); Cognitive Neuroscience (Q1); Experimental and Cognitive Psychology (Q1)";1023.0;303.0;United Kingdom;1963-2020;10.1016/j.neuropsychologia.2020.107500;0.02238;206.0;;00283932;18733514;00283932;18733514;3.139;Probabilistic graphical model, Bayesian, fMRI, Cognitive neuroscience, Big data, Factor model, Matrix normal;Elsevier Ltd.;;8150.0;Western Europe;1439.0;Q1;18009.0;Neuropsychologia;Incorporating structured assumptions with probabilistic graphical models in fmri data analysis;28496.0;3257.0;333.0;1036.0;27141.0;journal;article;2020
Existing logo detection methods usually consider a small number of logo classes, limited images per class and assume fine-gained object bounding box annotations. This limits their scalability to real-world dynamic applications. In this work, we tackle these challenges by exploring a web data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL2), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset “WebLogo-2M” by designing an automatic data collection and processing method. Extensive comparative evaluations demonstrate the superiority of SL2 over the state-of-the-art strongly and weakly supervised detection models and contemporary web data learning approaches.;Hang Su and Shaogang Gong and Xiatian Zhu;"Artificial Intelligence (Q1); Computer Vision and Pattern Recognition (Q1); Signal Processing (Q1); Software (Q1)";1168.0;1035.0;United Kingdom;1968-2021;10.1016/j.patcog.2019.107003;0.03271;210.0;;00313203;00313203;00313203;00313203;7.74;Object detection, Logo recognition, Logo dataset, Web data mining, Self-Learning, Co-Learning;Elsevier Ltd.;;4738.0;Western Europe;1492.0;Q1;24823.0;Pattern recognition;Scalable logo detection by self co-learning;33363.0;12476.0;412.0;1180.0;19522.0;journal;article;2020
Petrographic analysis based on microfacies identification in thin sections is widely used in sedimentary environment interpretation and paleoecological reconstruction. Fossil recognition from microfacies is an essential procedure for petrographers to complete this task. Distinguishing the morphological and microstructural diversity of skeletal fragments requires extensive prior knowledge of fossil morphotypes in microfacies and long training sessions under the microscope. This requirement engenders certain challenges for sedimentologists and paleontologists, especially novices. However, a machine classifier can help address this challenge. In this study, we collected a microfacies image dataset comprising both public data from 1133 references and our own materials (including a total of 30,815 images of 22 fossil and abiotic grain groups). We employed a high-performance workstation to implement four classic deep convolutional neural networks, which have proven to be highly efficient in computer vision. Our framework uses a transfer learning technique, which reuses the pre-trained parameters that are trained on a larger ImageNet dataset as initialization for the network to achieve high accuracy with low computing costs. We obtained up to 95% of the top one and 99% of the top three test accuracies in the Inception ResNet v2 architecture. The machine classifier exhibited 0.99 precision on minerals such as dolomite and pyrite. Although it had some difficulty on samples having similar morphologies, such as bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88 precision. Our machine learning framework demonstrates high accuracy with reproducibility and bias avoidance that is comparable to those of human classifiers. Its application can thus eliminate much of the tedious, manually intensive efforts by human experts conducting routine identification.;Xiaokang Liu and Haijun Song;"Geology (Q1); Stratigraphy (Q1)";401.0;338.0;Netherlands;1967-2020;10.1016/j.sedgeo.2020.105790;0.00845;113.0;;00370738;00370738;00370738;00370738;3.397;Microfossils, Minerals, Sedimentary structures, Machine learning, Transfer learning;Elsevier;;9285.0;Western Europe;1234.0;Q1;26319.0;Sedimentary geology;Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks;14860.0;1466.0;127.0;403.0;11792.0;journal;article;2020
The high-speed rail (HSR) of China has developed and expanded rapidly and made great achievements in the past twenty years. The ongoing HSR plan is expected to have a significant impact on the urban economy and spatial structure in China. However, relevant data-driven research is still lacking. Traditional data collection approaches such as field surveys are costly to assure the accuracy of materials. In this study, a new remote sensing perspective of night-time light (NTL) was adopted to observe the long-term impact of the HSR on cities along the rail. More specifically, we investigated the impact of the Beijing–Guangzhou High-Speed Railway (BGHSR) on urban economic development by using night-time light data from 2002 to 2018. Such a line connects the capital (located in the north of China) and southern China and lies on the most important geographic axis of the country. Our results find that the construction of BGHSR line has a considerable positive impact on economies of first-tier cities (e.g., Beijing and Guangzhou) and new-first-tier cities (e.g., Zhengzhou, Wuhan, and Changsha), but also hurt some second-tier and third-tier cities such as Baoding and Handan. Generally, the spatial economic pattern of cities along the BGHSR line has been rapidly reshaped with the change of the transportation system. Each city needs to reconsider its role and value in the coming regionalization process to adapt to the national strategy.;Yunxiang Guo and Wenhao Yu and Zhanlong Chen and Renwei Zou;"Geography, Planning and Development (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1); Economics and Econometrics (Q2); Statistics, Probability and Uncertainty (Q2)";137.0;509.0;United Kingdom;1967-2020;10.1016/j.seps.2020.100905;0.00155;52.0;;00380121;00380121;00380121;00380121;4.923;High-speed rail, Urban economic development, Nighttime light image, Urban agglomeration, China;Elsevier Ltd.;;5721.0;Western Europe;1020.0;Q1;18291.0;Socio-economic planning sciences;Impact of high-speed rail on urban economic development: an observation from the beijing-guangzhou line based on night-time light images;2347.0;706.0;200.0;141.0;11441.0;journal;article;2020
Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.;Robert Northcott;"History (Q1); History and Philosophy of Science (Q1)";134.0;122.0;United Kingdom;1970-1971, 1974-1978, 1980-1981, 1983-1986, 1988, 1990-2020;10.1016/j.shpsa.2019.09.002;;37.0;;00393681;00393681;18792510;00393681;;Big data, Prediction, Case studies, Explanation, Elections, Weather;Elsevier Ltd.;;5317.0;Western Europe;615.0;Q1;11600154632.0;Studies in history and philosophy of science part a;Big data and prediction: four case studies;;229.0;95.0;150.0;5051.0;journal;article;2020
The interrogation of established, large-scale datasets presents great opportunities in health data science for the linkage and mining of potentially disparate resources to create new knowledge in a fast and cost-efficient manner. The number of datasets that can be queried in the field of multimorbidity is vast, ranging from national administrative and audit datasets, large clinical, technical and biological cohorts, through to more bespoke data collections made available by individual organisations and laboratories. However, with these opportunities also come technical and regulatory challenges that require an informed approach. In this review, we outline the potential benefits of using previously collected data as a vehicle for research activity. We illustrate the added value of combining potentially disparate datasets to find answers to novel questions in the field. We focus on the legal, governance and logistical considerations required to hold and analyse data acquired from disparate sources and outline some of the solutions to these challenges. We discuss the infrastructure resources required and the essential considerations in data curation and informatics management, and briefly discuss some of the analysis approaches currently used.;Christopher Boulton and J. Mark Wilkinson;"Aging (Q1); Developmental Biology (Q2)";269.0;501.0;Ireland;1972-2020;10.1016/j.mad.2020.111310;0.00451;117.0;;00476374;18726216;00476374;18726216;5.432;Public datasets, Legislation, Information governance, Data curation, Multimorbidity;Elsevier Ireland Ltd;;9974.0;Western Europe;1534.0;Q1;29640.0;Mechanisms of ageing and development;Use of public datasets in the examination of multimorbidity: opportunities and challenges;7302.0;1383.0;144.0;279.0;14363.0;journal;article;2020
"Long-term mechanistic research and monitoring provides integral science support for ecosystem-based management (EBM) of resources, activities and services. Decades of oceanographic and ecological research by Bill Peterson and colleagues along the Newport Hydrographic Line (NH Line) provides essential context for understanding and managing Pacific salmon Oncorhynchus spp. and other marine resources in the California Current ecosystem. This research program helped federal scientists convey the significance of the northeast Pacific marine heatwave (2013–2016) to fisheries managers and stakeholders. Particularly illustrative were shifts in the composition of the copepod community, which reflected feeding conditions for Pacific salmon and other consumers. We identify six traits of the dataset produced by Peterson and colleagues that have made it especially valuable for informing management: (i) it has generated robust ecosystem indicators; (ii) it is long-term; (iii) it is associated with meaningful ecological mechanisms; (iv) it relates to ecosystem components of high societal value; (v) it can represent processes and lags at meaningful temporal scales; and (vi) it is part of a broader, integrative science effort. This research effort underscores the importance of developing and sustaining long-term mechanistic research and monitoring along the U.S. West Coast and elsewhere in the world.";Chris J. Harvey and Jennifer L. Fisher and Jameal F. Samhouri and Gregory D. Williams and Tessa B. Francis and Kym C. Jacobson and Yvonne L. deReynier and Mary E. Hunsicker and Newell Garfield;"Aquatic Science (Q1); Geology (Q1)";441.0;381.0;United Kingdom;1963-1965, 1969, 1973, 1976-2020;10.1016/j.pocean.2020.102418;0.01183;132.0;;00796611;00796611;00796611;00796611;4.08;Ecosystem-based fisheries management, Ecosystem indicators, California Current large marine ecosystem, Copepods, Salmon, Food web ecology, Climate, Long-term monitoring;Elsevier Ltd.;;8280.0;Western Europe;1487.0;Q1;28409.0;Progress in oceanography;The importance of long-term ecological time series for integrated ecosystem assessment and ecosystem-based management;10811.0;1822.0;161.0;448.0;13330.0;journal;article;2020
Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.;Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum;"Hematology (Q1); Oncology (Q1)";124.0;421.0;United Kingdom;1974-2020;10.1053/j.seminoncol.2020.02.006;0.00455;133.0;;00937754;00937754;15328708;00937754;4.929;Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics;W.B. Saunders Ltd;;5822.0;Western Europe;1812.0;Q1;13120.0;Seminars in oncology;Using big data in pediatric oncology: current applications and future directions;5713.0;690.0;51.0;150.0;2969.0;journal;article;2020
Faculty learning communities (FLCs) are year-long professional development opportunities available at many higher education institutions in the United States. While the literature reflects some librarian engagement with FLCs, it seems limited primarily to areas of traditional librarian expertise such as information literacy and outreach. This article describes a case study of a librarian-facilitated FLC focused on data literacy, which resulted in the development of a teaching toolkit, library-led data literacy instruction, and ongoing collaborations between librarians and faculty. The FLC structure proved to be a valuable framework that facilitated collaborative learning in topics relevant to both disciplinary faculty and librarians. In addition, the tangible work products produced by the FLC serve to advance the strategic, curricular goals of the university while giving the library an opportunity to showcase its value in the academic lifecycle.;Theresa Burress and Emily Mann and Tina Neville;"Education (Q1); Library and Information Sciences (Q1)";249.0;165.0;United Kingdom;1988-2020;10.1016/j.acalib.2019.102076;0.00237;58.0;;00991333;00991333;00991333;00991333;1.533;Librarian-faculty collaboration, Data literacy, Higher education, Centers for teaching and learning, Faculty learning communities;Elsevier BV;;4143.0;Western Europe;889.0;Q1;12791.0;Journal of academic librarianship;Exploring data literacy via a librarian-faculty learning community: a case study;1990.0;501.0;112.0;252.0;4640.0;journal;article;2020
This paper presents a hybrid deep forest approach for outlier detection and fault diagnosis. Isolation forest algorithm is combined with Pearson's correlation coefficient for outlier detection. The physical significance of outliers detected by the proposed algorithm is explained by origin analysis, which is rarely mentioned in existing studies. In addition, a novel non-neural network deep learning model-cascade forest model is proposed to fault diagnosis of HVAC system for the first time to achieve high precision accuracy in low-dimensional features. The proposed approach is validated with the refrigerant charge fault of VRF system. The results show that the isolation forest algorithm can improve the performance of fault diagnosis model and the mainly outliers of VRF system are defrosting data. The IF-CF model has short operation time, and high accuracy in low-dimensional features. When the dimension drops to 6, the accuracy of the IF-CF model is 94.16%, which is 5.26%, 10.02%, 5.87% and 3.34% higher than the IF-MLP, IF-BPNN, IF-SVM and IF-LSTM models, respectively. Moreover, IF-CF model does not require complex hyper-parameter optimization strategy because its maximum accuracy difference in different hyper-parameters is 2.04%. This study is enlightening which may inspire the potential of outlier detection technology and deep learning in HVAC field.;Yuke Zeng and Huanxin Chen and Chengliang Xu and Yahao Cheng and Qijian Gong;"Building and Construction (Q1); Mechanical Engineering (Q1)";1061.0;385.0;United Kingdom;1978-2020;10.1016/j.ijrefrig.2020.08.014;;116.0;;01407007;01407007;01407007;01407007;;Variable refrigerant flow system, Outlier detection, Fault diagnosis, Deep forest model, Système à débit de frigorigène variable, Méthode de détection des valeurs aberrantes, Diagnostic des défaillances, Modèle de forêt neuronale profonde;Elsevier Ltd.;;3476.0;Western Europe;1497.0;Q1;16113.0;International journal of refrigeration;A hybrid deep forest approach for outlier detection and fault diagnosis of variable refrigerant flow system;;4287.0;371.0;1104.0;12895.0;journal;article;2020
Web services playing a vital role in the World Wide Web and generates huge amount of information across various domains of internet. Due to this evolution data in the form of articles, reports, digital galleries and web data of companies were increased everyday. To handle the huge volume of data each and every day, automatic query classification based on internet is more significant method. Research and development community has developed various techniques for the web services discovery, where it offers the mandated data for the improvement method. With respect to the literature survey, most of the researchers are concentrating to provide the efficient web service discovery. The amount of data that is available in the web is keeps on increasing and also it is used to differentiate the services, explanation and work of art. In order to achieve this method, machine learning algorithm is applied extensively for domain categorization. Various machine learning algorithm like KNN is applied for web service discovery. The systems are effectively learning the input and evaluate the performance accuracy with the given datasets. This paper, proposes an improved fuzzy with KNN algorithm for effective web service classification. This is used to increase an outcome in the form of accuracy and performance measures.;C. Viji and J {Beschi Raja} and R.S. Ponmagal and S.T. Suganthi and P. Parthasarathi and Sanjeevi Pandiyan;"Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3)";423.0;234.0;Netherlands;1978-2020;10.1016/j.micpro.2020.103097;0.00207;38.0;;01419331;01419331;01419331;01419331;1.525;Web services, Classification, Improved fuzzy with KNN, KNN classification;Elsevier;;2771.0;Western Europe;323.0;Q3;15552.0;Microprocessors and microsystems;Efficient fuzzy based k-nearest neighbour technique for web services classification;1490.0;962.0;462.0;428.0;12804.0;journal;article;2020
Density log data is one of the key physical attributes used for reservoir characterization by quantifying and qualifying lithological attributes in a wellbore. The density log is most often acquired by geophysical wireline logging techniques after drilling. However, wireline logging can be difficult to execute in highly deviated wells and alternative data acquisition such as logging-while-drilling (LWD) can be very costly. This paper describes the process of using instantaneous drilling attributes together with a machine learning algorithm, extreme gradient boosting (XGBoost), to generate a pseudo density log. The mean absolute error (MAE) and root mean square error (RMSE) are used as the evaluation metrics. Case studies are performed using data from six coalbed methane (or coal seam gas) wells in the Surat Basin, Australia. The inputs include drilling data [weight on bit (WOB), rotations per minute (RPM), torque, true vertical depth (TVD) and rate of penetration (ROP)] and LWD data (i.e., natural gamma ray and hole diameter). The MAE of pseudo density log for most wells is between 0.08 and 0.11 g/cc (except Well 4 is 0.16 g/cc), which results in an average error rate less than 5%. It is found that TVD, gamma ray, ROP, and hole diameter are four most important features. Further experiment shows that the model with these four important features has almost the same performance as the model with all features. The proposed machine learning methodology can assist petroleum engineers and geologists in reservoir characterization by generating pseudo density logs from ongoing LWD and drilling data in real time. It can potentially mitigate the need to run wireline logging tools after drilling or costly LWD techniques whilst drilling.;Ruizhi Zhong and Raymond Johnson and Zhongwei Chen;"Economic Geology (Q1); Fuel Technology (Q1); Geology (Q1); Stratigraphy (Q1)";516.0;681.0;Netherlands;1980-2020;10.1016/j.coal.2020.103416;0.01168;136.0;;01665162;01665162;01665162;01665162;6.806;Density logging, Pseudo density log, Machine learning, Drilling, Coalbed methane (CBM), Coal seam gas (CSG), Extreme Gradient Boosting (XGBoost);Elsevier;;8187.0;Western Europe;2048.0;Q1;27524.0;International journal of coal geology;Generating pseudo density log from drilling and logging-while-drilling data using extreme gradient boosting (xgboost);17982.0;3772.0;178.0;520.0;14573.0;journal;article;2020
"Individualizing patient treatment is a core objective of the medical field. Reaching this objective has been elusive owing to the complex set of factors contributing to both disease and health; many factors, from genes to proteins, remain unknown in their role in human physiology. Accurately diagnosing, monitoring, and treating disorders requires advances in biomarker discovery, the subsequent development of accurate signatures that correspond with dynamic disease states, as well as therapeutic interventions that can be continuously optimized and modulated for dose and drug selection. This work highlights key breakthroughs in the development of enabling technologies that further the goal of personalized and precision medicine, and remaining challenges that, when addressed, may forge unprecedented capabilities in realizing truly individualized patient care.";Dean Ho and Stephen R. Quake and Edward R.B. McCabe and Wee Joo Chng and Edward K. Chow and Xianting Ding and Bruce D. Gelb and Geoffrey S. Ginsburg and Jason Hassenstab and Chih-Ming Ho and William C. Mobley and Garry P. Nolan and Steven T. Rosen and Patrick Tan and Yun Yen and Ali Zarrinpar;"Bioengineering (Q1); Biotechnology (Q1)";346.0;1186.0;United Kingdom;1983-2020;10.1016/j.tibtech.2019.12.021;0.01817;219.0;;01677799;01677799;18793096;01677799;19.536;personalized medicine, precision medicine, therapeutics, diagnostics, artificial intelligence, clinical trials;Elsevier Ltd.;;6710.0;Western Europe;3192.0;Q1;16146.0;Trends in biotechnology;Enabling technologies for personalized and precision medicine;20693.0;4561.0;167.0;397.0;11206.0;journal;article;2020
"Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.";Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso;"Hematology (Q1); Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";959.0;514.0;Ireland;1983-2020;10.1016/j.radonc.2020.09.054;0.02494;157.0;;01678140;18790887;01678140;18790887;6.28;Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making;Elsevier Ireland Ltd;;3159.0;Western Europe;1892.0;Q1;17876.0;Radiotherapy and oncology;From multisource data to clinical decision aids in radiation oncology: the need for a clinical data science community;22462.0;5131.0;470.0;1026.0;14846.0;journal;article;2020
Over the past ten years, plant phenotyping technologies that utilize sensing and data mining approaches to estimate crop traits in a high-throughput and objective manner, have been evaluated and applied in different crop improvement and breeding programs. Multiple platforms, from proximal to unmanned aerial systems based remote sensing, have been developed and applied to increase the throughput, efficiency, and objectivity during field phenotyping. In recent years, the development and availability of high-resolution satellite imagery from low-orbit satellites have offered yet another opportunity for phenotyping applications. This review demonstrates the applications of satellite imagery in agricultural production and crop phenotyping and suggests plant traits that can be evaluated using high-resolution satellite imagery. The review summarizes the merits (e.g. rapid/automated data capture from larger and multiple field sites) and challenges (e.g. cloud occlusion) of satellite-based phenotyping in crop breeding programs, and discusses future perspectives/opportunities of high-resolution satellite imagery as a phenotyping tool. High-resolution satellite imagery can serve as a phenotyping tool for the assessment of crop varieties, thus assisting plants breeders in the process of selecting high-yielding, stress (abiotic and biotic) tolerant variety that can contribute to addressing world food demand amidst climate change.;Chongyuan Zhang and Afef Marzougui and Sindhuja Sankaran;"Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)";1298.0;727.0;Netherlands;1985-2020;10.1016/j.compag.2020.105584;0.01646;115.0;;01681699;01681699;01681699;01681699;5.565;Phenomics, Crop improvement, Plant breeding, Low-orbiting satellite, Remote sensing;Elsevier;;4433.0;Western Europe;1208.0;Q1;30441.0;Computers and electronics in agriculture;High-resolution satellite imagery applications in crop phenotyping: an overview;17657.0;9479.0;648.0;1300.0;28725.0;journal;article;2020
With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.;Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães;"Instrumentation (Q2); Nuclear and High Energy Physics (Q3)";1750.0;142.0;Netherlands;1983-2020;10.1016/j.nimb.2020.05.027;;119.0;;0168583X;0168583X;0168583X;0168583X;;Ion beam analysis, Big data, Data quality assurance, Artificial intelligence;Elsevier;;3041.0;Western Europe;429.0;Q2;29068.0;Nuclear instruments and methods in physics research, section b: beam interactions with materials and atoms;Ion beam analysis and big data: how data science can support next-generation instrumentation;;2403.0;514.0;1766.0;15630.0;journal;article;2020
We describe a strategy for applying multilevel regression and post-stratification (MRP) methods to pre-election polling. Using a combination of contemporaneous polling, census data, past election polling, past election results, and other sources of information, we are able to construct probabilistic, internally consistent estimates of national votes and the sub-national electoral districts that determine seats or electoral votes in many electoral systems. We report on the performance of the general framework in three applications that were conducted and released publicly in advance of the 2016 UK Referendum on EU Membership, the 2016 US Presidential Election, and the 2017 UK General Election.;Benjamin E. Lauderdale and Delia Bailey and Jack Blumenau and Douglas Rivers;Business and International Management (Q1);261.0;450.0;Netherlands;1985-2020;10.1016/j.ijforecast.2019.05.012;0.00612;96.0;;01692070;01692070;01692070;01692070;3.779;Election forecasting, Polling, UK politics, US politics, Multilevel regression and stratification;Elsevier;;3928.0;Western Europe;1268.0;Q1;22706.0;International journal of forecasting;Model-based pre-election polling for national and sub-national outcomes in the us and uk;7207.0;1254.0;142.0;277.0;5578.0;journal;article;2020
"Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.";Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park;"Cancer Research (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine (Q1)";845.0;471.0;Ireland;1985-2020;10.1016/j.lungcan.2020.05.033;0.01966;129.0;;01695002;01695002;18728332;01695002;5.705;Real-time updated system, Big data, Real-world data, NSCLC, Treatment;Elsevier Ireland Ltd;;3011.0;Western Europe;1989.0;Q1;12391.0;Lung cancer;"Ten-year patient journey of stage iii non-small cell lung cancer patients: a single-center, observational, retrospective study in korea (realtime automatically updated data warehouse in health care; universe-root study)";15504.0;4671.0;349.0;917.0;10507.0;journal;article;2020
In recent years, the wealth of technological development revolutionised our ability to collect data in geosciences. Due to the unprecedented level of detail of these datasets, geomorphologists are facing new challenges, giving more in-depth answers to a broad(er) range of fundamental questions across the full spectrum of the Earth's (and Planetary) processes. This contribution builds on the existing literature of geomorphometry (the science of quantitative land-surface analysis) and feature extraction (translate land surface parameters into extents of geomorphological elements). It provides evidence of critical themes as well as emerging fields of future research in the digital realm, supporting the likely effectiveness of geomorphometry and feature extractions as they are advancing the theoretical, empirical and applied dimension of geomorphology. The review further discusses the role of geomorphometric legacies, and scientific reproducibility, and how they can be implemented, in the hope that this will facilitate action towards improving the transparency, and efficiency of scientific research, and accelerate discoveries in geomorphology. In the current landscape, substantial changes in landforms, ecosystems, land use, hydrological routing, and direct anthropogenic modifications impact systems across the full spectrum of geomorphological processes. Although uncertainties in the precise nature and likelihood of changes exist, geomorphometry and feature extraction can aid exploring process regimes and landscape responses. Taken together, they can revolutionise geomorphology by opening the doors to improved investigations crossing space and time scales, blurring the boundaries between traditional approaches and computer modelling, and facilitating cross-disciplinary research. Ultimately, the exploitation of the available wealth of digital information can help to translate our understanding of geomorphic processes, which is often based on observations of past or current conditions, into the rapidly changing future.;G. Sofia;Earth-Surface Processes (Q1);1105.0;391.0;Netherlands;1984, 1987-2020;10.1016/j.geomorph.2020.107055;0.01992;159.0;;0169555X;0169555X;1872695X;0169555X;4.139;Geomorphometry, Feature extraction, Lidar, Digital elevation model, Global;Elsevier;;8297.0;Western Europe;1346.0;Q1;26979.0;Geomorphology;Combining geomorphometry, feature extraction techniques and earth-surface processes research: the way forward;28906.0;5052.0;398.0;1130.0;33022.0;journal;article;2020
"Introduction
The public-private ADVANCE consortium (Accelerated development of vaccine benefit-risk collaboration in Europe) aimed to assess if electronic healthcare databases can provide fit-for purpose data for collaborative, distributed studies and monitoring of vaccine coverage, benefits and risks of vaccines.
Objective
To evaluate if European healthcare databases can be used to estimate vaccine coverage, benefit and/or risk using pertussis-containing vaccines as an example.
Methods
Characterisation was conducted using open-source Java-based (Jerboa) software and R scripts. We obtained: (i) The general characteristics of the database and data source (meta-data) and (ii) a detailed description of the database population (size, representatively of age/sex of national population, rounding of birth dates, delay between birth and database entry), vaccinations (number of vaccine doses, recording of doses, pattern of doses by age and coverage) and events of interest (diagnosis codes, incidence rates). A total of nine databases (primary care, regional/national record linkage) provided data on events (pertussis, pneumonia, death, fever, convulsions, injection site reactions, hypotonic hypo-responsive episode, persistent crying) and vaccines (acellular pertussis and whole cell pertussis) related to the pertussis proof of concept studies.
Results
The databases contained data for a total population of 44 million individuals. Seven databases had recorded doses of vaccines. The pertussis coverage estimates were similar to those reported by the World Health Organisation (WHO). Incidence rates of events were comparable in magnitude and age-distribution between databases with the same characteristics. Several conditions (persistent crying and somnolence) were not captured by the databases for which outcomes were restricted to hospital discharge diagnoses.
Conclusion
The database characterisation programs and workflows allowed for an efficient, transparent and standardised description and verification of electronic healthcare databases which may participate in pertussis vaccine coverage, benefit and risk studies. This approach is ready to be used for other vaccines/events to create readiness for participation in other vaccine related studies.";Miriam Sturkenboom and Toon Braeye and Lieke {van der Aa} and Giorgia Danieli and Caitlin Dodd and Talita Duarte-Salles and Hanne- Dorthe Emborg and Marius Gheorghe and Johnny Kahlert and Rosa Gini and Consuelo Huerta-Alvarez and Elisa Martín-Merino and Chris McGee and Simon {de Lusignan} and Gino Picelli and Giuseppe Roberto and Lara Tramontan and Marco Villa and Daniel Weibel and Lina Titievsky;"Immunology and Microbiology (miscellaneous)  (Q1); Infectious Diseases (Q1); Molecular Medicine (Q1); Public Health, Environmental and Occupational Health (Q1); Veterinary (miscellaneous) (Q1)";2957.0;327.0;Netherlands;1983-2020;10.1016/j.vaccine.2020.01.100;0.0618;184.0;;0264410X;0264410X;18732518;0264410X;3.641;Electronic health data, Big data, Fit-for-purpose, Vaccination, Real world evidence;Elsevier BV;;3684.0;Western Europe;1585.0;Q1;21376.0;Vaccine;Advance database characterisation and fit for purpose assessment for multi-country studies on the coverage, benefits and risks of pertussis vaccinations;50113.0;10913.0;1148.0;3184.0;42289.0;journal;article;2020
;Matthew S. Lebo and Limin Hao and Chiao-Feng Lin and Arti Singh;"Biochemistry (medical) (Q2); Clinical Biochemistry (Q3)";149.0;161.0;United Kingdom;1981-2020;10.1016/j.cll.2020.02.003;0.00186;55.0;;02722712;15579832;02722712;15579832;1.935;Genome sequencing, Exome sequencing, Alignment, Variant calling, Annotation, Filtration, Validation, Bioinformatic infrastructure;W.B. Saunders Ltd;;4063.0;Western Europe;690.0;Q2;26792.0;Clinics in laboratory medicine;Bioinformatics in clinical genomic sequencing;1772.0;398.0;52.0;173.0;2113.0;journal;article;2020
By analyzing the dynamic behavior of institutional and retail investors in the Indonesia Stock Exchange using their completed transactions (comprising over 250 million observations), this study highlights that their trading strategies and behavior, in which institutions play a more important role than individuals in the market, are indeed different. Specifically, past trading activities by individual (institutional) investors have significantly affected the current trading behaviors and strategies of individual investors (both investor types). Furthermore, retail (institutional) investors are most likely to perform contrarian (momentum) strategies and trade frequently (infrequently) with small (large) amounts of money and short (long) holding periods.;Deddy P. Koesrindartoto and Aurelius Aaron and Inka Yusgiantoro and Wirata A. Dharma and Abdurrohman Arroisi;"Business, Management and Accounting (miscellaneous) (Q1); Finance (Q2)";529.0;439.0;Netherlands;2004-2021;10.1016/j.ribaf.2019.101061;0.00412;42.0;;02755319;02755319;02755319;02755319;4.091;Market microstructure, Emerging market, Institutional investors, Individual investors, Trading strategies;Elsevier BV;;6051.0;Western Europe;767.0;Q1;12056.0;Research in international business and finance;Who moves the stock market in an emerging country – institutional or retail investors?;3698.0;2127.0;202.0;532.0;12223.0;journal;article;2020
Polycystic Ovary Syndrome (PCOS) is the most common endocrine disorder amongst women of reproductive age, whose aetiology remains unclear. To improve our understanding of the molecular mechanisms underlying the disease, we conducted a genome-wide DNA methylation profiling in granulosa lutein cells collected from 16 women suffering from PCOS, in comparison to 16 healthy controls. Samples were collected by follicular aspiration during routine egg collection for IVF treatment. Study groups were matched for age and BMI, did not suffer from other disease and were not taking confounding medication. Comparing women with polycystic versus normal ovarian morphology, after correcting for multiple comparisons, we identified 106 differentially methylated CpG sites with p-values <5.8 × 10−8 that were associated with 88 genes, several of which are known to relate either to PCOS or to ovarian function. Replication and validation of the experiment was done using pyrosequencing to analyse six of the identified differentially methylated sites. Pathway analysis indicated potential disruption in canonical pathways and gene networks that are, amongst other, associated with cancer, cardiogenesis, Hedgehog signalling and immune response. In conclusion, these novel findings indicate that women with PCOS display epigenetic changes in ovarian granulosa cells that may be associated with the heterogeneity of the disorder.;E. Makrinou and A.W. Drong and G. Christopoulos and A. Lerner and I. Chapa-Chorda and T. Karaderi and S. Lavery and K. Hardy and C.M. Lindgren and S. Franks;"Biochemistry (Q1); Endocrinology (Q1); Molecular Biology (Q2)";858.0;376.0;Ireland;1974-2020;10.1016/j.mce.2019.110611;0.01564;144.0;;03037207;03037207;18728057;03037207;4.102;PCOS, EWAS, DNA methylation, Metabolic syndrome, Reproduction;Elsevier Ireland Ltd;;8065.0;Western Europe;1296.0;Q1;26206.0;Molecular and cellular endocrinology;Genome-wide methylation profiling in granulosa lutein cells of women with polycystic ovary syndrome (pcos);18715.0;3543.0;293.0;885.0;23629.0;journal;article;2020
;;Ecological Modeling (Q2);814.0;293.0;Netherlands;1975-2020;10.1016/j.ecolmodel.2020.109257;0.01125;156.0;;03043800;18727026;03043800;18727026;2.974;;Elsevier;;6467.0;Western Europe;876.0;Q2;23274.0;Ecological modelling;Transitioning machine learning from theory to practice in natural resources management;22498.0;2539.0;283.0;826.0;18301.0;journal;article;2020
"In its traditional definition, a repair of an inconsistent database is a consistent database that differs from the inconsistent one in a “minimal way.” Often, repairs are not equally legitimate, as it is desired to prefer one over another; for example, one fact is regarded more reliable than another, or a more recent fact should be preferred to an earlier one. Motivated by these considerations, researchers have introduced and investigated the framework of preferred repairs, in the context of denial constraints and subset repairs. There, a priority relation between facts is lifted towards a priority relation between consistent databases, and repairs are restricted to the ones that are optimal in the lifted sense. Three notions of lifting (and preferred repairs) have been proposed: Pareto, global, and completion. In this article, we investigate the complexity of three problems on preferred repairs. The first is the problem of deciding whether the priority relation contains enough information to clean the database unambiguously, or in other words, whether there is exactly one preferred repair. We show that the different lifting semantics entail highly different complexities for this problem. Then, we study the ability to quantify ambiguity, by investigating two classes of problems. The first is that of counting the preferred repairs. We establish a dichotomy in data complexity for the entire space of (sets of) functional dependencies for all three notions. The second class of problems is that of enumerating (i.e., generating) the preferred repairs. We devise enumeration algorithms with efficiency guarantees on the delay between generated repairs, even for constraints represented as general conflict graphs or hypergraphs.";Benny Kimelfeld and Ester Livshits and Liat Peterfreund;"Computer Science (miscellaneous) (Q2); Theoretical Computer Science (Q3)";1144.0;150.0;Netherlands;1975-2020;10.1016/j.tcs.2020.05.016;0.01046;122.0;;03043975;03043975;03043975;03043975;0.827;Inconsistent databases, Repair, Subset repair, Preferred repair, Categoricity, Repair counting, Repair enumeration, Functional dependencies, Conflict hypergraph;Elsevier;;2768.0;Western Europe;464.0;Q2;20571.0;Theoretical computer science;Counting and enumerating preferred database repairs;8571.0;1839.0;587.0;1196.0;16251.0;journal;article;2020
This paper proposes an integrative approach to feature (input and output) selection in Data Envelopment Analysis (DEA). The DEA model is enriched with zero-one decision variables modelling the selection of features, yielding a Mixed Integer Linear Programming formulation. This single-model approach can handle different objective functions as well as constraints to incorporate desirable properties from the real-world application. Our approach is illustrated on the benchmarking of electricity Distribution System Operators (DSOs). The numerical results highlight the advantages of our single-model approach provide to the user, in terms of making the choice of the number of features, as well as modeling their costs and their nature.;Sandra Benítez-Peña and Peter Bogetoft and Dolores {Romero Morales};"Information Systems and Management (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1)";361.0;755.0;United Kingdom;1973-2020;10.1016/j.omega.2019.05.004;;142.0;;03050483;03050483;03050483;03050483;;Benchmarking, Data Envelopment Analysis, Feature Selection, Mixed Integer Linear Programming;Elsevier BV;;5155.0;Western Europe;2500.0;Q1;21915.0;Omega;Feature selection in data envelopment analysis: a mathematical optimization approach;;2965.0;196.0;363.0;10104.0;journal;article;2020
"Introduction
Recent advances in burn care have resulted in the transition of care from inpatient to outpatient. There is a growing appreciation that with improved survival, meaningful markers of quality need to include recovery of form, function, and reconstruction. Capture of the data describing care delivered in the outpatient setting is being missed.
Methods
Development of our outpatient database included providers, registrar, program manager, and outpatient nursing staff. Data points were included if they described the population, and epidemiology of our patients, were useful for programmatic changes and improvements as well as anticipated research focus areas.
Results
The database platform chosen was Midas+™ because it was in use by hospital quality and integrated with the electronic medical record. Fields were customized based on changing program needs and are updated for new programs or outcomes measures. Reports can be easily built and both outpatients and inpatients are included. This allows for longitudinal tracking of burn patients. Ongoing additions to original data points include variables to track outcomes related to laser therapy for scar management, time to custom garment donning, and to track functional outcomes. Epidemiologic data collected is used to target high-risk populations for prevention and outreach efforts. Outcome data is used for evaluation of programs and care.
Conclusions
High quality databases serve to measure effectiveness of care and offer insight for areas of improvement. There is a clear need for inclusion of outpatient activity in the National Burn Registry (NBR).";Rebecca Coffey and Rachel Penny and Larry Jones and J. Kevin Bailey;"Critical Care and Intensive Care Medicine (Q1); Emergency Medicine (Q1); Surgery (Q1); Medicine (miscellaneous) (Q2)";660.0;210.0;United Kingdom;1974-2020;10.1016/j.burns.2019.10.020;0.00766;101.0;;03054179;03054179;18791409;03054179;2.744;Outpatient registry, Database outcomes;Elsevier Ltd.;;2879.0;Western Europe;901.0;Q1;29574.0;Burns;One center’s experience developing a burn outpatient registry;10973.0;1893.0;367.0;836.0;10566.0;journal;article;2020
Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.;Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline;"Computer Science Applications (Q1); Information Systems (Q1); Library and Information Sciences (Q1); Management Science and Operations Research (Q1); Media Technology (Q1)";298.0;801.0;United Kingdom;1975-2020;10.1016/j.ipm.2019.102122;;101.0;;03064573;18735371;03064573;18735371;;Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics;Elsevier Ltd.;;5841.0;Western Europe;1061.0;Q1;12689.0;Information processing and management;Towards a real-time processing framework based on improved distributed recurrent neural network variants with fasttext for social big data analytics;;2449.0;250.0;303.0;14602.0;journal;article;2020
Compulsivity is recognized as a transdiagnostic phenotype, underlying a variety of addictive and obsessive–compulsive behaviors. However, current understanding of how it should be operationalized and the processes contributing to its development and maintenance is limited. The present study investigated if there was a relationship between the affective process Experiential Avoidance (EA), an unwillingness to tolerate negative internal experiences, and the frequency and severity of transdiagnostic compulsive behaviors. A large sample of adults (N = 469) completed online questionnaires measuring EA, psychological distress and the severity of seven obsessive–compulsive and addiction-related behaviors. Using structural equation modelling, results indicated a one-factor model of compulsivity was superior to the two-factor model (addictive- vs OCD-related behaviors). The effect of EA on compulsivity was fully mediated by psychological distress, which in turn had a strong direct effect on compulsivity. This suggests distress is a key mechanism in explaining why people with high EA are more prone to compulsive behaviors. The final model explained 41% of the variance in compulsivity, underscoring the importance of these constructs as likely risk and maintenance factors for compulsive behavior. Implications for designing effective psychological interventions for compulsivity are discussed.;Lauren {Den Ouden} and Jeggan Tiego and Rico S.C. Lee and Lucy Albertella and Lisa-Marie Greenwood and Leonardo Fontenelle and Murat Yücel and Rebecca Segrave;"Clinical Psychology (Q1); Medicine (miscellaneous) (Q1); Psychiatry and Mental Health (Q1); Toxicology (Q1)";1148.0;356.0;United Kingdom;1975-2021;10.1016/j.addbeh.2020.106464;0.02223;127.0;;03064603;03064603;03064603;03064603;3.913;Addiction, Obsessive–compulsive disorder, Gambling, Compulsive buying, Binge-eating, Experiential Avoidance;Elsevier Ltd.;;4711.0;Western Europe;1520.0;Q1;24763.0;Addictive behaviors;The role of experiential avoidance in transdiagnostic compulsive behavior: a structural model analysis;17650.0;4787.0;345.0;1180.0;16254.0;journal;article;2020
One of the most relevant inputs for hydrological modeling is the soil map. The soil sources and scales for the soil properties are diverse, and the quality of soil mapping is increasing, but soil surveying is time-consuming and large area campaigns are expensive. The taxonomic unit approach for soil mapping is common and limited to one layer of data. This limitation causes errors in simulated water fluxes through the soil when taxonomic units approach is implemented during hydrological modeling analysis. Some strategies using geostatistics and machine learning algorithms such as Kriging and Self-Organizing maps (SOM) are improving the taxonomic units’ approach and could serve as an alternative for soil mapping for hydrological purposes. The aim of this work is to study the influence of different soil maps and resolutions on the main hydrological components of a sub-arid watershed in central Spain. For this, the Soil Water and Assessment Tool (SWAT) was parameterized with three different soil maps. A first one was based on Harmonized World Soil database from FAO, at scale 1:1,000,000 (HWSD). The other two were based on a Kriging interpolation at 100 × 100 m from soil samples. To obtain soil properties map from it, two strategies were applied: one was to average the soil properties following the official taxonomic soil units at 1:400,000 scale (Agricultural Technological Institute of Castilla and Leon - ITACyL) and the other was to applied Self-organizing map (SOM) to create the soil units (SOMM). The results suggest that scale and soil properties mapping influence HRU definition, which in turn affects water flow through the soils. Statistical metrics of model performance were improved from R2 =0.62 and NSE=0.46 with HWSD soil map to R2 =0.86 and NSE=0.84 with SOM and similar values were achieved during validation. Thus, the SOM is presented as an innovative algorithm applied for hydrological modeling with SWAT, significantly increasing the level of model accuracy to stream flow in sub-arid watersheds.;David Rivas-Tabares and Ángel {de Miguel} and Bárbara Willaarts and Ana M. Tarquis;"Applied Mathematics (Q1); Modeling and Simulation (Q1)";1602.0;540.0;United States;1976-2021;10.1016/j.apm.2020.06.044;0.02616;112.0;;0307904X;0307904X;0307904X;0307904X;5.129;Soil properties, Self-organizing Maps, Hydrological modeling, SWAT, Soils spatial patterns, SOM;Elsevier Inc.;;4323.0;Northern America;1011.0;Q1;28065.0;Applied mathematical modelling;Self-organizing map of soil properties in the context of hydrological modeling;24972.0;7948.0;578.0;1606.0;24985.0;journal;article;2020
Saturation pressure is a vital parameter of oil reservoir which can reflect the oilfield characteristics and determine the oilfield development process, and it is determined by experiments in the laboratory in general. However, there was only one well with saturation pressure test in this target reservoir, and it is necessary to determine whether this parameter is right or not. In this work, we present a new method for quickly determining saturation pressure using machine learning algorithms, including random forest regressor (RF), support vector machine (SVM), decision trees (DT), and artificial neural network (ANN or NN). Using these approaches, saturation pressure was obtained by using the initial solution gas-oil ratio (GOR), temperature, API gravity and other reservoir-fluid data available in the oilfields. Compared with the empirical formula for saturation pressure calculation, the calculated result shows that the accuracy given from machine learning is higher than that from other formulas at home and abroad, and has a good match with the lab test. On the basis of the calculated saturation pressure, it can determine whether the reservoir enters into the stage of dissolved gas drive or not, which also provides the basis for maintaining the reservoir pressure by water injection in advance, rational development decision-making and work over measures. This approach above can provide technical guidance for predicting the saturation pressure in the development of different kinds of reservoirs, including the sandstone reservoirs and carbonate reservoirs.;Guoyi Yu and Feng Xu and Yingzhi Cui and Xiangling Li and Chujuan Kang and Cheng Lu and Siyu Li and Lin Bai and Shuheng Du;"Condensed Matter Physics (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Renewable Energy, Sustainability and the Environment (Q1)";8065.0;575.0;United Kingdom;1976-2020;10.1016/j.ijhydene.2020.08.042;0.08541;215.0;;03603199;03603199;03603199;03603199;5.816;Oil reservoir, Saturation pressure, Random forest, Decision tree, ANN, Empirical formula;Elsevier Ltd.;;5409.0;Western Europe;1212.0;Q1;26991.0;International journal of hydrogen energy;A new method of predicting the saturation pressure of oil reservoir and its application;120051.0;45807.0;3124.0;8134.0;168963.0;journal;article;2020
Biological entities are involved in intricate and complex interactions, in which uncovering the biological information from the network concepts are of great significance. Benefiting from the advances of network science and high-throughput biomedical technologies, studying the biological systems from network biology has attracted much attention in recent years, and networks have long been central to our understanding of biological systems, in the form of linkage maps among genotypes, phenotypes, and the corresponding environmental factors. In this review, we summarize the recent developments of computational network biology, first introducing various types of biological networks and network structural properties. We then review the network-based approaches, ranging from some network metrics to the complicated machine-learning methods, and emphasize how to use these algorithms to gain new biological insights. Furthermore, we highlight the application in neuroscience, human disease, and drug developments from the perspectives of network science, and we discuss some major challenges and future directions. We hope that this review will draw increasing interdisciplinary attention from physicists, computer scientists, and biologists.;Chuang Liu and Yifang Ma and Jing Zhao and Ruth Nussinov and Yi-Cheng Zhang and Feixiong Cheng and Zi-Ke Zhang;Physics and Astronomy (miscellaneous) (Q1);147.0;3602.0;Netherlands;1971-2020;10.1016/j.physrep.2019.12.004;;297.0;;03701573;03701573;03701573;03701573;;Complex networks, Network biology, Disease module, Machine learning;Elsevier;;40140.0;Western Europe;6914.0;Q1;29229.0;Physics reports;Computational network biology: data, models, and applications;;5960.0;53.0;147.0;21274.0;journal;article;2020
This study reviews soil water balance (SWB) model approaches to determine crop irrigation requirements and scheduling irrigation adopting the FAO56 method. The Kc-ETo approach is discussed with consideration of baseline concepts namely standard vs. actual Kc concepts, as well as single and dual Kc approaches. Requirements for accurate SWB and appropriate parameterization and calibration are introduced. The one-step vs. the two-step computational approaches is discussed before the review of the FAO56 method to compute and partition crop evapotranspiration and related soil water balance. A brief review on transient state models is also included. Baseline information is concluded with a discussion on yields prediction and performance indicators related to water productivity. The study is continued with an overview on models development and use after publication of FAO24, essentially single Kc models, followed by a review on models following FAO56, particularly adopting the dual Kc approach. Features of dual Kc modeling approaches are analyzed through a few applications of the SWB model SIMDualKc, mainly for derivation of basal and single Kc, extending the basal Kc approach to relay intercrop cultivation, assessing alternative planting dates, determining beneficial and non-beneficial uses of water by an irrigated crop, and assessing the groundwater contribution to crop ET in the presence of a shallow water table. The review finally discusses the challenges placed to SWB modeling for real time irrigation scheduling, particularly the new modeling approaches for large scale multi-users application, use of cloud computing and adopting the internet of things (IoT), as well as an improved wireless association of modeling with soil and plant sensors. Further challenges refer to the use of remote sensing energy balance and vegetation indices to map Kc, ET and crop water and irrigation requirements. Trends are expected to change research issues relative to SWB modeling, with traditional models mainly used for research while new, fast-responding and multi-users models based on cloud and IoT technologies will develop into applications to the farm practice. Likely, the Kc-ETo will continue to be used, with ETo from gridded networks, re-analysis and other sources, and Kc data available in real time from large databases and remote sensing.;L.S. Pereira and P. Paredes and N. Jovanovic;"Agronomy and Crop Science (Q1); Earth-Surface Processes (Q1); Soil Science (Q1); Water Science and Technology (Q1)";1201.0;472.0;Netherlands;1976-1977, 1979-2021;10.1016/j.agwat.2020.106357;0.01577;128.0;;03783774;03783774;03783774;03783774;4.516;Crop coefficients, Crop evapotranspiration, Dual K approach, Real time irrigation management, Water use assessment, SIMDualKc model;Elsevier;;5822.0;Western Europe;1493.0;Q1;35824.0;Agricultural water management;Soil water balance models for determining crop water and irrigation requirements and irrigation scheduling focusing on the fao56 method and the dual kc approach;22091.0;6257.0;429.0;1207.0;24976.0;journal;article;2020
Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.;João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma;"Agronomy and Crop Science (Q1); Soil Science (Q1)";795.0;539.0;Netherlands;1978-2020;10.1016/j.fcr.2020.107828;0.01607;150.0;;03784290;18726852;03784290;18726852;5.224;Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands;Elsevier;;5586.0;Western Europe;1951.0;Q1;78796.0;Field crops research;Can big data explain yield variability and water productivity in intensive cropping systems?;24118.0;4752.0;191.0;798.0;10669.0;journal;article;2020
The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.;Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie;"Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)";246.0;894.0;Netherlands;1977-2020;10.1016/j.im.2020.103361;;162.0;;03787206;03787206;03787206;03787206;;Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS;Elsevier;;8758.0;Western Europe;2147.0;Q1;12303.0;Information and management;The role of information governance in big data analytics driven innovation;;2482.0;130.0;248.0;11385.0;journal;article;2020
This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.;Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa;"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1)";1158.0;425.0;Netherlands;1977-2021;10.1016/j.epsr.2020.106788;0.01287;122.0;;03787796;03787796;03787796;03787796;3.414;Electricity grids, Analytics, Big data, Decision-making;Elsevier BV;;3258.0;Western Europe;845.0;Q1;16044.0;Electric power systems research;Big data analytics for future electricity grids;13115.0;5369.0;643.0;1165.0;20949.0;journal;article;2020
Artificial intelligence (AI) will continue to cause substantial changes within the field of radiology, and it will become increasingly important for clinicians to be familiar with several concepts behind AI algorithms in order to effectively guide their clinical implementation. This review aims to give medical professionals the basic information needed to understand AI development and research. The general concepts behind several AI algorithms, including their data requirements, training, and evaluation methods are explained. The potential legal implications of using AI algorithms in clinical practice are also discussed.;Marly {van Assen} and Scott J. Lee and Carlo N. {De Cecco};"Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1)";1082.0;325.0;Ireland;1981-2020;10.1016/j.ejrad.2020.109083;0.01605;115.0;;0720048X;18727727;0720048X;18727727;3.528;Artificial intelligence, Cardiac, Chest;Elsevier Ireland Ltd;;3029.0;Western Europe;1025.0;Q1;16677.0;European journal of radiology;Artificial intelligence from a to z: from neural network to legal framework;16452.0;3680.0;518.0;1115.0;15691.0;journal;article;2020
Advanced manufacturing and 3D printing are transformative technologies currently undergoing rapid adoption in healthcare, a traditionally non-manufacturing sector. Recent development in this field, largely enabled by merging different disciplines, has led to important clinical applications from anatomical models to regenerative bioscaffolding and devices. Although much research to-date has focussed on materials, designs, processes, and products, little attention has been given to the design and requirements of facilities for enabling clinically relevant biofabrication solutions. These facilities are critical to overcoming the major hurdles to clinical translation, including solving important issues such as reproducibility, quality control, regulations, and commercialization. To improve process uniformity and ensure consistent development and production, large-scale manufacturing of engineered tissues and organs will require standardized facilities, equipment, qualification processes, automation, and information systems. This review presents current and forward-thinking guidelines to help design biofabrication laboratories engaged in engineering model and tissue constructs for therapeutic and non-therapeutic applications.;Henry W. Sanicola and Caleb E. Stewart and Michael Mueller and Farzad Ahmadi and Dadong Wang and Sean K. Powell and Korak Sarkar and Kenneth Cutbush and Maria A. Woodruff and David A. Brafman;"Applied Microbiology and Biotechnology (Q1); Bioengineering (Q1); Biotechnology (Q1)";321.0;1368.0;United States;1983-2020;10.1016/j.biotechadv.2020.107652;0.017;191.0;;07349750;18731899;07349750;18731899;14.227;Bioprinting, Biofabrication, Tissue engineering, Cloud manufacturing, Deep learning;Elsevier Inc.;;19805.0;Northern America;2772.0;Q1;15461.0;Biotechnology advances;Guidelines for establishing a 3-d printing biofabrication laboratory;23792.0;4598.0;130.0;326.0;25746.0;journal;article;2020
;Jessica Spence and C. David Mazer;Anesthesiology and Pain Medicine (Q3);34.0;116.0;United States;2003-2020;10.1016/j.aan.2020.09.003;;7.0;;07376146;07376146;07376146;07376146;;Cardiothoracic anesthesia, Research, Trial design, Future;Academic Press Inc.;;6433.0;Northern America;371.0;Q3;21401.0;Advances in anesthesia;The future directions of research in cardiac anesthesiology;;49.0;15.0;37.0;965.0;book series;article;2020
;Lee Squitieri and Kevin C. Chung;"Orthopedics and Sports Medicine (Q2); Surgery (Q2)";181.0;152.0;United Kingdom;1985-2020;10.1016/j.hcl.2020.01.011;0.00176;55.0;;07490712;15581969;07490712;15581969;1.907;Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis;W.B. Saunders Ltd;;3727.0;Western Europe;742.0;Q2;29770.0;Hand clinics;Deriving evidence from secondary data in hand surgery: strengths, limitations, and future directions;2329.0;332.0;67.0;207.0;2497.0;journal;article;2020
"ABSTRACT
Objectives
Predictive risk models are advocated in psychosocial oncology practice to provide timely and appropriate support to those likely to experience the emotional and psychological consequences of cancer and its treatments. New digital technologies mean that large scale and routine data collection are becoming part of everyday clinical practice. Using these data to try to identify those at greatest risk for late psychosocial effects of cancer is an attractive proposition in a climate of unmet need and limited resource. In this paper, we present a framework to support the development of high-quality predictive risk models in psychosocial and supportive oncology. The aim is to provide awareness and increase accessibility of best practice literature to support researchers in psychosocial and supportive care to undertake a structured evidence-based approach.
Data Sources
Statistical prediction risk model publications.
Conclusion
In statistical modeling and data science different approaches are needed if the goal is to predict rather than explain. The deployment of a poorly developed and tested predictive risk model has the potential to do great harm. Recommendations for best practice to develop predictive risk models have been developed but there appears to be little application within psychosocial and supportive oncology care.
Implications for Nursing Practice
Use of best practice evidence will ensure the development and validation of predictive models that are robust as these are currently lacking. These models have the potential to enhance supportive oncology care through harnessing routine digital collection of patient-reported outcomes and the targeting of interventions according to risk characteristics.";Jenny Harris and Edward Purssell and Emma Ream and Anne Jones and Jo Armes and Victoria Cornelius;Oncology (nursing) (Q2);178.0;186.0;United Kingdom;1985-2020;10.1016/j.soncn.2020.151089;0.00169;45.0;;07492081;18783449;07492081;18783449;2.315;Predictive risk models, Regression models, Cancer, Psychosocial, Supportive care, Psychological, Distress;W.B. Saunders Ltd;;4950.0;Western Europe;596.0;Q2;29925.0;Seminars in oncology nursing;How to develop statistical predictive risk models in oncology nursing to enhance psychosocial and supportive care;1412.0;396.0;66.0;195.0;3267.0;journal;article;2020
"Background
The volume of unicompartmental knee arthroplasty (UKA) has increased dramatically in recent years with good reported long-term outcomes. UKA can be performed under general or neuraxial (ie, spinal) anesthesia; however, little is known as to whether there is a difference in outcomes based on anesthesia type. The purpose of the present study is to compare perioperative outcomes between anesthesia types for patients undergoing primary elective UKA.
Methods
Patients who underwent primary elective UKA from 2007 to 2017 were identified from the American College of Surgeons-National Surgical Quality Improvement Program Database. Operating room times, length of stay (LOS), 30-day adverse events, and readmission rates were compared between patients who received general anesthesia and those who received spinal anesthesia. Propensity-adjusted multivariate analysis was used to control for selection bias and baseline patient characteristics.
Results
A total of 8639 patients underwent UKA and met the inclusion criteria for this study. Of these, 4728 patients (54.7%) received general anesthesia and 3911 patients (45.3%) received spinal anesthesia. On propensity-adjusted multivariate analyses, general anesthesia was associated with increased operative time (P < .001) and the occurrence of any severe adverse event (odds ratio [OR], 1.39; 95% confidence interval [95% CI], 1.04-1.84; P = .024). In addition, general anesthesia was associated with higher rates of deep venous thrombosis (OR, 2.26; 95% CI, 1.11-4.6; P = .024) and superficial surgical site infection (OR, 1.04; 95% CI, 0.6-1.81; P < .001). Finally, general anesthesia was also associated with a reduced likelihood of discharge to home (OR, 0.72; 95% CI, 0.59-0.88; P < .001). No difference existed in postoperative hospital LOS or readmission rates among cohorts.
Conclusion
General anesthesia was associated with an increased rate of adverse events and increased operating room times as well as a reduced likelihood of discharge to home. There was no difference in hospital LOS or postoperative readmission rates between anesthesia types.";Yining Lu and William M. Cregar and J. Brett Goodloe and Zain Khazi and Brian Forsythe and Tad L. Gerlinger;Orthopedics and Sports Medicine (Q1);1949.0;410.0;United States;1986-2020;10.1016/j.arth.2020.03.012;0.03779;135.0;;08835403;08835403;15328406;08835403;4.757;unicompartmental arthroplasty, general anesthesia, spinal anesthesia, complications, National Surgical Quality Improvement Project, NSQIP;Churchill Livingstone;;2954.0;Northern America;2766.0;Q1;12191.0;Journal of arthroplasty;General anesthesia leads to increased adverse events compared with spinal anesthesia in patients undergoing unicompartmental knee arthroplasty;27716.0;9335.0;810.0;2159.0;23925.0;journal;article;2020
Benchmarking is a common and effective method for measuring and analyzing ICU performance. With the existence of national registries, objective information can now be obtained to allow benchmarking of ICU care within and between countries. The present manuscript briefly describes the current status of benchmarking in healthcare and critical care and presents the LOGIC project, an initiative to promote international benchmarking for intensive care units. Currently 13 registries have joined LOGIC. We showed large differences in the utilization of ICU as well as resources and in outcomes. Despite the need for careful interpretation of differences due to variation in definitions and limited risk adjustment, LOGIC is a growing worldwide initiative that allows access to insightful epidemiologic data from ICUs in multiple databases and registries.;D.A. Dongelmans and David Pilcher and Abigail Beane and Marcio Soares and Maria {del Pilar Arias Lopez} and Ariel Fernandez and Bertrand Guidet and Rashan Haniffa and Jorge I.F. Salluh;Critical Care and Intensive Care Medicine (Q1);847.0;253.0;Netherlands;1986-2020;10.1016/j.jcrc.2020.08.031;0.01662;83.0;;08839441;15578615;08839441;15578615;3.425;Critical care, Improvement science, Benchmarking, Global, Quality, Quality registry;Elsevier BV;;3014.0;Western Europe;1149.0;Q1;12212.0;Journal of critical care;Linking of global intensive care (logic): an international benchmarking in critical care initiative;9746.0;2889.0;300.0;1095.0;9042.0;journal;article;2020
To comprehensively understand the decision information system for the information processing of the intelligent manufacturing under Internet of Things, an intelligent decision support system (DSS) based on data mining technology is applied to enterprises to establish an Internet of Things-based intelligent DSS for manufacturing industry, thereby supporting the decision-makers in making intelligent decisions through the intelligent DSS. The research results show that data mining technology can analyze the statistical data from multiple angles and perspectives by modeling, classifying, and clustering a large amount of data, as well as discovering the correlations between the data. Also, in statistical work, the data are counted, and their correlations are utilized to support the decision analysis. Therefore, it can be concluded that the establishment of intelligent DSS for enterprises in manufacturing industry and the utilization of data mining technology as the key technology to achieve the system can make the decision-making of the manufacturing enterprises more effective and scientific. Eventually, the satisfactory decision-making results can be obtained.;Yuan Guo and Nan Wang and Ze-Yin Xu and Kai Wu;"Aerospace Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Mechanical Engineering (Q1); Signal Processing (Q1)";2017.0;795.0;United States;1987-2021;10.1016/j.ymssp.2020.106630;0.03775;167.0;;08883270;08883270;10961216;08883270;6.823;Intelligent decision support system, Data mining, Decision tree;Elsevier;;4336.0;Northern America;2275.0;Q1;21080.0;Mechanical systems and signal processing;The internet of things-based decision support system for information processing in intelligent manufacturing using data mining technology;30686.0;16068.0;538.0;2031.0;23327.0;journal;article;2020
;Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort;"Cardiology and Cardiovascular Medicine (Q2); Medicine (miscellaneous) (Q2); Surgery (Q2)";1527.0;137.0;United States;1986-2020;10.1016/j.avsg.2020.04.022;0.00916;74.0;;08905096;16155947;08905096;16155947;1.466;;Elsevier Inc.;;2003.0;Northern America;635.0;Q2;20492.0;Annals of vascular surgery;Artificial intelligence in vascular surgery: moving from big data to smart data;7121.0;2242.0;844.0;1561.0;16904.0;journal;article;2020
Advancements in modern mineral processing has been driven by technology and fuelled by market economics of supply and demand. Over the last three decades, the demand for various minerals has steadily increased, while the mineral processing industry has seen an unavoidable increase in the treatment of complex ores, continuous decline in plant feed grade and poor plant performance partly due to blending of ores with dissimilar properties. Despite these challenges, production plant data that are routinely generated are usually underutilised. In this contribution and aligned with the direction of the 4th industrial revolution, we highlight the value of legacy metallurgical plant data and the concept of a dry laboratory approach. This study is presented in two parts. In the current paper (Part I), a comprehensive review of the potential for the combination of modern analytical technology with data analytics to generate a new competence for process optimisation are provided. To demonstrate the value of data within the extractive metallurgy discipline, we employ data analytics and simulation to examine gold plant performance and the flotation process in two separate case studies in the second paper (Part II). This was done with the aim of showcasing relevant plant data insights, and extract parameters that should be targeted for plant design and performance optimisation. We identify several promising technologies that integrate well with existing mineral processing plants and testing laboratories to exploit the concept of a dry laboratory, in order to enhance pre-existing mineral processing chains. It also sets the passage in terms of the value of innovative analysis of existing and simulation data as part of the new world of data analytics. Using data- and technology-driven initiatives, we propose the establishment of dry laboratories and data banks to ultimately leverage integrated data, analytics and process simulation for effective plant design and improved performance.;Yousef Ghorbani and Glen T. Nwaila and Steven E. Zhang and Martyn P. Hay and Lunga C. Bam and Pratama Istiadi Guntoro;"Chemistry (miscellaneous) (Q1); Control and Systems Engineering (Q1); Geotechnical Engineering and Engineering Geology (Q1); Mechanical Engineering (Q1)";954.0;481.0;United Kingdom;1988-2020;10.1016/j.mineng.2020.106646;0.01028;108.0;;08926875;08926875;08926875;08926875;4.765;Data analytics, Dry laboratories, Data bank, Legacy metallurgical data;Elsevier Ltd.;;4011.0;Western Europe;1092.0;Q1;16936.0;Minerals engineering;Repurposing legacy metallurgical data part i: a move toward dry laboratories and data bank;18516.0;4768.0;409.0;962.0;16406.0;journal;article;2020
Global warming may increase the frequency of climate extremes, but systematic examinations at different temperature thresholds are unknown over the Tibetan Plateau (TP). Changes in surface temperature and precipitation extreme indices derived from a multi-model ensemble mean (MMEM) of the Coupled Model Inter-comparison Project Phase 5 (CMIP5) models are examined under global warming of 1.5 °C (RCP2.6), 2 °C (RCP4.5) and 3 °C (RCP8.5) above pre-industrial levels. The TP amplification of future temperature and precipitation changes is evident for all three scenarios, with greater trend magnitudes in extreme indices than those for the whole China, regions between 25°N and 40°N, Northern Hemisphere (land only), Northern Hemisphere and the global mean. The TP amplification is also projected to intensify in each scenario, resulting in faster changes in intensity, duration and frequency of climate extremes. There appears to be greater difference for precipitation-based indices between 2 °C and 3 °C than for temperature, and the differences between 1.5 °C and 2 °C are less dramatic. Overall changes in climate extremes at 2 °C are greater than at 1.5 °C, but differences are less discernible between 3 °C and 2 °C. The Kolmogorov-Smirnov test between simulated and scaled temperature distributions shows that accelerated warming over the TP from 1.5 °C to 2 °C follows a broadly linear response, but the nonlinearity occurs between 2 °C and 3 °C. This suggests that the rate of warming might make a large difference to the future TP amplification at different thresholds.;Qinglong You and Fangying Wu and Liucheng Shen and Nick Pepin and Zhihong Jiang and Shichang Kang;"Global and Planetary Change (Q1); Oceanography (Q1)";506.0;490.0;Netherlands;1989-2020;10.1016/j.gloplacha.2020.103261;0.01393;133.0;;09218181;09218181;09218181;09218181;5.114;Tibetan Plateau, CMIP5, 1.5 °C, 2 °C and 3 °C, Linearity analysis;Elsevier;;9203.0;Western Europe;1706.0;Q1;27049.0;Global and planetary change;Tibetan plateau amplification of climate extremes under global warming of 1.5 °c, 2 °c and 3 °c;13351.0;2726.0;204.0;518.0;18775.0;journal;article;2020
"Glucocorticoids reduce phobic fear in anxiety disorders and enhance psychotherapy, possibly by reducing the retrieval of fear memories and enhancing the consolidation of new corrective memories. Glucocorticoid signaling in the basolateral amygdala can influence connected fear and memory-related cortical regions, but this is not fully understood. Previous studies investigated specific pathways moderated by glucocorticoids, for example, visual-temporal pathways; however, these analyses were limited to a-priori selected regions. Here, we performed whole-brain pattern analysis to localize phobic stimulus decoding related to the fear-reducing effect of glucocorticoids. We reanalyzed functional magnetic resonance imaging (fMRI) data from a previously published study with spider-phobic patients and healthy controls. The patients received glucocorticoids or a placebo before the exposure to spider images. There was moderate evidence that patients with phobia had higher decoding of phobic content in the anterior cingulate cortex (ACC) and the left and right anterior insula compared to controls. Decoding in the ACC and the right insula showed strong evidence for correlation with experienced fear. Patients with cortisol reported a reduction of fear by 10–13%; however, there was only weak evidence for changes in neural decoding compared to placebo which was found in the precuneus, the opercular cortex, and the left cerebellum.";Simon Schwab and Andrea Federspiel and Yosuke Morishima and Masahito Nakataki and Werner Strik and Roland Wiest and Markus Heinrichs and Dominique {de Quervain} and Leila M. Soravia;"Radiology, Nuclear Medicine and Imaging (Q1); Neuroscience (miscellaneous) (Q2); Psychiatry and Mental Health (Q2)";386.0;240.0;Ireland;1990-2020;10.1016/j.pscychresns.2020.111066;;108.0;;09254927;18727506;09254927;18727506;;Phobia, Anxiety disorder, Cortisol, fMRI, Pattern analysis;Elsevier Ireland Ltd;;5717.0;Western Europe;1030.0;Q1;18730.0;Psychiatry research - neuroimaging;Glucocorticoids and cortical decoding in the phobic brain;;1046.0;105.0;389.0;6003.0;journal;article;2020
The diary food factories in Japan are facing serious challenges of severe labor shortage and the increased diversity of demand. Food manufacturing companies are forced to improve factories to be more productive and flexible to deal with the expanding market scale in the future and also the product diversity. To improve the productivity and the flexibility, automation technologies have been implemented in manufacturing system with the popularization of Industrial 4.0 and Smart Factory. Based on the actual system construction practice of a dairy factory which is as a case study, this paper proposes a five-level horizontal model with automation technologies, aiming to realize high efficiency, rapid integration and relocation of the manufacturing system. This paper introduces the composition, the specifications and the functions of the horizontal model, and evaluates the function of each level. Finally, through the case study and numerical comparison on cost and labor hours, we verify the superiority of the proposed horizontal hierarchical system model for food factories.;Takao Matsumoto and Yijun Chen and Akihiro Nakatsuka and Qunzhi Wang;"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)";881.0;831.0;Netherlands;1991-2021;10.1016/j.ijpe.2020.107616;0.0228;185.0;;09255273;09255273;09255273;09255273;7.885;Dairy product, Industry 4.0, Smart factory, Operation technology, Plant control system, Manufacturing execution system;Elsevier;;6640.0;Western Europe;2406.0;Q1;19165.0;International journal of production economics;Research on horizontal system model for food factories: a case study of process cheese manufacturer;32606.0;8124.0;327.0;891.0;21712.0;journal;article;2020
Naturalistic driving studies (NDS) are increasingly being used to investigate driver on-road behavior. In parallel, smartphones are gaining interest as data acquisition systems (DAS) in NDS instead of costly in-vehicle DAS. However, smartphone and in-vehicle DAS differ across several attributes and no current document outlines the implications of using smartphones as DAS in NDS. In this document, we present a comparative review of the advantages and disadvantages of using smartphone and in-vehicle DAS in NDS and discuss their implications. In addition, we present a brief account on prospective technological developments that might have further implications for using smartphones for studying and advancing road safety. Researchers and practitioners can use this review as a general guide to decide which DAS (smartphone or in-vehicle) to use in their NDS. For example, smartphones would be a cost-effective alternative for studying driving style (e.g., braking and speeding), but an inferior alternative to in-vehicle DAS for reconstructing crashes or near crashes and for studying short-term relationships between events (e.g., smartphone usage and hard braking). Researchers and practitioners can also use this review as an aid for the design of NDS with smartphones. For example, we show that it would be advisable to use beacons to know if participants were driving their vehicle or riding the bus, and that data completeness and accuracy would depend on battery charge and using a cradle. Prospective technologies might mitigate the shortcomings that we have outlined and might even dim the distinction between the different types of DAS.;Einat Grimberg and Assaf Botzer and Oren Musicant;"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)";1020.0;549.0;Netherlands;1991-2020;10.1016/j.ssci.2020.104917;0.01488;111.0;;09257535;09257535;09257535;09257535;4.877;;Elsevier;;5826.0;Western Europe;1178.0;Q1;12332.0;Safety science;Smartphones vs. in-vehicle data acquisition systems as tools for naturalistic driving studies: a comparative review;17184.0;5909.0;451.0;1047.0;26276.0;journal;article;2020
Matching-type estimators using the propensity score are the major workhorse in active labour market policy evaluation. This work investigates if machine learning algorithms for estimating the propensity score lead to more credible estimation of average treatment effects on the treated using a radius matching framework. Considering two popular methods, the results are ambiguous: We find that using LASSO based logit models to estimate the propensity score delivers more credible results than conventional methods in small and medium sized high dimensional datasets. However, the usage of Random Forests to estimate the propensity score may lead to a deterioration of the performance in situations with a low treatment share. The application reveals a positive effect of the training programme on days in employment for long-term unemployed. While the choice of the “first stage” is highly relevant for settings with low number of observations and few treated, machine learning and conventional estimation becomes more similar in larger samples and higher treatment shares.;Daniel Goller and Michael Lechner and Andreas Moczall and Joachim Wolff;"Economics and Econometrics (Q1); Organizational Behavior and Human Resource Management (Q1)";267.0;175.0;Netherlands;1993-2020;10.1016/j.labeco.2020.101855;0.00733;75.0;;09275371;09275371;09275371;09275371;1.772;Programme evaluation, active labour market policy, causal machine learning, treatment effects, radius matching, propensity score;Elsevier;;4949.0;Western Europe;1899.0;Q1;16922.0;Labour economics;Does the estimation of the propensity score by machine learning improve matching estimation? the case of germany's programmes for long term unemployed;4051.0;592.0;116.0;270.0;5741.0;journal;article;2020
OrBiTo was a precompetitive collaboration focused on the development of the next generation of Oral Biopharmaceutics Tools. The consortium included world leading scientists from nine universities, one regulatory agency, one non-profit research organisation, three small/medium sized specialist technology companies together with thirteen pharmaceutical companies. The goal of the OrBiTo project was to deliver a framework for rational application of predictive biopharmaceutics tools for oral drug delivery. This goal was achieved through novel prospective investigations to define new methodologies or refinement of existing tools. Extensive validation has been performed of novel and existing biopharmaceutics tools using historical datasets supplied by industry partners as well as laboratory ring studies. A combination of high quality in vitro and in vivo characterizations of active drugs and formulations have been integrated into physiologically based in silico biopharmaceutics models capturing the full complexity of gastrointestinal drug absorption and some of the best practices has been highlighted. This approach has given an unparalleled opportunity to deliver transformational change in European industrial research and development towards model based pharmaceutical product development in accordance with the vision of model-informed drug development.;B. Abrahamsson and M. McAllister and P. Augustijns and P. Zane and J. Butler and R. Holm and P. Langguth and A. Lindahl and A. Müllertz and X. Pepin and A. Rostami-Hodjegan and E. Sjögren and M. Berntsson and H. Lennernäs;"Biotechnology (Q1); Medicine (miscellaneous) (Q1); Pharmaceutical Science (Q1)";844.0;535.0;Netherlands;1991-2020;10.1016/j.ejpb.2020.05.008;0.01324;158.0;;09396411;18733441;09396411;18733441;5.571;Biopharmaceutics, PBPK, IVIVC, Dissolution, Drug absorption, Permeability;Elsevier;;5607.0;Western Europe;1103.0;Q1;21332.0;European journal of pharmaceutics and biopharmaceutics;Six years of progress in the oral biopharmaceutics area – a summary from the imi orbito project;20326.0;4755.0;259.0;849.0;14522.0;journal;article;2020
As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.;Leah M. Hamilton and Jacob Lahne;"Food Science (Q1); Nutrition and Dietetics (Q1)";548.0;543.0;United Kingdom;1988-1991, 1993-2021;10.1016/j.foodqual.2020.103926;0.00829;120.0;;09503293;09503293;09503293;09503293;5.565;Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning;Elsevier Ltd.;;5414.0;Western Europe;1135.0;Q1;23161.0;Food quality and preference;Fast and automated sensory analysis: using natural language processing for descriptive lexicon development;13058.0;3000.0;227.0;558.0;12290.0;journal;article;2020
"Background
Online crowdsourcing methods have proved useful for studies of diverse designs in the behavioral and addiction sciences. The remote and online setting of crowdsourcing research may provide easier access to unique participant populations and improved comfort for these participants in sharing sensitive health or behavioral information. To date, few studies have evaluated the use of qualitative research methods on crowdsourcing platforms and even fewer have evaluated the quality of data gathered. The purpose of the present analysis was to document the feasibility and validity of using crowdsourcing techniques for collecting qualitative data among people who use drugs.
Methods
Participants (N = 60) with a history of non-medical prescription opioid use with transition to heroin or fentanyl use were recruited using Amazon Mechanical Turk (mTurk). A battery of qualitative questions was included indexing beliefs and behaviors surrounding opioid use, transition pathways to heroin and/or fentanyl use, and drug-related contacts with structural institutions (e.g., health care, criminal justice).
Results
Qualitative data recruitment was feasible as evidenced by the rapid sampling of a relatively large number of participants from diverse geographic regions. Computerized text analysis indicated high ratings of authenticity for the provided narratives. These authenticity percentiles were higher than the average of general normative writing samples as well as than those collected in experimental settings.
Conclusions
These findings support the feasibility and quality of qualitative data collected in online settings, broadly, and crowdsourced settings, specifically. Future work among people who use drugs may leverage crowdsourcing methods and the access to hard-to-sample populations to complement existing studies in the human laboratory and clinic as well as those using other digital technology methods.";Justin C. Strickland and Grant A. Victor;"Health Policy (Q1); Medicine (miscellaneous) (Q1)";634.0;434.0;Netherlands;1998-2020;10.1016/j.drugpo.2019.10.013;0.01545;79.0;;09553959;18734758;09553959;18734758;5.009;Crowdsourcing, Heroin, Mechanical Turk, Opioid, Qualitative, Mixed method;Elsevier;;5202.0;Western Europe;1649.0;Q1;25290.0;International journal of drug policy;Leveraging crowdsourcing methods to collect qualitative data in addiction science: narratives of non-medical prescription opioid, heroin, and fentanyl use;8075.0;3372.0;322.0;715.0;16752.0;journal;article;2020
Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.;Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang;"Biomedical Engineering (Q1); Biophysics (Q1); Biotechnology (Q1); Electrochemistry (Q1); Medicine (miscellaneous) (Q1); Nanoscience and Nanotechnology (Q1)";2718.0;1020.0;United Kingdom;1990-2020;10.1016/j.bios.2020.112412;;192.0;;09565663;18734235;09565663;18734235;;Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare;Elsevier Ltd.;;5468.0;Western Europe;2546.0;Q1;15437.0;Biosensors and bioelectronics;Artificial intelligence biosensors: challenges and prospects;;27420.0;701.0;2720.0;38330.0;journal;article;2020
Many applications of intelligent systems involve understanding a group of contrastively different outcome (e.g., all survivors of a deadly cancer, a top performing team in a large corporation). The intelligent system needs to identify attributes (features) which best describe or explain the group versus its alternatives. In data mining, this problem is studied under the framework of contrast set mining (CSM). Although CSM is not new, the era of big data has produced new computational and statistical challenges. In particular, existing algorithms fail (1) to perform efficiently in terms of runtime on large-scale datasets and (2) to accommodate simultaneous inference on an overwhelming array of features which are often repetitive and collinear. In this paper, we develop a CSM algorithm which addresses both challenges. The computational challenge is addressed with a tree structure and two theorems while the statistical challenge is addressed with the application of false discovery rate for multiple testing. The computational and statistical advantages of the proposed algorithm over three state-of-the-art algorithms are demonstrated with comprehensive experiments. In addition, we also show the effectiveness of our proposed method in an intelligence-system application involving hospital process redesign. The proposed method not only improves the performance of machine learning systems, but also generates succinct and insightful patterns directly relevant to clinical decision-making.;Dang Nguyen and Wei Luo and Bay Vo and Witold Pedrycz;"Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)";1943.0;867.0;United Kingdom;1990-2021;10.1016/j.eswa.2020.113670;0.04053;207.0;;09574174;09574174;09574174;09574174;6.954;Data mining, Contrast set mining, Classification, False discovery rate, Emergency department, Length of stay (LOS);Elsevier Ltd.;;5495.0;Western Europe;1368.0;Q1;24201.0;Expert systems with applications;Succinct contrast sets via false positive controlling with an application in clinical process redesign;55444.0;17345.0;770.0;1945.0;42314.0;journal;article;2020
As the problem of urban environment and air pollution continues to intensify, the public has developed a very strong perception of the surrounding environment. With this growing perception, their emotions and satisfaction about the environment may also be greatly affected. This article explores the main factors that affect the public’s environmental satisfaction with two main novelties. The first is to develop a data acquisition platform. Based on socially aware computing and a participatory sensing method, the public’s subjective environmental satisfaction is measured by quantifying the collected sensing data. The second is to establish a multiple regression model and a correlation model to analyze the relationship between public’s environmental satisfaction and atmospheric environmental factors, and to compare the influence of each factor. Lanzhou City was taken as a case study, and a total of 33505 data points was acquired over a year of experiment. The experimental results show: (1) Air pollution in Lanzhou affects public environmental satisfaction more strangely than meteorological factors. (2) There is a negative correlation between public environmental satisfaction and different air pollutants in Lanzhou. (3) PM2.5, PM10 and NO2 are the main influencing factors of the public’s environmental satisfaction in Lanzhou. Overall, our study demonstrates a viable method for capturing public environmental satisfaction and identifying probable drivers of it, and our results provide important information for policy makers to prioritize areas that can more effectively improve public satisfaction.;Qiang Zhang and Tianze Gao and Xueyan Liu and Yun Zheng;"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)";10577.0;956.0;United Kingdom;1993-2021;10.1016/j.jclepro.2020.121774;0.18299;200.0;;09596526;09596526;18791786;09596526;9.297;Atmospheric environment, Socially aware computing, Participatory sensing, Analysis of correlation;Elsevier Ltd.;;5940.0;Western Europe;1937.0;Q1;19167.0;Journal of cleaner production;Exploring the influencing factors of public environmental satisfaction based on socially aware computing;170352.0;103295.0;5126.0;10603.0;304498.0;journal;article;2020
Increasing air pollutants significantly affect the proportion of diffuse (Rd) to global (Rs) solar radiation. This study proposed three new hybrid support vector machines (SVM) with particle swarm optimization algorithm (SVM-PSO), bat algorithm (SVM-BAT) and whale optimization algorithm (SVM-WOA) for predicting daily Rd in air-polluted regions. These models were further compared to standalone SVM, multivariate adaptive regression spline (MARS) and extreme gradient boosting (XGBoost) models. The results showed that models with suspended particulate matter with aerodynamic diameter smaller than 2.5 μm and 10 μm (PM2.5 and PM10) and ozone (O3) produced more accurate daily Rd estimates than those without air pollution parameters, with average relative decreases in root mean square deviation (RMSD) of 11.1%, 10.0% and 10.4% for sunshine duration-based, Rs-based and combined models, respectively. SVM showed better accuracy than XGBoost and MARS. However, compared to SVM, SVM-BAT further enhanced the prediction accuracy and convergence rate in daily Rd modeling, followed by SVM-WOA and SVM-PSO, with relative decreases in RMSD of 2.9%–5.6%, 1.9%–4.9% and 1.1%–3.3%, respectively. The results highlighted the significance of incorporating air pollutants for more accurate estimation of daily Rd in air-polluted regions. Heuristic algorithms, especially BAT, are highly recommended for improving performance of standalone machine learning models.;Junliang Fan and Lifeng Wu and Xin Ma and Hanmi Zhou and Fucang Zhang;Renewable Energy, Sustainability and the Environment (Q1);3587.0;839.0;United Kingdom;1991-2021;10.1016/j.renene.2019.07.104;0.06925;191.0;;09601481;09601481;09601481;09601481;8.001;Air pollution, Support vector machines, Extreme gradient boosting, Particle swarm optimization algorithm, Bat algorithm, Whale optimization algorithm;Elsevier BV;;4707.0;Western Europe;1825.0;Q1;27569.0;Renewable energy;Hybrid support vector machines with heuristic algorithms for prediction of daily diffuse solar radiation in air-polluted regions;72390.0;29828.0;2467.0;3605.0;116113.0;journal;article;2020
Embracing the concept of resilience within coastal management marks a step change in thinking, building on the inputs of more traditional risk assessments, and further accounting for capacities to respond, recover and implement contingency measures. Nevertheless, many past resilience assessments have been theoretical and have failed to address the requirements of practitioners. Assessment methods can also be subjective, relying on opinion-based judgements, and can lack empirical validation. Scope exists to address these challenges through drawing on rapidly emerging sources of data and smart analytics. This, alongside the careful selection of the metrics used in assessment of resilience, can facilitate more robust assessment methods. This work sets out to establish a set of core metrics, and data sources suitable for inclusion within a data-driven coastal resilience assessment. A case study region of East Anglia, UK, is focused on, and data types and sources associated with a set of proven assessment metrics were identified. Virtually all risk-specific metrics could be satisfied using available or derived data sources. However, a high percentage of the resilience-specific metrics would still require human input. This indicates that assessment of resilience is inherently more subjective than assessment of risk. Yet resilience assessments incorporate both risk and resilience specific variables. As such it was possible to link 75% of our selected metrics to empirical sources. Through taking a case study approach and discussing a set of requirements outlined by a coastal authority, this paper reveals scope for the incorporation of rapidly progressing data collection, dissemination, and analytical methods, within dynamic coastal resilience assessments. This could facilitate more sustainable evidence-based management of coastal regions.;Alexander G. Rumson and Andres Payo Garcia and Stephen H. Hallett;"Aquatic Science (Q1); Management, Monitoring, Policy and Law (Q1); Oceanography (Q1)";878.0;334.0;United Kingdom;1992-2020;10.1016/j.ocecoaman.2019.105004;;84.0;;09645691;09645691;09645691;09645691;;Coastal management, Resilience metrics, Geospatial data, Open source data, Big data;Elsevier BV;;6915.0;Western Europe;916.0;Q1;28333.0;Ocean and coastal management;The role of data within coastal resilience assessments: an east anglia, uk, case study;;3103.0;281.0;907.0;19431.0;journal;article;2020
In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.;Guangjie Chen and Zhiqiang Ge;"Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1)";611.0;442.0;United Kingdom;1993-2020;10.1016/j.conengprac.2020.104344;0.00717;119.0;;09670661;09670661;09670661;09670661;3.475;Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis;Elsevier Ltd.;;4087.0;Western Europe;1175.0;Q1;18174.0;Control engineering practice;Robust bayesian networks for low-quality data modeling and process monitoring applications;8368.0;2779.0;196.0;615.0;8010.0;journal;article;2020
Customer engagement (CE) is critical for firms to cultivate and improve customer brand experience in the customer journey. However, few studies are available on the effects of customer-based driving forces on CE in a defined brand experience context. Given the multidimensional nature of CE, the interrelationships among CE dimensions and various dimensional effects on the customer–brand relationship, represented by brand intimacy, have not been thoroughly explored. To address these research gaps, this study explores three customer- and context-based forces that drive CE in social media contexts from a consumer’s perspective. CE is operationalized as a second-order construct consisting of three dimensions (i.e., consumption, contribution, and creation) to reflect its multifaceted nature. An online survey was used to collect data. The results suggest that customer-based forces, advice seeking, and self-image expression exert positive influences on behavioral CE dimensions. The effect of a context-based factor, that is, fashion involvement, is salient only when gender difference is integrated. Moreover, the three facets of behavioral CE affect brand intimacy to different extents. Brand intimacy is the most affected by creation followed by consumption. The research findings contribute to the literature on CE and brand intimacy and also offer practical insights on marketing communications and segmentation.;Tien Wang and Fu-Yu Lee;Marketing (Q1);550.0;777.0;United Kingdom;1994-2021;10.1016/j.jretconser.2020.102035;0.00876;89.0;;09696989;09696989;09696989;09696989;7.135;Customer engagement, Social media, Brand intimacy, Advice seeking, Self-image expression, Fashion involvement;Elsevier Ltd.;;8030.0;Western Europe;1568.0;Q1;22992.0;Journal of retailing and consumer services;Examining customer engagement and brand intimacy in social media context;10506.0;4555.0;346.0;555.0;27784.0;journal;article;2020
The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.;Iris Hausladen and Maximilian Schosser;"Law (Q1); Management, Monitoring, Policy and Law (Q1); Strategy and Management (Q1); Transportation (Q1)";349.0;468.0;United Kingdom;1994-1995, 1997-2020;10.1016/j.jairtraman.2019.101721;0.00394;75.0;;09696997;09696997;09696997;09696997;4.134;Maturity model, Network planning, Big data analytics, Airlines, Case study;Elsevier Ltd.;;5195.0;Western Europe;1220.0;Q1;20532.0;Journal of air transport management;Towards a maturity model for big data analytics in airline network planning;4929.0;1612.0;142.0;356.0;7377.0;journal;article;2020
Discovering new materials with excellent performance is a hot issue in the materials genome initiative. Traditional experiments and calculations often waste large amounts of time and money and are also limited by various conditions. Therefore, it is imperative to develop a new method to accelerate the discovery and design of new materials. In recent years, material discovery and design methods using machine learning have attracted much attention from material experts and have made some progress. This review first outlines available materials database and material data analytics tools and then elaborates on the machine learning algorithms used in materials science. Next, the field of application of machine learning in materials science is summarized, focusing on the aspects of structure determination, performance prediction, fingerprint prediction, and new material discovery. Finally, the review points out the problems of data and machine learning in materials science and points to future research. Using machine learning algorithms, the authors hope to achieve amazing results in material discovery and design.;Yingli Liu and Chen Niu and Zhuo Wang and Yong Gan and Yan Zhu and Shuhong Sun and Tao Shen;"Ceramics and Composites (Q1); Materials Chemistry (Q1); Mechanical Engineering (Q1); Mechanics of Materials (Q1); Metals and Alloys (Q1); Polymers and Plastics (Q1)";883.0;814.0;Netherlands;1993-2021;10.1016/j.jmst.2020.01.067;;68.0;;10050302;10050302;10050302;10050302;;Materials genome initiative (MGI), Materials database, Machine learning, Materials properties prediction, Materials design and discovery;Elsevier BV;;5264.0;Western Europe;1743.0;Q1;12330.0;Journal of materials science and technology;Machine learning in materials genome initiative: a review;;6659.0;572.0;890.0;30108.0;journal;article;2020
High-quality data are the foundation to monitor the progress and evaluate the effects of road traffic injury prevention measures. Unfortunately, official road traffic injury statistics delivered by governments worldwide, are often believed somewhat unreliable and invalid. We summarized the reported problems concerning the road traffic injury statistics through systematically searching and reviewing the literature. The problems include absence of regular data, under-reporting, low specificity, distorted cause spectrum of road traffic injury, inconsistency, inaccessibility, and delay of data release. We also explored the mechanisms behind the problematic data and proposed the solutions to the addressed challenges for road traffic statistics.;Fang-Rong Chang and He-Lai Huang and David C. Schwebel and Alan H.S. Chan and Guo-Qing Hu;"Orthopedics and Sports Medicine (Q3); Surgery (Q3)";222.0;156.0;Netherlands;2001-2020;10.1016/j.cjtee.2020.06.001;;26.0;;10081275;10081275;10081275;10081275;;Traffic injury data, Reported problems, Mechanisms behind the data;Elsevier;;2900.0;Western Europe;442.0;Q3;29582.0;Chinese journal of traumatology - english edition;Global road traffic injury statistics: challenges, mechanisms and solutions;;423.0;78.0;227.0;2262.0;journal;article;2020
;Pierre-Yves Cordier and Eliott Gaudray and Edouard Martin and Raphaël Paris and Salah Boussen and Philippe Goutorbe;"Critical Care Nursing (Q1); Emergency Nursing (Q1)";151.0;215.0;Ireland;1992-2020;10.1016/j.aucc.2019.08.001;0.00179;36.0;;10367314;10367314;10367314;10367314;2.737;;Elsevier Ireland Ltd;;3664.0;Western Europe;732.0;Q1;27139.0;Australian critical care;Comprehensive care documentation: a first step not to be missed;1194.0;437.0;117.0;186.0;4287.0;journal;article;2020
This paper presents the first palaeodemographic results of a newly assembled region-wide radiocarbon record of the Arctic regions of northern Norway. The dataset contains a comprehensive collection of radiocarbon dates in the area (N = 1205) and spans the 10,000-year period of hunter-gatherer settlement history from 11500 to 1500 cal BP. Utilizing local, high-resolution palaeoclimate data, the paper performs multi-proxy correlation testing of climate and demographic dynamics, looking for hunter-gatherer responses to climate variability. The paper compares both long-term climate trends and short-term disruptive climate events with the demographic development in the region. The results demonstrate marked demographic fluctuations throughout the period, characterized by a general increase, punctuated by three significant boom and bust-cycles centred on 6000, 3800 and 2200 cal BP, interpreted as instances of climate forcing of human demographic responses. The results strongly suggest the North Cape Current as a primary driver in the local environment and supports the patterns of covariance between coastal climate proxies and the palaeodemographic model. A mechanism of climate forcing mediation through marine trophic webs is proposed as a tentative explanation of the observed demographic fluxes, and a comparison with inter-regional results demonstrate remarkable similarity in demographic trends across mid-Holocene north and west Europe. The results of the north Norwegian radiocarbon record are thus consistent with independent, international efforts, corroborating the existing pan-European results and help further substantiate super-regional climate variability as the primary driver of population dynamics regardless of economic adaptation.;Erlend Kirkeng Jørgensen;Earth-Surface Processes (Q1);1841.0;207.0;United Kingdom;1989-2020;10.1016/j.quaint.2018.05.014;0.02478;106.0;;10406182;10406182;10406182;10406182;2.13;Summed probability distribution (SPD), Palaeodemographic modelling, Human/climate covariation, Northern Norway, Archaeology, Human ecology;Elsevier Ltd.;;8079.0;Western Europe;927.0;Q1;25776.0;Quaternary international;The palaeodemographic and environmental dynamics of prehistoric arctic norway: an overview of human-climate covariation;18778.0;4556.0;656.0;1961.0;53000.0;journal;article;2020
This paper is a review of the literature on fintech and its interaction with banking. Included in fintech are innovations in payment systems (including cryptocurrencies), credit markets (including P2P lending), and insurance, with Blockchain-assisted smart contracts playing a role. The paper provides a definition of fintech, examines some statistics and stylized facts, and then reviews the theoretical and empirical literature. The review is organized around four main research questions. The paper summarizes our knowledge on these questions and concludes with questions for future research.;Anjan V. Thakor;"Economics and Econometrics (Q1); Finance (Q1)";83.0;482.0;United States;1990-2020;10.1016/j.jfi.2019.100833;0.00467;77.0;;10429573;10960473;10429573;10960473;5.179;Fintech, Cryptocurrencies, P2P lending, Banking, Systemic risk;Academic Press Inc.;;4358.0;Northern America;5445.0;Q1;28996.0;Journal of financial intermediation;Fintech and banking: what do we know?;3216.0;436.0;48.0;87.0;2092.0;journal;article;2020
Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.;Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene;"Applied Psychology (Q1); Business and International Management (Q1); Organizational Behavior and Human Resource Management (Q1); Sociology and Political Science (Q1)";142.0;994.0;United States;1990-2020;10.1016/j.leaqua.2020.101426;0.00895;151.0;;10489843;10489843;10489843;10489843;10.517;Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects;Elsevier Inc.;;12072.0;Northern America;4989.0;Q1;21149.0;Leadership quarterly;Determining causal relationships in leadership research using machine learning: the powerful synergy of experiments and data science;14059.0;1576.0;78.0;150.0;9416.0;journal;article;2020
The attacks, faults, and severe communication/system conditions in Mobile Crowd Sensing (MCS) make false data detection a critical problem. Observing the intrinsic low dimensionality of general monitoring data and the sparsity of false data, false data detection can be performed based on the separation of normal data and anomalies. Although the existing separation algorithm based on Direct Robust Matrix Factorization (DRMF) is proven to be effective, requiring iteratively performing Singular Value Decomposition (SVD) for low-rank matrix approximation would result in a prohibitively high accumulated computation cost when the data matrix is large. In this work, we observe the quick false data location feature from our empirical study of DRMF, based on which we propose an intelligent Light weight Low Rank and False Matrix Separation algorithm (LightLRFMS) that can reuse the previous result of the matrix decomposition to deduce the one for the current iteration step. Depending on the type of data corruption, random or successive/mass, we design two versions of LightLRFMS. From a theoretical perspective, we validate that LightLRFMS only requires one round of SVD computation and thus has very low computation cost. We have done extensive experiments using a PM 2.5 air condition trace and a road traffic trace. Our results demonstrate that LightLRFMS can achieve very good false data detection performance with the same highest detection accuracy as DRMF but with up to 20 times faster speed thanks to its lower computation cost.;Li, Xiaocan and Xie, Kun and Wang, Xin and Xie, Gaogang and Xie, Dongliang and Li, Zhenyu and Wen, Jigang and Diao, Zulong and Wang, Tian;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)";652.0;540.0;United States;1993-2020;10.1109/TNET.2020.2982685;;174.0;;10636692;10636692;10636692;10636692;;;Institute of Electrical and Electronics Engineers Inc.;IEEE Press;3740.0;Northern America;1022.0;Q1;27237.0;Ieee/acm transactions on networking;Quick and accurate false data detection in mobile crowd sensing;;3573.0;187.0;653.0;6994.0;journal;article;2020
Firms are increasingly turning towards new-age technologies such as artificial intelligence (AI), the internet of things (IoT), blockchain, and drones, among others, to assist in interacting with their customers. Further, with the prominence of personalization and customer engagement as the go-to customer management strategies, it is essential for firms to understand how to integrate new-age technologies into their existing practices to aid seamlessly in the generation of actionable insights. Towards this end, this study proposes an organizing framework to understand how firms can use digital analytics, within the changing technology landscape, to generate consumer insights. The proposed framework begins by recognizing the forces that are external to the firm then lead to the generation of specific capabilities by the firm. Further, the firms capabilities can lead to the generation of insights for decision-making that can be data-driven and/or analytics-driven. Finally, the proposed framework identifies the creation of value-based outcomes for firms and customers resulting from the insights generated. Additionally, we identify moderators that influence: (a) the impact of external forces on the development of firm capabilities, and (b) the creation of insights and subsequent firm outcomes. This study also identifies questions for future research that combines the inclusion of new-age technologies, generation of strategic insights, and the achievement of established firm outcomes.;Shaphali Gupta and Agata Leszkiewicz and V. Kumar and Tammo Bijmolt and Dmitriy Potapov;"Business and International Management (Q1); Marketing (Q1)";95.0;661.0;United States;1997-2021;10.1016/j.intmar.2020.04.003;0.00291;106.0;;10949968;10949968;15206653;10949968;6.258;Digital analytics, Internet of things, Artificial intelligence, Drones, Blockchain, Firm capabilities;Elsevier Ltd.;;8590.0;Northern America;2605.0;Q1;22928.0;Journal of interactive marketing;Digital analytics: modeling for insights and new methods;5433.0;850.0;31.0;96.0;2663.0;journal;article;2020
The SHEILA framework provides a policy and strategy framework informing the strategic implementation and use of learning analytics. However, as evidenced in several ‘ground-up’ implementations of tools, the institutional preparedness and the governance around use often comes secondary to the policy. In this paper we depart from familiar approaches and evaluate one such example of a tool's development (SRES – Student Relationship Engagement System). SRES' adoption and scaling across two institutions are evaluated using an auto-ethnographic approach scaffolded through the dimensions of the SHEILA framework, focusing on the individual perspectives of the institutional champions who have been central to this journey. This practical approach and the emerging insights may enable other institutions to identify areas of potential improvement and inform senior academic managers about the strategic requirements to scale the approach, accounting for aspects not considered in the initial ‘organic growth’ of the implementation.;Lorenzo Vigentini and Danny Y.T. Liu and Natasha Arthars and Mollie Dollinger;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Education (Q1); E-learning (Q1)";73.0;860.0;United Kingdom;1998-2020;10.1016/j.iheduc.2020.100728;0.00413;90.0;;10967516;10967516;10967516;10967516;7.178;Learning analytics, Adoption framework, Scalability and sustainability of LA, Institutional adoption, Analytic autoethnography;Elsevier BV;;6461.0;Western Europe;3143.0;Q1;16965.0;Internet and higher education;Evaluating the scaling of a la tool through the lens of the sheila framework: a comparison of two cases from tinkerers to institutional adoption;4427.0;740.0;23.0;73.0;1486.0;journal;article;2020
"Summary
The CRISPR-Cas system offers a programmable platform for eukaryotic genome and epigenome editing. The ability to perform targeted genetic and epigenetic perturbations enables researchers to perform a variety of tasks, ranging from investigating questions in basic biology to potentially developing novel therapeutics for the treatment of disease. While CRISPR systems have been engineered to target DNA and RNA with increased precision, efficiency, and flexibility, assays to identify off-target editing are becoming more comprehensive and sensitive. Furthermore, techniques to perform high-throughput genome and epigenome editing can be paired with a variety of readouts and are uncovering important cellular functions and mechanisms. These technological advances drive and are driven by accompanying computational approaches. Here, we briefly present available CRISPR technologies and review key computational advances and considerations for various CRISPR applications. In particular, we focus on the analysis of on- and off-target editing and CRISPR pooled screen data.";Kendell Clement and Jonathan Y. Hsu and Matthew C. Canver and J. Keith Joung and Luca Pinello;"Cell Biology (Q1); Molecular Biology (Q1)";1177.0;1389.0;United States;1997-2020;10.1016/j.molcel.2020.06.012;0.16184;395.0;;10972765;10974164;10972765;10974164;17.97;;Cell Press;;6040.0;Northern America;12615.0;Q1;18606.0;Molecular cell;Technologies and computational analysis strategies for crispr applications;86299.0;17825.0;413.0;1182.0;24946.0;journal;article;2020
The present study evaluates the performance of PNN models for porosity prediction using seismic attributes. Four seismic datasets and more than 20 wells from different sedimentary basins located in Libya, Iraq, Egypt and USA are employed to characterize the effective attributes for porosity prediction. Verification and testing error analysis is adopted for evaluating the prediction performance. Results indicated that the porosity prediction models are primarily dependent to the propagation related attributes with frequency attributes as the most effective parameters in porosity prediction. In addition, the data quality and processing history strongly control the prediction model performance with relatively limited effects to dataset dimensionality (2D versus 3D) and the number of wells utilized in model construction. Such results are important to better understand and evaluate the performance of PNN porosity prediction models using various seismic attributes.;Abdulaziz M. Abdulaziz;"Fuel Technology (Q1); Organic Chemistry (Q1); Process Chemistry and Technology (Q1); Catalysis (Q2); Geochemistry and Petrology (Q2); Renewable Energy, Sustainability and the Environment (Q2)";288.0;525.0;Egypt;2011-2020;10.1016/j.ejpe.2019.12.001;;38.0;;11100621;11100621;20902468;11100621;;Porosity prediction, Seismic attributes, PNN, Al Ghani field, Sooner field, Kifl field, Baltim field;Egyptian Petroleum Research Institute;;3891.0;Africa/Middle East;942.0;Q1;21100819607.0;Egyptian journal of petroleum;The effective seismic attributes in porosity prediction for different rock types: some implications from four case studies;;1586.0;33.0;288.0;1284.0;journal;article;2020
Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.;Haitham Ghallab and Hanan Fahmy and Mona Nasr;"Information Systems (Q1); Computer Science Applications (Q2); Management Science and Operations Research (Q2)";59.0;694.0;Egypt;2010-2020;10.1016/j.eij.2019.12.001;0.00093;34.0;;11108665;11108665;11108665;11108665;3.943;Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs;Faculty of Computers and Information, Cairo University;;3521.0;Africa/Middle East;728.0;Q1;19700182731.0;Egyptian informatics journal;Detection outliers on internet of things using big data technology;820.0;415.0;47.0;59.0;1655.0;journal;article;2020
"Background
Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID).
Objectives
We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID.
Sources
References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019.
Content
We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%).
Implications
Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.";N. Peiffer-Smadja and T.M. Rawson and R. Ahmad and A. Buchard and P. Georgiou and F.-X. Lescure and G. Birgand and A.H. Holmes;"Infectious Diseases (Q1); Medicine (miscellaneous) (Q1); Microbiology (medical) (Q1)";683.0;504.0;United Kingdom;1995-2020;10.1016/j.cmi.2019.09.009;0.03171;154.0;;1198743X;1198743X;14690691;1198743X;8.067;Artificial intelligence, Clinical decision support system, Infectious diseases, Information technology, Machine learning;Elsevier Ltd.;;2529.0;Western Europe;2884.0;Q1;29316.0;Clinical microbiology and infection;Machine learning for clinical decision support in infectious diseases: a narrative review of current applications;24871.0;5153.0;503.0;1003.0;12723.0;journal;article;2020
;;Computer Science (miscellaneous) (Q1);267.0;489.0;Saudi Arabia;2014-2020;10.1016/j.jksuci.2020.11.016;;33.0;;13191578;22131248;13191578;22131248;;;King Saud University;;3987.0;Middle East;617.0;Q1;21100389724.0;Journal of king saud university - computer and information sciences;Erratum regarding missing declaration of competing interest statements in previously published articles;;1739.0;314.0;341.0;12519.0;journal;article;2020
This review highlights recent advances in atopic dermatitis (AD) and food allergy (FA), particularly on molecular mechanisms and disease endotypes, recent developments in global strategies for the management of patients, pipeline for future treatments, primary and secondary prevention and psychosocial aspects. During the recent years, there has been major advances in personalized/precision medicine linked to better understanding of disease pathophysiology and precision treatment options of AD. A greater understanding of the molecular and cellular mechanisms of AD through substantial progress in epidemiology, genetics, skin immunology and psychological aspects resulted in advancements in the precision management of AD. However, the implementation of precision medicine in the management of AD still requires the validation of reliable biomarkers, which will provide more tailored management, starting from prevention strategies towards targeted therapies for more severe diseases. Cutaneous exposure to food via defective barriers is an important route of sensitization to food allergens. Studies on the role of the skin barrier genes demonstrated their association with the development of IgE-mediated FA, and suggest novel prevention and treatment strategies for type 2 diseases in general because of their link to barrier defects not only in AD and FA, but also in asthma, chronic rhinosinusitis, allergic rhinitis and inflammatory bowel disease. The development of more accurate diagnostic tools, biomarkers for early prediction, and innovative solutions require a better understanding of molecular mechanisms and the pathophysiology of FA. Based on these developments, this review provides an overview of novel developments and advances in AD and FA, which are reported particularly during the last two years.;Kazunari Sugita and Cezmi A. Akdis;"Medicine (miscellaneous) (Q1); Immunology and Allergy (Q2)";201.0;316.0;Japan;1996-2020;10.1016/j.alit.2019.08.013;0.00452;58.0;;13238930;14401592;13238930;14401592;5.836;Atopic dermatitis, Barrier, Food allergy, Precision medicine, Mechanisms and pathophysiology;Japanese Society of Allergology;;3217.0;Asiatic Region;1490.0;Q1;20184.0;Allergology international;Recent developments and advances in atopic dermatitis and food allergy;3122.0;1245.0;111.0;335.0;3571.0;journal;article;2020
Hyperspectral imaging technology has evolved for over thirty years and is widely used for geologic mapping, environmental monitoring, vegetation analysis, atmospheric characterization, biological and chemical detection, etc. With advances in technology, hyperspectral imagery not only determines the presence of materials and objects, but more importantly, also quantifies the variability and abundance of the identified materials or objects. Airborne hyperspectral imagers still perform a vital role in remote sensing fields due to advantages of spatial resolution, performance capabilities in a cloudy atmosphere, and onboard maintenance as compared to similar imagers aboard spaceborne platforms. To date, hundreds of airborne hyperspectral systems have been designed, built, and operated. Here, a review of key technologies for airborne hyperspectral imaging technology during past three decades is presented. First discussed will be high throughput imaging modes, high quality spectroscopic subsystems, and high sensitivity detector technology used on current airborne hyperspectral imagers. Particularly, the importance of data-processing such as calibration, geometric rectification, and atmospheric correction are discussed. Next, several new and novel applications are presented on the basis of state-of-the-art airborne hyperspectral technology. Finally, an outlook of challenges and future technology directions is presented along with general advice for designing and realizing novel high-performance airborne hyperspectral systems in this rapidly evolving field. By illustrating the status and prospects of typical airborne hyperspectral imagers, this overview provides a comparison of the technologies employed in previous hyperspectral imaging systems, current imaging technology research programs and prospects for innovative technology in future airborne hyperspectral imaging platforms.;Jianxin Jia and Yueming Wang and Jinsong Chen and Ran Guo and Rong Shu and Jianyu Wang;"Atomic and Molecular Physics, and Optics (Q2); Condensed Matter Physics (Q2); Electronic, Optical and Magnetic Materials (Q2)";914.0;292.0;Netherlands;1994-2020;10.1016/j.infrared.2019.103115;;65.0;;13504495;13504495;13504495;13504495;;Airborne, Hyperspectral, Key technology, Surface reflectance, Application;Elsevier;;3501.0;Western Europe;542.0;Q2;12121.0;Infrared physics and technology;Status and application of advanced airborne hyperspectral imaging technology: a review;;2807.0;391.0;918.0;13688.0;journal;article;2020
As a much finer particle, particulate matter less than 1 μm (PM1) plays an important role on the haze formation and human health. However, the capability of mapping PM1 concentration is severely impaired by coarse temporal resolution and low estimation accuracy, largely due to the neglect of spatial or temporal autocorrelation of PM1. In order to improve the estimation of high-resolution PM1, here we developed a novel spatiotemporal model named extreme gradient boosting (XGBoost)-geographically and temporally weighted regression (GTWR) using Himawari-8 aerosol optical depth (AOD), meteorological factors, and geographical covariates. The estimation of PM1 over Zhejiang province showed that XGBoost-GTWR method was characterized by greater predictive ability (10-fold cross-validation R2 = 0.83, root mean squared error (RMSE) = 10.72 μg/m3) compared with other 11 models. Additionally, the extrapolation test was performed to validate the robustness of the hybrid model and the result demonstrated that XGBoost-GTWR can accurately predict the out-of-band PM1 concentration (R2 = 0.75 (0.60), RMSE = 12.71 (12.58) μg/m3). The PM1 concentration displayed pronounced spatial heterogeneity, with the highest value in Quzhou (34.72 ± 1.77 μg/m3) and the lowest in Zhoushan (26.39 ± 1.56 μg/m3), respectively. In terms of the seasonality, the highest PM1 concentration was observed in winter (39.06 ± 3.08 μg/m3), followed by those in spring (32.54 ± 3.09 μg/m3) and autumn (30.97 ± 4.50 μg/m3), and the lowest one in summer (25.57 ± 5.22 μg/m3). The high aerosol emission and adverse meteorological conditions (e.g., low boundary layer height and lack of precipitation) were key factors accounting for the peak PM1 concentration observed in winter. Also, the PM1 concentration exhibited significant diurnal variation, peaking at 1500 local solar time (LST) but reaching the lowest value at 1000 LST. This method enhances our capability of estimating hourly PM1 from space, and lays a solid data foundation for improving the assessment of the fine particle-related health effect.;Rui Li and Lulu Cui and Hongbo Fu and Ya Meng and Junlin Li and Jianping Guo;"Atmospheric Science (Q1); Environmental Science (miscellaneous) (Q1)";2034.0;493.0;United Kingdom;1968-1989, 1994-2021;10.1016/j.atmosenv.2020.117434;0.03998;240.0;;13522310;13522310;18732844;13522310;4.798;AOD, PM, XGBoost, GTWR, Zhejiang;Elsevier Ltd.;;5833.0;Western Europe;1400.0;Q1;23357.0;Atmospheric environment;Estimating high-resolution pm1 concentration from himawari-8 combining extreme gradient boosting-geographically and temporally weighted regression (xgboost-gtwr);68466.0;10479.0;609.0;2046.0;35523.0;journal;article;2020
This is the first systematic review to comprehensively capture Global Positioning Systems’ (GPS) utilization in active living research by investigating the influence of physical contexts and social environment on all intensities of physical activity and sedentary behavior among all age groups. An extensive search of peer-reviewed literature was conducted using six databases. Out of 2026 articles identified, 129 studies met the inclusion criteria. After describing the evolution of GPS use across four themes (study designs and methods, physical contexts and social environment, active transportation, and behaviors), evidence-based recommendations for active living research, policy, and practice were generated.;Tarun R. Katapally and Jasmin Bhawra and Pinal Patel;"Geography, Planning and Development (Q1); Health (social science) (Q1); Life-span and Life-course Studies (Q1); Public Health, Environmental and Occupational Health (Q1); Sociology and Political Science (Q1)";444.0;397.0;United Kingdom;1995-2020;10.1016/j.healthplace.2020.102453;;109.0;;13538292;13538292;13538292;13538292;;Global positioning systems, Active living, Physical activity, Sedentary behavior, Physical contexts, Social environment;Elsevier Ltd.;;6251.0;Western Europe;1341.0;Q1;29102.0;Health and place;A systematic review of the evolution of gps use in active living research: a state of the evidence for research, policy, and practice;;2110.0;174.0;448.0;10877.0;journal;article;2020
Nuclear magnetic resonance (NMR) spectroscopy acts as the best tool that can be used in tissue engineering scaffolds to investigate unknown metabolites. Moreover, metabolomics is a systems approach for examining in vivo and in vitro metabolic profiles, which promises to provide data on cancer metabolic alterations. However, metabolomic profiling allows for the activity of small molecules and metabolic alterations to be measured. Furthermore, metabolic profiling also provides high-spectral resolution, which can then be linked to potential metabolic relationships. An altered metabolism is a hallmark of cancer that can control many malignant properties to drive tumorigenesis. Metabolite targeting and metabolic engineering contribute to carcinogenesis by proliferation, and metabolic differentiation. The resulting the metabolic differences are examined with traditional chemometric methods such as principal component analysis (PCA), and partial least squares-discriminate analysis (PLS-DA). In this review, we examine NMR-based activity metabolomic platforms that can be used to analyze various fluxomics and for multivariant statistical analysis in cancer. We also aim to provide the reader with a basic understanding of NMR spectroscopy, cancer metabolomics, target profiling, chemometrics, and multifunctional tools for metabolomics discrimination, with a focus on metabolic phenotypic diversity for cancer therapeutics.;Ganesan Raja and Youngmi Jung and Sang Hoon Jung and Tae-Jin Kim;"Applied Microbiology and Biotechnology (Q2); Bioengineering (Q2); Biochemistry (Q3)";960.0;366.0;United Kingdom;1950, 1953-1955, 1973-1975, 1979-2020;10.1016/j.procbio.2020.08.023;0.00836;157.0;;13595113;00329592;13595113;00329592;3.757;Cancer, Metabolomics, Metabolic engineering, Target profiling, Software, Therapeutics;Elsevier BV;;5291.0;Western Europe;689.0;Q2;16134.0;Process biochemistry;1h-nmr-based metabolomics for cancer targeting and metabolic engineering –a review;21378.0;3554.0;380.0;963.0;20104.0;journal;article;2020
Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.;Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu;"Drug Discovery (Q1); Pharmacology (Q1)";630.0;717.0;United Kingdom;1996-2020;10.1016/j.drudis.2020.07.005;0.0174;175.0;;13596446;18785832;13596446;18785832;7.851;;Elsevier Ltd.;;7656.0;Western Europe;1778.0;Q1;21196.0;Drug discovery today;Advancing computer-aided drug discovery (cadd) by big data and data-driven machine learning modeling;18695.0;5142.0;258.0;667.0;19752.0;journal;article;2020
"Persistent lack of non-motorized traffic counts can affect the evidence-based decisions of transportation planning and safety-concerned agencies in making reliable investments in bikeway and other non-motorized facilities. Researchers have used various approaches to estimate bicycles counts, such as scaling, direct-demand modeling, time series, and others. In recent years, an increasing number of studies have tried to use crowdsourced data for estimating the bicycle counts. Crowdsourced data only represents a small percentage of cyclists. This percentage, on the other hand, can change based on the location, facility type, meteorological, and other factors. Moreover, the autocorrelation observed in bicycle counts may be different from the autocorrelation structure observed among crowdsourced platform users, such as Strava. Strava users are more consistent; hence, the time series count data may be stationary, while bicycle demand may vary based on seasonal factors. In addition to seasonal variation, several time-invariant contributing factors (e.g., facility type, roadway characteristics, household income) affect bicycle demand, which needs to be accounted for when developing direct demand models. In this paper, we use a mixed-effects model with autocorrelated errors to predict daily bicycle counts from crowdsourced data across the state of Texas. Additionally, we supplement crowdsourced data with other spatial and temporal factors such as roadway facility, household income, population demographics, population density and weather conditions to predict bicycle counts. The results show that using a robust methodology, we can predict bicycle demand with a 29% margin of error, which is significantly lower than merely scaling the crowdsourced data (41%).";Bahar Dadashova and Greg P. Griffin;"Civil and Structural Engineering (Q1); Environmental Science (miscellaneous) (Q1); Transportation (Q1)";821.0;610.0;United Kingdom;1996-2020;10.1016/j.trd.2020.102368;;99.0;;13619209;13619209;13619209;13619209;;Bicycle Counts, crowdsourced data, Strava, Travel demand, Time series data, Random parameter models;Elsevier Ltd.;;5678.0;Western Europe;1600.0;Q1;20894.0;Transportation research, part d: transport and environment;Random parameter models for estimating statewide daily bicycle counts using crowdsourced data;;5627.0;370.0;834.0;21008.0;journal;article;2020
This study examines the dynamic responses of monthly Australian inbound tourist arrivals to higher frequency daily movements in the exchange rate. Many existing models of inbound tourism utilize currency values averaged over months or quarters to match available visitor data, thus discarding valuable dynamic information from higher frequency variations in the exchange rate. We employ a Mixed Data Sampling (MIDAS) approach, modelling monthly Australian inbound tourism from 1998 to 2018, while utilising daily observations of the domestic currency as an explanatory variable. The findings suggest that models that take averages of exchange rate data to match frequencies may substantially underestimate how sensitive tourists are in responding to short-term fluctuations in the exchange rate. Indeed, the overwhelming majority of such responses typically are observed within 15–20 days prior to travel. After considering time horizons of up to 90 days, our findings indicate that travelers are predominantly concerned with exchange rate variations again only within three weeks prior to their departures. The results suggest that the Mixed Data Sampling (MIDAS) methodology can be a useful supplement to traditional approaches to modelling tourism demand. MIDAS models can provide rich insights into the duration and patterns of the dynamic interplay between the variables of interest.;Jeremy Nguyen and Abbas Valadkhani;Tourism, Leisure and Hospitality Management (Q1);190.0;544.0;United Kingdom;2006-2020;10.1016/j.jhtm.2020.07.003;0.00232;34.0;;14476770;18395260;14476770;18395260;5.959;MIDAS, Tourist arrivals, Australia, The exchange rate, Economic policy uncertainty;Elsevier BV;;7926.0;Western Europe;1310.0;Q1;21100255484.0;Journal of hospitality and tourism management;Dynamic responses of tourist arrivals in australia to currency fluctuations;2467.0;1119.0;153.0;194.0;12127.0;journal;article;2020
We use gravity information obtained from the XGM2016 global gravitational model together with topographic, bathymetric and seismic data to interpret the crustal structure beneath Cameroon and adjoined geological regions. For this purpose, we apply the regularized non-linear gravity inversion for a gravimetric determination of the Moho depth utilizing existing results of seismic data analysis as constraints. The estimated Moho model reflects regional tectonic configuration and geological structure of this region, mainly consisting of two major geological units, i.e. the Cameroon Volcanic Line and the Congo Craton. A validation of gravimetric result at sites of the Cameroon Broadband Seismic Experiment (CBSE) reveals overall similarities between gravimetric and seismic estimates. A comparison of our result is also conducted with previously published results. The cross-comparison of these results reveals a good agreement between them, particularly beneath the Cameroon Volcanic Line, the Adamawa Plateau and the Garoua Rift. Nevertheless, some relatively large inconsistencies roughly reaching ±10 km in estimated values of the Moho depth are identified in geological regions of the Congo Craton and the Yaoundé domain. The spatial correlation analysis between the Moho geometry and the topography indicates an isostatic state of particular geological units, suggesting their compensation stage. Our result closely agrees with the assumption that most of isostatically over compensated geological structures were formed during a compressional tectonic regime, except for the Garoua Rift that was likely formed during an extensional regime. We also computed the Bouguer gravity data at different constant elevations above sea level in order to supress a gravitational signature of shallower sources, while enhancing a gravitational signature from deeper crustal and lithospheric structures, focusing primarily on cores of major cratonic formations. The Bouguer gravity maps indicate that the Yaoundé domain likely represents the crustal manifestation of the suture zone between the Congo Craton and the Adamawa-Yadé domain, acting as a micro-continent.;Franck Eitel {Kemgang Ghomsi} and Nguiya Sévérin and Animesh Mandal and Françoise Enyegue A. Nyam and Robert Tenzer and Alain P. {Tokam Kamga} and Robert Nouayou;"Earth-Surface Processes (Q2); Geology (Q2)";912.0;199.0;United Kingdom;1983-2020;10.1016/j.jafrearsci.2019.103657;0.00628;76.0;;1464343X;1464343X;1464343X;1464343X;2.046;Cameroon, Crust, Gravity inversion, Moho, Seismic data;Elsevier Ltd.;;7282.0;Western Europe;572.0;Q2;31815.0;Journal of african earth sciences;Cameroon's crustal configuration from global gravity and topographic models and seismic data;8527.0;1920.0;277.0;922.0;20170.0;journal;article;2020
"Microglia–astrocyte interactions represent a delicate balance affecting neural cell functions in health and disease. Tightly controlled to maintain homeostasis during physiological conditions, rapid and prolonged departures during disease, infection, and following trauma drive multiple outcomes: both beneficial and detrimental. Recent sequencing studies at the bulk and single-cell level in humans and rodents provide new insight into microglia–astrocyte communication in homeostasis and disease. However, the complex changing ways these two cell types functionally interact has been a barrier to understanding disease initiation, progression, and disease mechanisms. Single cell sequencing is providing new insights; however, many questions remain. Here, we discuss how to bridge transcriptional states to specific functions so we can develop therapies to mediate negative effects of altered microglia–astrocyte interactions.";Shane A. Liddelow and Samuel E. Marsh and Beth Stevens;"Immunology (Q1); Immunology and Allergy (Q1)";286.0;1067.0;United Kingdom;1987-1990, 1993-1994, 2001-2020;10.1016/j.it.2020.07.006;0.02457;226.0;;14714906;14714906;14714981;14714906;16.687;microglia, astrocytes, neuroimmune, single cell sequencing, neuroinflammation, neurodegeneration;Elsevier Ltd.;;7350.0;Western Europe;6349.0;Q1;21365.0;Trends in immunology;Microglia and astrocytes in disease: dynamic duo or partners in crime?;16915.0;3648.0;112.0;297.0;8232.0;journal;article;2020
;;"Molecular Medicine (Q1); Pathology and Forensic Medicine (Q1)";269.0;395.0;United States;1999-2020;10.1016/S1525-1578(20)30513-4;0.00965;95.0;;15251578;15251578;19437811;15251578;5.568;;Elsevier Inc.;;3540.0;Northern America;2420.0;Q1;15983.0;Journal of molecular diagnostics;Association for molecular pathology 2020 annual meeting abstracts;5407.0;1431.0;144.0;283.0;5097.0;journal;article;2020
"Summary
Extreme weather events can cause heat stress that decreases crop production. Recent studies have demonstrated that protein degradation and rRNA homeostasis as well as transcription factors are involved in the thermoresponse in plants. However, how RNA modifications contribute to temperature stress response in plant remains largely unknown. Herein, we identified OsNSUN2 as an RNA 5-methylcytosine (m5C) methyltransferase in rice. osnsun2 mutant displayed severe temperature- and light-dependent lesion-mimic phenotypes and heat-stress hypersensitivity. Heat stress enhanced the OsNSUN2-dependent m5C modification of mRNAs involved in photosynthesis and detoxification systems, such as β-OsLCY, OsHO2, OsPAL1, and OsGLYI4, which increased protein synthesis. Furthermore, the photosystem of osnsun2 mutant was vulnerable to high ambient temperature and failed to undergo repair under tolerable heat stress. Thus, OsNSUN2 mutation reduced photosynthesis efficiency and accumulated excessive reactive oxygen species upon heat treatment. Our findings demonstrate an important mechanism of mRNA m5C-dependent heat acclimation in rice.";Yongyan Tang and Chun-Chun Gao and Ying Gao and Ying Yang and Boyang Shi and Jia-Li Yu and Cong Lyu and Bao-Fa Sun and Hai-Lin Wang and Yunyuan Xu and Yun-Gui Yang and Kang Chong;"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Cell Biology (Q1); Developmental Biology (Q1); Molecular Biology (Q1)";896.0;760.0;United States;2001-2020;10.1016/j.devcel.2020.03.009;0.05835;264.0;;15345807;15345807;18781551;15345807;12.27;OsNSUN2, mC, RNA modification, methyltransferase, rice, heat stress, photosynthesis, reactive oxygen species;Cell Press;;5732.0;Northern America;5284.0;Q1;18525.0;Developmental cell;Osnsun2-mediated 5-methylcytosine mrna modification enhances rice adaptation to high temperature;36177.0;7055.0;298.0;918.0;17082.0;journal;article;2020
"The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.";Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won;Electrical and Electronic Engineering (Q1);360.0;3755.0;United States;2005-2020;10.1109/COMST.2020.2973314;0.03684;197.0;;1553877X;1553877X;1553877X;1553877X;25.249;"Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks";Institute of Electrical and Electronics Engineers Inc.;;19942.0;Northern America;6605.0;Q1;17900156715.0;Ieee communications surveys and tutorials;The future of healthcare internet of things: a survey of emerging technologies;22146.0;15073.0;91.0;368.0;18147.0;journal;article;2020
Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.;Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang;"Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1)";312.0;1573.0;Netherlands;2000-2021;10.1016/j.inffus.2019.06.016;0.0126;107.0;;15662535;18726305;15662535;18726305;12.975;Urban computing, Big data, Data fusion, Deep learning;Elsevier;;8130.0;Western Europe;2776.0;Q1;26099.0;Information fusion;Urban big data fusion based on deep learning: an overview;9059.0;5599.0;168.0;318.0;13659.0;journal;article;2020
Over the past decade, the global proliferation of cyanobacterial harmful algal blooms (CyanoHABs) have presented a major risk to the public and wildlife, and ecosystem and economic services provided by inland water resources. As a consequence, water resources, environmental, and healthcare agencies are in need of early information about the development of these blooms to mitigate or minimize their impact. Results from various components of a novel multi-cloud cyber-infrastructure referred to as “CyanoTRACKER” for initial detection and continuous monitoring of spatio-temporal growth of CyanoHABs is highlighted in this study. The novelty of the CyanoTRACKER framework is the collection and integration of combined community reports (social cloud), remote sensing data (sensor cloud) and digital image analytics (computation cloud) to detect and differentiate between regular algal blooms and CyanoHABs. Individual components of CyanoTRACKER include a reporting website, mobile application (App), remotely deployable solar powered automated hyperspectral sensor (CyanoSense), and a cloud-based satellite data processing and integration tool. All components of CyanoTRACKER provided important data related to CyanoHABs assessments for regional and global water bodies. Reports and data received via social cloud including the mobile App, Twitter, Facebook, and CyanoTRACKER website, helped in identifying the geographic locations of CyanoHABs affected water bodies. A significant increase (124.92%) in tweet numbers related to CyanoHABs was observed between 2011 (total relevant tweets = 2925) and 2015 (total relevant tweets = 6579) that reflected an increasing trend of the harmful phenomena across the globe as well as an increased awareness about CyanoHABs among Twitter users. The CyanoHABs affected water bodies extracted via the social cloud were categorized, and smaller water bodies were selected for the deployment of CyanoSense, and satellite data analysis was performed for larger water bodies. CyanoSense was able to differentiate between ordinary algae and CyanoHABs through the use of their characteristic absorption feature at 620 nm. The results and products from this infrastructure can be rapidly disseminated via the CyanoTRACKER website, social media, and direct communication with appropriate management agencies for issuing warnings and alerting lake managers, stakeholders and ordinary citizens to the dangers posed by these environmentally harmful phenomena.;Deepak R. Mishra and Abhishek Kumar and Lakshmish Ramaswamy and Vinay K. Boddula and Moumita C. Das and Benjamin P. Page and Samuel J. Weber;"Aquatic Science (Q1); Plant Science (Q1)";338.0;413.0;Netherlands;2002-2020;10.1016/j.hal.2020.101828;0.00574;91.0;;15689883;15689883;15689883;15689883;4.273;CyanoHABs, Remote sensing, Wireless sensors, Social media, Satellite data, Cyberinfrastructure;Elsevier;;8483.0;Western Europe;1162.0;Q1;19723.0;Harmful algae;Cyanotracker: a cloud-based integrated multi-platform architecture for global observation of cyanobacterial harmful algal blooms;7904.0;1511.0;148.0;339.0;12555.0;journal;article;2020
The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.;Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala;"Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)";61.0;1249.0;Ireland;2007-2020;10.1016/j.cosrev.2020.100303;0.00195;44.0;;15740137;15740137;15740137;15740137;7.872;Internet of Things, Deep Learning, Smart city, Big data analytics, Review;Elsevier Ireland Ltd;;14424.0;Western Europe;1646.0;Q1;8000153138.0;Computer science review;Leveraging deep learning and iot big data analytics to support the smart cities development: review and future directions;1249.0;891.0;54.0;61.0;7789.0;journal;article;2020
How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.;Lin, Mengting and Zhao, Youping;"Computer Networks and Communications (Q2); Electrical and Electronic Engineering (Q2)";607.0;353.0;China;2008-2020;10.23919/JCC.2020.03.006;0.00373;42.0;;16735447;16735447;16735447;16735447;2.688;"Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management";China Institute of Communication;;2922.0;Asiatic Region;508.0;Q2;19700177325.0;China communications;Artificial intelligence-empowered resource management for future wireless communications: a survey;2891.0;2012.0;227.0;629.0;6632.0;journal;article;2020
Accurately mapping plate boundary types and locations through time is essential for understanding the evolution of the plate-mantle system and the exchange of material between the solid Earth and surface environments. However, the complexity of the Earth system and the cryptic nature of the geological record make it difficult to discriminate tectonic environments through deep time. Here we present a new method for identifying tectonic paleo-environments on Earth through a data mining approach using global geochemical data. We first fingerprint a variety of present-day tectonic environments utilising up to 136 geochemical data attributes in any available combination. A total of 38301 geochemical analyses from basalts aged from 5–0 Ma together with a well-established plate reconstruction model are used to construct a suite of discriminatory models for the first order tectonic environments of subduction and mid-ocean ridge as distinct from intraplate hotspot oceanic environments, identifying 41, 35, and 39 key discriminatory geochemical attributes, respectively. After training and validation, our model is applied to a global geochemical database of 1547 basalt samples of unknown tectonic origin aged between 1000–410 Ma, a relatively ill-constrained period of Earth's evolution following the breakup of the Rodinia supercontinent, producing 56 unique global tectonic environment predictions throughout the Neoproterozoic and Early Paleozoic. Predictions are used to discriminate between three alternative published Rodinia configuration models, identifying the model demonstrating the closest spatio-temporal consistency with the basalt record, and emphasizing the importance of integrating geochemical data into plate reconstructions. Our approach offers an extensible framework for constructing full-plate, deep-time reconstructions capable of assimilating a broad range of geochemical and geological observations, enabling next generation Earth system models.;Michael G. Tetley and Zheng-Xiang Li and Kara J. Matthews and Simon E. Williams and R. Dietmar Müller;Earth and Planetary Sciences (miscellaneous) (Q1);382.0;609.0;China;2010-2021;10.1016/j.gsf.2019.05.002;0.00927;57.0;;16749871;16749871;16749871;16749871;6.853;Plate tectonics, Geochemistry, Geodynamics, Supercontinents, Rodinia, Big data;China University of Geosciences (Beijing) and Peking University;;9211.0;Asiatic Region;1842.0;Q1;19700182749.0;Geoscience frontiers;Decoding earth's plate tectonic history using sparse geochemical data;5390.0;2198.0;165.0;400.0;15198.0;journal;article;2020
To enhance the credibility of human reliability analysis, various kinds of data have been recently collected and analyzed. Although it is obvious that the quality of data is critical, the practices or considerations for securing data quality have not been sufficiently discussed. In this work, based on the experience of the recent human reliability data extraction projects, which produced more than fifty thousand data-points, we derive a number of issues to be considered for generating meaningful data. As a result, thirteen considerations are presented here as pertaining to the four different data extraction activities: preparation, collection, analysis, and application. Although the lessons were acquired from a single kind of data collection framework, it is believed that these results will guide researchers to consider important issues in the process of extracting data.;Yochan Kim;Nuclear Energy and Engineering (Q2);602.0;240.0;South Korea;2008-2020;10.1016/j.net.2020.01.034;0.00525;40.0;;17385733;2234358X;17385733;2234358X;2.341;Data analytics, Human reliability analysis, HuREX framework, Lesson learned, Simulation data;Korean Nuclear Society;;2662.0;Asiatic Region;737.0;Q2;11700154337.0;Nuclear engineering and technology;Considerations for generating meaningful hra data: lessons learned from hurex data collection;3095.0;1465.0;404.0;604.0;10754.0;journal;article;2020
"Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.";Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr;"Education (Q2); Health Policy (Q3); Medicine (miscellaneous) (Q3)";221.0;81.0;Germany;2008-2020;10.1016/j.zefq.2020.11.002;;28.0;;18659217;22120289;18659217;22120289;;Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases;Urban und Fischer Verlag Jena;;2979.0;Western Europe;423.0;Q2;11300153728.0;Zeitschrift fur evidenz, fortbildung und qualitat im gesundheitswesen;Evaluation eines zukunftsszenarios zur nutzung von big-data-anwendungen für die verbesserung der versorgung von menschen mit seltenen erkrankungen;;267.0;73.0;244.0;2175.0;journal;article;2020
"Background and aims
With no approved vaccines for treating COVID-19 as of August 2020, many health systems and governments rely on contact tracing as one of the prevention and containment methods. However, there have been instances when the infected person forgets his/her contact-persons and does not have their contact details. Therefore, this study aimed at analyzing possible opportunities and challenges of integrating emerging technologies into COVID-19 contact tracing.
Methods
The study applied literature search from Google Scholar, Science Direct, PubMed, Web of Science, IEEE and WHO COVID-19 reports and guidelines analyzed.
Results
While the integration of technology-based contact tracing applications to combat COVID-19 and break transmission chains promise to yield better results, these technologies face challenges such as technical limitations, dealing with asymptomatic individuals, lack of supporting ICT infrastructure and electronic health policy, socio-economic inequalities, deactivation of mobile devices’ WIFI, GPS services, interoperability and standardization issues, security risks, privacy issues, political and structural responses, ethical and legal risks, consent and voluntariness, abuse of contact tracing apps, and discrimination.
Conclusion
Integrating emerging technologies into COVID-19 contact tracing is seen as a viable option that policymakers, health practitioners and IT technocrats need to seriously consider in mitigating the spread of coronavirus. Further research is also required on how best to improve efficiency and effectiveness in the utilisation of emerging technologies in contact tracing while observing the security and privacy of people in fighting the COVID-19 pandemic.";Elliot Mbunge;"Internal Medicine (Q2); Medicine (miscellaneous) (Q2); Endocrinology, Diabetes and Metabolism (Q3)";900.0;246.0;Netherlands;2007-2020;10.1016/j.dsx.2020.08.029;;40.0;;18714021;18780334;18714021;18780334;;COVID-19, Contact tracing, Emerging technologies;Elsevier BV;;3117.0;Western Europe;684.0;Q2;5700165201.0;Diabetes and metabolic syndrome: clinical research and reviews;Integrating emerging technologies into covid-19 contact tracing: opportunities, challenges and pitfalls;;2232.0;403.0;910.0;12562.0;journal;article;2020
Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.;Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime;"Aerospace Engineering (Q1); Applied Mathematics (Q1); Automotive Engineering (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1)";3060.0;780.0;United States;1967-2020;10.1109/TVT.2020.2991372;0.05223;178.0;;19399359;00189545;19399359;00189545;5.978;"Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet";Institute of Electrical and Electronics Engineers Inc.;;3495.0;Northern America;1365.0;Q1;17393.0;Ieee transactions on vehicular technology;Trading: traffic aware data offloading for big data enabled intelligent transportation system;36492.0;23276.0;1427.0;3074.0;49867.0;journal;article;2020
DNA methylation (5mC) and hydroxymethylation (5hmC) are chemical modifications of cytosine bases which play a crucial role in epigenetic gene regulation. However, cost, data complexity and unavailability of comprehensive analytical tools is one of the major challenges in exploring these epigenetic marks. Hydroxymethylation-and Methylation-Sensitive Tag sequencing (HMST-seq) is one of the most cost-effective techniques that enables simultaneous detection of 5mC and 5hmC at single base pair resolution. We present HMST-Seq-Analyzer as a comprehensive and robust method for performing simultaneous differential methylation analysis on 5mC and 5hmC data sets. HMST-Seq-Analyzer can detect Differentially Methylated Regions (DMRs), annotate them, give a visual overview of methylation status and also perform preliminary quality check on the data. In addition to HMST-Seq, our tool can be used on whole-genome bisulfite sequencing (WGBS) and reduced representation bisulfite sequencing (RRBS) data sets as well. The tool is written in Python with capacity to process data in parallel and is available at (https://hmst-seq.github.io/hmst/).;Amna Farooq and Sindre Grønmyr and Omer Ali and Torbjørn Rognes and Katja Scheffler and Magnar Bjørås and Junbai Wang;"Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2)";255.0;789.0;Sweden;2012-2020;10.1016/j.csbj.2020.09.038;0.00677;45.0;;20010370;20010370;20010370;20010370;7.271;Methylation analysis, Hydroxy methylation, Differential methylation, Hydroxymethylation-and methylation-sensitive tag sequencing, Whole genome bisulfite sequencing;Research Network of Computational and Structural Biotechnology;;7869.0;Western Europe;1908.0;Q1;21100318415.0;Computational and structural biotechnology journal;Hmst-seq-analyzer: a new python tool for differential methylation and hydroxymethylation analysis in various dna methylation sequencing data;3620.0;2022.0;357.0;255.0;28093.0;journal;article;2020
Soil erosion is one of the most severe global environmental problems, and soil erosion surveys are the scientific basis for planning soil conservation and ecological development. To improve soil erosion sampling survey methods and accurately and rapidly estimate the actual rates of soil erosion, a Pan-Third Pole region was taken as an example to study a methodology of soil erosion sampling survey based on high-spatial-resolution remote sensing images. The sampling units were designed using a stratified variable probability systematic sampling method. The spatiotemporal characteristics of soil erosion and conservation were taken into account, and finer-resolution freely available and accessible images in Google Earth were used. Through the visual interpretation of the free high-resolution remote sensing images, detailed information on land use and soil conservation measures was obtained. Then, combined with the regional soil erosion factor data products, such as rainfall-runoff erosivity factor (R), soil erodibility factor (K), and slope length and steepness factor (LS), the soil loss rates of some sampling units were calculated. The results show that, based on these high-resolution remote sensing images, the land use and soil conservation measures of the sampling units can be quickly and accurately extracted. The interpretation accuracy in 4 typical cross sections was more than 80%, and sampling accuracy, described by histogram similarity in 11 large sampling sites, show that the landuse of sampling uints can represent the structural characteristics of regional land use. Based on the interpretation of data from the sample survey and the regional soil erosion factor data products, the calculation of the soil erosion rate can be completed quickly. The calculation results can reflect the actual conditions of soil erosion better than the potential soil erosion rates calculated by using the coarse-resolution remote sensing method.;Qinke Yang and Mengyang Zhu and Chunmei Wang and Xiaoping Zhang and Baoyuan Liu and Xin Wei and Guowei Pang and Chaozhen Du and Lihua Yang;"Agronomy and Crop Science (Q1); Nature and Landscape Conservation (Q1); Soil Science (Q1); Water Science and Technology (Q1)";113.0;660.0;Netherlands;2013-2020;10.1016/j.iswcr.2020.07.005;0.00235;29.0;;20956339;20956339;20956339;20956339;6.027;Pan -third pole area, Land use, Soil conservation measures, Remote sensing, Variable probability sampling;;;5291.0;Western Europe;1541.0;Q1;21100817619.0;International soil and water conservation research;Study on a soil erosion sampling survey in the pan-third pole region based on higher-resolution images;1612.0;775.0;47.0;113.0;2487.0;journal;article;2020
Road accidents are one of the most relevant causes of injuries and death worldwide, and therefore, they constitute a significant field of research on the use of advanced algorithms and techniques to analyze and predict traffic accidents and determine the most relevant elements that contribute to road accidents. The research of road accident prediction aims to respond to the challenge of offer tools to generate a more secure mobility environment, and ultimately, save lives. This paper aims to provide an overview of the state of the art in the prediction of road accidents through machine learning algorithms and advanced techniques for analyzing information, such as convolutional neural networks and long short-term memory networks, among other deep learning architectures. Furthermore, in this article, a compendium and study of the most used data sources for the road accident forecast is made. And a classification is proposed according to its origin and characteristics, such as open data, measurement technologies, onboard equipment and social media data. For the analysis of the information, the different algorithms employed to make predictions about road accidents are listed and compared, as well as their applicability depending on the types of data being analyzed, along with the results obtained and their ease of interpretation and analysis. The best results reported by the authors are obtained when two or more analytic techniques are combined, in such a way that analysis of the obtained results is strengthened. Among the future challenges in road traffic forecasting lies the enhancement of the scope of the proposed models and predictions by the incorporation of heterogeneous data sources, that include geo spatial data, information from traffic volume, traffic statistics, video, sound, text and sentiment from social media, that many authors concur that can improve the precision and accuracy of the analysis and predictions.;Camilo Gutierrez-Osorio and César Pedraza;"Civil and Structural Engineering (Q2); Transportation (Q2)";144.0;357.0;Netherlands;2014-2020;10.1016/j.jtte.2020.05.002;;26.0;;20957564;20957564;20957564;20957564;;Traffic engineering, Data analysis, Machine learning, Road accident forecasting, Traffic accident prediction;Elsevier BV;;5660.0;Western Europe;656.0;Q2;21100784267.0;Journal of traffic and transportation engineering (english edition);Modern data sources and techniques for analysis and forecast of road accidents: a review;;542.0;73.0;147.0;4132.0;journal;article;2020
;Yueting Zhuang and Ming Cai and Xuelong Li and Xiangang Luo and Qiang Yang and Fei Wu;"Chemical Engineering (miscellaneous) (Q1); Computer Science (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Materials Science (miscellaneous) (Q1)";324.0;673.0;United Kingdom;2015-2020;10.1016/j.eng.2020.01.009;0.00715;45.0;;20958099;20958099;20958099;20958099;7.553;;Elsevier Ltd.;;5252.0;Western Europe;1376.0;Q1;21100780794.0;Engineering;The next breakthroughs of artificial intelligence: the interdisciplinary nature of ai;4023.0;3152.0;186.0;372.0;9769.0;journal;article;2020
"In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the ""misjudgment rate"". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.";Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei;"Electrical and Electronic Engineering (Q4); Electronic, Optical and Magnetic Materials (Q4); Energy (miscellaneous) (Q4)";148.0;18.0;United States;2020;10.17775/CSEEJPES.2020.04080;0.0027;4.0;;20960042;20960042;20960042;20960042;3.938;"Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest";Institute of Electrical and Electronics Engineers Inc.;;3800.0;Northern America;118.0;Q4;21101017898.0;Csee journal of power and energy systems;A big data cleaning method based on improved clof and random forest for distribution network;1205.0;27.0;43.0;148.0;1634.0;journal;article;2020
;Luxia Zhang and Ming-Hui Zhao and Li Zuo and Yue Wang and Feng Yu and Hong Zhang and Haibo Wang and Rui Chen and Hong Chu and Xinwei Deng and Lanxia Gan and Bixia Gao and Yifang Jiang and Lili Liu and Jianyan Long and Ying Shi and Zaiming Su and Xiaoyu Sun and Wen Tang and Fang Wang and Huai-Yu Wang and Jinwei Wang and Song Wang and Chao Yang and Dongliang Zhang and Xinju Zhao and Liren Zheng and Zhiye Zhou;Nephrology (Q1);22.0;854.0;United States;2011-2020;10.1016/j.kisu.2020.09.001;0.00197;38.0;;21571716;21571716;21571724;21571716;10.545;;Elsevier Inc.;;3917.0;Northern America;2855.0;Q1;21100258425.0;Kidney international supplements;China kidney disease network (ck-net) 2016 annual data report;3207.0;195.0;12.0;26.0;470.0;journal;article;2020
We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.;Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng;"Artificial Intelligence (Q1); Theoretical Computer Science (Q1)";192.0;980.0;United States;2010-2020;10.1145/3428080;0.00463;63.0;;21576904;21576912;21576904;21576912;4.654;charging scheduling, charging pattern, MDP, Electric bus, data driven;Association for Computing Machinery (ACM);Association for Computing Machinery;5920.0;Northern America;914.0;Q1;19700190323.0;Acm transactions on intelligent systems and technology;Pricing-aware real-time charging scheduling and charging station expansion for large-scale electric buses;4104.0;1489.0;70.0;199.0;4144.0;journal;article;2020
Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.;Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.;"Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Computer Science Applications (Q1); Software (Q1)";1107.0;1251.0;United States;2012-2020;10.1109/TNNLS.2020.3001602;0.04868;212.0;;21622388;2162237X;21622388;2162237X;10.451;"Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model";IEEE Computational Intelligence Society;;3746.0;Northern America;2882.0;Q1;21100235616.0;Ieee transactions on neural networks and learning systems;A wide-deep-sequence model-based quality prediction method in industrial process analysis;36361.0;14914.0;609.0;1117.0;22815.0;journal;article;2020
The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.;Liu, Yangxiaoyue and Yang, Yaping and Jing, Wenlong;"Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)";24200.0;448.0;United States;2013-2020;10.1109/ACCESS.2020.3009977;0.15396;127.0;;21693536;21693536;21693536;21693536;3.367;"Microwave radiometry;Meteorology;Satellite broadcasting;Sensors;Soil moisture;Synthetic aperture radar;Soil measurements;Gap-filling;satellite retrieved soil moisture;the essential climate variable soil moisture;the soil moisture active passive soil moisture";Institute of Electrical and Electronics Engineers Inc.;;4275.0;Northern America;587.0;Q1;21100374601.0;Ieee access;Potential applicability of smap in ecv soil moisture gap-filling: a case study in europe;105968.0;116691.0;18036.0;24267.0;771081.0;journal;article;2020
Climate change mitigation strategies are multifaceted and require collaboration among a range of stakeholder groups. The objective of this paper was to develop an overarching Renewable Energy and Energy Conservation Area Policy (REECAP) framework. The framework was developed based on a comprehensive literature review, in which seven principles for Renewable Energy and Energy Conservation Policies were identified. The paper also includes a case study to demonstrate an application of the REECAP framework. The novelty of the framework stems from its integration of carbon-energy-cash flows among different decision-making spheres, scales and area specific characteristics. The framework provides a mathematical understanding of how energy strategies can be transformed and optimised in a cost-effective manner by integrating stakeholders under a shared vision.;Abel S. Vieira and Rodney A. Stewart and Roberto Lamberts and Cara D. Beal;Energy (miscellaneous) (Q1);245.0;719.0;Netherlands;2012-2020;10.1016/j.esr.2020.100544;0.00348;33.0;;2211467X;2211467X;2211467X;2211467X;6.425;REECAP framework, Carbon emission reduction, Renewable energy, Energy conservation, Carbon-energy-cash flows;Elsevier;;6401.0;Western Europe;1639.0;Q1;21100199818.0;Energy strategy reviews;Renewable energy and energy conservation area policy (reecap) framework: a novel methodology for bottom-up and top-down principles integration;2011.0;1645.0;98.0;247.0;6273.0;journal;article;2020
Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.;Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei;Tourism, Leisure and Hospitality Management (Q1);274.0;677.0;United States;2012-2020;10.1016/j.tmp.2020.100715;0.00445;43.0;;22119736;22119736;22119736;22119736;6.586;Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic;Elsevier USA;;8644.0;Northern America;1454.0;Q1;21100202157.0;Tourism management perspectives;Knowledge mapping of tourism demand forecasting research;3902.0;1824.0;140.0;277.0;12102.0;journal;article;2020
"Background
The Longitudinal Epidemiologic Assessment of Diabetes Risk (LEADR) study uses a novel Electronic Health Record (EHR) data approach as a tool to assess the epidemiology of known and new risk factors for type 2 diabetes mellitus (T2DM) and study how prevention interventions affect progression to and onset of T2DM. We created an electronic cohort of 1.4 million patients having had at least 4 encounters with a healthcare organization for at least 24-months; were aged ≥18 years in 2010; and had no diabetes (i.e., T1DM or T2DM) at cohort entry or in the 12 months following entry. EHR data came from patients at nine healthcare organizations across the U.S. between January 1, 2010–December 31, 2016.
Results
Approximately 5.9% of the LEADR cohort (82,922 patients) developed T2DM, providing opportunities to explore longitudinal clinical care, medication use, risk factor trajectories, and diagnoses for these patients, compared with patients similarly matched prior to disease onset.
Conclusions
LEADR represents one of the largest EHR databases to have repurposed EHR data to examine patients’ T2DM risk. This paper is first in a series demonstrating this novel approach to studying T2DM.
Implications
Chronic conditions that often take years to develop can be studied efficiently using EHR data in a retrospective design.
Level of evidence
While much is already known about T2DM risk, this EHR's cohort's 160 M data points for 1.4 M people over six years, provides opportunities to investigate new unique risk factors and evaluate research hypotheses where results could modify public health practice for preventing T2DM.";Howard A. Fishbein and Rebecca Jeffries Birch and Sunitha M. Mathew and Holly L. Sawyer and Gerald Pulver and Jennifer Poling and David Kaelber and Russell Mardon and Maurice C. Johnson and Wilson Pace and Keith D. Umbel and Xuanping Zhang and Karen R. Siegel and Giuseppina Imperatore and Sundar Shrestha and Krista Proia and Yiling Cheng and Kai {McKeever Bullard} and Edward W. Gregg and Deborah Rolka and Meda E. Pavkov;Health Policy (Q1);124.0;158.0;Netherlands;2013-2020;10.1016/j.hjdsi.2020.100458;0.0043;19.0;;22130764;22130772;22130764;22130772;2.645;Chronic disease, Diabetes mellitus, Epidemiologic methods, Epidemiologic research design, Big data, Electronic health records, Public health informatics, Public health practice;Elsevier BV;;2527.0;Western Europe;917.0;Q1;21100361200.0;Healthcare;The longitudinal epidemiologic assessment of diabetes risk (leadr): unique 1.4 m patient electronic health record cohort;2175.0;259.0;74.0;126.0;1870.0;journal;article;2020
"Summary
Background
Oral rehydration solution (ORS) is a form of oral rehydration therapy (ORT) for diarrhoea that has the potential to drastically reduce child mortality; yet, according to UNICEF estimates, less than half of children younger than 5 years with diarrhoea in low-income and middle-income countries (LMICs) received ORS in 2016. A variety of recommended home fluids (RHF) exist as alternative forms of ORT; however, it is unclear whether RHF prevent child mortality. Previous studies have shown considerable variation between countries in ORS and RHF use, but subnational variation is unknown. This study aims to produce high-resolution geospatial estimates of relative and absolute coverage of ORS, RHF, and ORT (use of either ORS or RHF) in LMICs.
Methods
We used a Bayesian geostatistical model including 15 spatial covariates and data from 385 household surveys across 94 LMICs to estimate annual proportions of children younger than 5 years of age with diarrhoea who received ORS or RHF (or both) on continuous continent-wide surfaces in 2000–17, and aggregated results to policy-relevant administrative units. Additionally, we analysed geographical inequality in coverage across administrative units and estimated the number of diarrhoeal deaths averted by increased coverage over the study period. Uncertainty in the mean coverage estimates was calculated by taking 250 draws from the posterior joint distribution of the model and creating uncertainty intervals (UIs) with the 2·5th and 97·5th percentiles of those 250 draws.
Findings
While ORS use among children with diarrhoea increased in some countries from 2000 to 2017, coverage remained below 50% in the majority (62·6%; 12 417 of 19 823) of second administrative-level units and an estimated 6 519 000 children (95% UI 5 254 000–7 733 000) with diarrhoea were not treated with any form of ORT in 2017. Increases in ORS use corresponded with declines in RHF in many locations, resulting in relatively constant overall ORT coverage from 2000 to 2017. Although ORS was uniformly distributed subnationally in some countries, within-country geographical inequalities persisted in others; 11 countries had at least a 50% difference in one of their units compared with the country mean. Increases in ORS use over time were correlated with declines in RHF use and in diarrhoeal mortality in many locations, and an estimated 52 230 diarrhoeal deaths (36 910–68 860) were averted by scaling up of ORS coverage between 2000 and 2017. Finally, we identified key subnational areas in Colombia, Nigeria, and Sudan as examples of where diarrhoeal mortality remains higher than average, while ORS coverage remains lower than average.
Interpretation
To our knowledge, this study is the first to produce and map subnational estimates of ORS, RHF, and ORT coverage and attributable child diarrhoeal deaths across LMICs from 2000 to 2017, allowing for tracking progress over time. Our novel results, combined with detailed subnational estimates of diarrhoeal morbidity and mortality, can support subnational needs assessments aimed at furthering policy makers' understanding of within-country disparities. Over 50 years after the discovery that led to this simple, cheap, and life-saving therapy, large gains in reducing mortality could still be made by reducing geographical inequalities in ORS coverage.
Funding
Bill & Melinda Gates Foundation.";Kirsten E Wiens and Paulina A Lindstedt and Brigette F Blacker and Kimberly B Johnson and Mathew M Baumann and Lauren E Schaeffer and Hedayat Abbastabar and Foad Abd-Allah and Ahmed Abdelalim and Ibrahim Abdollahpour and Kedir Hussein Abegaz and Ayenew Negesse Abejie and Lucas Guimarães Abreu and Michael R M Abrigo and Ahmed Abualhasan and Manfred Mario Kokou Accrombessi and Dilaram Acharya and Maryam Adabi and Abdu A Adamu and Oladimeji M Adebayo and Rufus Adesoji Adedoyin and Victor Adekanmbi and Olatunji O Adetokunboh and Beyene Meressa Adhena and Mohsen Afarideh and Sohail Ahmad and Keivan Ahmadi and Anwar E Ahmed and Muktar Beshir Ahmed and Rushdia Ahmed and Temesgen Yihunie Akalu and Fares Alahdab and Ziyad Al-Aly and Noore Alam and Samiah Alam and Genet Melak Alamene and Turki M Alanzi and Jacqueline Elizabeth Alcalde-Rabanal and Beriwan Abdulqadir Ali and Mehran Alijanzadeh and Vahid Alipour and Syed Mohamed Aljunid and Ali Almasi and Amir Almasi-Hashiani and Hesham M Al-Mekhlafi and Khalid A Altirkawi and Nelson Alvis-Guzman and Nelson J Alvis-Zakzuk and Saeed Amini and Arianna Maever L Amit and Catalina Liliana Andrei and Mina Anjomshoa and Amir Anoushiravani and Fereshteh Ansari and Carl Abelardo T Antonio and Benny Antony and Ernoiz Antriyandarti and Jalal Arabloo and Hany Mohamed Amin Aref and Olatunde Aremu and Bahram Armoon and Amit Arora and Krishna K Aryal and Afsaneh Arzani and Mehran Asadi-Aliabadi and Hagos Tasew Atalay and Seyyed Shamsadin Athari and Seyyede Masoume Athari and Sachin R Atre and Marcel Ausloos and Nefsu Awoke and Beatriz Paulina {Ayala Quintanilla} and Getinet Ayano and Martin Amogre Ayanore and Yared Asmare {Aynalem IV} and Samad Azari and Peter S Azzopardi and Ebrahim Babaee and Tesleem Kayode Babalola and Alaa Badawi and Mohan Bairwa and Shankar M Bakkannavar and Senthilkumar Balakrishnan and Ayele Geleto Bali and Maciej Banach and Joseph Adel Mattar Banoub and Aleksandra Barac and Till Winfried Bärnighausen and Huda Basaleem and Sanjay Basu and Vo Dinh Bay and Mohsen Bayati and Estifanos Baye and Neeraj Bedi and Mahya Mahya Beheshti Beheshti and Masoud Behzadifar and Meysam Behzadifar and Bayu Begashaw Bekele and Yaschilal Muche Belayneh and Michellr L Bell and Derrick A Bennett and Dessalegn Ajema Berbada and Robert S Bernstein and Anusha Ganapati Bhat and Krittika Bhattacharyya and Suraj Bhattarai and Soumyadeep Bhaumik and Zulfiqar A Bhutta and Ali Bijani and Boris Bikbov and Binyam Minuye {Birihane IV} and Raaj Kishore Biswas and Somayeh Bohlouli and Hunduma Amensisa Amensisa {Bojia I} and Soufiane Boufous and Oliver J Brady and Nicola Luigi Bragazzi and Andrey Nikolaevich Briko and Nikolay Ivanovich Briko and Gabrielle B Britton and Sharath {Burugina Nagaraja} and Reinhard Busse and Zahid A Butt and Luis LA Alberto Cámera and Ismael R Campos-Nonato and Jorge Cano and Josip Car and Rosario Cárdenas and Felix Carvalho and Carlos A Castañeda-Orjuela and Franz Castro and Wagaye Fentahun Chanie and Pranab Chatterjee and Vijay Kumar Chattu and Tesfaye Yitna Yitna Chichiabellu and Ken Lee Chin and Devasahayam J Christopher and Dinh-Toi Chu and Natalie Maria Cormier and Vera Marisa Costa and Carlos Culquichicon and Matiwos Soboka Daba and Giovanni Damiani and Lalit Dandona and Rakhi Dandona and Anh Kim Dang and Aso Mohammad Darwesh and Amira Hamed Darwish and Ahmad Daryani and Jai K Das and Rajat {Das Gupta} and Aditya Prasad Dash and Gail Davey and Claudio Alberto Dávila-Cervantes and Adrian C Davis and Dragos Virgil Davitoiu and Fernando Pio {De la Hoz} and Asmamaw Bizuneh Demis and Dereje Bayissa Demissie and Getu Debalkie Demissie and Gebre Teklemariam Demoz and Edgar Denova-Gutiérrez and Kebede Deribe and Assefa Desalew and Aniruddha Deshpande and Samath Dhamminda Dharmaratne and Preeti Dhillon and Meghnath Dhimal and Govinda Prasad Dhungana and Daniel Diaz and Isaac Oluwafemi Dipeolu and Shirin Djalalinia and Kerrie E Doyle and Eleonora Dubljanin and Bereket Duko and Andre Rodrigues Duraes and Mohammad {Ebrahimi Kalan} and Hisham Atan Edinur and Andem Effiong and Aziz Eftekhari and Nevine {El Nahas} and Iman {El Sayed} and Maysaa {El Sayed Zaki} and Maha {El Tantawi} and Teshome Bekele {Elema I} and Hala Rashad Elhabashy and Shaimaa I El-Jaafary and Hajer Elkout and Aisha Elsharkawy and Iqbal RF Elyazar and Aklilu Endalamaw and Daniel Adane Endalew and Sharareh Eskandarieh and Alireza Esteghamati and Sadaf Esteghamati and Arash Etemadi and Oluchi Ezekannagha and Mohammad Fareed and Roghiyeh Faridnia and Farshad Farzadfar and Mehdi Fazlzadeh and Valery L Feigin and Seyed-Mohammad Fereshtehnejad and Eduarda Fernandes and Irina Filip and Florian Fischer and Nataliya A Foigt and Morenike Oluwatoyin Folayan and Masoud Foroutan and Richard Charles Franklin and Takeshi Fukumoto and Mohamed M Gad and Reta Tsegaye Gayesa and Teshome Gebre and Ketema Bizuwork Gebremedhin and Gebreamlak Gebremedhn Gebremeskel and Hailay Abrha Gesesew and Kebede Embaye Gezae and Keyghobad Ghadiri and Ahmad Ghashghaee and Pramesh Raj Ghimire and Paramjit Singh Gill and Tiffany K Gill and Themba G G Ginindza and Nelson G M Gomes and Sameer Vali Gopalani and Alessandra C Goulart and Bárbara Niegia Garcia Goulart and Ayman Grada and Mohammed Ibrahim Mohialdeen Gubari and Harish Chander Gugnani and Davide Guido and Rafael Alves Guimarães and Yuming Guo and Rajeev Gupta and Nima Hafezi-Nejad and Dessalegn H Haile and Gessessew Bugssa Hailu and Arvin Haj-Mirzaian and Arya Haj-Mirzaian and Randah R Hamadeh and Samer Hamidi and Demelash Woldeyohannes Handiso and Hamidreza Haririan and Ninuk Hariyani and Ahmed I Hasaballah and Md Mehedi Hasan and Edris Hasanpoor and Amir Hasanzadeh and Hadi Hassankhani and Hamid Yimam Hassen and Mohamed I Hegazy and Behzad Heibati and Behnam Heidari and Delia Hendrie and Nathaniel J Henry and Claudiu Herteliu and Fatemeh Heydarpour and Hagos Degefa de {Hidru I} and Thomas R Hird and Chi Linh Hoang and Enayatollah {Homaie Rad} and Praveen Hoogar and Mohammad Hoseini and Naznin Hossain and Mostafa Hosseini and Mehdi Hosseinzadeh and Mowafa Househ and Mohamed Hsairi and Guoqing Hu and Mohammedaman Mama Hussen and Segun Emmanuel Ibitoye and Ehimario U Igumbor and Olayinka Stephen Ilesanmi and Milena D Ilic and Mohammad Hasan Imani-Nasab and Usman Iqbal and Seyed Sina Naghibi Irvani and Sheikh Mohammed Shariful Islam and Chinwe Juliana Iwu and Neda Izadi and Anelisa Jaca and Nader Jahanmehr and Mihajlo Jakovljevic and Amir Jalali and Achala Upendra Jayatilleke and Ravi Prakash Jha and Vivekanand Jha and John S Ji and Jost B Jonas and Jacek Jerzy Jozwiak and Ali Kabir and Zubair Kabir and Amaha Kahsay and Hamed Kalani and Tanuj Kanchan and Behzad {Karami Matin} and André Karch and Mohd Anisul Karim and Hamidreza Karimi-Sari and Surendra Karki and Amir Kasaeian and Gebremicheal Gebreslassie Kasahun and Yawukal chane Kasahun and Habtamu Kebebe Kasaye and Gebrehiwot G Kassa and Getachew Mullu Kassa and Gbenga A Kayode and Ali {Kazemi Karyani} and Mihiretu M Kebede and Peter Njenga Keiyoro and Abraham Getachew Kelbore and Andre Pascal Kengne and Daniel Bekele Ketema and Yousef Saleh Khader and Morteza Abdullatif Khafaie and Nauman Khalid and Rovshan Khalilov and Ejaz Ahmad Khan and Junaid Khan and Md Nuruzzaman {Khan I} and Muhammad Shahzeb Khan and Khaled Khatab and Amir M Khater and Mona M Khater and Maryam Khayamzadeh and Mohammad Khazaei and Salman Khazaei and Mohammad Hossein Khosravi and Jagdish Khubchandani and Ali Kiadaliri and Yun Jin Kim and Ruth W Kimokoti and Adnan Kisa and Sezer Kisa and Niranjan Kissoon and Shivakumar KM Marulasiddaiah M KMShivakumar and Sonali Kochhar and Tufa Kolola and Hamidreza Komaki and Soewarta Kosen and Parvaiz A Koul and Ai Koyanagi and Moritz U G Kraemer and Kewal Krishan and Nuworza Kugbey and G Anil Kumar and Manasi Kumar and Pushpendra Kumar and Vivek Kumar and Dian Kusuma and Carlo {La Vecchia} and Ben Lacey and Sheetal D Lad and Dharmesh Kumar Lal and Felix Lam and Faris Hasan Lami and Prabhat Lamichhane and Van Charles Lansingh and Savita Lasrado and Avula Laxmaiah and Paul H Lee and Kate E LeGrand and Mostafa Leili and Tsegaye Lolaso Lenjebo and Cheru Tesema Leshargie and Aubrey J Levine and Shanshan Li and Shai Linn and Shiwei Liu and Simin Liu and Rakesh Lodha and Joshua Longbottom and Jaifred Christian F Lopez and Hassan {Magdy Abd El Razek} and Muhammed {Magdy Abd El Razek} and D R {Mahadeshwara Prasad} and Phetole Walter Mahasha and Narayan B Mahotra and Azeem Majeed and Reza Malekzadeh and Deborah Carvalho Malta and Abdullah A Mamun and Navid Manafi and Ana Laura Manda and Narendar Dawani Dawanu Manohar and Mohammad Ali Mansournia and Chabila Christopher Mapoma and Joemer C Maravilla and Gabriel Martinez and Santi Martini and Francisco Rogerlândio Martins-Melo and Anthony Masaka and Benjamin Ballard Massenburg and Manu Raj Mathur and Benjamin K Mayala and Mohsen Mazidi and Colm McAlinden and Birhanu Geta Meharie and Man Mohan Mehndiratta and Kala M Mehta and Tefera C Chane Mekonnen and Gebrekiros Gebremichael Meles and Peter T N Memiah and Ziad A Memish and Walter Mendoza and Ritesh G Menezes and Seid Tiku Mereta and Tuomo J Meretoja and Tomislav Mestrovic and Bartosz Miazgowski and Kebadnew Mulatu Mihretie and Ted R Miller and GK Mini and Erkin M Mirrakhimov and Babak Moazen and Bahram Mohajer and Amjad Mohamadi-Bolbanabad and Dara K Mohammad and Karzan Abdulmuhsin Mohammad and Yousef Mohammad and Naser {Mohammad Gholi Mezerji} and Roghayeh Mohammadibakhsh and Noushin Mohammadifard and Jemal Abdu Mohammed and Shafiu Mohammed and Farnam Mohebi and Ali H Mokdad and Mariam Molokhia and Lorenzo Monasta and Yoshan Moodley and Catrin E Moore and Ghobad Moradi and Masoud Moradi and Mohammad Moradi-Joo and Maziar Moradi-Lakeh and Paula Moraga and Linda Morales and Ilais {Moreno Velásquez} and Abbas Mosapour and Simin Mouodi and Seyyed Meysam Mousavi and Miliva {Mozaffor I} and Kindie Fentahun Muchie and Getahun Fentaw Mulaw and Sandra B Munro and Moses K Muriithi and Christopher J L Murray and GVS Murthy and Kamarul Imran Musa and Ghulam Mustafa and Saravanan Muthupandian and Ashraf F Nabhan and Mehdi Naderi and Ahamarshan Jayaraman Nagarajan and Kovin S Naidoo and Gurudatta Naik and Farid Najafi and Vinay Nangia and Jobert Richie Nansseu and Bruno Ramos Nascimento and Javad Nazari and Duduzile Edith Ndwandwe and Ionut Negoi and Henok Biresaw Netsere Netsere and Josephine W Ngunjiri and Cuong Tat Nguyen and Huong Lan Thi Nguyen and Trang Huyen Nguyen and Dabere Nigatu and Solomon Gedlu Nigatu and Dina Nur Anggraini Ningrum and Chukwudi A Nnaji and Marzieh Nojomi and Vuong Minh Nong and Ole F Norheim and Jean Jacques Noubiap and Soraya {Nouraei Motlagh} and Bogdan Oancea and Okechukwu Samuel Ogah and Felix Akpojene Ogbo and In-Hwan Oh and Andrew T Olagunju and Tinuke O Olagunju and Bolajoko Olubukunola Olusanya and Jacob Olusegun Olusanya and Obinna E Onwujekwe and Eyal Oren and Doris V V Ortega-Altamirano and Osayomwanbo Osarenotor and Frank B Osei and Mayowa O Owolabi and Mahesh {P A} and Jagadish Rao Padubidri and Smita Pakhale and Sangram Kishor Patel and Angel J Paternina-Caicedo and Ashish Pathak and George C Patton and Deepak Paudel and Kebreab Paulos and Veincent Christian Filipino Pepito and Alexandre Pereira and Norberto Perico and Aslam Pervaiz and Julia Moreira Pescarini and Bakhtiar Piroozi and Meghdad Pirsaheb and Maarten J Postma and Hadi Pourjafar and Farshad Pourmalek and Akram Pourshams and Hossein Poustchi and Sergio I Prada and Narayan Prasad and Liliana Preotescu and Hedley Quintana and Navid Rabiee and Amir Radfar and Alireza Rafiei and Fakher Rahim and Afarin Rahimi-Movaghar and Vafa Rahimi-Movaghar and Mohammad Hifz Ur Rahman and Muhammad Aziz Rahman and SHAFIUR Rahman and Fatemeh Rajati and Saleem Muhammad Rana and Chhabi Lal Ranabhat and Davide Rasella and David Laith Rawaf and Salman Rawaf and Lal Rawal and Wasiq Faraz Rawasia and Vishnu Renjith and Andre M N Renzaho and Serge Resnikoff and Melese Abate Reta and Negar Rezaei and Mohammad sadegh Rezai and Seyed Mohammad Riahi and Ana Isabel Ribeiro and Jennifer Rickard and Maria Rios-Blancas and Leonardo Roever and Luca Ronfani and Elias Merdassa Roro and Jennifer M Ross and Enrico Rubagotti and Salvatore Rubino and Anas M Saad and Yogesh Damodar Sabde and Siamak Sabour and Ehsan Sadeghi and Yahya Safari and Roya Safari-Faramani and Rajesh Sagar and Amirhossein Sahebkar and Mohammad Ali Sahraian and S Mohammad Sajadi and Mohammad Reza Salahshoor and Nasir Salam and Payman Salamati and Hosni Salem and Marwa R Rashad {Salem I} and Yahya Salimi and Hamideh Salimzadeh and Abdallah M Samy and Juan Sanabria and Milena M Santric-Milicevic and Bruno Piassi {Sao Jose} and Sivan Yegnanarayana Iyer Saraswathy and Kaushik Sarkar and Abdur Razzaque Sarker and Nizal {Sarrafzadegan I} and Benn Sartorius and Brijesh Sathian and Thirunavukkarasu Sathish and Monika Sawhney and Sonia Saxena and David C Schwebel and Anbissa Muleta {Senbeta IV} and Subramanian Senthilkumaran and Sadaf G Sepanlou and Edson Serván-Mori and Hosein Shabaninejad and Azadeh Shafieesabet and Masood Ali Shaikh and Ali S Shalash and Seifadin Ahmed Shallo and Mehran Shams-Beyranvand and MohammadBagher Shamsi and Morteza Shamsizadeh and Mohammed Shannawaz and Kiomars Sharafi and Hamid Sharifi and Hatem Samir Shehata and Aziz Sheikh and B Suresh Kumar Shetty and Kenji Shibuya and Wondimeneh Shibabaw Shiferaw and Desalegn Markos Shifti and Mika Shigematsu and Jae Il Shin and Rahman Shiri and Reza Shirkoohi and Soraya Siabani and Tariq Jamal Siddiqi and Diego Augusto Santos Silva and Ambrish Singh and Jasvinder A Singh and Narinder Pal Singh and Virendra Singh and Malede Mequanent Sisay and Eirini Skiadaresi and Mohammad Reza Sobhiyeh and Anton Sokhan and Shahin Soltani and Ranjani Somayaji and Moslem Soofi and Muluken Bekele Sorrie and Ireneous N Soyiri and Chandrashekhar T Sreeramareddy and Agus Sudaryanto and Mu'awiyyah Babale Sufiyan and Hafiz Ansar Rasul Suleria and Marufa Sultana and Bruno Fokas Sunguya and Bryan L Sykes and Rafael Tabarés-Seisdedos and Takahiro Tabuchi and Degena Bahrey Tadesse and Ingan Ukur Tarigan and Aberash Abay Tasew and Yonatal Mesfin Tefera and Merhawi Gebremedhin Tekle and Mohamad-Hani Temsah and Berhe Etsay {Tesfay I} and Fisaha Haile Haile Tesfay and Belay Tessema and Zemenu Tadesse Tessema and Kavumpurathu Raman Thankappan and Nihal Thomas and Alemayehu Toma Toma and Roman Topor-Madry and Marcos Roberto Roberto Tovani-Palone and Eugenio Traini and Bach Xuan Tran and Khanh Bao Tran and Irfan Ullah and Bhaskaran Unnikrishnan and Muhammad Shariq Usman and Benjamin S Chudi Uzochukwu and Pascual R Valdez and Santosh Varughese and Francesco S Violante and Sebastian Vollmer and Feleke Gebremeskel W/hawariat and Yasir Waheed and Mitchell Taylor Wallin and Yafeng Wang and Yuan-Pang Wang and Marcia Weaver and Bedilu Girma Weji and Girmay Teklay Weldesamuel and Catherine A Welgan and Andrea Werdecker and Ronny Westerman and Taweewat Wiangkham and Charles Shey Wiysonge and Haileab Fekadu Wolde and Dawit Zewdu Wondafrash and Tewodros Eshete Wonde and Getasew Taddesse Worku and Ai-Min Wu and Gelin Xu and Ali Yadollahpour and Seyed Hossein {Yahyazadeh Jabbari} and Tomohide Yamada and Hiroshi Yatsuya and Alex Yeshaneh and Christopher Sabo Yilgwan and Mekdes Tigistu Yilma and Paul Yip and Engida Yisma and Naohiro Yonemoto and Seok-Jun Yoon and Mustafa Z Younis and Mahmoud Yousefifard and Hebat-Allah Salah A Yousof and Chuanhua Yu and Hasan Yusefzadeh and Siddhesh Zadey and Zoubida Zaidi and Sojib Bin Zaman and Mohammad Zamani and Hamed Zandian and Nejimu Biza Zepro and Taddese Alemu Zerfu and Yunquan Zhang and Xiu-Ju George Zhao and Arash Ziapour and Sanjay Zodpey and Yves Miel H Zuniga and Simon I Hay and Robert C Reiner;Medicine (miscellaneous) (Q1);299.0;617.0;United Kingdom;2013-2020;10.1016/S2214-109X(20)30230-8;;88.0;;2214109X;2214109X;2214109X;2214109X;;;Elsevier BV;;1506.0;Western Europe;7970.0;Q1;21100265444.0;Lancet global health,the;Mapping geographical inequalities in oral rehydration therapy coverage in low-income and middle-income countries, 2000–17;;6192.0;332.0;972.0;4999.0;journal;article;2020
The number of chemicals with potential to reach the environment is still largely unknown, which poses great challenges for both environmental scientists and analytical chemists. Liquid chromatography coupled to high-resolution mass spectrometry (LC-HRMS) is currently the instrumentation of choice for identification of wide-scope polar chemicals of concern (CECs) in water. This review critically evaluates all steps involved in screening for polar CECs in water, including sampling and extraction, analysis by LC-HRMS, data (pre-)treatment, evaluation and reporting. Passive samplers and direct injection, in combination with LC-HRMS, provide new opportunities compared with conventional grab water sampling, as do instrumental advances such as ion-mobility spectrometry coupled to HRMS (IM-HRMS). In this paper, we argue that target, suspect and non-target screening should not be viewed as three separate principles, but rather as conceptual approaches to general data treatment strategies that can be linked together. Due to the large amount of data generated, smart prioritisation strategies are needed, in particular for non-target screening, to reduce complexity and focus on data of high interest. We critically evaluate existing strategies and consider that each prioritisation step will result in data loss (as any other step in a screening study), requiring compromises depending on the research question to be tackled. Many different data treatment strategies have been developed in recent years, but structure elucidation remains a challenging and time-consuming task. We discuss current and potential future trends, e.g. effect-based methods that can be used as future prioritisation tools, technological advances like IM-HRMS and improved software solutions that can enable new data treatment strategies.;Frank Menger and Pablo Gago-Ferrero and Karin Wiberg and Lutz Ahrens;"Analytical Chemistry (Q1); Environmental Chemistry (Q1)";31.0;940.0;Netherlands;2014-2020;10.1016/j.teac.2020.e00102;0.0008;25.0;;22141588;22141588;22141588;22141588;9.6;Aquatic environment, Organic, Emerging micropollutant, Non-Target screening, Suspect screening, LC, HRMS, Water analysis;Elsevier BV;;10439.0;Western Europe;1568.0;Q1;21100316072.0;Trends in environmental analytical chemistry;Wide-scope screening of polar contaminants of concern in water: a critical review of liquid chromatography-high resolution mass spectrometry-based strategies;647.0;287.0;31.0;31.0;3236.0;journal;article;2020
The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.;Jing Zhang and Rong Tan and Chunhua Su and Wen Si;"Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)";292.0;543.0;United Kingdom;2013-2020;10.1016/j.jisa.2020.102659;0.00209;40.0;;22142126;22142126;22142134;22142126;3.872;Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting;Elsevier Ltd.;;4677.0;Western Europe;610.0;Q2;21100332403.0;Journal of information security and applications;Design and application of a personal credit information sharing platform based on consortium blockchain;1526.0;1526.0;183.0;297.0;8559.0;journal;article;2020
Due to the comparatively low frequency of long-distance and overnight travel, it can be challenging to measure using traditional travel surveys. In response to this dearth of data, several recent surveys have included self-reported frequency questions. However, the value and accuracy of these questions is unclear. This study leverages data from 628 panel members who completed a year-long, online survey in 2013–2014 by comparing their one-time self-assessment of typical overnight trip frequency to those reported in 12 subsequent monthly surveys. The self-assessed frequency of overnight tours, airplane tours and tours to non-North American destinations are consistent for only 68% and 70% of respondents for work and personal tours respectively. For most tour types, consistency is highest for low frequency (never and <1 per year) suggesting, unsurprisingly, that individuals are good at knowing they do not travel. Inconsistent estimators both over- and under-estimated trip-making suggest that not only is recall an issue but that prestige bias and the complexity and variability of long-distance travel are factors in recall as well. Few demographic factors were statistically significant in estimating whether participants were consistent reporters. Only 9% of participants consistently either over- or under-estimated both work and personal trips suggesting that there is simply a general inaccuracy in these survey questions for measuring long-distance travel. The aggregate trip rate across people and trip types was accurate suggesting this crude frequency measure may be acceptable for total frequency but not for understanding relative patterns or details in overnight long-distance travel.;Jonathan Dowds and Lisa Aultman-Hall and Jeffrey J. LaMondia;Transportation (Q1);151.0;545.0;Netherlands;2014-2021;10.1016/j.tbs.2019.12.004;0.00222;26.0;;2214367X;2214367X;2214367X;2214367X;4.983;Travel survey, Long-distance travel, Trip recall, Overnight travel;Elsevier BV;;5671.0;Western Europe;1695.0;Q1;21100316002.0;Travel behaviour and society;Comparing alternative methods of collecting self-assessed overnight long-distance travel frequencies;1218.0;822.0;94.0;155.0;5331.0;journal;article;2020
The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.;Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin;"Applied Microbiology and Biotechnology (Q1); Food Science (Q1)";286.0;516.0;Netherlands;2015-2021;10.1016/j.cofs.2020.11.006;0.00491;38.0;;22147993;22147993;22147993;22147993;6.031;;Elsevier BV;;5725.0;Western Europe;1297.0;Q1;21100370190.0;Current opinion in food science;Big data in food safety- a review;3298.0;1714.0;89.0;318.0;5095.0;journal;article;2020
Within CONNECTING Nature, we are dealing with developing innovative nature-based solutions (NBS) for climate change adaptation, health and well-being, social cohesion and sustainable economic development in European cities. In order to enable “learning by comparing” and “generating new knowledge” from multiple NBS related studies, a novel data and knowledge base is needed which requires a specified methodological approach for its development. This paper provides conceptual and methodological context and techniques for constructing such a data and knowledge base that will systematically support the process of NBS monitoring and assessment:•A methodology presents the comprehensive, multi-step approach to the NBS data and knowledge development that helps to guide work and influence the quality of an information included.•The paper describes the methodology and main steps/phases for developing a large data and knowledge base of NBS that will allow further systematic review.•The suggested methodology explains how to build NBS related databases from the conceptualization and requirements phases through to implementation and maintenance. In this regard, such a methodology is iterative, with extensive NBS stakeholders’ and end-user's involvement that are packaged with reusable templates or deliverables offering a good opportunity for success when used by practitioners and other end-users.•The NBS data and knowledge base gathers information about different NBS models and generations into one easy-to-find, easy-to-use place and provides detailed descriptions of each of the 1490 NBS cases from urban centers in Europe.•The data and knowledge base thus helps users identify the best and most appropriated NBS model/type for addressing the particular goals and, at the same time, considers the local context and potential.•The data obtained can be used for the further meta-analysis by applying statistics or searching for specific sample cases and thus enables to generate and expand the knowledge from multiple NBS related studies, in both qualitative and quantitative ways.;Diana Dushkova and Dagmar Haase;"Medical Laboratory Technology (Q2); Clinical Biochemistry (Q3)";548.0;184.0;Netherlands;2014-2020;10.1016/j.mex.2020.101096;;23.0;;22150161;22150161;22150161;22150161;;Nature-based solutions (NBS), Data- and knowledge base, Climate change, Societal challenges, Sustainability, Resilience, Urban Europe;Elsevier BV;;1809.0;Western Europe;356.0;Q2;21100317906.0;Methodsx;Methodology for development of a data and knowledge base for learning from existing nature-based solutions in europe: the connecting nature project;;1058.0;423.0;548.0;7652.0;journal;article;2020
Big data analytics and artificial intelligence, paired with blockchain technology, the Internet of Things, and other emerging technologies, are poised to revolutionise urban management. With massive amounts of data collected from citizens, devices, and traditional sources such as routine and well-established censuses, urban areas across the world have – for the first time in history – the opportunity to monitor and manage their urban infrastructure in real-time. This simultaneously provides previously unimaginable opportunities to shape the future of cities, but also gives rise to new ethical challenges. This paper provides a transdisciplinary synthesis of the developments, opportunities, and challenges for urban management and planning under this ongoing ‘digital revolution’ to provide a reference point for the largely fragmented research efforts and policy practice in this area. We consider both top-down systems engineering approaches and the bottom-up emergent approaches to coordination of different systems and functions, their implications for the existing physical and institutional constraints on the built environment and various planning practices, as well as the social and ethical considerations associated with this transformation from non-digital urban management to data-driven urban management.;Zeynep Engin and Justin {van Dijk} and Tian Lan and Paul A. Longley and Philip Treleaven and Michael Batty and Alan Penn;"Urban Studies (Q1); Geography, Planning and Development (Q2); Public Administration (Q2)";60.0;313.0;Netherlands;2012-2020;10.1016/j.jum.2019.12.001;;11.0;;22265856;25890360;22265856;25890360;;Data-driven society, Urban management and applications, Evidence-based decision making;Elsevier BV;;4028.0;Western Europe;587.0;Q1;21100907127.0;Journal of urban management;Data-driven urban management: mapping the landscape;;210.0;32.0;68.0;1289.0;journal;article;2020
The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.;Ezzine, Imane and Benhlima, Laila;"Computer Science Applications; Information Systems and Management; Management Science and Operations Research; Signal Processing";128.0;114.0;United States;2015;10.1109/CiSt49399.2021.9357200;;11.0;;23271884;2327185X;23271884;2327185X;;"COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance";;;0.0;Northern America;170.0;-;21100400809.0;Colloquium in information science and technology, cist;Technology against covid-19 a blockchain-based framework for data quality;;139.0;0.0;132.0;0.0;conference and proceedings;inproceedings;2020
Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.;Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.;"Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)";1555.0;1237.0;United States;2014-2020;10.1109/JIOT.2020.2994556;0.03208;97.0;;23274662;23274662;23274662;23274662;9.471;"Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD)";Institute of Electrical and Electronics Engineers Inc.;;3472.0;Northern America;2075.0;Q1;21100338350.0;Ieee internet of things journal;Quality estimation for scarce scenarios within mobile crowdsensing systems;21151.0;20461.0;1163.0;1594.0;40380.0;journal;article;2020
The prediction of building energy use is the basis for smart building operation, which optimizes building performance through control and low-energy strategy. For reducing computation complexity and improving calculation accuracy, a comparative study of online electricity data predictions for different types of buildings was conducted. This study is also intended to assess the capability and accuracy of the supervised machine learning methods, with which the kernel algorithms of predictions were developed. Specifically, in this study, large-scale real data collected from the building energy management system were used in the online energy consumption forecasting, which is specially designed for optimized control, real-time fault detection, diagnosis and abnormality alarms. Firstly, the characteristics of building energy profiles and data reliability were addressed. Mathematical algorithms were introduced and their previous applications in building energy usage prediction were summarized, including the evaluation criteria that are effective for energy use predictions in buildings. The reliability and efficiency of the proposed algorithms were then demonstrated through the comparison between the monitored actual data and the predicted results. It is found that Gaussian Process Regression (GPR) can give acceptable predictions on the energy consumption of office buildings with an equilibrium of data prediction accuracy with the average deviations of below 15% and low computation time. Additionally, the statistical evaluation criteria proposed by ASHRAE can also be satisfied. For hotels and shopping malls where complex functions were applied in these buildings, their accuracy are not better or even the same as those of simplified models, due to the significant effects of the factors involving occupant's activities and schedules as well as data reliability on building energy usage. Our result revealed that GPR is a reliable method and can still generate highly accurate predictions when a large data set with a small time interval and complex energy use patterns obtained from real building measurements rather than simulated data are involved.;Aaron Zeng and Hodde Ho and Yao Yu;"Architecture (Q1); Building and Construction (Q1); Civil and Structural Engineering (Q1); Mechanics of Materials (Q1); Safety, Risk, Reliability and Quality (Q1)";720.0;570.0;Netherlands;2015-2021;10.1016/j.jobe.2019.101054;0.00753;39.0;;23527102;23527102;23527102;23527102;5.318;Energy use prediction, Machine learning, Electricity consumption, Gaussian process regression, Online building energy;Elsevier BV;;4917.0;Western Europe;974.0;Q1;21100389518.0;Journal of building engineering;Prediction of building electricity usage using gaussian process regression;5990.0;4094.0;777.0;721.0;38204.0;journal;article;2020
Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation.;Marília Barandas and Duarte Folgado and Letícia Fernandes and Sara Santos and Mariana Abreu and Patrícia Bota and Hui Liu and Tanja Schultz and Hugo Gamboa;"Computer Science Applications (Q2); Software (Q2)";235.0;257.0;Netherlands;2015-2020;10.1016/j.softx.2020.100456;0.00813;21.0;;23527110;23527110;23527110;23527110;1.959;Time series, Machine learning, Feature extraction, Python;Elsevier BV;;2778.0;Western Europe;528.0;Q2;21100422153.0;Softwarex;Tsfel: time series feature extraction library;2553.0;654.0;162.0;235.0;4501.0;journal;article;2020
Improving female empowerment is an important human rights and development goal that needs better monitoring. A number of indices have been developed to track female empowerment at the national level, but these are incomplete and may obscure important sub-national variation. We developed the Female Empowerment Index (FEMI) to track multiple domains of women's empowerment at the sub-national level. The index is based on six categories of empowerment: violence against women, employment, education, reproductive healthcare, decision making, and access to contraceptives. The FEMI has a range of zero to one (low to high empowerment), and it is calculated as the mean proportion of positive outcomes in the six categories. To provide a proof of concept, we computed the FEMI for Nigeria and its 36 states from five Demographic and Health Surveys between the years of 1990 and 2013, using questions asked to 98,542 women between 15 and 49 years old. At the national level, the FEMI increased from 0.34 to 0.48. However, there was substantial sub-national variation, with state-level values ranging from 0.16-0.60 in 1990 to 0.19–0.73 in 2013. Our findings thus illustrate the importance of considering sub-national variation in female empowerment. The FEMI can be readily computed for other countries, and its ability to track spatial and temporal variation in woman's empowerment across a broad set of categories may make it more useful than existing approaches.;Erica M. Rettig and Stephen E. Fick and Robert J. Hijmans;Multidisciplinary (Q1);2688.0;285.0;Netherlands;2015-2020;10.1016/j.heliyon.2020.e03829;;28.0;;24058440;24058440;24058440;24058440;;Africa, Nigeria, Women's empowerment, Demographic and health surveys, Gender inequality, Machine learning, Data analytics, Data visualization, Big data, Data mining, Human geography, Social inequality, Human rights, Geography, Sociology, Information science;Elsevier BV;;4878.0;Western Europe;455.0;Q1;21100411756.0;Heliyon;The female empowerment index (femi): spatial and temporal variation in women's empowerment in nigeria;;7639.0;2721.0;2689.0;132717.0;journal;article;2020
Recent empirical studies reveal that predictive maintenance is essential for accomplishing business objectives of manufacturing enterprises. Knowledge-based maintenance strategies for optimal operation of industrial machines and physical assets reasonably require explaining and predicting long term economic impacts, based on exploring historical data. This paper examines how supervised machine learning (ML) techniques may enhance anticipating the economic impact of maintenance on profitability (IMP). Planning and monitoring of maintenance activities supported by various statistical learning and supervised ML algorithms have been investigated in the literature of production management. However, data-driven prediction of IMP has not been largely addressed. A novel data-driven framework is proposed comprising cause-and-effect dependencies between maintenance and profitability, which constructs a set of appropriate features as independent variables.;Kai Schenkelberg and Ulrich Seidenberg and Fazel Ansari;Control and Systems Engineering (Q3);8351.0;113.0;Austria;2002-2019;10.1016/j.ifacol.2020.12.2830;;72.0;;24058963;24058963;24058963;24058963;;Maintenance, Profitability, Supervised learning, Machine learning, Regression, Knowledge-Based Maintenance;IFAC Secretariat;;1663.0;Western Europe;308.0;Q3;21100456158.0;Ifac-papersonline;Supervised machine learning for knowledge-based analysis of maintenance impact on profitability;;9863.0;115.0;8407.0;1913.0;journal;article;2020
Supply chain disruptions have serious consequences for society and this has made supply chain risk management (SCRM) an attractive area for researchers and managers. In this paper, we use an objective literature mapping approach to identify, classify, and analyze decision-making models and support systems for SCRM, providing an agenda for future research. Through bibliometric networks of articles published in the Scopus database, we analyze the most influential decision-making models and support systems for SCRM, evaluate the main areas of current research, and provide insights for future research in this field. The main results are the following: we found that the identity of the area is structured in three groups of risk decision support models: (i) quantitative multicriteria decision models, (ii) stochastic decision-making models, and (iii) computational simulation/optimization models. We mapped six current research clusters: (i) conceptual and qualitative risk models, (ii) upstream supply chain risk models, (iii) downstream supply chain risk models, (iv) supply chain sustainability risk models, (v) stochastic and multicriteria decision risk models, and (vi) emerging techniques risk models. We identified seven future research clusters, with insights from further studies for: (i) tools to operate SCRM data, (ii) validation of risk models, (iii) computational improvement for data analysis, (iv) multi-level and multi-period supply chains, (v) agrifood risks, (vi) energy risks and (vii) sustainability risks. Finally, the future research agenda should prioritize SCRM's holistic vision, the relationship between Big Data, Industry 4.0 and SCRM, as well as emerging social and environmental risks.;Marcus Vinicius Carvalho Fagundes and Eduardo Oliveira Teles and Silvio A.B. {Vieira de Melo} and Francisco Gaudêncio Mendonça Freires;"Business and International Management (Q1); Marketing (Q1); Strategy and Management (Q1); Economics and Econometrics (Q2)";63.0;633.0;Spain;2016-2020;10.1016/j.iedeen.2020.02.001;0.00054;18.0;;24448834;24448834;24448834;24448834;5.024;risk model, multicriteria decision, stochastic and computational model, bibliometrics;European Academy of Management and Business Economics;;5595.0;Western Europe;1024.0;Q1;21100465205.0;European research on management and business economics;Decision-making models and support systems for supply chain risk: literature mapping and future research agenda;395.0;427.0;22.0;63.0;1231.0;journal;article;2020
The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.;Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh;Genetics (Q4);441.0;72.0;Netherlands;2015-2020;10.1016/j.genrep.2020.100869;;9.0;;24520144;24520144;24520144;24520144;;Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning;Elsevier BV;;4164.0;Western Europe;235.0;Q4;21100445644.0;Gene reports;Big data in biology: the hope and present-day challenges in it;;331.0;407.0;442.0;16946.0;journal;article;2020
Systems biology involves network-oriented, computational approaches to modeling biological systems through analysis of big biological data. To contribute maximally to scientific progress, big biological data should be FAIR: findable, accessible, interoperable, and reusable. Here, we describe high-throughput sequencing data that characterize the vast diversity of B- and T-cell clones comprising the adaptive immune receptor repertoire (AIRR-seq data) and its contribution to our understanding of COVID-19 (coronavirus disease 19). We describe the accomplishments of the AIRR community, a grass-roots network of interdisciplinary laboratory scientists, bioinformaticians, and policy wonks, in creating and publishing standards, software and repositories for AIRR-seq data based on the FAIR principles.;Jamie K. Scott and Felix Breden;"Applied Mathematics (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Computer Science Applications (Q1); Drug Discovery (Q1); Modeling and Simulation (Q1)";239.0;290.0;United Kingdom;2017-2020;10.1016/j.coisb.2020.10.001;;21.0;;24523100;24523100;24523100;24523100;;Adaptive immunity, B-cell and T-cell receptor repertoires, FAIR Principles, Open source, Adaptive immune receptor repertoire (AIRR) community;Elsevier Ltd.;;6363.0;Western Europe;1576.0;Q1;21100857212.0;Current opinion in systems biology;The adaptive immune receptor repertoire community as a model for fair stewardship of big immunology data;;800.0;43.0;269.0;2736.0;journal;article;2020
"Purpose
Though the domain of big data and artificial intelligence in health care continues to evolve, there is a lack of systemic methods to improve data quality and streamline the preparation process. To address this, we aimed to develop an automated sorting system (RetiSort) that accurately labels the type and laterality of retinal photographs.
Design
Cross-sectional study.
Participants
RetiSort was developed with retinal photographs from the Singapore Epidemiology of Eye Diseases (SEED) study.
Methods
The development of RetiSort was composed of 3 steps: 2 deep-learning (DL) algorithms and 1 rule-based classifier. For step 1, a DL algorithm was developed to locate the optic disc, the “landmark feature.” For step 2, based on the location of the optic disc derived from step 1, a rule-based classifier was developed to sort retinal photographs into 3 types: macular-centered, optic disc–centered, or related to other fields. Step 2 concurrently distinguished laterality (i.e., the left or right eye) of macular-centered photographs. For step 3, an additional DL algorithm was developed to differentiate the laterality of disc-centered photographs. Via the 3 steps, RetiSort sorted and labeled retinal images into (1) right macular–centered, (2) left macular–centered, (3) right optic disc–centered, (4) left optic disc–centered, and (5) images relating to other fields. Subsequently, the accuracy of RetiSort was evaluated on 5000 randomly selected retinal images from SEED as well as on 3 publicly available image databases (DIARETDB0, HEI-MED, and Drishti-GS). The main outcome measure was the accuracy for sorting of retinal photographs.
Results
RetiSort mislabeled 48 out of 5000 retinal images from SEED, representing an overall accuracy of 99.0% (95% confidence interval [CI], 98.7–99.3). In external tests, RetiSort mislabeled 1, 0, and 2 images, respectively, from DIARETDB0, HEI-MED, and Drishti-GS, representing an accuracy of 99.2% (95% CI, 95.8–99.9), 100%, and 98.0% (95% CI, 93.1–99.8), respectively. Saliency maps consistently showed that the DL algorithm in step 3 required pixels in the central left lateral border and optic disc of optic disc–centered retinal photographs to differentiate the laterality.
Conclusions
RetiSort is a highly accurate automated sorting system. It can aid in data preparation and has practical applications in DL research that uses retinal photographs.";Tyler Hyungtaek Rim and Zhi Da Soh and Yih-Chung Tham and Henrik Hee Seung Yang and Geunyoung Lee and Youngnam Kim and Simon Nusinovici and Daniel Shu Wei Ting and Tien Yin Wong and Ching-Yu Cheng;Ophthalmology (Q1);394.0;234.0;United States;2017-2020;10.1016/j.oret.2020.03.007;;18.0;;24686530;24686530;24686530;24686530;;;Elsevier Inc.;;1897.0;Northern America;1517.0;Q1;21100925772.0;Ophthalmology retina;Deep learning for automated sorting of retinal photographs;;1327.0;254.0;586.0;4819.0;journal;article;2020
"Summary
Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.";Yadi Zhou and Fei Wang and Jian Tang and Ruth Nussinov and Feixiong Cheng;"Decision Sciences (miscellaneous) (Q1); Health Informatics (Q1); Health Information Management (Q1); Medicine (miscellaneous) (Q1)";26.0;394.0;United Kingdom;2019-2020;10.1016/S2589-7500(20)30192-8;;14.0;;25897500;25897500;25897500;25897500;;;Elsevier Ltd.;;1670.0;Western Europe;2346.0;Q1;21100922606.0;Lancet digital health, the;Artificial intelligence in covid-19 drug repurposing;;319.0;138.0;81.0;2305.0;journal;article;2020
"Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.";Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis;"Cardiology and Cardiovascular Medicine (Q4); Internal Medicine (Q4)";16.0;71.0;Canada;2019-2020;10.1016/j.ijchy.2020.100027;;2.0;;25900862;25900862;25900862;25900862;;Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network;Canadian Medical Association;;2998.0;Northern America;194.0;Q4;21100922613.0;International journal of cardiology: hypertension;Uses and opportunities for machine learning in hypertension research;;12.0;43.0;17.0;1289.0;journal;article;2020
Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.;Ann Hill Duin and Jason Tham;"Language and Linguistics (Q1); Linguistics and Language (Q1); Computer Science (miscellaneous) (Q2); Education (Q2)";102.0;150.0;United Kingdom;1983-2020;10.1016/j.compcom.2020.102544;;35.0;;87554615;87554615;87554615;87554615;;Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access;Elsevier Ltd.;;5244.0;Western Europe;521.0;Q1;26168.0;Computers and composition;The current state of analytics: implications for learning management system (lms) use in writing pedagogy;;159.0;43.0;112.0;2255.0;journal;article;2020
