abstract: "Big data are large volumes of digital data that can be collected from disparate\
  \ sources and are challenging to analyze. These data are often described with the\
  \ five \u201CVs\u201D: volume, velocity, variety, veracity, and value. Perioperative\
  \ nurses contribute to big data through documentation in the electronic health record\
  \ during routine surgical care, and these data have implications for clinical decision\
  \ making, administrative decisions, quality improvement, and big data science. This\
  \ article explores methods to improve the quality of perioperative nursing data\
  \ and provides examples of how these data can be combined with broader nursing data\
  \ for quality improvement. We also discuss a national action plan for nursing knowledge\
  \ and big data science and how perioperative nurses can engage in collaborative\
  \ actions to transform health care. Standardized perioperative nursing data has\
  \ the potential to affect care far beyond the original patient."
author: Bonnie L. Westra and Jessica J. Peterson
categories: Medical and Surgical Nursing (Q2)
citable_docs._(3years): 475.0
cites_/_doc._(2years): 25.0
country: United States
coverage: 1963-2020
doi: 10.1016/j.aorn.2016.07.009
eigenfactor_score: 0.0013
h_index: 43.0
isbn: null
issn: 00012092
issn1: '18780369'
issn2: 00012092
issn3: '18780369'
jcr_value: '0.676'
keywords: big data, perioperative nursing, quality care, nursing knowledge, nursing
  informatics
publisher_x: John Wiley &amp; Sons Inc.
publisher_y: null
ref._/_doc.: 855.0
region: Northern America
scimago_value: 222.0
sjr_best_quartile: Q2
sourceid: 27052.0
title_bib: Big data and perioperative nursing
title_csv: Aorn journal
total_cites: 1553.0
total_cites_(3years): 276.0
total_docs._(2020): 288.0
total_docs._(3years): 682.0
total_refs.: 2461.0
type: journal
type_publication: article
year: 2016
---
abstract: "The railroad industry plays a principal role in the transportation infrastructure\
  \ and economic prosperity of the United States, and safety is of the utmost importance.\
  \ Trespassing is the leading cause of rail-related fatalities and there has been\
  \ little progress in reducing the trespassing frequency and deaths for the past\
  \ ten years in the United States. Although the widespread deployment of surveillance\
  \ cameras and vast amounts of video data in the railroad industry make witnessing\
  \ these events achievable, it requires enormous labor-hours to monitor real-time\
  \ videos or archival video data. To address this challenge and leverage this big\
  \ data, this study develops a robust Artificial Intelligence (AI)-aided framework\
  \ for the automatic detection of trespassing events. This deep learning-based tool\
  \ automatically detects trespassing events, differentiates types of violators, generates\
  \ video clips, and documents basic information of the trespassing events into one\
  \ dataset. This study aims to provide the railroad industry with state-of-the-art\
  \ AI tools to harness the untapped potential of video surveillance infrastructure\
  \ through the risk analysis of their data feeds in specific locations. In the case\
  \ study, the AI has analyzed over 1,600\_h of archival video footage and detected\
  \ around 3,000 trespassing events from one grade crossing in New Jersey. The data\
  \ generated from these big video data will potentially help understand human factors\
  \ in railroad safety research and contribute to specific trespassing proactive safety\
  \ risk management initiatives and improve the safety of the train crew, rail passengers,\
  \ and road users through engineering, education, and enforcement solutions to trespassing."
author: Zhipeng Zhang and Asim Zaman and Jinxuan Xu and Xiang Liu
categories: Human Factors and Ergonomics (Q1); Law (Q1); Public Health, Environmental
  and Occupational Health (Q1); Safety, Risk, Reliability and Quality (Q1)
citable_docs._(3years): 1034.0
cites_/_doc._(2years): 555.0
country: United Kingdom
coverage: 1969-2020
doi: 10.1016/j.aap.2022.106594
eigenfactor_score: 0.0204
h_index: 152.0
isbn: null
issn: '00014575'
issn1: '18792057'
issn2: '00014575'
issn3: '18792057'
jcr_value: '4.993'
keywords: Trespassing, Railroad safety, Artificial Intelligence, Computer vision,
  Risk management
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5321.0
region: Western Europe
scimago_value: 1816.0
sjr_best_quartile: Q1
sourceid: 19532.0
title_bib: 'Artificial intelligence-aided railroad trespassing detection and data
  analytics: methodology and a case study'
title_csv: Accident analysis and prevention
total_cites: 25323.0
total_cites_(3years): 5984.0
total_docs._(2020): 385.0
total_docs._(3years): 1041.0
total_refs.: 20484.0
type: journal
type_publication: article
year: 2022
---
abstract: "Purpose\nTo characterize the role of Big Data in evaluating quality of\
  \ care in ophthalmology, to highlight opportunities for studying quality improvement\
  \ using data available in the American Academy of Ophthalmology Intelligent Research\
  \ in Sight (IRIS) Registry, and to show how Big Data informs us about rare events\
  \ such as endophthalmitis after cataract surgery.\nDesign\nReview of published studies,\
  \ analysis of public-use Medicare claims files from 2010 to 2013, and analysis of\
  \ IRIS Registry from 2013 to 2014.\nMethods\nStatistical analysis of observational\
  \ data.\nResults\nThe overall rate of endophthalmitis after cataract surgery was\
  \ 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the\
  \ endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals.\
  \ Endophthalmitis rates tended to be higher in eyes with combined cataract surgery\
  \ and anterior vitrectomy (P\_= .051), although only 0.08% of eyes had this combined\
  \ procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative\
  \ endophthalmitis measured 1\u20137\_days postoperatively were logMAR 0.58 (standard\
  \ deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31\
  \ (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes\
  \ with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative\
  \ VA worse than 20/40.\nConclusions\nBig Data drawing on Medicare claims and IRIS\
  \ Registry records can help identify additional areas for quality improvement, such\
  \ as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse\
  \ than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities\
  \ for further research on rare complications such as postoperative endophthalmitis\
  \ and outcomes from uncommon procedures such as cataract surgery combined with anterior\
  \ vitrectomy. But privacy and data-security concerns associated with Big Data should\
  \ not be taken lightly."
author: Anne Louise Coleman
categories: Ophthalmology (Q1)
citable_docs._(3years): 848.0
cites_/_doc._(2years): 373.0
country: United States
coverage: 1918-2020
doi: 10.1016/j.ajo.2015.09.028
eigenfactor_score: 0.0263
h_index: 186.0
isbn: null
issn: 00029394
issn1: 00029394
issn2: '18791891'
issn3: 00029394
jcr_value: '5.258'
keywords: null
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 2799.0
region: Northern America
scimago_value: 2704.0
sjr_best_quartile: Q1
sourceid: 13266.0
title_bib: 'How big data informs us about cataract surgery: the lxxii edward jackson
  memorial lecture'
title_csv: American journal of ophthalmology
total_cites: 32566.0
total_cites_(3years): 4453.0
total_docs._(2020): 428.0
total_docs._(3years): 1146.0
total_refs.: 11979.0
type: journal
type_publication: article
year: 2015
---
abstract: Efficient and reliable analysis of chemical analytical data is a great challenge
  due to the increase in data size, variety and velocity. New methodologies, approaches
  and methods are being proposed not only by chemometrics but also by other data scientific
  communities to extract relevant information from big datasets and provide their
  value to different applications. Besides common goal of big data analysis, different
  perspectives and terms on big data are being discussed in scientific literature
  and public media. The aim of this comprehensive review is to present common trends
  in the analysis of chemical analytical data across different data scientific fields
  together with their data type-specific and generic challenges. Firstly, common data
  science terms used in different data scientific fields are summarized and discussed.
  Secondly, systematic methodologies to plan and run big data analysis projects are
  presented together with their steps. Moreover, different analysis aspects like assessing
  data quality, selecting data pre-processing strategies, data visualization and model
  validation are considered in more detail. Finally, an overview of standard and new
  data analysis methods is provided and their suitability for big analytical chemical
  datasets shortly discussed.
author: "Ewa Szyma\u0144ska"
categories: Analytical Chemistry (Q1); Biochemistry (Q1); Environmental Chemistry
  (Q1); Spectroscopy (Q1)
citable_docs._(3years): 2228.0
cites_/_doc._(2years): 627.0
country: Netherlands
coverage: 1947-2020
doi: 10.1016/j.aca.2018.05.038
eigenfactor_score: 0.03972
h_index: 203.0
isbn: null
issn: '00032670'
issn1: '00032670'
issn2: '18734324'
issn3: '00032670'
jcr_value: '6.558'
keywords: Chemometrics, Data science, Big data, Chemical analytical data, Methodology
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5021.0
region: Western Europe
scimago_value: 1403.0
sjr_best_quartile: Q1
sourceid: 23911.0
title_bib: "Modern data science for analytical chemical data \u2013 a comprehensive\
  \ review"
title_csv: Analytica chimica acta
total_cites: 58170.0
total_cites_(3years): 14014.0
total_docs._(2020): 897.0
total_docs._(3years): 2235.0
total_refs.: 45034.0
type: journal
type_publication: article
year: 2018
---
abstract: "Despite the development of several tools for the analysis of the transcriptome\
  \ data, non-availability of a standard pipeline for analyzing the low quality and\
  \ fragmented mRNA samples pose a major challenge to the computational molecular\
  \ biologist for effective interpretation of the data. Hence the present study aimed\
  \ to establish a bioinformatics pipeline for analyzing the biologically fragmented\
  \ sperm RNA. Sperm transcriptome data (2 x 75 PE sequencing) generated from bulls\
  \ (n\_=\_8) of high-fertile (n\_=\_4) and low-fertile (n\_=\_4) classified based\
  \ on the fertility rate (41.52\_\xB1\_1.07 vs 36.04\_\xB1\_1.04%) were analyzed\
  \ with different bioinformatics tools for alignment, quantitation, and differential\
  \ gene expression studies. TopHat2 was effectual compared to HISAT2 and STAR for\
  \ sperm mRNA due to the higher exonic (6% vs 2%) mapping percentage and quantitating\
  \ the low expressed genes. TopHat2 also had significantly strong correlation with\
  \ STAR (0.871, p\_=\_0.05) and HISAT2 (0.933, p\_=\_0.01). TopHat2 and Cufflinks\
  \ combo quantitated the number of genes higher than the other combinations. Among\
  \ the tools (Cuffdiff, DESeq, DESeq2, edgeR, and limma) used for the differential\
  \ gene expression analysis, edgeR and limma identified the largest number of significantly\
  \ differentially expressed genes (p\_<\_0.05) with biological relevance. The concordance\
  \ analysis concurred that edgeR had an edge over the other tools. It also identified\
  \ a higher number (9.5%) of fertility-related genes to be differentially expressed\
  \ between the two groups. The present study established that TopHat2, Cufflinks,\
  \ and edgeR as a suitable pipeline for the analysis of fragmented mRNA from bovine\
  \ spermatozoa."
author: Laxman Ramya and Divakar Swathi and Santhanahalli Siddalingappa Archana and
  Maharajan Lavanya and Sivashanmugam Parthipan and Sellappan Selvaraju
categories: Biophysics (Q2); Biochemistry (Q3); Cell Biology (Q3); Molecular Biology
  (Q3)
citable_docs._(3years): 901.0
cites_/_doc._(2years): 319.0
country: United States
coverage: 1960-2020
doi: 10.1016/j.ab.2021.114141
eigenfactor_score: 0.010029999999999999
h_index: 190.0
isbn: null
issn: 00032697
issn1: '10960309'
issn2: 00032697
issn3: '10960309'
jcr_value: '3.365'
keywords: Bioinformatics pipeline, Fragmented transcripts, Transcriptomics, Differential
  gene expression, Bovine spermatozoa
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 4080.0
region: Northern America
scimago_value: 633.0
sjr_best_quartile: Q2
sourceid: 16789.0
title_bib: Establishment of bioinformatics pipeline for deciphering the biological
  complexities of fragmented sperm transcriptome
title_csv: Analytical biochemistry
total_cites: 42950.0
total_cites_(3years): 2942.0
total_docs._(2020): 334.0
total_docs._(3years): 908.0
total_refs.: 13628.0
type: journal
type_publication: article
year: 2021
---
abstract: Human reliability analysis plays an important role in the safety assessment
  and management of rail operations. This paper discusses how the increasing availability
  of operational data can be used to develop an understanding of train driver reliability.
  The paper derives human reliability data for two driving tasks, stopping at red
  signals and controlling speed on approach to buffer stops. In the first of these
  cases, a tool has been developed that can estimate the number of times a signal
  is approached at red by trains on the Great Britain (GB) rail network. The tool
  has been developed using big data techniques and ideas, recording and analysing
  millions of pieces of data from live operational feeds to update and summarise statistics
  from thousands of signal locations in GB on a daily basis. The resulting driver
  reliability data are compared to similar analyses of other train driving tasks.
  This shows human reliability approaching the currently accepted limits of human
  performance. It also shows higher error rates amongst freight train drivers than
  passenger train drivers for these tasks. The paper highlights the importance of
  understanding the task specific performance limits if further improvements in human
  reliability are sought. It also provides a practical example of how big data could
  play an increasingly important role in system error management, whether from the
  perspective of understanding normal performance and the limits of performance for
  specific tasks or as the basis for dynamic safety indicators which, if not leading,
  could at least become closer to real time.
author: Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw
  Gibson and Alice Monk
categories: Engineering (miscellaneous) (Q1); Human Factors and Ergonomics (Q1); Physical
  Therapy, Sports Therapy and Rehabilitation (Q1); Safety, Risk, Reliability and Quality
  (Q1)
citable_docs._(3years): 675.0
cites_/_doc._(2years): 417.0
country: United Kingdom
coverage: 1969-2021
doi: 10.1016/j.apergo.2022.103795
eigenfactor_score: 0.00851
h_index: 98.0
isbn: null
issn: 00036870
issn1: '18729126'
issn2: 00036870
issn3: '18729126'
jcr_value: '3.661'
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4877.0
region: Western Europe
scimago_value: 1093.0
sjr_best_quartile: Q1
sourceid: 13929.0
title_bib: At the limit? using operational data to estimate train driver human reliability
title_csv: Applied ergonomics
total_cites: 9523.0
total_cites_(3years): 2995.0
total_docs._(2020): 232.0
total_docs._(3years): 682.0
total_refs.: 11314.0
type: journal
type_publication: article
year: 2022
---
abstract: Soil salinization is an important factor that restricts crop quality and
  yield and causes an enormous toll to human beings. Salt stress and abscisic acid
  (ABA) stress will occur in the process of soil salinization. In this study, transcriptome
  sequencing of tobacco leaves under salt and ABA stress in order to further study
  the resistance mechanism of tobacco. Compared with controlled groups, 1654 and 3306
  DEGs were obtained in salt and ABA stress, respectively. The genes function enrichment
  analysis showed that the up-regulated genes in salt stress were mainly concentrated
  in transcription factor WRKY family and PAR1 resistance gene family, while the up-regulated
  genes were mainly concentrated on bHLH transcription factor, Kunitz-type protease
  inhibitor, dehydrin (Xero1) gene and CAT (Catalase) family protein genes in ABA
  stress. Tobacco MAPK cascade triggered stress response through up-regulation of
  gene expression in signal transduction. The expression products of these up-regulated
  genes can improve the abiotic stress resistance of plants. These results have an
  important implication for further understanding the mechanism of salinity tolerance
  in plants.
author: Hui Wu and Huayang Li and Wenhui Zhang and Heng Tang and Long Yang
categories: Biophysics (Q1); Biochemistry (Q2); Molecular Biology (Q2); Cell Biology
  (Q3)
citable_docs._(3years): 6438.0
cites_/_doc._(2years): 331.0
country: United States
coverage: 1959-2020
doi: 10.1016/j.bbrc.2021.07.011
eigenfactor_score: 0.06934
h_index: 263.0
isbn: null
issn: 0006291X
issn1: 0006291X
issn2: '10902104'
issn3: 0006291X
jcr_value: '3.575'
keywords: Tobacco, Transcriptome, MAPK, Soil salinization mechanism
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 3030.0
region: Northern America
scimago_value: 998.0
sjr_best_quartile: Q1
sourceid: 16845.0
title_bib: Transcriptional regulation and functional analysis of nicotiana tabacum
  under salt and aba stress
title_csv: Biochemical and biophysical research communications
total_cites: 105148.0
total_cites_(3years): 21713.0
total_docs._(2020): 1943.0
total_docs._(3years): 6445.0
total_refs.: 58869.0
type: journal
type_publication: article
year: 2021
---
abstract: The tremendous expansion of data analytics and public and private big datasets
  presents an important opportunity for pre-clinical drug discovery and development.
  In the field of life sciences, the growth of genetic, genomic, transcriptomic and
  proteomic data is partly driven by a rapid decline in experimental costs as biotechnology
  improves throughput, scalability, and speed. Yet far too many researchers tend to
  underestimate the challenges and consequences involving data integrity and quality
  standards. Given the effect of data integrity on scientific interpretation, these
  issues have significant implications during preclinical drug development. We describe
  standardized approaches for maximizing the utility of publicly available or privately
  generated biological data and address some of the common pitfalls. We also discuss
  the increasing interest to integrate and interpret cross-platform data. Principles
  outlined here should serve as a useful broad guide for existing analytical practices
  and pipelines and as a tool for developing additional insights into therapeutics
  using big data.
author: John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross
  and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca
  Kusko
categories: Biochemistry (Q1); Pharmacology (Q1)
citable_docs._(3years): 983.0
cites_/_doc._(2years): 529.0
country: United States
coverage: 1958-2020
doi: 10.1016/j.bcp.2018.03.014
eigenfactor_score: 0.01736
h_index: 198.0
isbn: null
issn: 00062952
issn1: '18732968'
issn2: 00062952
issn3: '18732968'
jcr_value: '5.858'
keywords: Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 7833.0
region: Northern America
scimago_value: 1595.0
sjr_best_quartile: Q1
sourceid: 20063.0
title_bib: Integrity, standards, and qc-related issues with big data in pre-clinical
  drug discovery
title_csv: Biochemical pharmacology
total_cites: 33633.0
total_cites_(3years): 5594.0
total_docs._(2020): 513.0
total_docs._(3years): 993.0
total_refs.: 40185.0
type: journal
type_publication: article
year: 2018
---
abstract: 'Complete blood count (CBC) analysis is one of the most commonly ordered
  laboratory tests and is a critical first step in patients'' clinical evaluation.
  However, CBC analyzers are limited in their ability to positively identify several
  types of white blood cells (WBC), and cells with substantial clinical significance,
  such as immature granulocytes or blasts, are merely marked as flags. Also, CBC analyzers
  fall short of recognizing informative red blood cell (RBC) morphology, such as schistocytes,
  and often provide inaccurate platelets count. Flags and clinically non-sufficient
  CBC-derived data reflex to generation of blood smear (BS), and BS review comprises
  a substantial portion of the workload in routine hematology laboratories. For accurate
  identification and classification of WBC, BS analysis (BSA) requires detailed observation
  of cells with high-magnification objective (60-100X), which provides a relatively
  narrow Field of View (FOV). This physical limitation restricts current BSA to either
  low resolution/wide FOV or to high resolution/narrow FOV data generation (Fig. 1A).
  Hence, key issues of BSA such as the effects of the smearing process on the distribution
  of blood components, the effects of cells distribution on their morphology and further
  classification, as well as many other attributes, are addressed only qualitatively
  or empirically, leaving the real topology of the BS obscure. The computational imaging
  microscopy system presented herein uses a low resolution and wide FOV objective,
  and records a plurality of images under different illumination conditions, of the
  same sample area (Fig. 1B). An algorithm reconstructs a high resolution and aberration
  free image of whole specimens, as can be observed in the attached link (https://tinyurl.com/Scopio-Labs-X100-ASH-2020).
  High resolution images are critical not only for manual BSA, but also for artificial
  intelligence (AI)-derived BSA, since data quality is of prime importance for deep-learning
  processes, and to a large extent determine their outcome. Thus, the combination
  of high resolution/wide FOV turns each BS into a big data analytic field, rendering
  the measurement of yet undetermined cell characteristics. In order to elucidate
  the basic topology, 60 normal BS (28 females, 32 males) were subjected to analysis
  utilizing this novel computational imaging microscopy. For convenience of analysis
  and comparison with current BSA methodology, BS were segmented into strips according
  to RBC density (Fig. 1C, D). The average length of smear from females (F) was higher
  by nearly 28% compared with smear from males (M), and the presence of acute inflammation
  (A) resulted in a significant 33% increase in overall smear length compared to normal
  (N) average (Fig. 1E). As expected, RBC density formed a linear gradient (Fig. 1C)
  along the axis of sample smearing, however, RBC morphology was affected by location
  within the BS. For example, strips 4-5 contained RBC with the appearance of spherocytes
  (Fig. 1F; arrows), while in strips with increased RBC density, cells aggregated
  resembling rouleaux formation (Fig. 1F; arrowheads). Platelets distribution was
  non-linear, with only a few of them reaching the feathered edge of the smear (Fig.
  1G). Since the variance of both RBC/FOV and platelets/FOV concentrations drops starting
  with strip 4, BS-derived platelets number estimates should not be performed in strips
  1-3. On average, a normal BS contains 890+399 WBC in the scanned area (strips 1-8).
  Similar to RBC, the location of individual WBC throughout the BS may affect their
  morphology, and hence their classification. WBC in the feathered edge (strips 1-3)
  are generally more stretched, and often squeezed between RBC, rendering their classification
  by AI-based tools challenging (Fig. 1H). In strips 4-7, WBC morphology is optimal
  for a classification task, enabling favorable outcomes for either manual or AI cell
  analysis (Fig. 1H). These data indicate that BSA can be taken to a sensitivity level
  of at least 10-3 of WBC analysis, provided that a large portion of the BS is scanned.
  Our system provides a novel combination of computational imaging microscopy and
  AI-based classification tools to unravel the complex topology of blood smears, and
  upgrade the data obtained in BSA. This approach enables the establishment of quantitative
  rules to scientifically direct the objective analysis of cellular blood components
  both manually, and by AI-tools. Figure

  Disclosures

  Katz: Scopio Labs: Consultancy.'
author: Ben Zion Katz and Irit Avivi and Dan Benisty and Shahar Karni and Hadar Shimoni
  and Omri Grooper and Olga Pozdnyakova
categories: Biochemistry (Q1); Cell Biology (Q1); Hematology (Q1); Immunology (Q1)
citable_docs._(3years): 2041.0
cites_/_doc._(2years): 741.0
country: United States
coverage: 1946-2020
doi: 10.1182/blood-2020-134903
eigenfactor_score: 0.18725
h_index: 465.0
isbn: null
issn: 00064971
issn1: '15280020'
issn2: 00064971
issn3: '15280020'
jcr_value: '22.113'
keywords: null
publisher_x: American Society of Hematology
publisher_y: null
ref._/_doc.: 3106.0
region: Northern America
scimago_value: 5515.0
sjr_best_quartile: Q1
sourceid: 25454.0
title_bib: 'A novel approach to blood smear analysis based on specimen topology: implications
  for human and artificial intelligence decision making'
title_csv: Blood
total_cites: 200027.0
total_cites_(3years): 22558.0
total_docs._(2020): 853.0
total_docs._(3years): 2755.0
total_refs.: 26498.0
type: journal
type_publication: article
year: 2020
---
abstract: X-ray computed tomography (XCT) is increasingly being used for evaluating
  quality and conformance of complex products, including assemblies and additively
  manufactured parts. The metrological performance and traceability of XCT nevertheless
  remains an important research area that is reviewed in this paper. The error sources
  influencing XCT measurement results are discussed, along with related qualification,
  calibration and optimization procedures. Moreover, progress on performance verification
  testing and on the determination of task-specific measurement uncertainty is covered.
  Results of interlaboratory comparisons are summarized and performance in various
  dimensional measurement fields is illustrated. Conclusions and an outlook for future
  research activities are also provided.
author: Wim Dewulf and Harald Bosse and Simone Carmignato and Richard Leach
categories: Industrial and Manufacturing Engineering (Q1); Mechanical Engineering
  (Q1)
citable_docs._(3years): 472.0
cites_/_doc._(2years): 527.0
country: United States
coverage: 1969-2020
doi: 10.1016/j.cirp.2022.05.001
eigenfactor_score: .nan
h_index: 155.0
isbn: null
issn: 00078506
issn1: 00078506
issn2: '17260604'
issn3: 00078506
jcr_value: null
keywords: X-ray, Metrology, Traceability
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 3110.0
region: Northern America
scimago_value: 2370.0
sjr_best_quartile: Q1
sourceid: 19804.0
title_bib: Advances in the metrological traceability and performance of x-ray computed
  tomography
title_csv: Cirp annals - manufacturing technology
total_cites: .nan
total_cites_(3years): 3073.0
total_docs._(2020): 144.0
total_docs._(3years): 472.0
total_refs.: 4478.0
type: journal
type_publication: article
year: 2022
---
abstract: "Chondrules and matrix are the major components of chondritic meteorites\
  \ and represent a significant evolutionary step in planet formation. The formation\
  \ and evolution of chondrules and matrix and, in particular, the mechanics of chondrule\
  \ formation remain the biggest unsolved challenge in meteoritics. A large number\
  \ of studies of these major components not only helped to understand these in ever\
  \ greater detail, but also produced a remarkably large body of data. Studying all\
  \ available data has become known as \u2039big data\u203A analyses and promises\
  \ deep insights \u2013 in this case \u2013 to chondrule and matrix formation and\
  \ relationships. Looking at all data may also allow one to better understand the\
  \ mechanism of chondrule formation or, equally important, what information we might\
  \ be missing to identify this process. A database of all available chondrule and\
  \ matrix data further provides an overview and quick visualisation, which will not\
  \ only help to solve actual problems, but also enable students and future researchers\
  \ to quickly access and understand all we know about these components. We collected\
  \ all available data on elemental bulk chondrule and matrix compositions in a database\
  \ that we call ChondriteDB. The database also contains petrographic and petrologic\
  \ information on chondrules. Currently, ChondriteDB contains about 2388 chondrule\
  \ and 1064 matrix data from 70 different publications and 161 different chondrites.\
  \ Future iterations of ChondriteDB will include isotope data and information on\
  \ other chondrite components. Data quality is of critical importance. However, as\
  \ we discuss, quality is not an objective category, but a subjective judgement.\
  \ Quantifiable data acquisition categories are required that allow selecting the\
  \ appropriate data from a database in the context of a given research problem. We\
  \ provide a comprehensive overview on the contents of ChondriteDB. The database\
  \ is available as an Excel file upon request from the senior author of this paper,\
  \ or can be accessed through MetBase."
author: Dominik C. Hezel and Markus Harak and Guy Libourel
categories: Geophysics (Q1); Geochemistry and Petrology (Q2)
citable_docs._(3years): 118.0
cites_/_doc._(2years): 249.0
country: Germany
coverage: 1978-1990, 1993-2020
doi: 10.1016/j.chemer.2017.05.003
eigenfactor_score: .nan
h_index: 53.0
isbn: null
issn: 00092819
issn1: 00092819
issn2: 00092819
issn3: 00092819
jcr_value: null
keywords: Chondrules, Matrix, Elemental composition, ChondritedDB, Database
publisher_x: Elsevier GmbH
publisher_y: null
ref._/_doc.: 7944.0
region: Western Europe
scimago_value: 988.0
sjr_best_quartile: Q1
sourceid: 25163.0
title_bib: 'What we know about elemental bulk chondrule and matrix compositions: presenting
  the chondritedb database'
title_csv: Chemie der erde
total_cites: .nan
total_cites_(3years): 371.0
total_docs._(2020): 96.0
total_docs._(3years): 120.0
total_refs.: 7626.0
type: journal
type_publication: article
year: 2018
---
abstract: Celiac disease (CD) has been on the rise in the world and a large part of
  it remains undiagnosed. Novel methods are required to address the gaps in prompt
  detection and management. Artificial intelligence (AI) has seen an exponential surge
  in the last decade worldwide. With the advent of big data and powerful computational
  ability, we now have self-driving cars and smart devices in our daily lives. Huge
  databases in the form of electronic medical records and images have rendered healthcare
  a lucrative sector where AI can prove revolutionary. It is being used extensively
  to overcome the barriers in clinical workflows. From the perspective of a disease,
  it can be deployed in multiple steps i.e. screening tools, diagnosis, developing
  novel therapeutic agents, proposing management plans, and defining prognostic indicators,
  etc. We review the areas where it may augment physicians in the delivery of better
  healthcare by summarizing current literature on the use of AI in healthcare using
  CD as a model. We further outline major barriers to its large-scale implementations
  and prospects from the healthcare point of view.
author: Muhammad Khawar Sana and Zeshan M. Hussain and Pir Ahmad Shah and Muhammad
  Haisum Maqsood
categories: Computer Science Applications (Q1); Health Informatics (Q2)
citable_docs._(3years): 923.0
cites_/_doc._(2years): 559.0
country: United Kingdom
coverage: 1970-2020
doi: 10.1016/j.compbiomed.2020.103996
eigenfactor_score: 0.011859999999999999
h_index: 94.0
isbn: null
issn: 00104825
issn1: 00104825
issn2: '18790534'
issn3: 00104825
jcr_value: '4.589'
keywords: Artificial intelligence, Machine learning, Deep learning, Celiac disease,
  Systematic review
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5216.0
region: Western Europe
scimago_value: 884.0
sjr_best_quartile: Q1
sourceid: 17957.0
title_bib: Artificial intelligence in celiac disease
title_csv: Computers in biology and medicine
total_cites: 9751.0
total_cites_(3years): 5223.0
total_docs._(2020): 383.0
total_docs._(3years): 936.0
total_refs.: 19977.0
type: journal
type_publication: article
year: 2020
---
abstract: Studies have indicated that detection of circulating tumor DNA (ctDNA) prior
  to treatment is a negative prognostic marker in non-small cell lung cancer (NSCLC).
  ctDNA is currently identified by detection of tumor mutations. Commercial next-generation
  sequencing (NGS) assays for mutation analysis of ctDNA for routine practice usually
  include small gene panels and are not suitable for general mutation analysis. In
  this study, we investigated whether mutation analysis of cfDNA could be performed
  using a commercially available comprehensive NGS gene panel and bioinformatics workflow.
  Tumor DNA, plasma DNA and peripheral blood leukocyte DNA from 30 NSCLC patients
  were sequenced. In two patients (7%), tumor mutations in cfDNA were immediately
  called by the bioinformatic workflow. In 13 patients (43%), tumor mutations were
  not called, but were present in ctDNA and were identified based on the known tumor
  mutation profile. In the remaining 15 patients (50%), no concordant mutations were
  detected. In conclusion, we were able to identify tumor mutations in ctDNA from
  57% of NSCLC patients using a comprehensive gene panel. We demonstrated that sequencing
  paired tumor DNA was helpful to interpret data and confirm ctDNA, and thus increased
  the ratio of patients with detectable ctDNA. This approach might be feasible for
  mutation analysis of ctDNA in routine diagnostic practice, especially in case of
  suboptimal plasma quality and quantity.
author: "Anine Larsen Ottestad and Sissel G.F. Wahl and Bj\xF8rn Henning Gr\xF8nberg\
  \ and Frank Skorpen and Hong Yan Dai"
categories: Clinical Biochemistry (Q2); Pathology and Forensic Medicine (Q2); Molecular
  Biology (Q3)
citable_docs._(3years): 327.0
cites_/_doc._(2years): 306.0
country: United States
coverage: 1962-1995, 1997-2020
doi: 10.1016/j.yexmp.2019.104347
eigenfactor_score: 0.0036899999999999997
h_index: 68.0
isbn: null
issn: 00144800
issn1: '10960945'
issn2: 00144800
issn3: '10960945'
jcr_value: '3.362'
keywords: Non-small cell lung cancer (NSCLC), Next-generation sequencing (NGS), Circulating
  tumor DNA (ctDNA)
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 4805.0
region: Northern America
scimago_value: 791.0
sjr_best_quartile: Q2
sourceid: 13247.0
title_bib: The relevance of tumor mutation profiling in interpretation of ngs data
  from cell-free dna in non-small cell lung cancer patients
title_csv: Experimental and molecular pathology
total_cites: 4792.0
total_cites_(3years): 986.0
total_docs._(2020): 170.0
total_docs._(3years): 329.0
total_refs.: 8168.0
type: journal
type_publication: article
year: 2020
---
abstract: "Objective\nTo report the utilization, effectiveness, and safety of practices\
  \ in assisted reproductive technology (ART) globally in 2013 and assess global trends\
  \ over time.\nDesign\nRetrospective, cross-sectional survey on the utilization,\
  \ effectiveness, and safety of ART procedures performed globally during\_2013.\n\
  Setting\nSeventy-five countries and 2,639 ART clinics.\nPatient(s)\nWomen and men\
  \ undergoing ART procedures.\nIntervention(s)\nAll ART.\nMain Outcome Measure(s)\n\
  The ART cycles and outcomes on country-by-country, regional, and global levels.\
  \ Aggregate country data were processed and analyzed based on methods developed\
  \ by the International Committee for Monitoring Assisted Reproductive Technology\
  \ (ICMART).\nResult(s)\nA total of 1,858,500 ART cycles were conducted for the treatment\
  \ year 2013 across 2,639 clinics in 75 participating countries with a global participation\
  \ rate of 73.6%. Reported and estimated data suggest 1,160,474 embryo transfers\
  \ (ETs) were performed resulting in >344,317 babies. From 2012 to 2013, the number\
  \ of reported aspiration and frozen ET cycles increased by 3% and 16.4%, respectively.\
  \ The proportion of women aged >40 years undergoing nondonor ART increased from\
  \ 25.2% in 2012 to 26.3% in 2013. As a percentage of nondonor aspiration cycles,\
  \ intracytoplasmic sperm injection (ICSI) was similar to results for 2012. The in\_\
  vitro fertilization (IVF)/ICSI combined delivery rates per fresh aspiration and\
  \ frozen ET cycles were 24.2% and 22.8%, respectively. In fresh nondonor cycles,\
  \ single ET increased from 33.7% in 2012 to 36.5% in 2013, whereas the average number\
  \ of transferred embryos was 1.81\u2014again with wide country variation. The rate\
  \ of twin deliveries after fresh nondonor transfers was 17.9%; the triplet rate\
  \ was 0.7%. In frozen ET cycles performed in 2013, single ET was used in 57.6%,\
  \ with an average of 1.49 embryos transferred and twin and triplet rates of 10.8%\
  \ and 0.4%, respectively. The cumulative delivery rate per aspiration was 30.4%,\
  \ similar to that in 2012. Perinatal mortality rate per 1,000 births was 22.2% after\
  \ fresh IVF/ICSI and 16.8% after frozen ET. The data presented depended on the quality\
  \ and completeness of the data submitted by individual countries. This report covers\
  \ approximately two-thirds of world ART activity. Continued efforts to improve the\
  \ quality and consistency of reporting ART data by registries are still needed.\n\
  Conclusion(s)\nReported ART cycles, effectiveness, and safety increased between\
  \ 2012 and 2013 with adoption of a better method for estimating unreported cycles.\n\
  Comit\xE9 Internacional para la monitorizaci\xF3n de las Tecnolog\xEDas de Reproducci\xF3\
  n Asistida (ICMART): informe mundial.\nObjetivo\nInformar sobre la utilizaci\xF3\
  n, la eficacia y la seguridad de las pr\xE1cticas de la tecnolog\xEDa de reproducci\xF3\
  n asistida (TRA) a nivel mundial en 2013 y evaluar las tendencias mundiales a lo\
  \ largo del tiempo.\nDise\xF1o\nEncuesta retrospectiva y transversal sobre la utilizaci\xF3\
  n, la eficacia y la seguridad de los procedimientos de TRA realizados a nivel mundial\
  \ durante el a\xF1o 2013.\nEntorno\nSetenta y cinco pa\xEDses y 2639 cl\xEDnicas\
  \ de TRA.\nPacientes\nMujeres y hombres sometidos a procedimientos de TRA.\nIntervenci\xF3\
  n(es)\nTodas las TRA.\nMedida(s) principal(es) del resultado\nLos ciclos de TRA\
  \ y los resultados a nivel de pa\xEDs, regional y mundial. Los datos agregados de\
  \ los pa\xEDses se procesaron y analizaron seg\xFAn los m\xE9todos desarrollados\
  \ por el Comit\xE9 Internacional para la Vigilancia de las Tecnolog\xEDas de Reproducci\xF3\
  n Asistida (ICMART). Tecnolog\xEDa de Reproducci\xF3n Asistida (ICMART).\nResultados\n\
  En el a\xF1o de tratamiento 2013 se realizaron un total de 1.858.500 ciclos de TRA\
  \ en 2.639 cl\xEDnicas de 75 pa\xEDses participantes con una tasa de participaci\xF3\
  n global del 73,6%. Los datos informados y estimados sugieren que se realizaron\
  \ 1.160.474 transferencias de embriones (TE) que dieron lugar a >344.317 beb\xE9\
  s. De 2012 a 2013, el n\xFAmero de ciclos de aspiraci\xF3n y de TE congelados notificados\
  \ aument\xF3 un 3% y un 16,4%, respectivamente. La proporci\xF3n de mujeres de >40\
  \ a\xF1os que se sometieron a TRA aut\xF3loga aument\xF3 del 25,2% en 2012 al 26,3%\
  \ en 2013. Como porcentaje de ciclos de aspiraci\xF3n aut\xF3loga, la inyecci\xF3\
  n intracitoplasm\xE1tica de espermatozoides (ICSI) fue similar a los resultados\
  \ de 2012. Las tasas de parto combinadas de fecundaci\xF3n in vitro (FIV)/ICSI por\
  \ ciclos de aspiraci\xF3n en fresco y TE congelada fueron del 24,2% y el 22,8%,\
  \ respectivamente. En los ciclos aut\xF3logos en fresco,la TE \xFAnica aument\xF3\
  \ del 33,7% en 2012 al 36,5% en 2013, mientras que el n\xFAmero medio de embriones\
  \ transferidos fue de 1,81 -de nuevo, con una amplia variaci\xF3n entre pa\xEDses.\
  \ La tasa de partos gemelares tras transferencias aut\xF3logas en fresco fue del\
  \ 17,9%; la tasa de trillizos fue del 0,7%. En los ciclos de TE de congelados realizados\
  \ en 2013, se utiliz\xF3 la TE simple en el 57,6%, con una media de 1,49 embriones\
  \ transferidos y unas tasas de gemelos y trillizos del 10,8% y 0,4%, respectivamente.\
  \ La tasa acumulada de partos por aspiraci\xF3n fue del 30,4%, similar a la de 2012.\
  \ La tasa de mortalidad perinatal por cada 1.000 nacimientos fue del 22,2% tras\
  \ la FIV/ICSI en fresco y del 16,8% tras la TE de congelados. Los datos presentados\
  \ depend\xEDan de la calidad y la exhaustividad de los datos presentados por cada\
  \ pa\xEDs. Este informe abarca aproximadamente dos tercios de la actividad mundial\
  \ de TRA. Los esfuerzos continuos para mejorar la calidad y la coherencia de los\
  \ datos presentados por los registros sobre la TRA deben seguir siendo objeto de\
  \ esfuerzos continuos.\nConclusi\xF3n(es)\nLos ciclos de TRA notificados, la eficacia\
  \ y la seguridad aumentaron entre 2012 y 2013 con la adopci\xF3n de un mejor m\xE9\
  todo para estimar los ciclos no reportados."
author: Manish Banker and Silke Dyer and Georgina M. Chambers and Osamu Ishihara and
  Markus Kupka and Jacques {de Mouzon} and Fernando Zegers-Hochschild and G. David
  Adamson
categories: Obstetrics and Gynecology (Q1); Reproductive Medicine (Q1)
citable_docs._(3years): 883.0
cites_/_doc._(2years): 371.0
country: United States
coverage: 1950-2020
doi: 10.1016/j.fertnstert.2021.03.039
eigenfactor_score: 0.03305
h_index: 208.0
isbn: null
issn: 00150282
issn1: '15565653'
issn2: 00150282
issn3: '15565653'
jcr_value: '7.329'
keywords: Assisted reproductive technology, IVF/ICSI outcome, frozen embryo transfer,
  ICMART, cumulative live birth rate, registry
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2386.0
region: Northern America
scimago_value: 2272.0
sjr_best_quartile: Q1
sourceid: 12705.0
title_bib: 'International committee for monitoring assisted reproductive technologies
  (icmart): world report on assisted reproductive technologies, 2013'
title_csv: Fertility and sterility
total_cites: 45818.0
total_cites_(3years): 5714.0
total_docs._(2020): 502.0
total_docs._(3years): 1277.0
total_refs.: 11976.0
type: journal
type_publication: article
year: 2021
---
abstract: "ABSTRACT\nBackground and aims\nPublicly available databases containing\
  \ colonoscopic imaging data are valuable resources for artificial intelligence (AI)\
  \ research. Currently, little is known regarding the available number and content\
  \ of these databases. This review aimed to describe the availability, accessibility\
  \ and usability of publicly available colonoscopic imaging databases, focusing on\
  \ polyp detection, polyp characterization and quality of colonoscopy.\nMethods\n\
  A systematic literature search was performed in MEDLINE and Embase to identify AI-studies\
  \ describing publicly available colonoscopic imaging datasets published after 2010.\
  \ Second, a targeted search using Google\u2019s Dataset Search, Google Search, GitHub\
  \ and Figshare was done to identify datasets directly. Datasets were included if\
  \ they contained data about polyp detection, polyp characterization or quality of\
  \ colonoscopy. To assess accessibility of datasets the following categories were\
  \ defined: open access, open access with barriers and regulated access. To assess\
  \ the potential usability of the included datasets, essential details of each dataset\
  \ were extracted using a checklist derived from the CLAIM-checklist.\nResults\n\
  We identified 22 datasets with open access, 3 datasets open access with barriers\
  \ and 15 datasets with regulated access. The 22 open access databases containing\
  \ 19,463 images and 952 videos. Nineteen of these databases focused on polyp detection,\
  \ localization and/or segmentation, six on polyp characterization and three on quality\
  \ of colonoscopy. Only half of these databases have been used by other researcher\
  \ to develop, train or benchmark their AI-system. Although technical details were\
  \ in general well-reported, important details such as polyp and patient demographics\
  \ and the annotation process were underreported in almost all databases.\nConclusion\n\
  This review provides greater insight on public availability of colonoscopic imaging\
  \ databases for AI-research. Incomplete reporting of important details limits the\
  \ ability of researchers to assess the usability of the current databases."
author: Britt B.S.L. Houwen and Karlijn J. Nass and Jasper L.A. Vleugels and Paul
  Fockens and Yark Hazewinkel and Evelien Dekker
categories: Gastroenterology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)
citable_docs._(3years): 963.0
cites_/_doc._(2years): 347.0
country: United States
coverage: 1965-2020
doi: 10.1016/j.gie.2022.08.043
eigenfactor_score: 0.030869999999999998
h_index: 200.0
isbn: null
issn: '00165107'
issn1: '10976779'
issn2: '00165107'
issn3: '10976779'
jcr_value: '9.427'
keywords: artificial intelligence, machine learning, colonoscopy, colorectal cancer,
  colorectal polyps
publisher_x: Mosby Inc.
publisher_y: null
ref._/_doc.: 1731.0
region: Northern America
scimago_value: 2365.0
sjr_best_quartile: Q1
sourceid: 28350.0
title_bib: 'Comprehensive review of publicly available colonoscopic imaging databases
  for artificial intelligence research: availability, accessibility and usability'
title_csv: Gastrointestinal endoscopy
total_cites: 28955.0
total_cites_(3years): 5062.0
total_docs._(2020): 580.0
total_docs._(3years): 1496.0
total_refs.: 10041.0
type: journal
type_publication: article
year: 2022
---
abstract: The efficiency of attribute reduction is one of the important challenges
  being faced in the field of Big Data processing. Although many quick attribute reduction
  algorithms have been proposed, they are tightly coupled with their corresponding
  indiscernibility relations, and it is difficult to extend specific acceleration
  policies to other reduction models. In this paper, we propose a generalized indiscernibility
  reduction model(GIRM) and a concept of the granular structure in GIRM, which is
  a quantitative measurement induced from multiple indiscernibility relations and
  which can be used to represent the computation cost of varied models. Then, we prove
  that our GIRM is compatible with three typical reduction models. Based on the proposed
  GIRM, we present a generalized attribute reduction algorithm and a generalized positive
  region computing algorithm. We perform a quantitative analysis of the computation
  complexities of two algorithms using the granular structure. For the generalized
  attribute reduction, we present systematic acceleration policies that can reduce
  the computational domain and optimize the computation of the positive region. Based
  on the granular structure, we propose acceleration policies for the computation
  of the generalized positive region, and we also propose fast positive region computation
  approaches for three typical reduction models. Experimental results for various
  datasets prove the efficiency of our acceleration policies in those three typical
  reduction models.
author: Fan Jing and Jiang Yunliang and Liu Yong
categories: Artificial Intelligence (Q1); Computer Science Applications (Q1); Control
  and Systems Engineering (Q1); Information Systems and Management (Q1); Software
  (Q1); Theoretical Computer Science (Q1)
citable_docs._(3years): 2168.0
cites_/_doc._(2years): 789.0
country: United States
coverage: 1968-2021
doi: 10.1016/j.ins.2017.02.032
eigenfactor_score: 0.04908
h_index: 184.0
isbn: null
issn: '00200255'
issn1: '00200255'
issn2: '00200255'
issn3: '00200255'
jcr_value: '6.795'
keywords: Generalized indiscernibility relation, Attribute reduction, Granular structure,
  Acceleration policy
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 4311.0
region: Northern America
scimago_value: 1524.0
sjr_best_quartile: Q1
sourceid: 15134.0
title_bib: Quick attribute reduction with generalized indiscernibility models
title_csv: Information sciences
total_cites: 44038.0
total_cites_(3years): 17554.0
total_docs._(2020): 928.0
total_docs._(3years): 2184.0
total_refs.: 40007.0
type: journal
type_publication: article
year: 2017
---
abstract: 'ObjectivesL There is a need for monitoring dental health and healthcare,
  as support for quality development, allocation of resources and long-term planning
  of dental care. The aim of this paper is to describe the concept and implementation
  of the Swedish Quality Registry for Caries and Periodontal Diseases (SKaPa). Materials
  and methods: The SKaPa receives information by automatic transfer of data daily
  from electronic patient dental records via secure connections from affiliated dental
  care organisations (DCOs). The registry stores information about DCOs, dental professionals
  and patients. Information on a patient level includes personal identifier, gender,
  age, living area, dental status, risk assessments for caries and periodontitis,
  and dental care provided. In addition, data generated from a global question on
  patient-perceived oral health are uploaded. In total, more than 400 variables are
  transferred to the registry and updated daily. Results: In 2018, all of the 21 public
  DCOs and the largest private DCO in Sweden were affiliated to SKaPa, representing
  a total of 1,089 public and 234 private dental clinics. The accumulated amount of
  information on dental healthcare covers 6.9 million individuals out of the total
  Swedish population of 10 million. SKaPa produces reports on de-identified data,
  both cross-sectional and longitudinal. Conclusion: As a nationwide registry based
  on automatic retrieval of data directly from patient records, SKaPa offers the basis
  for a new era of systematic evaluation of oral health and quality of dental care.
  The registry supports clinical and epidemiological research, data mining and external
  validation of results from randomised controlled trials'
author: "Inger {von B\xFCltzingsl\xF6wen} and Hans \xD6stholm and Lars Gahnberg and\
  \ Dan Ericson and Jan L. Wennstr\xF6m and J\xF6rgen Paulander"
categories: Dentistry (miscellaneous) (Q1)
citable_docs._(3years): 176.0
cites_/_doc._(2years): 216.0
country: United States
coverage: 1960-1962, 1965-2020
doi: 10.1111/idj.12481
eigenfactor_score: 0.0016699999999999998
h_index: 64.0
isbn: null
issn: 00206539
issn1: 00206539
issn2: 00206539
issn3: 00206539
jcr_value: '2.512'
keywords: Odontology, epidemiology, oral health, big data, quality registry
publisher_x: Wiley-Blackwell
publisher_y: null
ref._/_doc.: 3749.0
region: Northern America
scimago_value: 840.0
sjr_best_quartile: Q1
sourceid: 25609.0
title_bib: "Swedish quality registry for caries and periodontal diseases \u2013 a\
  \ framework for quality development in dentistry"
title_csv: International dental journal
total_cites: 3063.0
total_cites_(3years): 512.0
total_docs._(2020): 81.0
total_docs._(3years): 204.0
total_refs.: 3037.0
type: journal
type_publication: article
year: 2019
---
abstract: "The process of cleaning motion capture data of aberrant points has been\
  \ described as \u201Cthe bane of motion capture operators\u201D. Yet, managing the\
  \ high volume kinematic data generated through in-home neurogames requires data\
  \ quality control that, executed insufficiently, jeopardizes accuracy of outcomes.\
  \ To begin to address this issue at the intersection of biomechanics and \u201C\
  big data\u201D, we performed a secondary analysis of a neurogame, evaluating gesture\
  \ count as well as shoulder and elbow joint angle outcomes calculated from kinematic\
  \ data in which valid gestures were identified through 3 methods: visual review\
  \ of regions of interest by an expert (BP); manufacturer-recommended data smoothing\
  \ (MS); and automated methods (AI). We hypothesized that upper extremity kinematic\
  \ outcomes from BP would be matched by AI but not MS methods. From one person with\
  \ post-stroke hemiparesis, upper-extremity kinematic data were collected for 6\_\
  days over 2\_weeks using a Microsoft Kinect\u2122-based neurogame. We calculated\
  \ gesture count, shoulder angle, and elbow angle outcomes from data managed using\
  \ BP, MS, and AI methods. BP identified 1929 valid gestures total over 6\_days which\
  \ was different than the other two methods (p\_=\_0.0015). In contrast, the AI algorithm\
  \ with best precision identified 4372 and MS identified 4459 valid gestures. Furthermore,\
  \ angle outcomes calculated from AI and MS methods resulted in different values\
  \ than BP (p\_<\_0.001 for 5 of 6 variables). More research is needed to automate\
  \ treatment of high volume, low quality motion data to support investigation of\
  \ motion associated with in-home rehabilitation neurogames."
author: Lise C. Worthen-Chaudhari and Michael P. McNally and Akshay Deshpande and
  Vivek Bakaraju
categories: Rehabilitation (Q1); Biomedical Engineering (Q2); Biophysics (Q2); Orthopedics
  and Sports Medicine (Q2); Sports Science (Q2)
citable_docs._(3years): 1361.0
cites_/_doc._(2years): 266.0
country: United Kingdom
coverage: 1968-2020
doi: 10.1016/j.jbiomech.2020.109726
eigenfactor_score: 0.018619999999999998
h_index: 199.0
isbn: null
issn: 00219290
issn1: '18732380'
issn2: 00219290
issn3: '18732380'
jcr_value: '2.712'
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 3796.0
region: Western Europe
scimago_value: 826.0
sjr_best_quartile: Q1
sourceid: 15846.0
title_bib: 'In-home neurogaming: demonstrating the impact of valid gesture recognition
  method on high volume kinematic outcomes'
title_csv: Journal of biomechanics
total_cites: 35320.0
total_cites_(3years): 3948.0
total_docs._(2020): 510.0
total_docs._(3years): 1383.0
total_refs.: 19358.0
type: journal
type_publication: article
year: 2020
---
abstract: It is well known that processing big graph data can be costly on Cloud.
  Processing big graph data introduces complex and multiple iterations that raise
  challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To
  tackle the challenges, we propose a novel technique for effectively processing big
  graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal
  features on Cloud. By exploring spatial data correlation, we partition a graph data
  set into clusters. In a cluster, the workload can be shared by the inference based
  on time series similarity. By exploiting temporal correlation, in each time series
  or a single graph edge, temporal data compression is conducted. A novel data driven
  scheduling is also developed for data processing optimisation. The experiment results
  demonstrate that the spatiotemporal compression and scheduling achieve significant
  performance gains in terms of data size and data fidelity loss.
author: Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and
  Kotagiri Ramamohanarao and Jinjun Chen
categories: Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer
  Networks and Communications (Q2); Theoretical Computer Science (Q2)
citable_docs._(3years): 245.0
cites_/_doc._(2years): 187.0
country: United States
coverage: 1967-2021
doi: 10.1016/j.jcss.2014.04.022
eigenfactor_score: 0.00268
h_index: 99.0
isbn: null
issn: '00220000'
issn1: '10902724'
issn2: '00220000'
issn3: '10902724'
jcr_value: '1.023'
keywords: Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 3535.0
region: Northern America
scimago_value: 573.0
sjr_best_quartile: Q2
sourceid: 12370.0
title_bib: A spatiotemporal compression based approach for efficient big data processing
  on cloud
title_csv: Journal of computer and system sciences
total_cites: 4516.0
total_cites_(3years): 568.0
total_docs._(2020): 46.0
total_docs._(3years): 252.0
total_refs.: 1626.0
type: journal
type_publication: article
year: 2014
---
abstract: Crises provide an opportunity for the field to take stock, as do the articles
  in this special issue. Constructive advice for 21st century publication standards
  includes appropriate theory, internal validity, and external validity. First, well-grounded
  theory can produce a priori plausibility, testable logic, and a focus on the ideas
  involved, all cumulatively informed by meta-analysis across studies. Second, internal
  validity benefits from both exploratory work and confirmatory analyses on well-powered
  samples that require systematic detection and principled decisions about data quality.
  Inferences benefit from manipulated mediation analysis and from careful interpretation
  without over-claiming. Finally, external validity profits from a variety of exact
  and conceptual replications, best evaluated by meta-analysis.
author: Susan T. Fiske
categories: Social Psychology (Q1); Sociology and Political Science (Q1)
citable_docs._(3years): 435.0
cites_/_doc._(2years): 337.0
country: United States
coverage: 1965-2020
doi: 10.1016/j.jesp.2016.01.006
eigenfactor_score: 0.01439
h_index: 142.0
isbn: null
issn: '00221031'
issn1: '00221031'
issn2: '10960465'
issn3: '00221031'
jcr_value: '3.603'
keywords: Experiments, Replicability, Internal validity, External validity, Theory
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 6022.0
region: Northern America
scimago_value: 2401.0
sjr_best_quartile: Q1
sourceid: 15499.0
title_bib: How to publish rigorous experiments in the 21st century
title_csv: Journal of experimental social psychology
total_cites: 15958.0
total_cites_(3years): 1800.0
total_docs._(2020): 110.0
total_docs._(3years): 437.0
total_refs.: 6624.0
type: journal
type_publication: article
year: 2016
---
abstract: "Urban waterlogging often causes urban disasters, and the rapid early warning\
  \ and comprehensive analysis of the urban waterlogging can help disaster defenses.\
  \ However, the warning of waterlogging through the monitoring data cannot give grid\
  \ distribution and the forecast of hydrological models cannot ensure rapid early\
  \ warning. To obtain a grid rapid early warning result for a region, like an urban\
  \ area, a method needs to be proposed which can meet the above problems. In this\
  \ research, AutoML (automatic machine learning based on genetic algorithm) was recommended\
  \ to construct the rapid early warning and comprehensive analysis models for urban\
  \ waterlogging by compared with the other three machine learning algorithms, CatBoost\
  \ (Categorical Boosting), XGBoost (eXtreme Gradient Boosting), and BPDNN (Back Propagation\
  \ Deep Learning Neural Network). In the models, the forecast and historical precipitation\
  \ obtained from the Integrated Nowcasting through Comprehensive analysis system\
  \ (INCA), the difference of elevation, and the urban waterlogging risk maps provided\
  \ by Tianjin Meteorological Administration were employed as the input sources. The\
  \ input precipitation duration was determined as 12\_h based on the sensitivity\
  \ analysis of the influence of various precipitation duration on waterlogging depths.\
  \ Due to the non-digital (discrete dataset) features, the urban waterlogging risk\
  \ maps were transformed to the weight of each corresponding risk level according\
  \ to the area of each risk level and the number of samples falling in each risk\
  \ level. The difference of elevation was characterized by the average elevations\
  \ of various distances from the points of concern. The output waterlogging depths\
  \ were compared with the waterlogging depths monitored in Tianjin, China, whose\
  \ quality was controlled by eliminating the records of the waterlogging depths lasting\
  \ for a long time after the end of rainfall. The comparison of the models constructed\
  \ by different methods demonstrated that the AutoML performed better (NSE and R2\_\
  >\_0.92, CC\_>\_0.95, RMSE1.1\u20131.9\_cm) than the other three models. The forecast\
  \ waterlogging depths by AutoML was also coherent with the monitoring waterlogging\
  \ depths (NSE and R2\_\u2265\_0.9, CC\_\u2265\_0.95, RMSE 1.7\u20132.2\_cm). For\
  \ that local topography and waterlogging risk are considered, the AutoML models\
  \ can be used in the area without the monitoring of water level, quickly predict\
  \ waterlogging depths and give spatial grid results for rapidly early warning."
author: Yuchen Guo and Lihong Quan and Lili Song and Hao Liang
categories: Water Science and Technology (Q1)
citable_docs._(3years): 2563.0
cites_/_doc._(2years): 576.0
country: Netherlands
coverage: 1949, 1963-2020
doi: 10.1016/j.jhydrol.2021.127367
eigenfactor_score: 0.04972
h_index: 226.0
isbn: null
issn: 00221694
issn1: 00221694
issn2: 00221694
issn3: 00221694
jcr_value: '5.722'
keywords: Urban waterlogging, Automatic machine learning algorithm based on genetic
  algorithms, Rapid early warning, XGBoost, CatBoost, BPDNN, Comprehensive analysis
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6550.0
region: Western Europe
scimago_value: 1684.0
sjr_best_quartile: Q1
sourceid: 50089.0
title_bib: Construction of rapid early warning and comprehensive analysis models for
  urban waterlogging based on automl and comparison of the other three machine learning
  algorithms
title_csv: Journal of hydrology
total_cites: 73620.0
total_cites_(3years): 15250.0
total_docs._(2020): 1336.0
total_docs._(3years): 2587.0
total_refs.: 87508.0
type: journal
type_publication: article
year: 2022
---
abstract: "Aims\nThe accurate identification of mothers at risk of postpartum psychiatric\
  \ admission would allow for preventive intervention or more timely admission. We\
  \ developed a prediction model to identify women at risk of postpartum psychiatric\
  \ admission.\nMethods\nData included administrative health data of all inpatient\
  \ live births in the Australian state of Queensland between January 2009 and October\
  \ 2014. Analyses were restricted to mothers with one or more indicator of mental\
  \ health problems during pregnancy (n\_=\_75,054 births). The predictors included\
  \ all maternal data up to and including the delivery, and neonatal data recorded\
  \ at delivery. We used multiple machine learning methods to predict hospital admission\
  \ in the 12 months following delivery in which the primary diagnosis was recorded\
  \ as an ICD-10 psychotic, bipolar or depressive disorders.\nResults\nThe boosted\
  \ trees algorithm produced the best performing model, predicting postpartum psychiatric\
  \ admission in the validation data with good discrimination [AUC\_=\_0.80; 95% CI\
  \ = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark\
  \ logistic regression model and an elastic net model. In addition to indicators\
  \ of maternal metal health history, maternal and neonatal anthropometric measures\
  \ and social/lifestyle factors were strong predictors.\nConclusion\nOur results\
  \ indicate the potential of a big data approach when aiming to identify mothers\
  \ at risk of postpartum psychiatric admission. Mothers at risk could be followed-up\
  \ and supported after neonatal discharge to either remove the need for admission\
  \ or facilitate more timely admission."
author: Kim S. Betts and Steve Kisely and Rosa Alati
categories: Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)
citable_docs._(3years): 755.0
cites_/_doc._(2years): 441.0
country: United Kingdom
coverage: 1961-1982, 1984-2020
doi: 10.1016/j.jpsychires.2020.07.002
eigenfactor_score: 0.020030000000000003
h_index: 136.0
isbn: null
issn: 00223956
issn1: '18791379'
issn2: 00223956
issn3: '18791379'
jcr_value: '4.791'
keywords: Administrative data linkage, Postpartum psychiatric admissions, Predictive
  models, Machine learning
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5173.0
region: Western Europe
scimago_value: 1875.0
sjr_best_quartile: Q1
sourceid: 16812.0
title_bib: Predicting postpartum psychiatric admission using a machine learning approach
title_csv: Journal of psychiatric research
total_cites: 20371.0
total_cites_(3years): 3624.0
total_docs._(2020): 357.0
total_docs._(3years): 776.0
total_refs.: 18469.0
type: journal
type_publication: article
year: 2020
---
abstract: "The paper examines the opportunities in and possibilities arising from\
  \ big data in retailing, particularly along five major data dimensions\u2014data\
  \ pertaining to customers, products, time, (geo-spatial) location and channel. Much\
  \ of the increase in data quality and application possibilities comes from a mix\
  \ of new data sources, a smart application of statistical tools and domain knowledge\
  \ combined with theoretical insights. The importance of theory in guiding any systematic\
  \ search for answers to retailing questions, as well as for streamlining analysis\
  \ remains undiminished, even as the role of big data and predictive analytics in\
  \ retailing is set to rise in importance, aided by newer sources of data and large-scale\
  \ correlational techniques. The Statistical issues discussed include a particular\
  \ focus on the relevance and uses of Bayesian analysis techniques (data borrowing,\
  \ updating, augmentation and hierarchical modeling), predictive analytics using\
  \ big data and a field experiment, all in a retailing context. Finally, the ethical\
  \ and privacy issues that may arise from the use of big data in retailing are also\
  \ highlighted."
author: Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti
categories: Marketing (Q1)
citable_docs._(3years): 87.0
cites_/_doc._(2years): 451.0
country: United Kingdom
coverage: 1993-2020
doi: 10.1016/j.jretai.2016.12.004
eigenfactor_score: 0.00458
h_index: 136.0
isbn: null
issn: 00224359
issn1: 00224359
issn2: 00224359
issn3: 00224359
jcr_value: '5.245'
keywords: Big data, Predictive analytics, Retailing, Pricing
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6532.0
region: Western Europe
scimago_value: 3184.0
sjr_best_quartile: Q1
sourceid: 22990.0
title_bib: The role of big data and predictive analytics in retailing
title_csv: Journal of retailing
total_cites: 10594.0
total_cites_(3years): 839.0
total_docs._(2020): 66.0
total_docs._(3years): 98.0
total_refs.: 4311.0
type: journal
type_publication: article
year: 2017
---
abstract: There is considerable scientific and societal concern about plastic pollution,
  which has resulted in citizen science projects to study the scale of the issue.
  Citizen science is a cost-effective way to gather data over a large geographical
  range while simultaneously raising public awareness on the problem. Because the
  experiences of researchers involved in these projects are not yet adequately covered,
  this paper presents the findings from ten semi-structured qualitative interviews
  with researchers leading a citizen science project on micro- or macroplastics. Our
  results show it is important to specify the goal(s) of the project and that expertise
  on communication and data science is needed. Furthermore, simple protocols, quality
  control, and engagement with volunteers and the public are key elements for successful
  projects. From these results, a framework with recommendations was drafted, which
  can be used by anyone who wants to develop or improve citizen science projects.
author: Liselotte Rambonnet and Suzanne C. Vink and Anne M. Land-Zandstra and Thijs
  Bosker
categories: Aquatic Science (Q1); Oceanography (Q1); Pollution (Q1)
citable_docs._(3years): 2512.0
cites_/_doc._(2years): 554.0
country: United Kingdom
coverage: 1970-2020
doi: 10.1016/j.marpolbul.2019.05.056
eigenfactor_score: 0.03696
h_index: 179.0
isbn: null
issn: 0025326X
issn1: 0025326X
issn2: '18793363'
issn3: 0025326X
jcr_value: '5.553'
keywords: Citizen science, Microplastics, Macroplastics, Global, Plastic pollution
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 6041.0
region: Western Europe
scimago_value: 1548.0
sjr_best_quartile: Q1
sourceid: 24024.0
title_bib: 'Making citizen science count: best practices and challenges of citizen
  science projects on plastics in aquatic environments'
title_csv: Marine pollution bulletin
total_cites: 48810.0
total_cites_(3years): 14671.0
total_docs._(2020): 1039.0
total_docs._(3years): 2537.0
total_refs.: 62770.0
type: journal
type_publication: article
year: 2019
---
abstract: "The upcoming Fluorescence Explorer (FLEX) satellite mission aims to provide\
  \ high quality radiometric measurements for subsequent retrieval of sun-induced\
  \ chlorophyll fluorescence (SIF). The combination of SIF with other observations\
  \ stemming from the FLEX/Sentinel-3 tandem mission holds the potential to assess\
  \ complex ecosystem processes. The calibration and validation (cal/val) of these\
  \ radiometric measurements and derived products are central but challenging components\
  \ of the mission. This contribution outlines strategies for the assessment of in\
  \ situ radiometric measurements and retrieved SIF. We demonstrate how in situ spectrometer\
  \ measurements can be analysed in terms of radiometric, spectral and spatial uncertainties.\
  \ The analysis of more than 200\_k spectra yields an average bias between two radiometric\
  \ measurements by two individual spectrometers of 8%, with a larger variability\
  \ in measurements of downwelling radiance (25%) compared to upwelling radiance (6%).\
  \ Spectral shifts in the spectrometer relevant for SIF retrievals are consistently\
  \ below 1 spectral pixel (up to 0.75). Found spectral shifts appear to be mostly\
  \ dependent on temperature (as measured by a temperature probe in the instrument).\
  \ Retrieved SIF shows a low variability of 1.8% compared with a noise reduced SIF\
  \ estimate based on APAR. A combination of airborne imaging and in situ non-imaging\
  \ fluorescence spectroscopy highlights the importance of a homogenous sampling surface\
  \ and holds the potential to further uncover SIF retrieval issues as here shown\
  \ for early evening acquisitions. Our experiments clearly indicate the need for\
  \ careful site selection, measurement protocols, as well as the need for harmonized\
  \ processing. This work thus contributes to guiding cal/val activities for the upcoming\
  \ FLEX mission."
author: Bastian Buman and Andreas Hueni and Roberto Colombo and Sergio Cogliati and
  Marco Celesti and Tommaso Julitta and Andreas Burkart and Bastian Siegmann and Uwe
  Rascher and Matthias Drusch and Alexander Damm
categories: Computers in Earth Sciences (Q1); Geology (Q1); Soil Science (Q1)
citable_docs._(3years): 1374.0
cites_/_doc._(2years): 1069.0
country: United States
coverage: 1969-2020
doi: 10.1016/j.rse.2022.112984
eigenfactor_score: 0.0534
h_index: 281.0
isbn: null
issn: '00344257'
issn1: '00344257'
issn2: '00344257'
issn3: '00344257'
jcr_value: '10.164'
keywords: Sun-induced chlorophyll fluorescence, Spectroradiometer, Uncertainty, Bias,
  Measurement variability, Spectral shift, FLEX, FloX
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 7598.0
region: Northern America
scimago_value: 3611.0
sjr_best_quartile: Q1
sourceid: 12503.0
title_bib: Towards consistent assessments of in situ radiometric measurements for
  the validation of fluorescence satellite missions
title_csv: Remote sensing of environment
total_cites: 73581.0
total_cites_(3years): 16246.0
total_docs._(2020): 496.0
total_docs._(3years): 1374.0
total_refs.: 37684.0
type: journal
type_publication: article
year: 2022
---
abstract: "Has the rise of data-intensive science, or \u2018big data\u2019, revolutionized\
  \ our ability to predict? Does it imply a new priority for prediction over causal\
  \ understanding, and a diminished role for theory and human experts? I examine four\
  \ important cases where prediction is desirable: political elections, the weather,\
  \ GDP, and the results of interventions suggested by economic experiments. These\
  \ cases suggest caution. Although big data methods are indeed very useful sometimes,\
  \ in this paper's cases they improve predictions either limitedly or not at all,\
  \ and their prospects of doing so in the future are limited too."
author: Robert Northcott
categories: History (Q1); History and Philosophy of Science (Q1)
citable_docs._(3years): 134.0
cites_/_doc._(2years): 122.0
country: United Kingdom
coverage: 1970-1971, 1974-1978, 1980-1981, 1983-1986, 1988, 1990-2020
doi: 10.1016/j.shpsa.2019.09.002
eigenfactor_score: .nan
h_index: 37.0
isbn: null
issn: 00393681
issn1: 00393681
issn2: '18792510'
issn3: 00393681
jcr_value: null
keywords: Big data, Prediction, Case studies, Explanation, Elections, Weather
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5317.0
region: Western Europe
scimago_value: 615.0
sjr_best_quartile: Q1
sourceid: 11600154632.0
title_bib: 'Big data and prediction: four case studies'
title_csv: Studies in history and philosophy of science part a
total_cites: .nan
total_cites_(3years): 229.0
total_docs._(2020): 95.0
total_docs._(3years): 150.0
total_refs.: 5051.0
type: journal
type_publication: article
year: 2020
---
abstract: "The term big data has been popularized over the past decade and is often\
  \ used to refer to data sets that are too large or complex to be analyzed by traditional\
  \ means. Although the term has been utilized for some time in business and engineering,\
  \ the concept of big data is relatively new to medicine. The reception from the\
  \ medical community has been mixed; however, the widespread utilization of electronic\
  \ health records in the United States, the creation of large clinical data sets\
  \ and national registries that capture information on numerous vectors affecting\
  \ healthcare delivery and patient outcomes, and the sequencing of the human genome\
  \ are all opportunities to leverage big data. This review was inspired by a lively\
  \ panel discussion on big data that took place at the 75th Central Surgical Association\
  \ Annual Meeting. The authors\u2019 aim was to describe big data, the methodologies\
  \ used to analyze big data, and their practical clinical application."
author: Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo
categories: Surgery (Q1)
citable_docs._(3years): 1108.0
cites_/_doc._(2years): 240.0
country: United States
coverage: 1937-2020
doi: 10.1016/j.surg.2018.06.022
eigenfactor_score: 0.02493
h_index: 162.0
isbn: null
issn: 00396060
issn1: '15327361'
issn2: 00396060
issn3: '15327361'
jcr_value: '3.982'
keywords: null
publisher_x: Mosby Inc.
publisher_y: null
ref._/_doc.: 2205.0
region: Northern America
scimago_value: 1532.0
sjr_best_quartile: Q1
sourceid: 22305.0
title_bib: 'Big data: more than big data sets'
title_csv: Surgery
total_cites: 25223.0
total_cites_(3years): 4267.0
total_docs._(2020): 487.0
total_docs._(3years): 1520.0
total_refs.: 10740.0
type: journal
type_publication: article
year: 2018
---
abstract: "Large population-based health administrative databases, clinical registries,\
  \ and data linkage systems are a rapidly expanding resource for health research.\
  \ Ophthalmic research has benefited from the use of these databases in expanding\
  \ the breadth of knowledge in areas such as disease surveillance, disease etiology,\
  \ health services utilization, and health outcomes. Furthermore, the quantity of\
  \ data available for research has increased exponentially in recent times, particularly\
  \ as e-health initiatives come online in health systems across the globe. We review\
  \ some big data concepts, the databases and data linkage systems used in eye research\u2014\
  including their advantages and limitations, the types of studies previously undertaken,\
  \ and the future direction for big data in eye research."
author: Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens
categories: Ophthalmology (Q1)
citable_docs._(3years): 199.0
cites_/_doc._(2years): 475.0
country: United States
coverage: 1956-1970, 1972-2020
doi: 10.1016/j.survophthal.2016.01.003
eigenfactor_score: 0.00487
h_index: 132.0
isbn: null
issn: 00396257
issn1: 00396257
issn2: '18793304'
issn3: 00396257
jcr_value: '6.048'
keywords: data linkage, clinical registry, health services research, ophthalmic epidemiology,
  big data
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 10132.0
region: Northern America
scimago_value: 2131.0
sjr_best_quartile: Q1
sourceid: 20107.0
title_bib: Big data and ophthalmic research
title_csv: Survey of ophthalmology
total_cites: 7296.0
total_cites_(3years): 1131.0
total_docs._(2020): 69.0
total_docs._(3years): 235.0
total_refs.: 6991.0
type: journal
type_publication: article
year: 2016
---
abstract: "Digital twin technology has a huge potential for widespread applications\
  \ in different industrial sectors such as infrastructure, aerospace, and automotive.\
  \ However, practical adoptions of this technology have been slower, mainly due to\
  \ a lack of application-specific details. Here we focus on a digital twin framework\
  \ for linear single-degree-of-freedom structural dynamic systems evolving in two\
  \ different operational time scales in addition to its intrinsic dynamic time-scale.\
  \ Our approach strategically separates into two components \u2013 (a) a physics-based\
  \ nominal model for data processing and response predictions, and (b) a data-driven\
  \ machine learning model for the time-evolution of the system parameters. The physics-based\
  \ nominal model is system-specific and selected based on the problem under consideration.\
  \ On the other hand, the data-driven machine learning model is generic. For tracking\
  \ the multi-timescale evolution of the system parameters, we propose to exploit\
  \ a mixture of experts as the data-driven model. Within the mixture of experts model,\
  \ Gaussian Process (GP) is used as the expert model. The primary idea is to let\
  \ each expert track the evolution of the system parameters at a single time-scale.\
  \ For learning the hyperparameters of the \u2018mixture of experts using GP\u2019\
  , an efficient framework that exploits expectation-maximization and sequential Monte\
  \ Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale\
  \ dynamical system with stiffness and/or mass variations. The digital twin is found\
  \ to be robust and yields reasonably accurate results. One exciting feature of the\
  \ proposed digital twin is its capability to provide reasonable predictions at future\
  \ time-steps. Aspects related to the data quality and data quantity are also investigated."
author: S. Chakraborty and S. Adhikari
categories: Civil and Structural Engineering (Q1); Computer Science Applications (Q1);
  Materials Science (miscellaneous) (Q1); Mechanical Engineering (Q1); Modeling and
  Simulation (Q1)
citable_docs._(3years): 465.0
cites_/_doc._(2years): 487.0
country: United Kingdom
coverage: 1971-2021
doi: 10.1016/j.compstruc.2020.106410
eigenfactor_score: .nan
h_index: 138.0
isbn: null
issn: 00457949
issn1: 00457949
issn2: 00457949
issn3: 00457949
jcr_value: null
keywords: Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process,
  Frequency
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4954.0
region: Western Europe
scimago_value: 1450.0
sjr_best_quartile: Q1
sourceid: 18171.0
title_bib: Machine learning based digital twin for dynamical systems with multiple
  time-scales
title_csv: Computers and structures
total_cites: .nan
total_cites_(3years): 2256.0
total_docs._(2020): 135.0
total_docs._(3years): 469.0
total_refs.: 6688.0
type: journal
type_publication: article
year: 2021
---
abstract: "Summary\nEvidence-based pathology advocates using a combination of best\
  \ available data (\u201Cevidence\u201D) from the literature and personal experience\
  \ for the diagnosis, estimation of prognosis, and assessment of other variables\
  \ that impact individual patient care. Evidence-based pathology relies on systematic\
  \ reviews of the literature, evaluation of the quality of evidence as categorized\
  \ by evidence levels and statistical tools such as meta-analyses, estimates of probabilities\
  \ and odds, and others. However, it is well known that previously \u201Cstatistically\
  \ significant\u201D information usually does not accurately forecast the future\
  \ for individual patients. There is great interest in \u201Ccognitive computing\u201D\
  \ in which \u201Cdata mining\u201D is combined with \u201Cpredictive analytics\u201D\
  \ designed to forecast future events and estimate the strength of those predictions.\
  \ This study demonstrates the use of IBM Watson Analytics software to evaluate and\
  \ predict the prognosis of 101 patients with typical and atypical pulmonary carcinoid\
  \ tumors in which Ki-67 indices have been determined. The results obtained with\
  \ this system are compared with those previously reported using \u201Croutine\u201D\
  \ statistical software and the help of a professional statistician. IBM Watson Analytics\
  \ interactively provides statistical results that are comparable to those obtained\
  \ with routine statistical tools but much more rapidly, with considerably less effort\
  \ and with interactive graphics that are intuitively easy to apply. It also enables\
  \ analysis of natural language variables and yields detailed survival predictions\
  \ for patient subgroups selected by the user. Potential applications of this tool\
  \ and basic concepts of cognitive computing are discussed."
author: Alberto M. Marchevsky and Ann E. Walts and Mark R. Wick
categories: Pathology and Forensic Medicine (Q1)
citable_docs._(3years): 838.0
cites_/_doc._(2years): 293.0
country: United Kingdom
coverage: 1970-2020
doi: 10.1016/j.humpath.2016.09.002
eigenfactor_score: 0.01257
h_index: 139.0
isbn: null
issn: 00468177
issn1: '15328392'
issn2: 00468177
issn3: '15328392'
jcr_value: '3.466'
keywords: Evidence-based, Pathology, Cognitive computing, Predictive analytics, IBM
  Watson
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 4179.0
region: Western Europe
scimago_value: 1213.0
sjr_best_quartile: Q1
sourceid: 14024.0
title_bib: 'Evidence-based pathology in its second decade: toward probabilistic cognitive
  computing'
title_csv: Human pathology
total_cites: 15964.0
total_cites_(3years): 2727.0
total_docs._(2020): 137.0
total_docs._(3years): 929.0
total_refs.: 5725.0
type: journal
type_publication: article
year: 2017
---
abstract: Big data, high dimensional data, sparse data, large scale data, and imaging
  data are all becoming new frontiers of statistics. Changing technologies have created
  this flood and have led to a real hunger for new modeling strategies and data analysis
  by scientists. In many cases data are not Euclidean; for example, in molecular biology,
  the data sit on manifolds. Even in a simple non-Euclidean manifold (circle), to
  summarize angles by the arithmetic average cannot make sense and so more care is
  needed. Thus non-Euclidean settings throw up many major challenges, both mathematical
  and statistical. This paper will focus on the PCA and clustering methods for some
  manifolds. Of course, the PCA and clustering methods in multivariate analysis are
  one of the core topics. We basically deal with two key manifolds from a practical
  point of view, namely spheres and tori. It is well known that dimension reduction
  on non-Euclidean manifolds with PCA-like methods has been a challenging task for
  quite some time but recently there has been some breakthrough. One of them is the
  idea of nested spheres and another is transforming a torus into a sphere effectively
  and subsequently use the technology of nested spheres PCA. We also provide a new
  method of clustering for multivariate analysis which has a fundamental property
  required for molecular biology that penalizes wrong assignments to avoid chemically
  no go areas. We give various examples to illustrate these methods. One of the important
  examples includes dealing with COVID-19 data.
author: Kanti V. Mardia and Henrik Wiechers and Benjamin Eltzner and Stephan F. Huckemann
categories: Numerical Analysis (Q1); Statistics and Probability (Q1); Statistics,
  Probability and Uncertainty (Q1)
citable_docs._(3years): 378.0
cites_/_doc._(2years): 190.0
country: United States
coverage: 1971-2021
doi: 10.1016/j.jmva.2021.104862
eigenfactor_score: 0.00836
h_index: 80.0
isbn: null
issn: 0047259X
issn1: 0047259X
issn2: '10957243'
issn3: 0047259X
jcr_value: '1.473'
keywords: Adaptive linkage clustering, Circular mode hunting, Dimension reduction,
  Multivariate wrapped normal, SARS-CoV-2 geometry, Stratified spheres, Torus PCA
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 3264.0
region: Northern America
scimago_value: 1283.0
sjr_best_quartile: Q1
sourceid: 23947.0
title_bib: Principal component analysis and clustering on manifolds
title_csv: Journal of multivariate analysis
total_cites: 6401.0
total_cites_(3years): 786.0
total_docs._(2020): 84.0
total_docs._(3years): 382.0
total_refs.: 2742.0
type: journal
type_publication: article
year: 2022
---
abstract: "Remote sensing image products (e.g. brightness of nighttime lights and\
  \ land cover/land use types) have been widely used to disaggregate census data to\
  \ produce gridded population maps for large geographic areas. The advent of the\
  \ geospatial big data revolution has created additional opportunities to map population\
  \ distributions at fine resolutions with high accuracy. A considerable proportion\
  \ of the geospatial data contains semantic information that indicates different\
  \ categories of human activities occurring at exact geographic locations. Such information\
  \ is often lacking in remote sensing data. In addition, the remarkable progress\
  \ in machine learning provides toolkits for demographers to model complex nonlinear\
  \ correlations between population and heterogeneous geographic covariates. In this\
  \ study, a typical type of geospatial big data, points-of-interest (POIs), was combined\
  \ with multi-source remote sensing data in a random forests model to disaggregate\
  \ the 2010 county-level census population data to 100\u202F\xD7\u202F100\u202Fm\
  \ grids. Compared with the WorldPop population dataset, our population map showed\
  \ higher accuracy. The root mean square error for population estimates in Beijing,\
  \ Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and\
  \ 34,193, respectively. The large under-allocation of the population in urban areas\
  \ and over-allocation in rural areas in the WorldPop dataset was greatly reduced\
  \ in this new population map. Apart from revealing the effectiveness of POIs in\
  \ improving population mapping, this study promises the potential of geospatial\
  \ big data for mapping other socioeconomic parameters in the future."
author: Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping
  Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and
  Peng Jia
categories: Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution
  (Q1); Waste Management and Disposal (Q1)
citable_docs._(3years): 13125.0
cites_/_doc._(2years): 796.0
country: Netherlands
coverage: 1970, 1972-2021
doi: 10.1016/j.scitotenv.2018.12.276
eigenfactor_score: 0.23081999999999997
h_index: 244.0
isbn: null
issn: 00489697
issn1: 00489697
issn2: '18791026'
issn3: 00489697
jcr_value: '7.963'
keywords: Points of interest, Population, Random forests, Nighttime light, China
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6581.0
region: Western Europe
scimago_value: 1795.0
sjr_best_quartile: Q1
sourceid: 25349.0
title_bib: Improved population mapping for china using remotely sensed and points-of-interest
  data within a random forests model
title_csv: Science of the total environment
total_cites: 210143.0
total_cites_(3years): 106837.0
total_docs._(2020): 6929.0
total_docs._(3years): 13278.0
total_refs.: 455970.0
type: journal
type_publication: article
year: 2019
---
abstract: The term big data is currently a buzzword in social science, however its
  precise meaning is ambiguous. In this paper we focus on administrative data which
  is a distinctive form of big data. Exciting new opportunities for social science
  research will be afforded by new administrative data resources, but these are currently
  under appreciated by the research community. The central aim of this paper is to
  discuss the challenges associated with administrative data. We emphasise that it
  is critical for researchers to carefully consider how administrative data has been
  produced. We conclude that administrative datasets have the potential to contribute
  to the development of high-quality and impactful social science research, and should
  not be overlooked in the emerging field of big data.
author: Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben
categories: Education (Q1); Sociology and Political Science (Q1)
citable_docs._(3years): 398.0
cites_/_doc._(2years): 228.0
country: United States
coverage: 1972-2020
doi: 10.1016/j.ssresearch.2016.04.015
eigenfactor_score: 0.00899
h_index: 89.0
isbn: null
issn: 0049089X
issn1: '10960317'
issn2: 0049089X
issn3: '10960317'
jcr_value: '2.322'
keywords: Big data, Administrative data, Data management, Data quality, Data access
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 7303.0
region: Northern America
scimago_value: 1042.0
sjr_best_quartile: Q1
sourceid: 26441.0
title_bib: The role of administrative data in the big data revolution in social science
  research
title_csv: Social science research
total_cites: 6976.0
total_cites_(3years): 1123.0
total_docs._(2020): 80.0
total_docs._(3years): 398.0
total_refs.: 5842.0
type: journal
type_publication: article
year: 2016
---
abstract: Head-mounted eye tracking is a new method that allows researchers to catch
  a glimpse of what infants and children see during naturalistic activities. In this
  chapter, we review how mobile, wearable eye trackers improve the construct validity
  of important developmental constructs, such as visual object experiences and social
  attention, in ways that would be impossible using screen-based eye tracking. Head-mounted
  eye tracking improves ecological validity by allowing researchers to present more
  realistic and complex visual scenes, create more interactive experimental situations,
  and examine how the body influences what infants and children see. As with any new
  method, there are difficulties to overcome. Accordingly, we identify what aspects
  of head-mounted eye-tracking study design affect the measurement quality, interpretability
  of the results, and efficiency of gathering data. Moreover, we provide a summary
  of best practices aimed at allowing researchers to make well-informed decisions
  about whether and how to apply head-mounted eye tracking to their own research questions.
author: John M. Franchak and Chen Yu
categories: Developmental and Educational Psychology (Q2); Pediatrics, Perinatology
  and Child Health (Q2); Behavioral Neuroscience (Q3)
citable_docs._(3years): 8.0
cites_/_doc._(2years): 181.0
country: United States
coverage: 1964-1965, 1967, 1969-1976, 1978-1980, 1982, 1984-1985, 1987, 1989, 1991,
  1993-1994, 1996, 1999, 2001-2020
doi: 10.1016/bs.acdb.2021.11.001
eigenfactor_score: 0.0011300000000000001
h_index: 41.0
isbn: null
issn: '00652407'
issn1: '00652407'
issn2: '00652407'
issn3: '00652407'
jcr_value: '2.182'
keywords: Eye movements, Head-mounted eye tracking, Mobile eye tracking, Ecological
  validity, Perceptual-motor development, Joint attention, Language development, Computer
  vision, Social attention
publisher_x: Academic Press Inc.
publisher_y: JAI
ref._/_doc.: 9344.0
region: Northern America
scimago_value: 767.0
sjr_best_quartile: Q2
sourceid: 29439.0
title_bib: 'Chapter three - beyond screen time: using head-mounted eye tracking to
  study natural behavior'
title_csv: Advances in child development and behavior
total_cites: 1055.0
total_cites_(3years): 106.0
total_docs._(2020): 18.0
total_docs._(3years): 55.0
total_refs.: 1682.0
type: journal
type_publication: incollection
year: 2022
---
abstract: "Registries are essential for health infrastructure planning, benchmarking,\
  \ continuous quality improvement, hypothesis generation, and real-world trials.\
  \ To date, data from these registries have predominantly been analyzed in isolated\
  \ \u201Csilos,\u201D hampering efforts to analyze \u201Cbig data\u201D at the international\
  \ level, an approach that provides wide-ranging benefits, including enhanced statistical\
  \ power, an ability to conduct international comparisons, and greater capacity to\
  \ study rare diseases. This review serves as a valuable resource to clinicians,\
  \ researchers, and policymakers, by comprehensively describing kidney failure registries\
  \ active in 2021, before proposing approaches for inter-registry research under\
  \ current conditions, as well as solutions to enhance global capacity for data collaboration.\
  \ We identified 79 kidney-failure registries spanning 77 countries worldwide. International\
  \ Society of Nephrology exemplar initiatives, including the Global Kidney Health\
  \ Atlas and Sharing Expertise to support the set-up of Renal Registries (SharE-RR),\
  \ continue to raise awareness regarding international healthcare disparities and\
  \ support the development of universal kidney-disease registries. Current barriers\
  \ to inter-registry collaboration include underrepresentation of lower-income countries,\
  \ poor syntactic and semantic interoperability, absence of clear consensus guidelines\
  \ for healthcare data sharing, and limited researcher incentives. This review represents\
  \ a call to action for international stakeholders to enact systemic change that\
  \ will harmonize the current fragmented approaches to kidney-failure registry data\
  \ collection and research."
author: "Monica S.Y. Ng and Vivek Charu and David W. Johnson and Michelle M. O\u2019\
  Shaughnessy and Andrew J. Mallett"
categories: Nephrology (Q1)
citable_docs._(3years): 749.0
cites_/_doc._(2years): 485.0
country: United States
coverage: 1972-2020
doi: 10.1016/j.kint.2021.09.024
eigenfactor_score: 0.03902
h_index: 276.0
isbn: null
issn: 00852538
issn1: 00852538
issn2: '15231755'
issn3: 00852538
jcr_value: '10.612'
keywords: data sharing, dialysis, inter-registry collaboration, kidney failure, registry,
  transplantation
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2681.0
region: Northern America
scimago_value: 3499.0
sjr_best_quartile: Q1
sourceid: 19992.0
title_bib: 'National and international kidney failure registries: characteristics,
  commonalities, and contrasts'
title_csv: Kidney international
total_cites: 52616.0
total_cites_(3years): 6461.0
total_docs._(2020): 470.0
total_docs._(3years): 1251.0
total_refs.: 12600.0
type: journal
type_publication: article
year: 2022
---
abstract: "Oncology is undergoing a data-driven metamorphosis. Armed with new and\
  \ ever more efficient molecular and information technologies, we have entered an\
  \ era where data is helping us spearhead the fight against cancer. This technology\
  \ driven data explosion, often referred to as \u201Cbig data\u201D, is not only\
  \ expediting biomedical discovery, but it is also rapidly transforming the practice\
  \ of oncology into an information science. This evolution is critical, as results\
  \ to-date have revealed the immense complexity and genetic heterogeneity of patients\
  \ and their tumors, a sobering reminder of the challenge facing every patient and\
  \ their oncologist. This can only be addressed through development of clinico-molecular\
  \ data analytics that provide a deeper understanding of the mechanisms controlling\
  \ the biological and clinical response to available therapeutic options. Beyond\
  \ the exciting implications for improved patient care, such advancements in predictive\
  \ and evidence-based analytics stand to profoundly affect the processes of cancer\
  \ drug discovery and associated clinical trials."
author: Guillaume Taglang and David B. Jackson
categories: Obstetrics and Gynecology (Q1); Oncology (Q1)
citable_docs._(3years): 1087.0
cites_/_doc._(2years): 441.0
country: United States
coverage: 1972-2020
doi: 10.1016/j.ygyno.2016.02.022
eigenfactor_score: 0.02767
h_index: 162.0
isbn: null
issn: 00908258
issn1: 00908258
issn2: '10956859'
issn3: 00908258
jcr_value: '5.482'
keywords: Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 3224.0
region: Northern America
scimago_value: 2105.0
sjr_best_quartile: Q1
sourceid: 27467.0
title_bib: "Use of \u201Cbig data\u201D in drug discovery and clinical trials"
title_csv: Gynecologic oncology
total_cites: 29012.0
total_cites_(3years): 5359.0
total_docs._(2020): 480.0
total_docs._(3years): 1150.0
total_refs.: 15477.0
type: journal
type_publication: article
year: 2016
---
abstract: Pediatric cancer is a rare disease with a low annual incidence, which presents
  a significant challenge in being able to collect enough data to fuel clinical discoveries.
  Big data registry trials hold promise to advance the study of pediatric cancers
  by allowing for the combination of traditional randomized controlled trials with
  the power of larger cohort sizes. The emergence of big data resources and data-sharing
  initiatives are becoming transformative for pediatric cancer diagnosis and treatment.
  This review discusses the uses of big data in pediatric cancer, existing pediatric
  cancer registry initiatives and research, the challenges in harmonizing these data
  to improve accessibility for study, and building pediatric data commons and other
  important future endeavors.
author: Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum
categories: Hematology (Q1); Oncology (Q1)
citable_docs._(3years): 124.0
cites_/_doc._(2years): 421.0
country: United Kingdom
coverage: 1974-2020
doi: 10.1053/j.seminoncol.2020.02.006
eigenfactor_score: 0.00455
h_index: 133.0
isbn: null
issn: 00937754
issn1: 00937754
issn2: '15328708'
issn3: 00937754
jcr_value: '4.929'
keywords: Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science,
  Informatics
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 5822.0
region: Western Europe
scimago_value: 1812.0
sjr_best_quartile: Q1
sourceid: 13120.0
title_bib: 'Using big data in pediatric oncology: current applications and future
  directions'
title_csv: Seminars in oncology
total_cites: 5713.0
total_cites_(3years): 690.0
total_docs._(2020): 51.0
total_docs._(3years): 150.0
total_refs.: 2969.0
type: journal
type_publication: article
year: 2020
---
abstract: Knowing who is an expert on social media is a challenging yet important
  task, especially in a world where misleading information is commonplace and where
  social media is an important information source for knowledge seekers. In this paper
  we investigate expertise heuristics by comparing features of experts versus non-experts
  in big data settings. We employ a large set of features to classify experts and
  non-experts using data collected on two social media platform (Twitter and reddit).
  Our results show a good ability to predict who is an expert, especially using language-based
  features, validating that heuristics can be developed to differentiate experts from
  novices organically, based on social media use. Our results contribute to the development
  of expertise location and identification systems as well as our understanding on
  how experts present themselves on social media.
author: Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel
categories: Computer Networks and Communications (Q1); Management Information Systems
  (Q1)
citable_docs._(3years): 76.0
cites_/_doc._(2years): 170.0
country: United States
coverage: 1969-2020
doi: 10.1145/3353401.3353406
eigenfactor_score: 0.0005200000000000001
h_index: 57.0
isbn: null
issn: 00950033
issn1: 00950033
issn2: '15320936'
issn3: 00950033
jcr_value: '1.828'
keywords: social media, data analytics., expertise location
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 6808.0
region: Northern America
scimago_value: 797.0
sjr_best_quartile: Q1
sourceid: 4900152206.0
title_bib: 'Recognizing experts on social media: a heuristics-based approach'
title_csv: Data base for advances in information systems
total_cites: 772.0
total_cites_(3years): 193.0
total_docs._(2020): 26.0
total_docs._(3years): 87.0
total_refs.: 1770.0
type: journal
type_publication: article
year: 2019
---
abstract: "Abstract\nA positional quality control method based on the application\
  \ of the International Standard ISO 2859 is proposed. This entails a common framework\
  \ for dealing with the control of all other spatial data quality components (e.g.,\
  \ completeness, consistency, etc.). We propose a relationship between the parameters\
  \ \u201Cacceptable quality level\u201D and \u201Climiting quality\u201D of the international\
  \ standard and positional quality by means of observed error models. This proposal\
  \ does not require any assumption for positional errors (e.g., normality), which\
  \ means that the application is universal. It can be applied to any type of positional\
  \ and geometric controls (points, line-strings), to any dimension (1D, 2D, 3D, etc.)\
  \ and with parametric or non-parametric error models (e.g., lidar). This paper introduces\
  \ ISO 2859, presents the statistical bases of the proposal and develops two examples\
  \ of application, the first dealing with a lot-by-lot control and the second, isolated\
  \ lot control."
author: "F.J. Ariza-L\xF3pez and J. Rodr\xEDguez-Avi"
categories: Computers in Earth Sciences (Q3)
citable_docs._(3years): 178.0
cites_/_doc._(2years): 110.0
country: United States
coverage: 1975-2020
doi: 10.14358/PERS.81.8.657
eigenfactor_score: 0.00235
h_index: 127.0
isbn: null
issn: 00991112
issn1: 00991112
issn2: 00991112
issn3: 00991112
jcr_value: '1.083'
keywords: null
publisher_x: American Society for Photogrammetry and Remote Sensing
publisher_y: null
ref._/_doc.: 2815.0
region: Northern America
scimago_value: 483.0
sjr_best_quartile: Q3
sourceid: 12366.0
title_bib: Using international standards to control the positional quality of spatial
  data
title_csv: Photogrammetric engineering and remote sensing
total_cites: 7415.0
total_cites_(3years): 294.0
total_docs._(2020): 13.0
total_docs._(3years): 240.0
total_refs.: 366.0
type: journal
type_publication: article
year: 2015
---
abstract: "Modern Chinese cities are defined from the administrative view and classified\
  \ into several administrative categories, which makes it inconsistent between Chinese\
  \ cities and their counterparts in western countries. Without easy access to fine-scale\
  \ data, researchers have to rely heavily on statistical and aggregated indicators\
  \ available in officially released yearbooks, to understand Chinese city system.\
  \ Not to mention the data quality of yearbooks, it is problematic that a large number\
  \ of towns or downtown areas of counties are not addressed in yearbooks. To address\
  \ this issue, as a following study of Long et\_al. (2016), we have redefined the\
  \ Chinese city system, using percolation theory in the light of newly emerging big/open\
  \ data. In this paper, we propose our alternative definition of a city with road/street\
  \ junctions, and present the methodology for extracting city system for the whole\
  \ country with national wide road junctions. A city is defined as \u201Ca spatial\
  \ cluster with a minimum of 100 road/street junctions within a 300\_m distance threshold\u201D\
  . Totally we identify 4629 redefined cities with a total urban area of 64,144\_\
  km2 for the whole China. We observe total city number increases from 2273 in 2009\
  \ to 4629 in 2014. We find that expanded urban area during 2009 and 2014, comparing\
  \ with urban areas in 2009 are associated with 73.3% road junction density, 25.3%\
  \ POI density and 5.5% online comment density. In addition, we benchmark our results\
  \ with the conventional Chinese city system by using yearbooks."
author: Ying Long
categories: Environmental Science (miscellaneous) (Q1); Forestry (Q1); Geography,
  Planning and Development (Q1); Tourism, Leisure and Hospitality Management (Q1)
citable_docs._(3years): 495.0
cites_/_doc._(2years): 440.0
country: Netherlands
coverage: 1980-2020
doi: 10.1016/j.apgeog.2016.08.002
eigenfactor_score: 0.00938
h_index: 99.0
isbn: null
issn: 01436228
issn1: 01436228
issn2: 01436228
issn3: 01436228
jcr_value: '4.240'
keywords: Urban morphology, Urban function, Human activity, Street network, City evolution
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6422.0
region: Western Europe
scimago_value: 1165.0
sjr_best_quartile: Q1
sourceid: 27471.0
title_bib: Redefining chinese city system with emerging new data
title_csv: Applied geography
total_cites: 10205.0
total_cites_(3years): 2353.0
total_docs._(2020): 156.0
total_docs._(3years): 497.0
total_refs.: 10019.0
type: journal
type_publication: article
year: 2016
---
abstract: 'Purpose

  Interest in leveraging real-world evidence (RWE) to support regulatory decision
  making for product effectiveness has been increasing globally as evident by the
  increasing number of regulatory frameworks and guidance documents. However, acceptance
  of RWE, especially before marketing for regulatory approval, differs across countries.
  In addition, guidance on the design and conduct of innovative clinical trials, such
  as randomized controlled registry studies, pragmatic trials, and other hybrid studies,
  is lacking.

  Methods

  We assessed the global regulatory environment with regard to RWE based on regional
  availability of the following 3 key regulatory elements: (1) RWE regulatory framework,
  (2) data quality and standards guidance. and (3) study methods guidance.

  Findings

  This article reviews the available frameworks and existing guidance from across
  the globe and discusses the observed gaps and opportunities for further development
  and harmonization.

  Implications

  Cross-country collaborations are encouraged to further shape and align RWE policies
  and help establish frameworks in countries without current policies with the goal
  of creating efficiencies when considering RWE to support regulatory decision-making
  globally.'
author: Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian
  and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell
categories: Pharmacology (Q2); Pharmacology (medical) (Q2)
citable_docs._(3years): 587.0
cites_/_doc._(2years): 233.0
country: United States
coverage: 1977-2020
doi: 10.1016/j.clinthera.2022.01.012
eigenfactor_score: 0.009640000000000001
h_index: 134.0
isbn: null
issn: 01492918
issn1: 1879114X
issn2: 01492918
issn3: 1879114X
jcr_value: '3.393'
keywords: Efficiency, Product effectiveness, Real-world evidence, Regulatory decision
  making
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 4049.0
region: Northern America
scimago_value: 925.0
sjr_best_quartile: Q2
sourceid: 29712.0
title_bib: 'Real-world evidence for regulatory decision-making: guidance from around
  the world'
title_csv: Clinical therapeutics
total_cites: 9397.0
total_cites_(3years): 1766.0
total_docs._(2020): 223.0
total_docs._(3years): 672.0
total_refs.: 9030.0
type: journal
type_publication: article
year: 2022
---
abstract: In this position paper we argue that the availability of "big" monitoring
  data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling
  approaches by violating their fundamental assumptions. However, big data alsobrings
  unique opportunities in its wake by enabling new modeling and analytics approaches
  as well as facilitating novel applications. We highlight a few key challenges andopportunities,
  and outline research directions for addressing them. To provide a proper context,
  we also summarize CPS modeling approaches, and discuss how modeling and analytics
  for CPS differs from general purpose IT systems.
author: Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru
  and Chen, Haifeng and Jiang, Guofei
categories: Computer Networks and Communications (Q3); Software (Q3); Hardware and
  Architecture (Q4)
citable_docs._(3years): 301.0
cites_/_doc._(2years): 104.0
country: United States
coverage: 1980, 1982, 1984, 1986-1989, 1994, 1996-2020
doi: 10.1145/2627534.2627558
eigenfactor_score: .nan
h_index: 80.0
isbn: null
issn: 01635999
issn1: 01635999
issn2: 01635999
issn3: 01635999
jcr_value: null
keywords: null
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 785.0
region: Northern America
scimago_value: 223.0
sjr_best_quartile: Q3
sourceid: 26742.0
title_bib: Modeling and analytics for cyber-physical systems in the age of big data
title_csv: Performance evaluation review
total_cites: .nan
total_cites_(3years): 337.0
total_docs._(2020): 72.0
total_docs._(3years): 323.0
total_refs.: 565.0
type: journal
type_publication: article
year: 2014
---
abstract: "Multi-signal joint reconstruction is critical in distributed compressed\
  \ sensing (DCS). We propose a joint reconstruction algorithm for subspace pursuit\
  \ of residual correlation steps to balance reconstruction efficiency and data quality.\
  \ The algorithm improves on two aspects. Firstly, the residuals are used as the\
  \ step feedback condition to achieve dynamic step adjustment, reducing the iteration\u2019\
  s overall number. Secondly, based on the mixed-support set model (MSM), the residual\
  \ non-decreasing surplus condition is set to reconstruct the common and the innovation\
  \ components of the target signal in groups, which reduces the mixing error in joint\
  \ reconstruction. This paper compares the performance of six algorithms of the same\
  \ type under the conditions of Gaussian sparse signal and measured shock wave field\
  \ sensor network data. The results show that the proposed algorithm can effectively\
  \ reduce the number of measurements required for reconstruction and improve efficiency\
  \ while maintaining accuracy."
author: Mingchi Ju and Man Zhao and Tailin Han and Hong Liu and Bo Xu and Xuan Liu
categories: Computer Vision and Pattern Recognition (Q1); Control and Systems Engineering
  (Q1); Electrical and Electronic Engineering (Q1); Signal Processing (Q1); Software
  (Q1)
citable_docs._(3years): 1057.0
cites_/_doc._(2years): 530.0
country: Netherlands
coverage: 1979-2021
doi: 10.1016/j.sigpro.2022.108747
eigenfactor_score: 0.01737
h_index: 136.0
isbn: null
issn: 01651684
issn1: 01651684
issn2: 01651684
issn3: 01651684
jcr_value: '4.662'
keywords: Distributed compressed sensing, Sparse reconstruction, Mixed-support set
  model
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4035.0
region: Western Europe
scimago_value: 907.0
sjr_best_quartile: Q1
sourceid: 25548.0
title_bib: A novel subspace pursuit of residual correlation step algorithm for distributed
  compressed sensing
title_csv: Signal processing
total_cites: 15960.0
total_cites_(3years): 5800.0
total_docs._(2020): 415.0
total_docs._(3years): 1060.0
total_refs.: 16747.0
type: journal
type_publication: article
year: 2023
---
abstract: "3D printing (3DP) is a progressive technology capable of transforming pharmaceutical\
  \ development. However, despite its promising advantages, its transition into clinical\
  \ settings remains slow. To make the vital leap to mainstream clinical practice\
  \ and improve patient care, 3DP must harness modern technologies. Machine learning\
  \ (ML), an influential branch of artificial intelligence, may be a key partner for\
  \ 3DP. Together, 3DP and ML can utilise intelligence based on human learning to\
  \ accelerate drug product development, ensure stringent quality control (QC), and\
  \ inspire innovative dosage-form design. With ML\u2019s capabilities, streamlined\
  \ 3DP drug delivery could mark the next era of personalised medicine. This review\
  \ details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly,\
  \ how it can expedite 3DP\u2019s integration into mainstream healthcare."
author: Moe Elbadawi and Laura E. McCoubrey and Francesca K.H. Gavins and Jun J. Ong
  and Alvaro Goyanes and Simon Gaisford and Abdul W. Basit
categories: Pharmacology (Q1); Toxicology (Q1)
citable_docs._(3years): 276.0
cites_/_doc._(2years): 993.0
country: United Kingdom
coverage: 1979-2020
doi: 10.1016/j.tips.2021.06.002
eigenfactor_score: 0.01488
h_index: 218.0
isbn: null
issn: '01656147'
issn1: '18733735'
issn2: '01656147'
issn3: '18733735'
jcr_value: '14.819'
keywords: additive manufacturing, 3D Printed drug products and formulations, Industry
  4.0 and digital health, personalized oral drug delivery systems and medical devices,
  biomedical engineering and pharmaceutical sciences, translational pharmaceutics
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6851.0
region: Western Europe
scimago_value: 3878.0
sjr_best_quartile: Q1
sourceid: 22321.0
title_bib: Disrupting 3d printing of medicines with machine learning
title_csv: Trends in pharmacological sciences
total_cites: 15308.0
total_cites_(3years): 3178.0
total_docs._(2020): 101.0
total_docs._(3years): 288.0
total_refs.: 6920.0
type: journal
type_publication: article
year: 2021
---
abstract: Increasing supply chain complexity poses new challenges to managers. On
  the other hand, evolving information and communication technology offers ample opportunity
  for more reliable supply chain management practices. Event processing has established
  itself in many applications in logistics. Although the topic has enjoyed increasing
  popularity, there is no study taking stock of prior developments and guiding future
  research. Therefore, a systematic literature review on the topic of event processing
  in supply chain management from 2005 until the present is undertaken. Extant literature
  is synthesized and analyzed from technological and supply chain management perspectives
  to inform scholars and practitioners of existing field developments. Additionally,
  to guide future scholarly endeavors, a research agenda is derived from promising
  topics raised in papers and unfulfilled practical requirements. We find that current
  solutions primarily focus on a limited number of supply chain core processes and
  a restricted number of supply chain actors. The majority of publications focused
  on time-temperature sensitive products. Additionally, the domination of road transportation
  can be observed, while other modes of transport are often ignored in solution implementations.
  Decision support in terms of object traceability within the supply chain is found
  in most articles. RFID, typically accompanied by the Electronic Product Code Information
  Services standard, is the dominant enabling technology. Future research should focus
  on the topics of standardization, granularity, data sources, and cooperation. Moreover,
  holistic event processing supported by big data and machine learning techniques
  could create interfaces with other legacy business intelligence applications. Another
  promising area includes the exploration of new technologies, i.e. IoT, to enable
  new smart solutions.
author: "Iurii Konovalenko and Andr\xE9 Ludwig"
categories: Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)
citable_docs._(3years): 319.0
cites_/_doc._(2years): 996.0
country: Netherlands
coverage: 1979-2020
doi: 10.1016/j.compind.2018.12.009
eigenfactor_score: 0.00584
h_index: 100.0
isbn: null
issn: '01663615'
issn1: '01663615'
issn2: '01663615'
issn3: '01663615'
jcr_value: '7.635'
keywords: Logistics, Supply chain event management, Event processing, Decision support,
  Literature review, Research agenda
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6023.0
region: Western Europe
scimago_value: 1432.0
sjr_best_quartile: Q1
sourceid: 19080.0
title_bib: "Event processing in supply chain management \u2013 the status quo and\
  \ research outlook"
title_csv: Computers in industry
total_cites: 6018.0
total_cites_(3years): 3165.0
total_docs._(2020): 115.0
total_docs._(3years): 323.0
total_refs.: 6926.0
type: journal
type_publication: article
year: 2019
---
abstract: "Data collection is an important process in the life cycle of big data processing.\
  \ It is the key part that must be completed first in all kinds of data applications,\
  \ which determines the results of data analysis and application service quality.\
  \ However, untrusted data sources and transmission links expose the data collection\
  \ process to attacks and malicious threats such as counterfeiting, replay, and denial\
  \ of service, and ultimately lead to untrustworthy data. In order to cope with the\
  \ threat of data collection process and ensure data quality, this paper proposes\
  \ trust evaluation scheme for data security collection based on wireless sensor\
  \ network, one of the data collection applications, including direct trust, recommendation\
  \ trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic\
  \ update of the trust of the data sources, a true data discovery and trust dynamic\
  \ update mechanism based on \u03C9-FCM (Weight Fuzzy C-Mean) algorithm is proposed.\
  \ The results of a large number of simulation experiments show that the proposed\
  \ scheme, model and algorithm can effectively evaluate the trust of data sources\
  \ and ensure the authenticity of the collected data."
author: Denglong Lv and Shibing Zhu
categories: Computer Science (miscellaneous) (Q1); Law (Q1)
citable_docs._(3years): 550.0
cites_/_doc._(2years): 675.0
country: United Kingdom
coverage: 1982-2020
doi: 10.1016/j.cose.2020.101937
eigenfactor_score: .nan
h_index: 92.0
isbn: null
issn: 01674048
issn1: 01674048
issn2: 01674048
issn3: 01674048
jcr_value: null
keywords: Big data collection, Trust evaluation, Trust model, True data discovery,
  Wireless sensor network
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5360.0
region: Western Europe
scimago_value: 861.0
sjr_best_quartile: Q1
sourceid: 28898.0
title_bib: Achieving secure big data collection based on trust evaluation and true
  data discovery
title_csv: Computers and security
total_cites: .nan
total_cites_(3years): 3843.0
total_docs._(2020): 321.0
total_docs._(3years): 559.0
total_refs.: 17204.0
type: journal
type_publication: article
year: 2020
---
abstract: 'Background and objectives

  Previous research has documented the role of different categories of psychosocial
  factors (i.e., sociodemographic factors, personality, subjective life circumstances,
  activity, physical health, and childhood circumstances) in predicting subjective
  well-being and quality of life among older adults. No previous study has simultaneously
  modeled a large number of these psychosocial factors using a well-powered sample
  and machine learning algorithms to predict quality of life, happiness, and life
  satisfaction among older adults. The aim of this paper was to investigate the correlates
  of quality of life, happiness, and life satisfaction among European adults older
  than 50 years using machine learning techniques.

  Research design and methods

  Data drawn from the Survey of Health, Ageing and Retirement in Europe (SHARE) Wave
  7 were used. Participants were 62,500 persons aged 50 years and over living in 26
  Continental EU Member States, Switzerland, and Israel. Multiple machine learning
  regression approaches were used.

  Results

  The algorithms captured 53%, 33%, and 18% of the variance of quality of life, life
  satisfaction, and happiness, respectively. The most important categories of correlates
  of quality of life and life satisfaction were physical health and subjective life
  circumstances. Sociodemographic factors (mostly country of residence) and psychological
  variables were the most important categories of correlates of happiness.

  Discussion and implications

  This study highlights subjective poverty, self-perceived health, country of residence,
  subjective survival probability, and personality factors (especially neuroticism)
  as important correlates of quality of life, happiness, and life satisfaction. These
  findings provide evidence-based recommendations for practice and/or policy implications.'
author: Gabriele Prati
categories: Gerontology (Q1); Health (social science) (Q1); Aging (Q2); Geriatrics
  and Gerontology (Q2)
citable_docs._(3years): 539.0
cites_/_doc._(2years): 310.0
country: Ireland
coverage: 1982-2020
doi: 10.1016/j.archger.2022.104791
eigenfactor_score: 0.00801
h_index: 75.0
isbn: null
issn: 01674943
issn1: 01674943
issn2: 01674943
issn3: 01674943
jcr_value: '3.250'
keywords: Well-being, Machine learning, Random forest, Gradient boosting
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 4267.0
region: Western Europe
scimago_value: 985.0
sjr_best_quartile: Q1
sourceid: 28541.0
title_bib: "Correlates of quality of life, happiness and life satisfaction among european\
  \ adults older than 50 years: a machine\u2010learning approach"
title_csv: Archives of gerontology and geriatrics
total_cites: 8210.0
total_cites_(3years): 1788.0
total_docs._(2020): 255.0
total_docs._(3years): 539.0
total_refs.: 10881.0
type: journal
type_publication: article
year: 2022
---
abstract: With the rapid development of the Internet of Things (IoT), Big Data technologies
  have emerged as a critical data analytics tool to bring the knowledge within IoT
  infrastructures to better meet the purpose of the IoT systems and support critical
  decision making. Although the topic of Big Data analytics itself is extensively
  researched, the disparity between IoT domains (such as healthcare, energy, transportation
  and others) has isolated the evolution of Big Data approaches in each IoT domain.
  Thus, the mutual understanding across IoT domains can possibly advance the evolution
  of Big Data research in IoT. In this work, we therefore conduct a survey on Big
  Data technologies in different IoT domains to facilitate and stimulate knowledge
  sharing across the IoT domains. Based on our review, this paper discusses the similarities
  and differences among Big Data technologies used in different IoT domains, suggests
  how certain Big Data technology used in one IoT domain can be re-used in another
  IoT domain, and develops a conceptual framework to outline the critical Big Data
  technologies across all the reviewed IoT domains.
author: Mouzhi Ge and Hind Bangui and Barbora Buhnova
categories: Computer Networks and Communications (Q1); Hardware and Architecture (Q1);
  Software (Q1)
citable_docs._(3years): 1868.0
cites_/_doc._(2years): 911.0
country: Netherlands
coverage: 1984-2021
doi: 10.1016/j.future.2018.04.053
eigenfactor_score: .nan
h_index: 119.0
isbn: null
issn: 0167739X
issn1: 0167739X
issn2: 0167739X
issn3: 0167739X
jcr_value: null
keywords: Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation,
  Building automation, Smart Cities
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4518.0
region: Western Europe
scimago_value: 1262.0
sjr_best_quartile: Q1
sourceid: 12264.0
title_bib: 'Big data for internet of things: a survey'
title_csv: Future generation computer systems
total_cites: .nan
total_cites_(3years): 16877.0
total_docs._(2020): 798.0
total_docs._(3years): 1935.0
total_refs.: 36054.0
type: journal
type_publication: article
year: 2018
---
abstract: Batch effects (BEs) are technical biases that may confound analysis of high-throughput
  biotechnological data. BEs are complex and effective mitigation is highly context-dependent.
  In particular, the advent of high-resolution technologies such as single-cell RNA
  sequencing presents new challenges. We first cover how BE modeling differs between
  traditional datasets and the new data landscape. We also discuss new approaches
  for measuring and mitigating BEs, including whether a BE is significant enough to
  warrant correction. Even with the advent of machine learning and artificial intelligence,
  the increased complexity of next-generation biotechnological data means increased
  complexities in BE management. We forecast that BEs will not only remain relevant
  in the age of big data but will become even more important.
author: Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong
categories: Bioengineering (Q1); Biotechnology (Q1)
citable_docs._(3years): 346.0
cites_/_doc._(2years): 1186.0
country: United Kingdom
coverage: 1983-2020
doi: 10.1016/j.tibtech.2022.02.005
eigenfactor_score: 0.01817
h_index: 219.0
isbn: null
issn: 01677799
issn1: 01677799
issn2: '18793096'
issn3: 01677799
jcr_value: '19.536'
keywords: artificial intelligence, batch effect, machine learning, RNA sequencing,
  single cell
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6710.0
region: Western Europe
scimago_value: 3192.0
sjr_best_quartile: Q1
sourceid: 16146.0
title_bib: Are batch effects still relevant in the age of big data?
title_csv: Trends in biotechnology
total_cites: 20693.0
total_cites_(3years): 4561.0
total_docs._(2020): 167.0
total_docs._(3years): 397.0
total_refs.: 11206.0
type: journal
type_publication: article
year: 2022
---
abstract: "Disconnected cancer research data management and lack of information exchange\
  \ about planned and ongoing research are complicating the utilisation of internationally\
  \ collected medical information for improving cancer patient care. Rapidly collecting/pooling\
  \ data can accelerate translational research in radiation therapy and oncology.\
  \ The exchange of study data is one of the fundamental principles behind data aggregation\
  \ and data mining. The possibilities of reproducing the original study results,\
  \ performing further analyses on existing research data to generate new hypotheses\
  \ or developing computational models to support medical decisions (e.g. risk/benefit\
  \ analysis of treatment options) represent just a fraction of the potential benefits\
  \ of medical data-pooling. Distributed machine learning and knowledge exchange from\
  \ federated databases can be considered as one beyond other attractive approaches\
  \ for knowledge generation within \u201CBig Data\u201D. Data interoperability between\
  \ research institutions should be the major concern behind a wider collaboration.\
  \ Information captured in electronic patient records (EPRs) and study case report\
  \ forms (eCRFs), linked together with medical imaging and treatment planning data,\
  \ are deemed to be fundamental elements for large multi-centre studies in the field\
  \ of radiation therapy and oncology. To fully utilise the captured medical information,\
  \ the study data have to be more than just an electronic version of a traditional\
  \ (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability,\
  \ utilisation of standards, data quality and privacy concerns, data ownership, rights\
  \ to publish, data pooling architecture and storage. This paper discusses a framework\
  \ for conceptual packages of ideas focused on a strategic development for international\
  \ research data exchange in the field of radiation therapy and oncology."
author: "Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas\
  \ Brunner and Volker Budach and Daniel B\xFCttner and J\xFCrgen Debus and Andre\
  \ Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild\
  \ Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin\
  \ L\xFChr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip\
  \ Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin\
  \ Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Bj\xF6\
  rn Zackrisson and Daniel Zips and Michael Baumann"
categories: Hematology (Q1); Oncology (Q1); Radiology, Nuclear Medicine and Imaging
  (Q1)
citable_docs._(3years): 959.0
cites_/_doc._(2years): 514.0
country: Ireland
coverage: 1983-2020
doi: 10.1016/j.radonc.2014.10.001
eigenfactor_score: 0.02494
h_index: 157.0
isbn: null
issn: 01678140
issn1: '18790887'
issn2: 01678140
issn3: '18790887'
jcr_value: '6.280'
keywords: Data pooling, Interoperability, Data exchange, Large scale studies, Public
  data, Radiotherapy
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 3159.0
region: Western Europe
scimago_value: 1892.0
sjr_best_quartile: Q1
sourceid: 17876.0
title_bib: 'Creating a data exchange strategy for radiotherapy research: towards federated
  databases and anonymised public datasets'
title_csv: Radiotherapy and oncology
total_cites: 22462.0
total_cites_(3years): 5131.0
total_docs._(2020): 470.0
total_docs._(3years): 1026.0
total_refs.: 14846.0
type: journal
type_publication: article
year: 2014
---
abstract: "The observation of growing \u201Cdifficulties\u201D in IT-infrastructures\
  \ in neuroscience research during the last years led to a search for reasons and\
  \ an analysis on how this phenomenon is reflected in the scientific literature.\
  \ With a retrospective analysis of nine examples of multicenter research projects\
  \ in the neurosciences and a literature review the observation was systematically\
  \ analyzed. Results show that the rise in complexity mainly stems from two reasons:\
  \ (1) more and more need for information on quality and context of research data\
  \ (metadata) and (2) long-term requirements to handle the consent and identity/pseudonyms\
  \ of study participants and biomaterials in relation to legal requirements. The\
  \ combination of these two aspects together with very long study times and data\
  \ evaluation periods are components of the subjectively perceived \u201Cdifficulties\u201D\
  . A direct consequence of this result is that big multicenter trials are becoming\
  \ part of integrated research data environments and are not standing alone for themselves\
  \ anymore. This drives up the resource needs regarding the IT-infrastructure in\
  \ neuroscience research. In contrast to these findings, literature on this development\
  \ is scarce and the problem probably underestimated."
author: Karoline Buckow and Matthias Quade and Otto Rienhoff and Sara Y. Nussbeck
categories: Medicine (miscellaneous) (Q1); Neuroscience (miscellaneous) (Q2)
citable_docs._(3years): 281.0
cites_/_doc._(2years): 290.0
country: Ireland
coverage: 1984-2020
doi: 10.1016/j.neures.2014.08.005
eigenfactor_score: 0.004529999999999999
h_index: 99.0
isbn: null
issn: 01680102
issn1: '18728111'
issn2: 01680102
issn3: '18728111'
jcr_value: '3.304'
keywords: IT-infrastructure, Metadata, Identity management, Data quality, Neuroscience,
  Infrastructure methodology
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 5507.0
region: Western Europe
scimago_value: 1234.0
sjr_best_quartile: Q1
sourceid: 18054.0
title_bib: Changing requirements and resulting needs for it-infrastructure for longitudinal
  research in the neurosciences
title_csv: Neuroscience research
total_cites: 5711.0
total_cites_(3years): 840.0
total_docs._(2020): 174.0
total_docs._(3years): 292.0
total_refs.: 9582.0
type: journal
type_publication: article
year: 2016
---
abstract: As the volume of data collected by various IoT sensors used in smart farm
  applications increases, the storing and processing of big data for agricultural
  applications become a huge challenge. The insight of this paper is that lossy compression
  can unleash the power of compression to IoT because, as compared with its counterpart
  (a lossless one), it can significantly reduce the data volume when the spatiotemporal
  characteristics of IoT sensor data are properly exploited. However, lossy compression
  faces the challenge of compressing too much data thus losing data fidelity, which
  might affect the quality of the data and potential analytics outcomes. To understand
  the impact of lossy compression on IoT data management and analytics, we evaluated
  four classification algorithms with reconstructed agricultural sensor data based
  on various energy concentration. Specifically, we applied three transformation-based
  lossy compression mechanisms to five real-world weather datasets collected at different
  sampling granularities from IoT weather stations. Our experimental results indicate
  that there is a strong positive correlation between the concentrated energy of the
  transformed coefficients and the compression ratio as well as the data quality.
  While we observed a general trend where much higher compression ratios can be achieved
  at the cost of a decrease in quality, we also observed that the impact on the classification
  accuracy varies among the data sets and algorithms we evaluated. Lastly, we show
  that the sampling granularity also influences the data fidelity in terms of the
  prediction performance and compression ratio.
author: Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son
categories: Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer
  Science Applications (Q1); Forestry (Q1); Horticulture (Q1)
citable_docs._(3years): 1298.0
cites_/_doc._(2years): 727.0
country: Netherlands
coverage: 1985-2020
doi: 10.1016/j.compag.2018.08.045
eigenfactor_score: 0.01646
h_index: 115.0
isbn: null
issn: 01681699
issn1: 01681699
issn2: 01681699
issn3: 01681699
jcr_value: '5.565'
keywords: Smart farm, Lossy compression, IoT, Signal processing, Data fidelity
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4433.0
region: Western Europe
scimago_value: 1208.0
sjr_best_quartile: Q1
sourceid: 30441.0
title_bib: Evaluating fidelity of lossy compression on spatiotemporal data from an
  iot enabled smart farm
title_csv: Computers and electronics in agriculture
total_cites: 17657.0
total_cites_(3years): 9479.0
total_docs._(2020): 648.0
total_docs._(3years): 1300.0
total_refs.: 28725.0
type: journal
type_publication: article
year: 2018
---
abstract: With a growing demand for accurate ion beam analysis on a large number of
  samples, it becomes an issue of how to ensure the quality standards and consistency
  over hundreds or thousands of samples. In this sense, a virtual assistant that checks
  the data quality, emitting certificates of quality, is highly desired. Even the
  processing of a massive number of spectra is a problem regarding the consistency
  of the analysis. In this work, we report the design and first results of a virtual
  layer under implementation in our laboratory. It consists of a series of systems
  running in the cloud that perform the mentioned tasks and serves as a virtual assistant
  for member staff and users. We aim to bring the concept of the Internet of Things
  and artificial intelligence closer to the laboratory to support a new generation
  of instrumentation.
author: "Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo\
  \ D.C. Pereira and Thiago B. Saramela and Renato O. Guimar\xE3es"
categories: Instrumentation (Q2); Nuclear and High Energy Physics (Q3)
citable_docs._(3years): 1750.0
cites_/_doc._(2years): 142.0
country: Netherlands
coverage: 1983-2020
doi: 10.1016/j.nimb.2020.05.027
eigenfactor_score: .nan
h_index: 119.0
isbn: null
issn: 0168583X
issn1: 0168583X
issn2: 0168583X
issn3: 0168583X
jcr_value: null
keywords: Ion beam analysis, Big data, Data quality assurance, Artificial intelligence
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 3041.0
region: Western Europe
scimago_value: 429.0
sjr_best_quartile: Q2
sourceid: 29068.0
title_bib: 'Ion beam analysis and big data: how data science can support next-generation
  instrumentation'
title_csv: 'Nuclear instruments and methods in physics research, section b: beam interactions
  with materials and atoms'
total_cites: .nan
total_cites_(3years): 2403.0
total_docs._(2020): 514.0
total_docs._(3years): 1766.0
total_refs.: 15630.0
type: journal
type_publication: article
year: 2020
---
abstract: 'Background

  The aim of this article is to evaluate the status, development, and perspectives
  of German claims data analyses in the international and health political context.

  Methods

  We conducted a comprehensive literature search in PubMed, Scopus, and DIMDI to identify
  empirical and methodological articles focusing on health insurance claims data studies
  published between 2000 and 2014. Inclusion criteria were (1) English/German full
  text articles or chapters in edited books that (2) focused on the claims data of
  statutory health insurance funds.

  Findings

  In total, 435 articles were included. Over time, the number of claims data studies
  has increased strongly and the frequency of policy-relevant research types increased.
  Along with the historical improvement path of claims data in Germany, we observed
  a rising percentage of international publications and an increase in the average
  quality of publications. In contrast to the US or Canada where comprehensive databases
  have been established, the most common data source in this search was data from
  a single SHI fund, while databases were rarely used.

  Conclusions

  Claims data are an important source of information for healthcare stakeholders,
  and their use for research purposes has further increased during recent years in
  Germany. Despite its potential in optimising the health system, we found a lack
  of German comprehensive all-payer claims databases compared to the US and Canada.'
author: Kristine Kreis and Sarah Neubauer and Mike Klora and Ansgar Lange and Jan
  Zeidler
categories: Health Policy (Q1)
citable_docs._(3years): 499.0
cites_/_doc._(2years): 294.0
country: Ireland
coverage: 1984-2020
doi: 10.1016/j.healthpol.2016.01.007
eigenfactor_score: 0.01001
h_index: 92.0
isbn: null
issn: 01688510
issn1: '18726054'
issn2: 01688510
issn3: '18726054'
jcr_value: '2.980'
keywords: Claims data analysis, Administrative data, Health services research, Germany,
  Statutory health insurance, Data source
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 4751.0
region: Western Europe
scimago_value: 1214.0
sjr_best_quartile: Q1
sourceid: 21306.0
title_bib: "Status and perspectives of claims data analyses in germany\u2014a systematic\
  \ review"
title_csv: Health policy
total_cites: 9058.0
total_cites_(3years): 1526.0
total_docs._(2020): 175.0
total_docs._(3years): 514.0
total_refs.: 8315.0
type: journal
type_publication: article
year: 2016
---
abstract: "Since the first version of the Entity\u2013Relationship (ER) model proposed\
  \ by Peter Chen over forty years ago, both the ER model and conceptual modeling\
  \ activities have been key success factors for modeling computer-based systems.\
  \ During the last decade, conceptual modeling has been recognized as an important\
  \ research topic in academia, as well as a necessity for practitioners. However,\
  \ there are many research challenges for conceptual modeling in contemporary applications\
  \ such as Big Data, data-intensive applications, decision support systems, e-health\
  \ applications, and ontologies. In addition, there remain challenges related to\
  \ the traditional efforts associated with methodologies, tools, and theory development.\
  \ Recently, novel research is uniting contributions from both the conceptual modeling\
  \ area and the Artificial Intelligence discipline in two directions. The first one\
  \ is efforts related to how conceptual modeling can aid in the design of Artificial\
  \ Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how\
  \ Artificial Intelligence and Machine Learning can be applied in model-based solutions,\
  \ such as model-based engineering, to infer and improve the generated models. For\
  \ the first time in the history of Conceptual Modeling (ER) conferences, we encouraged\
  \ the submission of papers based on AI and ML solutions in an attempt to highlight\
  \ research from both communities. In this paper, we present some of important topics\
  \ in current research in conceptual modeling. We introduce the selected best papers\
  \ from the 37th International Conference on Conceptual Modeling (ER\u201918) held\
  \ in Xi\u2019an, China and summarize some of the valuable contributions made based\
  \ on the discussions of these papers. We conclude with suggestions for continued\
  \ research."
author: Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda
  C. Storey
categories: Information Systems and Management (Q2)
citable_docs._(3years): 152.0
cites_/_doc._(2years): 255.0
country: Netherlands
coverage: 1985, 1987-2020
doi: 10.1016/j.datak.2021.101911
eigenfactor_score: .nan
h_index: 87.0
isbn: null
issn: 0169023X
issn1: 0169023X
issn2: 0169023X
issn3: 0169023X
jcr_value: null
keywords: Conceptual modeling, Big Data, Machine learning, Artificial Intelligence
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4086.0
region: Western Europe
scimago_value: 480.0
sjr_best_quartile: Q2
sourceid: 24437.0
title_bib: 'Conceptual modeling in the era of big data and artificial intelligence:
  research topics and introduction to the special issue'
title_csv: Data and knowledge engineering
total_cites: .nan
total_cites_(3years): 440.0
total_docs._(2020): 37.0
total_docs._(3years): 155.0
total_refs.: 1512.0
type: journal
type_publication: article
year: 2021
---
abstract: 'The advent of modern data collection and storage technologies has brought
  about a huge increase in data volumes with both traditional and machine learning
  tools struggling to effectively handle, manage and analyse the very large data quantities
  that are now available. The mineral exploration industry is by no means immune to
  this big data issue. Exploration decision-making has become much more complex in
  the wake of big data, in particular with respect to questions about how to best
  manage and use the data to obtain information, generate knowledge and gain insight.
  One of the ways in which the mineral exploration industry works with big data is
  by using a geographic information system (GIS). For example, GIS platforms are often
  used for integration, interrogation and interpretation of diverse geoscience and
  mineral exploration data with the goal of refining and prioritising known and identifying
  new targets. Here we (i) briefly discuss the importance of carefully translating
  conceptual ore deposit models into effective exploration targeting maps, (ii) propose
  and describe what we term exploration information systems (EIS): a new idea for
  an information system designed to better integrate the conceptual mineral deposit
  model (i.e., the critical and constituent processes of the targeted mineral system)
  with data available to support exploration targeting, and (iii) discuss how best
  to categorise mineral systems in an EIS as scale-dependent subsystems to form mineral
  deposits. Our vision for the future use of EIS in exploration targeting is one whereby
  the mappable ingredients of a targeted mineral system are translated and combined
  into a set of weighted evidence (or proxy) maps automatically, resulting in an auto-generated
  mineral prospectivity map and a series of ranked exploration targets. We do not
  envisage the EIS replacing human input and ingenuity; rather we envisage the EIS
  as an additional tool in the exploration toolbox and as an intelligence amplifying
  system in which humans are making use of machines to achieve the best possible results.'
author: "Mahyar Yousefi and Oliver P. Kreuzer and Vesa Nyk\xE4nen and Jon M.A. Hronsky"
categories: Economic Geology (Q1); Geochemistry and Petrology (Q1); Geology (Q1)
citable_docs._(3years): 1297.0
cites_/_doc._(2years): 377.0
country: Netherlands
coverage: 1986-2020
doi: 10.1016/j.oregeorev.2019.103005
eigenfactor_score: 0.01566
h_index: 97.0
isbn: null
issn: 01691368
issn1: 01691368
issn2: 01691368
issn3: 01691368
jcr_value: '3.809'
keywords: Exploration information systems (EIS), Mineral exploration targeting, Geographic
  information systems (GIS), Mineral systems approach
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 9404.0
region: Western Europe
scimago_value: 1413.0
sjr_best_quartile: Q1
sourceid: 25702.0
title_bib: "Exploration information systems \u2013 a proposal for the future use of\
  \ gis in mineral exploration targeting"
title_csv: Ore geology reviews
total_cites: 14843.0
total_cites_(3years): 5534.0
total_docs._(2020): 558.0
total_docs._(3years): 1326.0
total_refs.: 52477.0
type: journal
type_publication: article
year: 2019
---
abstract: "The last several decades have witnessed a rapid yet uneven urban expansion\
  \ in developing countries. The existing studies rely heavily on official statistical\
  \ yearbooks and remote sensing images. However, the former data sources have been\
  \ criticized due to its non-objectivity and low quality, while the latter is labor\
  \ and cost consuming in most cases. Recent efforts made by fractal analyses provide\
  \ alternatives to scrutinize the corresponding \u201Cnatural urban area\u201D. In\
  \ our proposed framework, the dynamics of internal urban contexts is reflected in\
  \ a quasi-real-time manner using emerging new data and the expansion is a fractal\
  \ concept instead of an absolute one based on the conventional Euclidean method.\
  \ We then evaluate the magnitude and pattern of natural cities and their expansion\
  \ in size and space. It turns out that the spatial expansion rate of official cities\
  \ (OCs) in our study area China has been largely underestimated when compared with\
  \ the results of natural cities (NCs). The perspective of NCs also provides a novel\
  \ way to understanding the quality of uneven urban expansion. We detail our analysis\
  \ for the 23 urban agglomerations in China, especially paying more attention to\
  \ the three most dominating urban agglomerations of China: Beijing-Tianjin-Hebei\
  \ (BTH), Yangtze River Delta (YRD) and Pearl River Delta (PRD). The findings from\
  \ the OC method are not consistent with the NC method. The distinctions may arise\
  \ from the definition of a city, and the bottom-up NC method contributes to our\
  \ comprehensive understanding of uneven urban expansion."
author: Ying Long and Weixin Zhai and Yao Shen and Xinyue Ye
categories: Ecology (Q1); Management, Monitoring, Policy and Law (Q1); Nature and
  Landscape Conservation (Q1)
citable_docs._(3years): 693.0
cites_/_doc._(2years): 616.0
country: Netherlands
coverage: 1986-2020
doi: 10.1016/j.landurbplan.2017.05.008
eigenfactor_score: 0.01602
h_index: 161.0
isbn: null
issn: 01692046
issn1: 01692046
issn2: 01692046
issn3: 01692046
jcr_value: '6.142'
keywords: Urban expansion, Social media, Head/tail division, New data, Open data,
  China
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6701.0
region: Western Europe
scimago_value: 1938.0
sjr_best_quartile: Q1
sourceid: 19041.0
title_bib: Understanding uneven urban expansion with natural cities using open data
title_csv: Landscape and urban planning
total_cites: 22743.0
total_cites_(3years): 5148.0
total_docs._(2020): 180.0
total_docs._(3years): 704.0
total_refs.: 12061.0
type: journal
type_publication: article
year: 2018
---
abstract: "Background and Objectives\nData Quality (DQ) programs are recognized as\
  \ a critical aspect of new-generation research platforms using electronic health\
  \ record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical\
  \ Data Repository aggregates EHR data from 37 hospitals to enable large-scale research\
  \ and secondary data analysis. This paper describes the DQ program currently in\
  \ place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.\n\
  Materials and Methods\nAs part of the AP-HP DQ program, two domains - patient identification\
  \ (PI) and healthcare services (HS) - were selected for conducting DQ campaigns\
  \ consisting of 5 phases: defining the scope, measuring, analyzing, improving and\
  \ controlling DQ. Semi-automated DQ profiling was conducted in two data sets \u2013\
  \ the PI data set containing 8.8\u202FM patients and the HS data set containing\
  \ 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined\
  \ and DQ issues were classified using a unified DQ reporting framework. For each\
  \ domain, actions plans were defined for improving and monitoring prioritized DQ\
  \ issues.\nResults\nEleven identified DQ issues (8 for the PI data set and 3 for\
  \ the HS data set) were categorized into completeness (n\u202F=\u202F6), conformance\
  \ (n\u202F=\u202F3) and plausibility (n\u202F=\u202F2) DQ issues. DQ issues were\
  \ caused by errors from data originators, ETL issues or limitations of the EHR data\
  \ entry tool. The action plans included sixteen actions (9 for the PI domain and\
  \ 7 for the HS domain). Though only partial implementation, the DQ campaigns already\
  \ resulted in significant improvement of DQ measures.\nConclusion\nDQ assessments\
  \ of hospital information systems are largely unpublished. The preliminary results\
  \ of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement\
  \ into a DQ program. The adoption of a unified DQ reporting framework enables the\
  \ communication of DQ findings in a well-defined manner with a shared vocabulary.\
  \ Dedicated tooling is needed to automate and extend the scope of the generic DQ\
  \ program. Specific DQ checks will be additionally defined on a per-study basis\
  \ to evaluate whether EHR data fits for specific uses."
author: "Christel Daniel and Patricia Serre and Nina Orlova and St\xE9phane Br\xE9\
  ant and Nicolas Paris and Nicolas Griffon"
categories: Computer Science Applications (Q1); Software (Q1); Health Informatics
  (Q2)
citable_docs._(3years): 753.0
cites_/_doc._(2years): 627.0
country: Ireland
coverage: 1985-2020
doi: 10.1016/j.cmpb.2018.10.016
eigenfactor_score: 0.01119
h_index: 102.0
isbn: null
issn: 01692607
issn1: 01692607
issn2: '18727565'
issn3: 01692607
jcr_value: '5.428'
keywords: Data accuracy, Data quality, Electronic health records, Data warehousing,
  Observational Studies as Topic
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 4409.0
region: Western Europe
scimago_value: 924.0
sjr_best_quartile: Q1
sourceid: 23604.0
title_bib: Initializing a hospital-wide data quality program. the ap-hp experience.
title_csv: Computer methods and programs in biomedicine
total_cites: 12277.0
total_cites_(3years): 4743.0
total_docs._(2020): 538.0
total_docs._(3years): 789.0
total_refs.: 23721.0
type: journal
type_publication: article
year: 2019
---
abstract: "A paradigm shift from current population based medicine to personalized\
  \ and participative medicine is underway. This transition is being supported by\
  \ the development of clinical decision support systems based on prediction models\
  \ of treatment outcome. In radiation oncology, these models \u2018learn\u2019 using\
  \ advanced and innovative information technologies (ideally in a distributed fashion\
  \ \u2014 please watch the animation: http://youtu.be/ZDJFOxpwqEA) from all available/appropriate\
  \ medical data (clinical, treatment, imaging, biological/genetic, etc.) to achieve\
  \ the highest possible accuracy with respect to prediction of tumor response and\
  \ normal tissue toxicity. In this position paper, we deliver an overview of the\
  \ factors that are associated with outcome in radiation oncology and discuss the\
  \ methodology behind the development of accurate prediction models, which is a multi-faceted\
  \ process. Subsequent to initial development/validation and clinical introduction,\
  \ decision support systems should be constantly re-evaluated (through quality assurance\
  \ procedures) in different patient datasets in order to refine and re-optimize the\
  \ models, ensuring the continuous utility of the models. In the reasonably near\
  \ future, decision support systems will be fully integrated within the clinic, with\
  \ data and knowledge being shared in a standardized, dynamic, and potentially global\
  \ manner enabling truly personalized and participative medicine."
author: "Philippe Lambin and Jaap Zindler and Ben G.L. Vanneste and Lien Van {De Voorde}\
  \ and Dani\xEBlle Eekers and Inge Compter and Kranthi Marella Panth and Jurgen Peerlings\
  \ and Ruben T.H.M. Larue and Timo M. Deist and Arthur Jochems and Tim Lustberg and\
  \ Johan {van Soest} and Evelyn E.C. {de Jong} and Aniek J.G. Even and Bart Reymen\
  \ and Nicolle Rekers and Marike {van Gisbergen} and Erik Roelofs and Sara Carvalho\
  \ and Ralph T.H. Leijenaar and Catharina M.L. Zegers and Maria Jacobs and Janita\
  \ {van Timmeren} and Patricia Brouwers and Jonathan A. Lal and Ludwig Dubois and\
  \ Ala Yaromina and Evert Jan {Van Limbergen} and Maaike Berbee and Wouter {van Elmpt}\
  \ and Cary Oberije and Bram Ramaekers and Andre Dekker and Liesbeth J. Boersma and\
  \ Frank Hoebers and Kim M. Smits and Adriana J. Berlanga and Sean Walsh"
categories: Pharmaceutical Science (Q1)
citable_docs._(3years): 443.0
cites_/_doc._(2years): 1323.0
country: Netherlands
coverage: 1987-2020
doi: 10.1016/j.addr.2016.01.006
eigenfactor_score: 0.0286
h_index: 313.0
isbn: null
issn: 0169409X
issn1: 0169409X
issn2: '18728294'
issn3: 0169409X
jcr_value: '15.470'
keywords: Radiotherapy, Decision support systems, Prediction models, Shared decision
  making
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 20675.0
region: Western Europe
scimago_value: 3475.0
sjr_best_quartile: Q1
sourceid: 19409.0
title_bib: Decision support systems for personalized and participative radiation oncology
title_csv: Advanced drug delivery reviews
total_cites: 43769.0
total_cites_(3years): 6945.0
total_docs._(2020): 138.0
total_docs._(3years): 485.0
total_refs.: 28532.0
type: journal
type_publication: article
year: 2017
---
abstract: "Introduction\nUntil the recent approval of immunotherapy after completing\
  \ concurrent chemoradiotherapy (CCRT), there has been little progress in treating\
  \ unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to\
  \ search real-world data (RWD) to better understand diagnosis and treatment patterns,\
  \ and outcomes.\nMethods\nThis non-interventional observational study used a unique,\
  \ novel algorithm for big data analysis to collect and assess anonymized patient\
  \ electronic medical records from a clinical data warehouse (CDW) over a 10-year\
  \ period to capture real-world patterns of diagnosis, treatment, and outcomes of\
  \ stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment\
  \ of patients with newly-diagnosed stage III NSCLC, and patients\u2019 characteristics,\
  \ and assessment of treatment outcomes.\nResults\nWe analyzed clinical variables\
  \ from 23,735 NSCLC patients. Stage III patients (N\u202F=\u202F4138, 18.2 %) were\
  \ diagnosed as IIIA (N\u202F=\u202F2,547, 11.2 %) or IIIB (N\u202F=\u202F1,591.\
  \ 7.0 %). Treated stage III patients (N\u202F=\u202F2530, 61.1 %) had a median age\
  \ of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of\
  \ 1 (65.2 %). Treatment comprised curative-intent surgery (N\u202F=\u202F1,254,\
  \ 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N\u202F=\u202F\
  648, 25.6 %); palliative CT (N\u202F=\u202F270, 10.7 %), or thoracic RT (N\u202F\
  =\u202F170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative\
  \ chemotherapy, lung RT alone, and supportive care was 49.2 (42.0\u201356.5), 52.5\
  \ (43.1\u201361.9), 30.3 (26.6\u201334.0), 14.7 (13.0\u201316.4), 8.8 (6.2\u2013\
  11.3), and 2.0 (1.0\u20133.0) months, respectively.\nConclusions\nThis unique in-house\
  \ algorithm enabled a rapid and comprehensive analysis of big data through a CDW,\
  \ with daily automatic updates that documented real-world PFS and OS consistent\
  \ with the published literature, and real-world treatment patterns and clinical\
  \ outcomes in stage III NSCLC patients."
author: Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju
  Ahn and Keunchil Park
categories: Cancer Research (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine
  (Q1)
citable_docs._(3years): 845.0
cites_/_doc._(2years): 471.0
country: Ireland
coverage: 1985-2020
doi: 10.1016/j.lungcan.2020.05.033
eigenfactor_score: 0.01966
h_index: 129.0
isbn: null
issn: 01695002
issn1: 01695002
issn2: '18728332'
issn3: 01695002
jcr_value: '5.705'
keywords: Real-time updated system, Big data, Real-world data, NSCLC, Treatment
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 3011.0
region: Western Europe
scimago_value: 1989.0
sjr_best_quartile: Q1
sourceid: 12391.0
title_bib: 'Ten-year patient journey of stage iii non-small cell lung cancer patients:
  a single-center, observational, retrospective study in korea (realtime automatically
  updated data warehouse in health care; universe-root study)'
title_csv: Lung cancer
total_cites: 15504.0
total_cites_(3years): 4671.0
total_docs._(2020): 349.0
total_docs._(3years): 917.0
total_refs.: 10507.0
type: journal
type_publication: article
year: 2020
---
abstract: While the past decade has witnessed an unprecedented growth of data generated
  and collected all over the world, existing data management approaches lack the ability
  to address the challenges of Big Data. One of the most promising tools for Big Data
  processing is the MapReduce paradigm. Although it has its limitations, the MapReduce
  programming model has laid the foundations for answering some of the Big Data challenges.
  In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce
  paradigm. Using as case study a Hadoop-based application, i.e., image similarity
  search, we present our experiences with the Hadoop framework when processing terabytes
  of data. The scale of the data and the application workload allowed us to test the
  limits of Hadoop and the efficiency of the tools it provides. We present a wide
  collection of experiments and the practical lessons we have drawn from our experience
  with the Hadoop environment. Our findings can be shared as best practices and recommendations
  to the Big Data researchers and practitioners.
author: Diana Moise and Denis Shestakov
categories: Applied Mathematics (Q4); Modeling and Simulation (Q4); Statistics and
  Probability (Q4)
citable_docs._(3years): 1.0
cites_/_doc._(2years): 112.0
country: Netherlands
coverage: 1980, 1982-1985, 1988, 1991, 1993-1994, 1996-1998, 2000-2001, 2003, 2005-2007,
  2009, 2012-2020
doi: 10.1016/B978-0-444-63492-4.00012-5
eigenfactor_score: .nan
h_index: 42.0
isbn: null
issn: 01697161
issn1: 01697161
issn2: 01697161
issn3: 01697161
jcr_value: null
keywords: Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment,
  SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves
publisher_x: Elsevier
publisher_y: Elsevier
ref._/_doc.: 5384.0
region: Western Europe
scimago_value: 125.0
sjr_best_quartile: Q4
sourceid: 17700156445.0
title_bib: Chapter 12 - terabyte-scale image similarity search
title_csv: Handbook of statistics
total_cites: .nan
total_cites_(3years): 82.0
total_docs._(2020): 19.0
total_docs._(3years): 89.0
total_refs.: 1023.0
type: book series
type_publication: incollection
year: 2015
---
abstract: Conventional static soft sensor is incapable of handling the dynamic of
  processes. With abundance of data, the problem of variable correlations and a large
  number of samples are encountered; moreover, the quality of the data for the construction
  of the soft sensors can be crucial for performance. An active learning strategy
  based on a latent variable model (LVM) to select representative data for efficient
  development of the dynamic soft sensor model is proposed. The uncertainty information
  for data selection is provided by the Gaussian process (GP) model. The developed
  LVM with the auxiliary GP model can handle the process dynamic. An active forward-update
  scheme which can update the soft sensor model in advance is proposed to reflect
  the current status of the process and improve the prediction performance without
  waiting for the quality measurements. Two case studies are done to demonstrate the
  features and the applicability of the proposed method.
author: Lester Lik Teck Chan and Qing-Yang Wu and Junghui Chen
categories: Analytical Chemistry (Q2); Computer Science Applications (Q2); Process
  Chemistry and Technology (Q2); Software (Q2); Spectroscopy (Q2)
citable_docs._(3years): 517.0
cites_/_doc._(2years): 359.0
country: Netherlands
coverage: 1986-2020
doi: 10.1016/j.chemolab.2018.01.015
eigenfactor_score: 0.006959999999999999
h_index: 126.0
isbn: null
issn: 01697439
issn1: 01697439
issn2: 01697439
issn3: 01697439
jcr_value: '3.491'
keywords: Active learning, Forward-update, Large database, Latent variable model,
  Model uncertainty, Soft sensor
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4317.0
region: Western Europe
scimago_value: 600.0
sjr_best_quartile: Q2
sourceid: 24595.0
title_bib: Dynamic soft sensors with active forward-update learning for selection
  of useful data from historical big database
title_csv: Chemometrics and intelligent laboratory systems
total_cites: 11735.0
total_cites_(3years): 1977.0
total_docs._(2020): 209.0
total_docs._(3years): 522.0
total_refs.: 9022.0
type: journal
type_publication: article
year: 2018
---
abstract: Cloud manufacturing adopts a cloud computing paradigm as the basis for delivering
  shared, on-demand manufacturing services. The result is customer-centric supply
  chains that can be configured for cost, quality, speed and customisation. While
  the technical capabilities required for cloud manufacturing are a current focus,
  there are many emerging questions relating to the impact, both positive and negative,
  on the people consuming or supporting cloud manufacturing services. Human factors
  can have a pivotal role in enabling the success and adoption of cloud manufacturing,
  while ensuring the safety, well-being and optimum user experience of those involved
  in a cloud manufacturing environment. This paper presents these issues, structured
  around groups of users (service providers, application providers and consumers).
  We also consider the issues of collaboration that are likely to arise from the manufacturing
  cloud. From this analysis we discuss the central role of human factors as an enabler
  of cloud manufacturing, and the opportunities that emerge.
author: David Golightly and Sarah Sharples and Harshada Patel and Svetan Ratchev
categories: Human Factors and Ergonomics (Q2); Public Health, Environmental and Occupational
  Health (Q2)
citable_docs._(3years): 342.0
cites_/_doc._(2years): 328.0
country: Netherlands
coverage: 1986-2020
doi: 10.1016/j.ergon.2016.05.011
eigenfactor_score: 0.00249
h_index: 79.0
isbn: null
issn: 01698141
issn1: 01698141
issn2: '18728219'
issn3: 01698141
jcr_value: '2.656'
keywords: Cloud manufacturing, Assembly, Production, Collaboration
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5101.0
region: Western Europe
scimago_value: 570.0
sjr_best_quartile: Q2
sourceid: 12385.0
title_bib: 'Manufacturing in the cloud: a human factors perspective'
title_csv: International journal of industrial ergonomics
total_cites: 4782.0
total_cites_(3years): 1089.0
total_docs._(2020): 119.0
total_docs._(3years): 344.0
total_refs.: 6070.0
type: journal
type_publication: article
year: 2016
---
abstract: 'Background

  Artificial intelligence methods for the classification of melanoma have been studied
  extensively. However, few studies compare these methods under the same standards.

  Objective

  To seek the best artificial intelligence method for diagnosis of melanoma.

  Methods

  The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions,
  and classifications were performed in sequence for evaluation of traditional machine
  learning algorithms. The recent popular convolutional neural network frameworks
  were used for transfer learning training classification.

  Results

  The region growing algorithm has the best segmentation performance, with an intersection
  over union of 70.06% and a false-positive rate of 17.67%. Classification performance
  was better with logistic regression, with a sensitivity of 76.36% and a specificity
  of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep
  learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the
  specificity was 85.64%.

  Limitations

  There was no division in the severity of melanoma samples used in this experiment.
  The data set was relatively small for deep learning.

  Conclusion

  The performance of traditional machine learning is satisfactory for the small data
  set of melanoma dermoscopic images, and the potential for deep learning in the future
  big data era is enormous.'
author: Xiaoyu Cui and Ran Wei and Lixin Gong and Ruiqun Qi and Zeyin Zhao and Hongduo
  Chen and Kaixin Song and Amer A.A. Abdulrahman and Yining Wang and John Z.S. Chen
  and Shuo Chen and Yue Zhao and Xinghua Gao
categories: Dermatology (Q1)
citable_docs._(3years): 1607.0
cites_/_doc._(2years): 349.0
country: United States
coverage: 1979-2020
doi: 10.1016/j.jaad.2019.06.042
eigenfactor_score: 0.03692
h_index: 208.0
isbn: null
issn: 01909622
issn1: '10976787'
issn2: 01909622
issn3: '10976787'
jcr_value: '11.527'
keywords: artificial intelligence, classification, deep learning, melanoma diagnosis,
  segmentation, traditional machine learning
publisher_x: Mosby Inc.
publisher_y: null
ref._/_doc.: 1367.0
region: Northern America
scimago_value: 1965.0
sjr_best_quartile: Q1
sourceid: 24245.0
title_bib: 'Assessing the effectiveness of artificial intelligence methods for melanoma:
  a retrospective review'
title_csv: Journal of the american academy of dermatology
total_cites: 40257.0
total_cites_(3years): 7754.0
total_docs._(2020): 1185.0
total_docs._(3years): 2153.0
total_refs.: 16196.0
type: journal
type_publication: article
year: 2019
---
abstract: "Clinical research often focuses on resource-intensive causal inference,\
  \ whereas the potential of predictive analytics with constantly increasing big data\
  \ sources remains largely unexplored. Basic prediction, divorced from causal inference,\
  \ is much easier with big data. Emergency care may benefit from this simpler application\
  \ of big data. Historically, predictive analytics have played an important role\
  \ in emergency care as simple heuristics for risk stratification. These tools generally\
  \ follow a standard approach: parsimonious criteria, easy computability, and independent\
  \ validation with distinct populations. Simplicity in a prediction tool is valuable,\
  \ but technological advances make it no longer a necessity. Emergency care could\
  \ benefit from clinical predictions built using data science tools with abundant\
  \ potential input variables available in electronic medical records. Patients\u2019\
  \ risks could be stratified more precisely with large pools of data and lower resource\
  \ requirements for comparing each clinical encounter to those that came before it,\
  \ benefiting clinical decisionmaking and health systems operations. The largest\
  \ value of predictive analytics comes early in the clinical encounter, in which\
  \ diagnostic and prognostic uncertainty are high and resource-committing decisions\
  \ need to be made. We propose an agenda for widening the application of predictive\
  \ analytics in emergency care. Throughout, we express cautious optimism because\
  \ there are myriad challenges related to database infrastructure, practitioner uptake,\
  \ and patient acceptance. The quality of routinely compiled clinical data will remain\
  \ an important limitation. Complementing big data sources with prospective data\
  \ may be necessary if predictive analytics are to achieve their full potential to\
  \ improve care quality in the emergency department."
author: Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip
  D. Levy
categories: Emergency Medicine (Q1)
citable_docs._(3years): 703.0
cites_/_doc._(2years): 147.0
country: United States
coverage: 1980-2020
doi: 10.1016/j.annemergmed.2015.06.024
eigenfactor_score: 0.01573
h_index: 153.0
isbn: null
issn: 01960644
issn1: 01960644
issn2: '10976760'
issn3: 01960644
jcr_value: '5.721'
keywords: null
publisher_x: Mosby Inc.
publisher_y: null
ref._/_doc.: 1603.0
region: Northern America
scimago_value: 1241.0
sjr_best_quartile: Q1
sourceid: 15220.0
title_bib: Exploring the potential of predictive analytics and big data in emergency
  care
title_csv: Annals of emergency medicine
total_cites: 14808.0
total_cites_(3years): 2154.0
total_docs._(2020): 392.0
total_docs._(3years): 1312.0
total_refs.: 6283.0
type: journal
type_publication: article
year: 2016
---
abstract: "In the face of green energy initiatives and progressively increasing shares\
  \ of more energy-efficient buildings, there is a pressing need to transform district\
  \ heating towards low-temperature district heating. The substantially lowered supply\
  \ temperature of low-temperature district heating broadens the opportunities and\
  \ challenges to integrate distributed renewable energy, which requires enhancement\
  \ on intelligent heating load prediction. Meanwhile, to fulfill the temperature\
  \ requirements for domestic hot water and space heating, separate energy conversion\
  \ units on user-side, such as building-sized boosting heat pumps shall be implemented\
  \ to upgrade the temperature level of the low-temperature district heating network.\
  \ This study conducted hybrid heating load prediction methods with long-term and\
  \ short-term prediction, and the main work consisted of four steps: (1) acquisition\
  \ and processing of district heating data of 20 district heating supplied nursing\
  \ homes in the Nordic climate (2016\u20132019); (2) long-term district heating load\
  \ prediction through linear regression, energy signature curve in hourly resolution,\
  \ providing an overall view and boundary conditions for the unit sizing; (3) short-term\
  \ district heating load prediction through two Artificial Neural Network models,\
  \ f72 and g120, with different prediction input parameters; (4) evaluation of the\
  \ predicted load profiles based on the measured data. Although the three prediction\
  \ models met the quality criteria, it was found that including the historical hourly\
  \ heating loads as the input to the forecasting model enhanced the prediction quality,\
  \ especially for the peak load and low-mild heating season. Furthermore, a possible\
  \ application of the heating load profiles was proposed by integrating two building-sized\
  \ heat pumps in low-temperature district heating, which may be a promising heat\
  \ supply method in low-temperature district heating."
author: "Yiyu Ding and Thomas Ohlson Timoudas and Qian Wang and Shuqin Chen and Helge\
  \ Bratteb\xF8 and Natasa Nord"
categories: Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Nuclear
  Energy and Engineering (Q1); Renewable Energy, Sustainability and the Environment
  (Q1)
citable_docs._(3years): 3458.0
cites_/_doc._(2years): 1003.0
country: United Kingdom
coverage: 1979-2020
doi: 10.1016/j.enconman.2022.116163
eigenfactor_score: 0.0823
h_index: 192.0
isbn: null
issn: 01968904
issn1: 01968904
issn2: 01968904
issn3: 01968904
jcr_value: '9.709'
keywords: Nursing homes, District heating load prediction, Linear regression, Artificial
  neural network, Low-temperature district heating
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5438.0
region: Western Europe
scimago_value: 2743.0
sjr_best_quartile: Q1
sourceid: 29372.0
title_bib: 'A study on data-driven hybrid heating load prediction methods in low-temperature
  district heating: an example for nursing homes in nordic countries'
title_csv: Energy conversion and management
total_cites: 80377.0
total_cites_(3years): 35051.0
total_docs._(2020): 1125.0
total_docs._(3years): 3479.0
total_refs.: 61183.0
type: journal
type_publication: article
year: 2022
---
abstract: "Urban studies attempt to identify the geographic areas with restricted\
  \ access to healthy and affordable foods (defined as food deserts in the literature).\
  \ While prior publications have reported the socioeconomic disparities in healthy\
  \ food accessibility, little evidence has been released from developing countries,\
  \ especially in China. This paper proposes a geo-big data approach to measuring\
  \ transit-varying healthy food accessibility and applies it to identify the food\
  \ deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest\
  \ the daily travel time from each community (8117) to each healthy food store (102)\
  \ from the Baidu Map under four transport modes (walking, public transit, private\
  \ car, and bicycle) during 17:30\u201320:30 in June 2016. Based on the travel time\
  \ calculations, we develop four travel time indicators to measure the healthy food\
  \ accessibility: the minimum, the maximum, the weighted average, and the standard\
  \ deviation. Results show that the four accessibility indicators generate different\
  \ estimations and the nearest service (minimum time) alone fails to reflect the\
  \ multidimensional nature of healthy food accessibility. The communities within\
  \ Shenzhen present quite different typology with respect to healthy food accessibility\
  \ under different transport modes. Multilevel additive regression is further applied\
  \ to examine the associations between healthy food accessibility and nested socioeconomic\
  \ characteristics at two geographic levels (community and district). We discover\
  \ that the associations are divergent with transport modes and with geographic levels.\
  \ More specifically, significant social equalities in healthy food accessibility\
  \ are identified via walking, public transit, and bicycle in Shenzhen. Based on\
  \ the associations, we finally map the food deserts and propose corresponding planning\
  \ strategies. The methods demonstrated in this study should offer deeper spatial\
  \ insights into intra-urban foodscapes and provide more nuanced understanding of\
  \ food deserts in urban settings of developing countries."
author: Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng
categories: Nature and Landscape Conservation (Q1); Urban Studies (Q1)
citable_docs._(3years): 361.0
cites_/_doc._(2years): 546.0
country: United Kingdom
coverage: 1970, 1976-2020
doi: 10.1016/j.habitatint.2017.04.007
eigenfactor_score: 0.00827
h_index: 78.0
isbn: null
issn: 01973975
issn1: 01973975
issn2: 01973975
issn3: 01973975
jcr_value: '5.369'
keywords: Food geography, Healthy food access, Accessibility, Social inequalities,
  Transport mode, Multilevel regression
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6843.0
region: Western Europe
scimago_value: 1542.0
sjr_best_quartile: Q1
sourceid: 13760.0
title_bib: 'A geo-big data approach to intra-urban food deserts: transit-varying accessibility,
  social inequalities, and implications for urban planning'
title_csv: Habitat international
total_cites: 8872.0
total_cites_(3years): 2114.0
total_docs._(2020): 117.0
total_docs._(3years): 365.0
total_refs.: 8006.0
type: journal
type_publication: article
year: 2017
---
abstract: "The introduction of clinical information systems (CIS) in Intensive Care\
  \ Units (ICUs) offers the possibility of storing a huge amount of machine-ready\
  \ clinical data that can be used to improve patient outcomes and the allocation\
  \ of resources, as well as suggest topics for randomized clinical trials. Clinicians,\
  \ however, usually lack the necessary training for the analysis of large databases.\
  \ In addition, there are issues referred to patient privacy and consent, and data\
  \ quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning\
  \ experts, statisticians, epidemiologists and other information scientists may overcome\
  \ these problems. A multidisciplinary event (Critical Care Datathon) was held in\
  \ Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical\
  \ Care Society (SEMICYUC), the event was organized by the Massachusetts Institute\
  \ of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit\
  \ and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting\
  \ Technologies group of Madrid Polytechnic University. After presentations referred\
  \ to big data in the critical care environment, clinicians, data scientists and\
  \ other health data science enthusiasts and lawyers worked in collaboration using\
  \ an anonymized database (MIMIC III). Eight groups were formed to answer different\
  \ clinical research questions elaborated prior to the meeting. The event produced\
  \ analyses for the questions posed and outlined several future clinical research\
  \ opportunities. Foundations were laid to enable future use of ICU databases in\
  \ Spain, and a timeline was established for future meetings, as an example of how\
  \ big data analysis tools have tremendous potential in our field.\nResumen\nLa aparici\xF3\
  n de los sistemas de informaci\xF3n cl\xEDnica (SIC) en el entorno de los cuidados\
  \ intensivos brinda la posibilidad de almacenar una ingente cantidad de datos cl\xED\
  nicos en formato electr\xF3nico durante el ingreso de los pacientes. Estos datos\
  \ pueden ser empleados posteriormente para obtener respuestas a preguntas cl\xED\
  nicas, para su uso en la gesti\xF3n de recursos o para sugerir l\xEDneas de investigaci\xF3\
  n que luego pueden ser explotadas mediante ensayos cl\xEDnicos aleatorizados. Sin\
  \ embargo, los m\xE9dicos cl\xEDnicos carecen de la formaci\xF3n necesaria para\
  \ la explotaci\xF3n de grandes bases de datos, lo que supone un obst\xE1culo para\
  \ aprovechar esta oportunidad. Adem\xE1s, existen cuestiones de \xEDndole legal\
  \ (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas\
  \ para poder utilizar esta potente herramienta. El trabajo multidisciplinar con\
  \ otros profesionales (analistas de datos, estad\xEDsticos, epidemi\xF3logos, especialistas\
  \ en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones\
  \ y permitir utilizar esta herramienta para investigaci\xF3n cl\xEDnica o an\xE1\
  lisis de resultados (benchmarking). Se describe la reuni\xF3n multidisciplinar (Critical\
  \ Care Datathon) realizada en Madrid los d\xEDas 1, 2 y 3 de diciembre de 2017.\
  \ Esta reuni\xF3n, celebrada bajo los auspicios de la Sociedad Espa\xF1ola de Medicina\
  \ Intensiva, Cr\xEDtica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada\
  \ por el Massachusetts Institute of Technology (MIT), la Unidad de Innovaci\xF3\
  n y el Servicio de Medicina Intensiva del Hospital Cl\xEDnico San Carlos, as\xED\
  \ como el grupo de investigaci\xF3n \xABLife Supporting Technologies\xBB de la Universidad\
  \ Polit\xE9cnica de Madrid. Tras unas ponencias de formaci\xF3n sobre big data,\
  \ seguridad y calidad de los datos, y su aplicaci\xF3n al entorno de la medicina\
  \ intensiva, un grupo de cl\xEDnicos, analistas de datos, estad\xEDsticos, expertos\
  \ en seguridad inform\xE1tica de datos realizaron sesiones de trabajo colaborativo\
  \ en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar\
  \ varias preguntas cl\xEDnicas establecidas previamente a la reuni\xF3n. El trabajo\
  \ colaborativo permiti\xF3 establecer resultados relevantes con respecto a las preguntas\
  \ planteadas y esbozar varias l\xEDneas de investigaci\xF3n cl\xEDnica a desarrollar\
  \ en el futuro. Adem\xE1s, se sentaron las bases para poder utilizar las bases de\
  \ datos de las UCI con las que contamos en Espa\xF1a, y se estableci\xF3 un calendario\
  \ de trabajo para planificar futuras reuniones contando con los datos de nuestras\
  \ unidades. El empleo de herramientas de big data y el trabajo colaborativo con\
  \ otros profesionales puede permitir ampliar los horizontes en aspectos como el\
  \ control de calidad de nuestra labor cotidiana, la comparaci\xF3n de resultados\
  \ entre unidades o la elaboraci\xF3n de nuevas l\xEDneas de investigaci\xF3n cl\xED\
  nica."
author: "Antonio {N\xFA\xF1ez Reiz} and Fernando {Mart\xEDnez Sagasti} and Manuel\
  \ {\xC1lvarez Gonz\xE1lez} and Antonio {Blesa Malpica} and Juan Carlos {Mart\xED\
  n Ben\xEDtez} and Mercedes {Nieto Cabrera} and \xC1ngela {del Pino Ram\xEDrez} and\
  \ Jos\xE9 Miguel {Gil Perdomo} and Jes\xFAs {Prada Alonso} and Leo Anthony Celi\
  \ and Miguel \xC1ngel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik\
  \ and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer\
  \ and Arturo {Gonz\xE1lez Ferrer} and \xC1ngel Rey and Henar {Gonz\xE1lez Luengo}\
  \ and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura L\xF3pez and\
  \ Beatriz Merino and Mar\xEDa Fernanda Cabrera and Mar\xEDa Teresa Arredondo and\
  \ Mar\xEDa Bod\xED and Josep G\xF3mez and Alejandro Rodr\xEDguez and Miguel {S\xE1\
  nchez Garc\xEDa}"
categories: Critical Care and Intensive Care Medicine (Q3)
citable_docs._(3years): 346.0
cites_/_doc._(2years): 98.0
country: Spain
coverage: 1988-2020
doi: 10.1016/j.medin.2018.06.002
eigenfactor_score: 0.00147
h_index: 28.0
isbn: null
issn: 02105691
issn1: '15786749'
issn2: 02105691
issn3: '15786749'
jcr_value: '2.491'
keywords: "Big data, Machine learning, Artificial intelligence, Clinical databases,\
  \ MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de\
  \ datos cl\xEDnicos, MIMIC III, Datathon, Trabajo colaborativo"
publisher_x: Ediciones Doyma, S.L.
publisher_y: null
ref._/_doc.: 1816.0
region: Western Europe
scimago_value: 336.0
sjr_best_quartile: Q3
sourceid: 18370.0
title_bib: 'Big data and machine learning in critical care: opportunities for collaborative
  research'
title_csv: Medicina intensiva
total_cites: 1177.0
total_cites_(3years): 416.0
total_docs._(2020): 237.0
total_docs._(3years): 401.0
total_refs.: 4303.0
type: journal
type_publication: article
year: 2019
---
abstract: "Focus\nThe transformative power of today's big data (BD) has allowed many\
  \ companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard\
  \ to decision-making, artificial intelligence (AI) takes task delegation to a new\
  \ level, and by employing AI-assisted tools, companies can provide their HR departments\
  \ with the means to manage the existing data and HR altogether.\nObjectives\nTo\
  \ determine how HR managers assess whether BD management is facilitated by AI, and\
  \ how they frame the changes necessary to meet the trends related to AI and its\
  \ implementation, namely their willingness to master its implementation and to meet\
  \ the possible challenges.\nMethodology\nContent analysis was conducted on interviews\
  \ held with a sample of 16 HR practitioners from a spectrum of areas, and the findings\
  \ were analysed using the big data maturity model (BDMM) framework. Domains covered\
  \ by this model allow the study of decision-making trends, in terms of preparedness\
  \ and willingness to tackle disruptive technology with the aim of improving and\
  \ gaining the competitive edge in decision-making.\nFindings\nThe central potential\
  \ of AI lies in faster data storage and processing power, thereby leading to more\
  \ insightful and effective decision-making. This article contains closer insights\
  \ into the challenges underlying the implementation of AI in decision-making processes,\
  \ specifically in terms of strategic alignment, governance, and implementation.\
  \ The results reflect the notions regarding the nature of AI \u2013 in assisting\
  \ HR \u2013 and lay out the path that precedes the extraction of BD, through the\
  \ delivery of advantageous intelligence, to augment decision-making in HR."
author: "Aleksandar Radonji\u0107 and Henrique Duarte and N\xE1dia Pereira"
categories: Strategy and Management (Q1)
citable_docs._(3years): 208.0
cites_/_doc._(2years): 572.0
country: United Kingdom
coverage: 1982-2020
doi: 10.1016/j.emj.2022.07.001
eigenfactor_score: 0.00371
h_index: 102.0
isbn: null
issn: '02632373'
issn1: '02632373'
issn2: '02632373'
issn3: '02632373'
jcr_value: '5.075'
keywords: Human resources, HR, HRM, e-HRM, Decision-making, Big data (BD), Big data
  maturity models (BDMM), Artificial intelligence (AI)
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 9046.0
region: Western Europe
scimago_value: 1365.0
sjr_best_quartile: Q1
sourceid: 22491.0
title_bib: "Artificial intelligence and hrm: hr managers\u2019 perspective on decisiveness\
  \ and challenges"
title_csv: European management journal
total_cites: 5790.0
total_cites_(3years): 1147.0
total_docs._(2020): 103.0
total_docs._(3years): 215.0
total_refs.: 9317.0
type: journal
type_publication: article
year: 2022
---
abstract: This study presents a broad view of the current state of the art of ML applications
  in the manufacturing sectors that have a considerable impact on sustainability and
  the environment, namely renewable energies (solar, wind, hydropower, and biomass),
  smart grids, the industry of catalysis and power storage and distribution. Artificial
  neural networks are the most preferred techniques over other ML algorithms because
  of their generalization capabilities. Demands for ML techniques in the energy sectors
  will increase considerably in the coming years, since there is a growing demand
  of academic programmes related to artificial intelligence in science, math, and
  engineering. Data generation, management, and safety are expected to play a key
  role for the successful implementation of ML algorithms that can be shared by major
  stakeholders in the energy sector, thereby promoting the development of ambitious
  energy management projects. New algorithms for producing reliable data and the addition
  of other sources of information (e.g., novel sensors) will enhance flow of information
  between ML and systems. It is expected that unsupervised and reinforcement learning
  will take a central role in the energy sector, but this will depend on the expansion
  of other major fields in data science such as big data analytics. Massive implementations,
  specialized algorithms, and new technologies like 5G will promote the development
  of sustainable applications of ML in non-industrial applications for energy management.
author: Daniel Rangel-Martinez and K.D.P. Nigam and Luis A. Ricardez-Sandoval
categories: Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1)
citable_docs._(3years): 1397.0
cites_/_doc._(2years): 384.0
country: United Kingdom
coverage: 1983-2020
doi: 10.1016/j.cherd.2021.08.013
eigenfactor_score: .nan
h_index: 99.0
isbn: null
issn: 02638762
issn1: '17443563'
issn2: 02638762
issn3: '17443563'
jcr_value: null
keywords: Machine learning, Artificial neural networks, Renewable energies, Catalysis,
  Power systems, Sustainability, Energy efficiency
publisher_x: Institution of Chemical Engineers
publisher_y: null
ref._/_doc.: 4680.0
region: Western Europe
scimago_value: 788.0
sjr_best_quartile: Q1
sourceid: 16411.0
title_bib: 'Machine learning on sustainable energy: a review and outlook on renewable
  energy systems, catalysis, smart grid and energy storage'
title_csv: Chemical engineering research and design
total_cites: .nan
total_cites_(3years): 5429.0
total_docs._(2020): 418.0
total_docs._(3years): 1403.0
total_refs.: 19563.0
type: journal
type_publication: article
year: 2021
---
abstract: 'How to reduce neighborhood socioeconomic status- (SES-) related health
  inequalities has been prioritized in the recent political literature. Park green
  spaces (PGSs) are essential neighborhood land use assets, as they are beneficial
  for population health and thus should mitigate SES-related health inequalities.
  This paper aims to elaborate the knowledge on the complex interrelationships among
  PGSs, health outcomes and social inequalities through developing a set of mixed
  indicators in an integrated manner. The data are obtained at the community level
  (N=8117) for three health outcomes (cardiopathy, chronic pneumonia and hypertension)
  and five SES variables. The PGS characteristics are described from three dimensions
  (coverage, quality and accessibility) at two geographical levels (15-minute walking
  distance (15 WD) neighborhoods and 30-minute walking distance (30 WD) neighborhoods).
  Spatial regressions reveal the following: 1) socioeconomically disadvantaged communities
  enjoy fewer PGSs and limited access to parks of high quality; 2) socioeconomically
  disadvantaged communities present higher incidences of diseases; and 3) PGS coverage
  within 30 WD neighborhoods and PGS accessibility within 15 WD neighborhoods are
  related to health outcomes. Structural equation modeling further confirms that PGSs,
  especially those of higher quality, could mitigate SES-related health inequalities.
  The findings of this study highlight the necessity of improving the PGS quality
  within walking distance of socioeconomically disadvantaged communities. We thus
  argue that land use policy makers should collaborate with social researchers and
  health professionals; and through health impact assessment, they can translate the
  professional knowledge into land use planning and consider health promotion into
  land use policies.'
author: Qian Wang and Zili Lan
categories: Forestry (Q1); Geography, Planning and Development (Q1); Management, Monitoring,
  Policy and Law (Q1); Nature and Landscape Conservation (Q1)
citable_docs._(3years): 1703.0
cites_/_doc._(2years): 557.0
country: United Kingdom
coverage: 1984-2021
doi: 10.1016/j.landusepol.2019.01.026
eigenfactor_score: 0.02454
h_index: 115.0
isbn: null
issn: 02648377
issn1: 02648377
issn2: 02648377
issn3: 02648377
jcr_value: '5.398'
keywords: park green spaces, health outcomes, inequalities, neighborhood socioeconomic
  status, indicators, greening policy
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6580.0
region: Western Europe
scimago_value: 1668.0
sjr_best_quartile: Q1
sourceid: 14500.0
title_bib: 'Park green spaces, public health and social inequalities: understanding
  the interrelationships for policy implications'
title_csv: Land use policy
total_cites: 22020.0
total_cites_(3years): 9371.0
total_docs._(2020): 730.0
total_docs._(3years): 1706.0
total_refs.: 48032.0
type: journal
type_publication: article
year: 2019
---
abstract: "Covering: 2010\u20132020 The digital revolution is driving significant\
  \ changes in how people store, distribute, and use information. With the advent\
  \ of new technologies around linked data, machine learning and large-scale network\
  \ inference, the natural products research field is beginning to embrace real-time\
  \ sharing and large-scale analysis of digitized experimental data. Databases play\
  \ a key role in this, as they allow systematic annotation and storage of data for\
  \ both basic and advanced applications. The quality of the content, structure, and\
  \ accessibility of these databases all contribute to their usefulness for the scientific\
  \ community in practice. This review covers the development of databases relevant\
  \ for microbial natural product discovery during the past decade (2010\u20132020),\
  \ including repositories of chemical structures/properties, metabolomics, and genomic\
  \ data (biosynthetic gene clusters). It provides an overview of the most important\
  \ databases and their functionalities, highlights some early meta-analyses using\
  \ such databases, and discusses basic principles to enable widespread interoperability\
  \ between databases. Furthermore, it points out conceptual and practical challenges\
  \ in the curation and usage of natural products databases. Finally, the review closes\
  \ with a discussion of key action points required for the field moving forward,\
  \ not only for database developers but for any scientist active in the field."
author: Jeffrey A. {van Santen} and Satria A. Kautsar and Marnix H. Medema and Roger
  G. Linington
categories: Biochemistry (Q1); Drug Discovery (Q1); Organic Chemistry (Q1)
citable_docs._(3years): 239.0
cites_/_doc._(2years): 845.0
country: United Kingdom
coverage: 1984-2020
doi: 10.1039/d0np00053a
eigenfactor_score: 0.01116
h_index: 177.0
isbn: null
issn: 02650568
issn1: 02650568
issn2: '14604752'
issn3: 02650568
jcr_value: '13.423'
keywords: null
publisher_x: Royal Society of Chemistry
publisher_y: null
ref._/_doc.: 13800.0
region: Western Europe
scimago_value: 2703.0
sjr_best_quartile: Q1
sourceid: 26371.0
title_bib: 'Microbial natural product databases: moving forward in the multi-omics
  era'
title_csv: Natural product reports
total_cites: 13293.0
total_cites_(3years): 2233.0
total_docs._(2020): 92.0
total_docs._(3years): 246.0
total_refs.: 12696.0
type: journal
type_publication: article
year: 2021
---
abstract: "The years-long high-precision photometric data observed by Kepler satellite\
  \ combining with huge amount of spectra observed by LAMOST provide a great opportunity\
  \ to study the relations between surface rotation and lithium abundance of li-rich\
  \ giants. In this study, we cross match the Kepler data with li-rich giants catalog\
  \ from LAMOST, and obtain 619 common sources. Then we measure 36 rotation periods\
  \ from the full set of 295 li-rich giants with good data quality which consists\
  \ of two sub-samples. The rotation periods of 14 stars was extracted from 205 stars\
  \ with evolutionary stages determined using asteroseismology, including 11 core\
  \ helium-burning stars (HeBs), 2 red giant branch stars (RGBs), and 1 unclassified\
  \ star. All the super lithium-rich giant stars (A(Li)>3.3\u2009\u200A\u200Adex)\
  \ are HeBs in our sample. The remaining 90 giants do not have evolutionary stages\
  \ confirmed, and in this sub-sample, 22 giants have their rotation period measured.\
  \ The rotation detection rate of the former sub-sample is 6.8%, which is significantly\
  \ higher than the detection rate of a large giant sample in previous studies (2.08%).\
  \ With the surface rotation period measured, we confirm the relation between stellar\
  \ rotation and lithium enrichment of giants. Meanwhile, we find that the less Li-enriched\
  \ stars have a relatively dispersed distribution of rotation periods, and the giants\
  \ with a high Li enrichment is concentrated on the rapidly rotating area, which\
  \ is consistent with earlier studies. The present work also shows a jump at A(Li)\u2248\
  3.3\u2009\u200A\u200Adex in the relation between rotation period and Li abundance\
  \ that coincidently is the boundary between Li-rich giants and super Li-rich giants\
  \ which may indicate the different mechanisms. The rotation periods of super Li-rich\
  \ giants become shorter as the lithium enrichment increases. This correlation provides\
  \ an evidence for the rotational induced extra-mixing mechanism responsible for\
  \ Li enrichment of giants."
author: DU Ming-hao and BI Shao-lan and SHI Jian-rong and YAN Hong-liang
categories: Astronomy and Astrophysics (Q4); Space and Planetary Science (Q4)
citable_docs._(3years): 109.0
cites_/_doc._(2years): 21.0
country: United Kingdom
coverage: 1981-2020
doi: 10.1016/j.chinastron.2021.02.003
eigenfactor_score: .nan
h_index: 12.0
isbn: null
issn: '02751062'
issn1: '02751062'
issn2: '02751062'
issn3: '02751062'
jcr_value: null
keywords: 'Stars: rotation, Stars: abundance, stars: chemically peculiar, method:
  data analysis'
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 2971.0
region: Western Europe
scimago_value: 132.0
sjr_best_quartile: Q4
sourceid: 27149.0
title_bib: Surface rotation of lamost-kepler li-rich giant stars
title_csv: Chinese astronomy and astrophysics
total_cites: .nan
total_cites_(3years): 33.0
total_docs._(2020): 34.0
total_docs._(3years): 109.0
total_refs.: 1010.0
type: journal
type_publication: article
year: 2021
---
abstract: The emergence of powerful software has created conditions and approaches
  for large datasets to be collected and analyzed which has led to informed decision-making
  towards tackling health issues. The objective of this study is to systematically
  review 804 scholarly publications related to big data analytics in health in order
  to identify the organizational and social values along with associated challenges.
  Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses
  (PRISMA) methodology were followed for conducting systematic reviews. Following
  a research path, we present the values, challenges and future directions of the
  scientific area using indicative examples from relevant published articles. The
  study reveals that one of the main values created is the development of analytical
  techniques which provides personalized health services to users and supports human
  decision-making using automated algorithms, challenging the power issues in the
  doctor-patient relationship and creating new working conditions. A main challenge
  to data analytics is data management and security when processing large volumes
  of sensitive, personal health data. Future research is directed towards the development
  of systems that will standardize and secure the process of extracting private healthcare
  datasets from relevant organizations. Our systematic literature review aims to provide
  to governments and health policy-makers a better understanding of how the development
  of a data driven strategy can improve public health and the functioning of healthcare
  organizations but also how can create challenges that need to be addressed in the
  near future to avoid societal malfunctions.
author: P. Galetsi and K. Katsaliaki and S. Kumar
categories: Health (social science) (Q1); History and Philosophy of Science (Q1);
  Medicine (miscellaneous) (Q1)
citable_docs._(3years): 1526.0
cites_/_doc._(2years): 439.0
country: United Kingdom
coverage: 1967-2020
doi: 10.1016/j.socscimed.2019.112533
eigenfactor_score: .nan
h_index: 243.0
isbn: null
issn: 02779536
issn1: '18735347'
issn2: 02779536
issn3: '18735347'
jcr_value: null
keywords: Systematic review, Big data analytics, Health-medicine, Decision-making,
  Organizational and societal values, Preferred reporting items for systematic reviews
  and meta-analyses
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5675.0
region: Western Europe
scimago_value: 1913.0
sjr_best_quartile: Q1
sourceid: 18983.0
title_bib: 'Values, challenges and future directions of big data analytics in healthcare:
  a systematic review'
title_csv: Social science and medicine
total_cites: .nan
total_cites_(3years): 7601.0
total_docs._(2020): 692.0
total_docs._(3years): 1607.0
total_refs.: 39274.0
type: journal
type_publication: article
year: 2019
---
abstract: In this paper, we introduce a model that incorporates features of the fully
  transparent hotel booking systems and enables estimates of hotel choice probabilities
  in a group based on the room charges. Firstly, we extract necessary information
  for the estimation from big data of online booking for major four hotels near Kyoto
  station.11The data were provided by National Institute of Informatics. Then, we
  consider a nested logit model as well as a multinomial logit model for the choice
  behavior of the customers, where the number of rooms available for booking for each
  hotel are possibly limited. In addition, we apply the model to an optimal room charge
  problem for a hotel that aims to maximize its expected sales of a certain room type
  in the transparent online booking systems. We show numerical examples of the maximization
  problem using the data of the four hotels of November 2012 which is a high season
  in Kyoto city. This model is useful in that hotel managers as well as hotel investors,
  such as hotel REITs and hotel funds, are able to predict the potential sales increase
  of hotels from online booking data and make use of the result as a tool for investment
  decisions.
author: Taiga Saito and Akihiko Takahashi and Hiroshi Tsuda
categories: Strategy and Management (Q1); Tourism, Leisure and Hospitality Management
  (Q1)
citable_docs._(3years): 490.0
cites_/_doc._(2years): 898.0
country: United Kingdom
coverage: 1982-2020
doi: 10.1016/j.ijhm.2016.06.006
eigenfactor_score: 0.011340000000000001
h_index: 122.0
isbn: null
issn: 02784319
issn1: 02784319
issn2: 02784319
issn3: 02784319
jcr_value: '9.237'
keywords: Hotels in Kyoto, Revenue management, Online booking, Discrete choice model
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 7886.0
region: Western Europe
scimago_value: 2321.0
sjr_best_quartile: Q1
sourceid: 28686.0
title_bib: Optimal room charge and expected sales under discrete choice models with
  limited capacity
title_csv: International journal of hospitality management
total_cites: 17219.0
total_cites_(3years): 4796.0
total_docs._(2020): 321.0
total_docs._(3years): 509.0
total_refs.: 25314.0
type: journal
type_publication: article
year: 2016
---
abstract: 'The definition of the threshold of sediment motion is critical for continental
  shelf sediment dynamics. The work by A. Shields laid the foundation for this research
  direction, leading to the well-known Shields curve. Here we review the most widely
  used threshold curves that have followed from the original Shields curve over the
  last 80 years, and propose that in terms of physical processes the threshold (critical
  Shields parameter) is a function of at least six variables, i.e. grain Reynolds
  number, grain size distribution, sphericity, roundness, particle cohesiveness and
  the scale effects of turbulence. Identifying these key factors, we paid a special
  attention to the role of the scale effects of turbulence. Turbulence was thought
  to be a random process, but the improvement of measurement techniques revealed that
  it has both temporal and spatial structures: the magnitude of instantaneous velocity
  fluctuations varies in time and in location, which can cause the deviation between
  in situ measurements and flume experiments. In coastal and shelf waters, in situ
  measurements of tidal currents and suspended sediment concentrations have revealed
  that resuspension takes place even though the bed shear stress is well below the
  Shields curve. Further process and mechanism studies are required to improve the
  theoretical framework regarding the turbulence structures and their interplay with
  sediment threshold. The scientific problems for future studies include the establishment
  of laboratory experiments, in situ measurements and process-based modelling under
  different water depths and hydrodynamic conditions to quantify the scale effects
  of turbulence; the development of new observation techniques for higher resolution
  and for extreme environments; development of new data processing methods, including
  big data methods to analyse turbulence structures; and the quantification of the
  effects of biological contributions and non-particle components on the family of
  Shields curves.'
author: Yang Yang and Shu Gao and Ya Ping Wang and Jianjun Jia and Jilian Xiong and
  Liang Zhou
categories: Aquatic Science (Q1); Geology (Q1); Oceanography (Q1)
citable_docs._(3years): 496.0
cites_/_doc._(2years): 234.0
country: United Kingdom
coverage: 1982-2020
doi: 10.1016/j.csr.2019.103960
eigenfactor_score: 0.006409999999999999
h_index: 111.0
isbn: null
issn: 02784343
issn1: 02784343
issn2: 02784343
issn3: 02784343
jcr_value: '2.391'
keywords: Sediment threshold, Shields curves, Critical shear stress, Turbulence structures,
  Scale effects, Continental shelf environments
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6453.0
region: Western Europe
scimago_value: 893.0
sjr_best_quartile: Q1
sourceid: 26817.0
title_bib: Revisiting the problem of sediment motion threshold
title_csv: Continental shelf research
total_cites: 11676.0
total_cites_(3years): 1380.0
total_docs._(2020): 122.0
total_docs._(3years): 501.0
total_refs.: 7873.0
type: journal
type_publication: article
year: 2019
---
abstract: "The upstream oil and gas industry has been contending with massive data\
  \ sets and monolithic files for many years, but \u201CBig Data\u201D is a relatively\
  \ new concept that has the potential to significantly re-shape the industry. Despite\
  \ the impressive amount of value that is being realized by Big Data technologies\
  \ in other parts of the marketplace, however, much of the data collected within\
  \ the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory\
  \ way. This viewpoint examines existing data management practices in the upstream\
  \ oil and gas industry, and compares them to practices and philosophies that have\
  \ emerged in organizations that are leading the way in Big Data. The comparison\
  \ shows that, in companies that are widely considered to be leaders in Big Data\
  \ analytics, data is regarded as a valuable asset\u2014but this is usually not true\
  \ within the oil and gas industry insofar as data is frequently regarded there as\
  \ descriptive information about a physical asset rather than something that is valuable\
  \ in and of itself. The paper then discusses how the industry could potentially\
  \ extract more value from data, and concludes with a series of policy-related questions\
  \ to this end."
author: Robert K. Perrons and Jesse W. Jensen
categories: Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1)
citable_docs._(3years): 2135.0
cites_/_doc._(2years): 629.0
country: United Kingdom
coverage: 1973-2020
doi: 10.1016/j.enpol.2015.02.020
eigenfactor_score: 0.04319
h_index: 217.0
isbn: null
issn: '03014215'
issn1: '03014215'
issn2: '03014215'
issn3: '03014215'
jcr_value: '6.142'
keywords: Big data, Oil and gas, Information technologies, Data
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5977.0
region: Western Europe
scimago_value: 2093.0
sjr_best_quartile: Q1
sourceid: 29403.0
title_bib: "Data as an asset: what the oil and gas sector can learn from other industries\
  \ about \u201Cbig data\u201D"
title_csv: Energy policy
total_cites: 60369.0
total_cites_(3years): 14111.0
total_docs._(2020): 679.0
total_docs._(3years): 2165.0
total_refs.: 40582.0
type: journal
type_publication: article
year: 2015
---
abstract: "Context\nThe optimal treatment for men with high-risk localized or locally\
  \ advanced prostate cancer (PCa) remains unknown.\nObjective\nTo perform a systematic\
  \ review of the existing literature on the effectiveness of the different primary\
  \ treatment modalities for high-risk localized and locally advanced PCa. The primary\
  \ oncological outcome is the development of distant metastases at \u22655 yr of\
  \ follow-up. Secondary oncological outcomes are PCa-specific mortality, overall\
  \ mortality, biochemical recurrence, and need for salvage treatment with \u2265\
  5 yr of follow-up. Nononcological outcomes are quality of life (QoL), functional\
  \ outcomes, and treatment-related side effects reported.\nEvidence acquisition\n\
  Medline, Medline In-Process, Embase, and the Cochrane Central Register of Randomized\
  \ Controlled Trials were searched. All comparative (randomized and nonrandomized)\
  \ studies published between January 2000 and May 2019 with at least 50 participants\
  \ in each arm were included. Studies reporting on high-risk localized PCa (International\
  \ Society of Urologic Pathologists [ISUP] grade 4\u20135 [Gleason score {GS} 8\u2013\
  10] or prostate-specific antigen [PSA] >20\u2009ng/ml or\u2009\u2265\u2009cT2c)\
  \ and/or locally advanced PCa (any PSA, cT3\u20134 or cN+, any ISUP grade/GS) or\
  \ where subanalyses were performed on either group were included. The following\
  \ primary local treatments were mandated: radical prostatectomy (RP), external beam\
  \ radiotherapy (EBRT) (\u226564\u2009Gy), brachytherapy (BT), or multimodality treatment\
  \ combining any of the local treatments above (\xB1any systemic treatment). Risk\
  \ of bias (RoB) and confounding factors were assessed for each study. A narrative\
  \ synthesis was performed.\nEvidence synthesis\nOverall, 90 studies met the inclusion\
  \ criteria. RoB and confounding factors revealed high RoB for selection, performance,\
  \ and detection bias, and low RoB for correction of initial PSA and biopsy GS. When\
  \ comparing RP with EBRT, retrospective series suggested an advantage for RP, although\
  \ with a low level of evidence. Both RT and RP should be seen as part of a multimodal\
  \ treatment plan with possible addition of (postoperative) RT and/or androgen deprivation\
  \ therapy (ADT), respectively. High levels of evidence exist for EBRT treatment,\
  \ with several randomized clinical trials showing superior outcome for adding long-term\
  \ ADT or BT to EBRT. No clear cutoff can be proposed for RT dose, but higher RT\
  \ doses by means of dose escalation schemes result in an improved biochemical control.\
  \ Twenty studies reported data on QoL, with RP resulting mainly in genitourinary\
  \ toxicity and sexual dysfunction, and EBRT in bowel problems.\nConclusions\nBased\
  \ on the results of this systematic review, both RP as part of multimodal treatment\
  \ and EBRT\u2009+\u2009long-term ADT can be recommended as primary treatment in\
  \ high-risk and locally advanced PCa. For high-risk PCa, EBRT\u2009+\u2009BT can\
  \ also be offered despite more grade 3 toxicity. Interestingly, for selected patients,\
  \ for example, those with higher comorbidity, a shorter duration of ADT might be\
  \ an option. For locally advanced PCa, EBRT\u2009+\u2009BT shows promising result\
  \ but still needs further validation. In this setting, it is important that patients\
  \ are aware that the offered therapy will most likely be in the context a multimodality\
  \ treatment plan. In particular, if radiation is used, the combination of local\
  \ with systemic treatment provides the best outcome, provided the patient is fit\
  \ enough to receive both. Until the results of the SPCG15 trial are known, the optimal\
  \ local treatment remains a matter of debate. Patients should at all times be fully\
  \ informed about all available options, and the likelihood of a multimodal approach\
  \ including the potential side effects of both local and systemic treatment.\nPatient\
  \ summary\nWe reviewed the literature to see whether the evidence from clinical\
  \ studies would tell us the best way of curing men with aggressive prostate cancer\
  \ that had not spread to other parts of the body such as lymph glands or bones.\
  \ Based on the results of this systematic review, there is good evidence that both\
  \ surgery and radiation therapy are good treatment options, in terms of prolonging\
  \ life and preserving quality of life, provided they are combined with other treatments.\
  \ In the case of surgery this means including radiotherapy (RT), and in the case\
  \ of RT this means either hormonal therapy or combined RT and brachytherapy."
author: "Lisa Moris and Marcus G. Cumberbatch and Thomas {Van den Broeck} and Giorgio\
  \ Gandaglia and Nicola Fossati and Brian Kelly and Raj Pal and Erik Briers and Philip\
  \ Cornford and Maria {De Santis} and Stefano Fanti and Silke Gillessen and Jeremy\
  \ P. Grummet and Ann M. Henry and Thomas B.L. Lam and Michael Lardas and Matthew\
  \ Liew and Malcolm D. Mason and Muhammad Imran Omar and Olivier Rouvi\xE8re and\
  \ Ivo G. Schoots and Derya Tilki and Roderick C.N. {van den Bergh} and Theodorus\
  \ H. {van Der Kwast} and Henk G. {van Der Poel} and Peter-Paul M. Willemse and Cathy\
  \ Y. Yuan and Badrinath Konety and Tanya Dorff and Suneil Jain and Nicolas Mottet\
  \ and Thomas Wiegel"
categories: Urology (Q1)
citable_docs._(3years): 538.0
cites_/_doc._(2years): 636.0
country: Netherlands
coverage: 1975-2020
doi: 10.1016/j.eururo.2020.01.033
eigenfactor_score: 0.06226
h_index: 216.0
isbn: null
issn: 03022838
issn1: 03022838
issn2: 1421993X
issn3: 03022838
jcr_value: '20.096'
keywords: Prostate cancer, Localized, Locally advanced, Primary therapy, Radical prostatectomy,
  External beam radiotherapy, Brachytherapy, Modality treatment, Systemic treatment,
  Systematic review
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 1525.0
region: Western Europe
scimago_value: 9799.0
sjr_best_quartile: Q1
sourceid: 19897.0
title_bib: 'Benefits and risks of primary treatments for high-risk localized and locally
  advanced prostate cancer: an international multidisciplinary systematic review'
title_csv: European urology
total_cites: 42109.0
total_cites_(3years): 10533.0
total_docs._(2020): 502.0
total_docs._(3years): 1501.0
total_refs.: 7656.0
type: journal
type_publication: article
year: 2020
---
abstract: Over the past few decades, data-driven machine learning (ML) has distinguished
  itself from hypothesis-driven studies and has recently received much attention in
  environmental toxicology. However, the use of ML in environmental toxicology remains
  in the early stages, with knowledge gaps, technical bottlenecks in data quality,
  high-dimensional/heterogeneous/small-sample data analysis and model interpretability,
  and a lack of an in-depth understanding of environmental toxicology. Given the above
  problems, we review the recent progress in the literature and highlight state-of-the-art
  toxicological studies using ML (such as learning and predicting toxicity in complicated
  biosystems and multiple-factor environmental scenarios of long-term and large-scale
  pollution). Beyond predicting simple biological endpoints by integrating untargeted
  omics and adverse outcome pathways, ML development should focus on revealing toxicological
  mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis
  and adverse outcome pathway frameworks) endows ML with widely promising application
  in revealing toxicological mechanisms. High-quality databases and interpretable
  algorithms are urgently needed for toxicology and environmental science. Addressing
  the core issues and future challenges for ML in this review may narrow the knowledge
  gap between environmental toxicity and computational science and facilitate the
  control of environmental risk in the future.
author: Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu
categories: Environmental Chemistry (Q1); Environmental Engineering (Q1); Health,
  Toxicology and Mutagenesis (Q1); Pollution (Q1); Waste Management and Disposal (Q1)
citable_docs._(3years): 2975.0
cites_/_doc._(2years): 1039.0
country: Netherlands
coverage: 1975-2021
doi: 10.1016/j.jhazmat.2022.129487
eigenfactor_score: 0.07094
h_index: 284.0
isbn: null
issn: 03043894
issn1: 03043894
issn2: 03043894
issn3: 03043894
jcr_value: '10.588'
keywords: Machine learning, Chemical, Toxicity, Environmental health, Big data
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5917.0
region: Western Europe
scimago_value: 2034.0
sjr_best_quartile: Q1
sourceid: 25858.0
title_bib: 'Machine learning in the identification, prediction and exploration of
  environmental toxicology: challenges and perspectives'
title_csv: Journal of hazardous materials
total_cites: 137983.0
total_cites_(3years): 31399.0
total_docs._(2020): 2143.0
total_docs._(3years): 2994.0
total_refs.: 126809.0
type: journal
type_publication: article
year: 2022
---
abstract: "As spatial technology has evolved and become integrated in to archaeology,\
  \ we face a new set of challenges posed by the sheer size and complexity of data\
  \ we use and produce. In this paper I discuss the prospects and problems of Geospatial\
  \ Big Data (GBD) \u2013 broadly defined as data sets with locational information\
  \ that exceed the capacity of widely available hardware, software, and/or human\
  \ resources. While the datasets we create today remain within available resources,\
  \ we nonetheless face the same challenges as many other fields that use and create\
  \ GBD, especially in apprehensions over data quality and privacy. After reviewing\
  \ the kinds of archaeological geospatial data currently available I discuss the\
  \ near future of GBD in writing culture histories, making decisions, and visualizing\
  \ the past. I use a case study from New Zealand to argue for the value of taking\
  \ a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology\
  \ be regularly accompanied by a Standalone Quality Report."
author: Mark D. McCoy
categories: Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1)
citable_docs._(3years): 362.0
cites_/_doc._(2years): 304.0
country: United States
coverage: 1974-2020
doi: 10.1016/j.jas.2017.06.003
eigenfactor_score: 0.01111
h_index: 126.0
isbn: null
issn: '03054403'
issn1: '03054403'
issn2: '10959238'
issn3: '03054403'
jcr_value: '3.216'
keywords: Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 7601.0
region: Northern America
scimago_value: 1572.0
sjr_best_quartile: Q1
sourceid: 31405.0
title_bib: 'Geospatial big data and archaeology: prospects and problems too great
  to ignore'
title_csv: Journal of archaeological science
total_cites: 17761.0
total_cites_(3years): 1239.0
total_docs._(2020): 137.0
total_docs._(3years): 365.0
total_refs.: 10414.0
type: journal
type_publication: article
year: 2017
---
abstract: "Incorporating big data analytics into a particular context brings various\
  \ challenges that rest on the model or framework through which individuals or organisations\
  \ adopt big data to achieve their objectives. Although these models have recently\
  \ triggered scholars\u2019 attention in various domains, in-depth knowledge of using\
  \ each of these models in big data research is still blurred. This study enriches\
  \ our knowledge on emerging models and theories that shape big data analytics adoption\
  \ (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles\
  \ and 86 conference papers) published in indexed sources between 2013 and 2019.\
  \ As a result, twenty models on BDAD have emerged (e.g., \u201CDynamic Capabilities\u201D\
  , \u201CResource-Based View\u201D, \u201CTechnology Acceptance Model\u201D, \u201C\
  Diffusion of Innovation\u201D, etc.). The analysis reveals that BDAD research to\
  \ demonstrate attributes suggestive of a topic at an initial stage of development\
  \ as it is broadly dispersed across different domains employs a wide range of models,\
  \ some of which overlap. Most of the applied models are generic in nature focusing\
  \ on variance-based relationships and snapshot prediction with little consensus.\
  \ There is a conspicuous dearth of process models, firm-level analysis and cultural\
  \ orientation in contemporary BDAD research. Insights of this bibliometric study\
  \ could guide rigorous big data research and practice in various contexts. The study\
  \ concludes with research implications and limitations that offer promising prospects\
  \ for forthcoming research."
author: Mohamed Aboelmaged and Samar Mouakket
categories: Computer Science Applications (Q1); Information Systems (Q1); Library
  and Information Sciences (Q1); Management Science and Operations Research (Q1);
  Media Technology (Q1)
citable_docs._(3years): 298.0
cites_/_doc._(2years): 801.0
country: United Kingdom
coverage: 1975-2020
doi: 10.1016/j.ipm.2020.102234
eigenfactor_score: .nan
h_index: 101.0
isbn: null
issn: '03064573'
issn1: '18735371'
issn2: '03064573'
issn3: '18735371'
jcr_value: null
keywords: Big data analytics, Technology adoption, Literature review, Bibliometric
  analysis, Theoretical models, Adoption frameworks
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5841.0
region: Western Europe
scimago_value: 1061.0
sjr_best_quartile: Q1
sourceid: 12689.0
title_bib: 'Influencing models and determinants in big data analytics research: a
  bibliometric analysis'
title_csv: Information processing and management
total_cites: .nan
total_cites_(3years): 2449.0
total_docs._(2020): 250.0
total_docs._(3years): 303.0
total_refs.: 14602.0
type: journal
type_publication: article
year: 2020
---
abstract: For the first time in history, it is possible to study human behavior on
  great scale and in fine detail simultaneously. Online services and ubiquitous computational
  devices, such as smartphones and modern cars, record our everyday activity. The
  resulting Big Data offers unprecedented opportunities for tracking and analyzing
  behavior. This paper hypothesizes the applicability and impact of Big Data technologies
  in the context of psychometrics both for research and clinical applications. It
  first outlines the state of the art, including the severe shortcomings with respect
  to quality and quantity of the resulting data. It then presents a technological
  vision, comprised of (i) numerous data sources such as mobile devices and sensors,
  (ii) a central data store, and (iii) an analytical platform, employing techniques
  from data mining and machine learning. To further illustrate the dramatic benefits
  of the proposed methodologies, the paper then outlines two current projects, logging
  and analyzing smartphone usage. One such study attempts to thereby quantify severity
  of major depression dynamically; the other investigates (mobile) Internet Addiction.
  Finally, the paper addresses some of the ethical issues inherent to Big Data technologies.
  In summary, the proposed approach is about to induce the single biggest methodological
  shift since the beginning of psychology or psychiatry. The resulting range of applications
  will dramatically shape the daily routines of researches and medical practitioners
  alike. Indeed, transferring techniques from computer science to psychiatry and psychology
  is about to establish Psycho-Informatics, an entire research direction of its own.
author: "Alexander Markowetz and Konrad B\u0142aszkiewicz and Christian Montag and\
  \ Christina Switala and Thomas E. Schlaepfer"
categories: Medicine (miscellaneous) (Q3)
citable_docs._(3years): 883.0
cites_/_doc._(2years): 140.0
country: United States
coverage: 1975-2020
doi: 10.1016/j.mehy.2013.11.030
eigenfactor_score: 0.005
h_index: 87.0
isbn: null
issn: 03069877
issn1: 03069877
issn2: '15322777'
issn3: 03069877
jcr_value: '1.538'
keywords: null
publisher_x: Churchill Livingstone
publisher_y: null
ref._/_doc.: 3513.0
region: Northern America
scimago_value: 441.0
sjr_best_quartile: Q3
sourceid: 17833.0
title_bib: 'Psycho-informatics: big data shaping modern psychometrics'
title_csv: Medical hypotheses
total_cites: 9727.0
total_cites_(3years): 1452.0
total_docs._(2020): 816.0
total_docs._(3years): 979.0
total_refs.: 28666.0
type: journal
type_publication: article
year: 2014
---
abstract: "One of the most relevant inputs for hydrological modeling is the soil map.\
  \ The soil sources and scales for the soil properties are diverse, and the quality\
  \ of soil mapping is increasing, but soil surveying is time-consuming and large\
  \ area campaigns are expensive. The taxonomic unit approach for soil mapping is\
  \ common and limited to one layer of data. This limitation causes errors in simulated\
  \ water fluxes through the soil when taxonomic units approach is implemented during\
  \ hydrological modeling analysis. Some strategies using geostatistics and machine\
  \ learning algorithms such as Kriging and Self-Organizing maps (SOM) are improving\
  \ the taxonomic units\u2019 approach and could serve as an alternative for soil\
  \ mapping for hydrological purposes. The aim of this work is to study the influence\
  \ of different soil maps and resolutions on the main hydrological components of\
  \ a sub-arid watershed in central Spain. For this, the Soil Water and Assessment\
  \ Tool (SWAT) was parameterized with three different soil maps. A first one was\
  \ based on Harmonized World Soil database from FAO, at scale 1:1,000,000 (HWSD).\
  \ The other two were based on a Kriging interpolation at 100\_\xD7\_100 m from soil\
  \ samples. To obtain soil properties map from it, two strategies were applied: one\
  \ was to average the soil properties following the official taxonomic soil units\
  \ at 1:400,000 scale (Agricultural Technological Institute of Castilla and Leon\
  \ - ITACyL) and the other was to applied Self-organizing map (SOM) to create the\
  \ soil units (SOMM). The results suggest that scale and soil properties mapping\
  \ influence HRU definition, which in turn affects water flow through the soils.\
  \ Statistical metrics of model performance were improved from R2 =0.62 and NSE=0.46\
  \ with HWSD soil map to R2 =0.86 and NSE=0.84 with SOM and similar values were achieved\
  \ during validation. Thus, the SOM is presented as an innovative algorithm applied\
  \ for hydrological modeling with SWAT, significantly increasing the level of model\
  \ accuracy to stream flow in sub-arid watersheds."
author: "David Rivas-Tabares and \xC1ngel {de Miguel} and B\xE1rbara Willaarts and\
  \ Ana M. Tarquis"
categories: Applied Mathematics (Q1); Modeling and Simulation (Q1)
citable_docs._(3years): 1602.0
cites_/_doc._(2years): 540.0
country: United States
coverage: 1976-2021
doi: 10.1016/j.apm.2020.06.044
eigenfactor_score: 0.026160000000000003
h_index: 112.0
isbn: null
issn: 0307904X
issn1: 0307904X
issn2: 0307904X
issn3: 0307904X
jcr_value: '5.129'
keywords: Soil properties, Self-organizing Maps, Hydrological modeling, SWAT, Soils
  spatial patterns, SOM
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 4323.0
region: Northern America
scimago_value: 1011.0
sjr_best_quartile: Q1
sourceid: 28065.0
title_bib: Self-organizing map of soil properties in the context of hydrological modeling
title_csv: Applied mathematical modelling
total_cites: 24972.0
total_cites_(3years): 7948.0
total_docs._(2020): 578.0
total_docs._(3years): 1606.0
total_refs.: 24985.0
type: journal
type_publication: article
year: 2020
---
abstract: The number of applications being developed that require access to knowledge
  about the real world has increased rapidly over the past two decades. Domain ontologies,
  which formalize the terms being used in a discipline, have become essential for
  research in areas such as Machine Learning, the Internet of Things, Robotics, and
  Natural Language Processing, because they enable separate systems to exchange information.
  The quality of these domain ontologies, however, must be ensured for meaningful
  communication. Assessing the quality of domain ontologies for their suitability
  to potential applications remains difficult, even though a variety of frameworks
  and metrics have been developed for doing so. This article reviews domain ontology
  assessment efforts to highlight the work that has been carried out and to clarify
  the important issues that remain. These assessment efforts are classified into five
  distinct evaluation approaches and the state of the art of each described. Challenges
  associated with domain ontology assessment are outlined and recommendations are
  made for future research and applications.
author: McDaniel, Melinda and Storey, Veda C.
categories: Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)
citable_docs._(3years): 365.0
cites_/_doc._(2years): 1916.0
country: United States
coverage: 1969-2020
doi: 10.1145/3329124
eigenfactor_score: 0.01438
h_index: 163.0
isbn: null
issn: '03600300'
issn1: '03600300'
issn2: '15577341'
issn3: '03600300'
jcr_value: '10.282'
keywords: domain ontology, evaluation, assessment, metrics, Ontology, ontology development,
  ontology application, applied ontology, task-ontology fit
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 14160.0
region: Northern America
scimago_value: 2079.0
sjr_best_quartile: Q1
sourceid: 23038.0
title_bib: 'Evaluating domain ontologies: clarification, classification, and challenges'
title_csv: Acm computing surveys
total_cites: 10790.0
total_cites_(3years): 6978.0
total_docs._(2020): 112.0
total_docs._(3years): 365.0
total_refs.: 15859.0
type: journal
type_publication: article
year: 2019
---
abstract: "There is a growing recognition of the importance of proper urban design\
  \ in the improvement of air flow and pollution dispersion and in reducing human\
  \ exposure to air pollution. However, a limited number of studies have been published\
  \ so far focusing on the development of standard procedures which could be applied\
  \ by urban planners to effectively evaluate urban conditions with respect to air\
  \ quality. To fill this gap, a new approach for the determination of urban Air Quality\
  \ Management Zones (AQMZs) was proposed and presented based on two case studies:\
  \ Antwerp, Belgium and Gda\u0144sk, Poland. The main objectives of the study were\
  \ to 1) formulate a theoretical framework for the management of urban ventilation\
  \ potential and human exposure to air pollution and to 2) develop methods for its\
  \ implementation by means of a geographic information system (GIS). As a result\
  \ of the analysis, the typologies that may be associated with decreased ventilation\
  \ potential and the areas that require close monitoring due to potential human exposure\
  \ to air pollution were identified for both cities. It is advocated that delimiting\
  \ these typologies \u2013 combined with investigating local climate, wind and topography\
  \ conditions and air pollution characteristics \u2013 could constitute a preliminary\
  \ step in the urban planning process aimed at air quality improvement. These methods\
  \ can be further applied to other urban areas in order to indicate where detailed\
  \ studies are required and to facilitate the development of planning guidelines.\
  \ Moreover, the directions for further research and urban planning strategies were\
  \ discussed."
author: Joanna Badach and Dimitri Voordeckers and Lucyna Nyka and Maarten {Van Acker}
categories: Building and Construction (Q1); Civil and Structural Engineering (Q1);
  Environmental Engineering (Q1); Geography, Planning and Development (Q1)
citable_docs._(3years): 1687.0
cites_/_doc._(2years): 690.0
country: United Kingdom
coverage: 1976-2020
doi: 10.1016/j.buildenv.2020.106743
eigenfactor_score: 0.02925
h_index: 154.0
isbn: null
issn: '03601323'
issn1: '03601323'
issn2: '03601323'
issn3: '03601323'
jcr_value: '6.456'
keywords: Air quality management, Urban ventilation, Urban planning, Urban morphology,
  GIS-Based analysis
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5827.0
region: Western Europe
scimago_value: 1736.0
sjr_best_quartile: Q1
sourceid: 26874.0
title_bib: "A framework for air quality management zones - useful gis-based tool for\
  \ urban planning: case studies in antwerp and gda\u0144sk"
title_csv: Building and environment
total_cites: 38699.0
total_cites_(3years): 12000.0
total_docs._(2020): 754.0
total_docs._(3years): 1692.0
total_refs.: 43938.0
type: journal
type_publication: article
year: 2020
---
abstract: "This review discusses practical benefits and limitations of novel data-driven\
  \ research for social scientists in general and criminologists in particular by\
  \ providing a comprehensive examination of the matter. Specifically, this study\
  \ is an attempt to critically evaluate \u2018big data\u2019, data-driven perspectives,\
  \ and their epistemological value for both scholars and practitioners, particularly\
  \ those working on crime. It serves as guidance for those who are interested in\
  \ data-driven research by pointing out new research avenues. In addition to the\
  \ benefits, the drawbacks associated with data-driven approaches are also discussed.\
  \ Finally, critical problems that are emerging in this era, such as privacy and\
  \ ethical concerns are highlighted."
author: Turgut Ozkan
categories: Sociology and Political Science (Q2); Social Psychology (Q3)
citable_docs._(3years): 194.0
cites_/_doc._(2years): 161.0
country: United States
coverage: 1978, 1980, 1982-2020
doi: 10.1016/j.soscij.2018.10.010
eigenfactor_score: 0.0016699999999999998
h_index: 39.0
isbn: null
issn: 03623319
issn1: 03623319
issn2: 03623319
issn3: 03623319
jcr_value: '2.376'
keywords: Social science, Big data, Crime, Social media, Data-driven social science
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 5978.0
region: Northern America
scimago_value: 349.0
sjr_best_quartile: Q2
sourceid: 26437.0
title_bib: 'Criminology in the age of data explosion: new directions'
title_csv: Social science journal
total_cites: 1817.0
total_cites_(3years): 340.0
total_docs._(2020): 138.0
total_docs._(3years): 198.0
total_refs.: 8250.0
type: journal
type_publication: article
year: 2019
---
abstract: The coupling of computational thermodynamics and kinetics has been the central
  research theme in Integrated Computational Material Engineering (ICME). Two major
  bottlenecks in implementing this coupling and performing efficient ICME-guided high-throughput
  multi-component industrial alloys discovery or process parameters optimization,
  are slow responses in kinetic calculations to a given set of compositions and processing
  conditions and the quality of a large amount of calculated thermodynamic data. Here,
  we employ machine learning techniques to eliminate them, including (1) intelligent
  corrupt data detection and re-interpolation (i.e. data purge/cleaning) to a big
  tabulated thermodynamic dataset based on an unsupervised learning algorithm and
  (2) parameterization via artificial neural networks of the purged big thermodynamic
  dataset into a non-linear equation consisting of base functions and parameterization
  coefficients. The two techniques enable the efficient linkage of high-quality data
  with a previously developed microstructure model. This proposed approach not only
  improves the model performance by eliminating the interference of the corrupt data
  and stability due to the boundedness and continuity of the obtained non-linear equation
  but also dramatically reduces the running time and demand for computer physical
  memory simultaneously. The high computational robustness, efficiency, and accuracy,
  which are prerequisites for high-throughput computing, are verified by a series
  of case studies on multi-component aluminum, steel, and high-entropy alloys. The
  proposed data purge and parameterization methods are expected to apply to various
  microstructure simulation approaches or to bridging the multi-scale simulation where
  handling a large amount of input data is required. It is concluded that machine
  learning is a valuable tool in fueling the development of ICME and high throughput
  materials simulations.
author: "Yue Li and Bj\xF8rn Holmedal and Boyu Liu and Hongxiang Li and Linzhong Zhuang\
  \ and Jishan Zhang and Qiang Du and Jianxin Xie"
categories: Chemical Engineering (miscellaneous) (Q1); Chemistry (miscellaneous) (Q1);
  Computer Science Applications (Q2)
citable_docs._(3years): 316.0
cites_/_doc._(2years): 209.0
country: United Kingdom
coverage: 1977-2020
doi: 10.1016/j.calphad.2020.102231
eigenfactor_score: .nan
h_index: 62.0
isbn: null
issn: 03645916
issn1: 03645916
issn2: 03645916
issn3: 03645916
jcr_value: null
keywords: Materials informatics, Machine learning, High-throughput computing, Microstructure
  simulation, Tabulation
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5057.0
region: Western Europe
scimago_value: 757.0
sjr_best_quartile: Q1
sourceid: 24593.0
title_bib: Towards high-throughput microstructure simulation in compositionally complex
  alloys via machine learning
title_csv: 'Calphad: computer coupling of phase diagrams and thermochemistry'
total_cites: .nan
total_cites_(3years): 734.0
total_docs._(2020): 120.0
total_docs._(3years): 317.0
total_refs.: 6068.0
type: journal
type_publication: article
year: 2021
---
abstract: "There is a critical need to establish a global geochemical observation\
  \ network to provide data for monitoring the chemical changes of the Earths near-surface\
  \ environment. The International Centre on Global-scale Geochemistry, under auspices\
  \ of UNESCO and Government of China, has initiated an International Scientific Cooperation\
  \ Project called Mapping Chemical Earth. The project focuses on the establishment\
  \ of Global Geochemical Observatory Network for documenting baselines and changes\
  \ of nearly all natural chemical elements in the Earths surface and creating a digital\
  \ Chemical Earth platform allowing anyone to access vast amounts of geochemical\
  \ data through the Internet. A total area of about 37 million km2, nearly accounting\
  \ for 27% of the global land, has been covered by global-/continental-scale sampling.\
  \ Comparing the data of China, the US, Europe and Australia, the percentage of sites\
  \ with toxic metals exceeding the risk limits of soil pollution according to \u201C\
  Environmental Quality Standard for Soil of China (GB 15618-1995)\u201D to the total\
  \ sample sites is 30.9%, 17.1%, 23.5% and 10.9% in Europe, China, USA, and Australia\
  \ respectively. Comparing the China datasets of 15\_years interval sampling between\
  \ 1994, 1995 and in 2008\u20132012, toxic metals of As, Cd, Cr, Cu, Hg, Ni, Pb and\
  \ Zn, particularly Cd at top soils significantly increase from 1990s to 2010s. The\
  \ proportion of top soil samples exceeding the China Standard risk limit of 0.2\_\
  mg/kg Cd increases from 12.2% to 24.9%. The facts show that chemical changes of\
  \ toxic metals induced by human activities can be well observed using catchment\
  \ sediment sampling."
author: Xueqiu Wang and Bimin Zhang and Lanshi Nie and Wei Wang and Jian Zhou and
  Shanfa Xu and Qinhua Chi and Dongsheng Liu and Hanliang Liu and Zhixuan Han and
  Qingqing Liu and Mi Tian and Baoyun Zhang and Hui Wu and Ruihong Li and Qinghai
  Hu and Taotao Yan and Yanfang Gao
categories: Economic Geology (Q1); Geochemistry and Petrology (Q2)
citable_docs._(3years): 586.0
cites_/_doc._(2years): 366.0
country: Netherlands
coverage: 1972-2020
doi: 10.1016/j.gexplo.2020.106578
eigenfactor_score: 0.00675
h_index: 87.0
isbn: null
issn: '03756742'
issn1: '03756742'
issn2: '03756742'
issn3: '03756742'
jcr_value: '3.746'
keywords: Mapping Chemical Earth, Global Geochemical Observatory Networks, Geochemical
  baselines, Progress and challenge
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 7016.0
region: Western Europe
scimago_value: 994.0
sjr_best_quartile: Q1
sourceid: 23222.0
title_bib: 'Mapping chemical earth program: progress and challenge'
title_csv: Journal of geochemical exploration
total_cites: 9705.0
total_cites_(3years): 2522.0
total_docs._(2020): 146.0
total_docs._(3years): 596.0
total_refs.: 10243.0
type: journal
type_publication: article
year: 2020
---
abstract: "Quantification and forecasting of cost uncertainty for aerospace innovations\
  \ is challenged by conditions of small data which arises out of having few measurement\
  \ points, little prior experience, unknown history, low data quality, and conditions\
  \ of deep uncertainty. Literature research suggests that no frameworks exist which\
  \ specifically address cost estimation under such conditions. In order to provide\
  \ contemporary cost estimating techniques with an innovative perspective for addressing\
  \ such challenges a framework based on the principles of spatial geometry is described.\
  \ The framework consists of a method for visualising cost uncertainty and a dependency\
  \ model for quantifying and forecasting cost uncertainty. Cost uncertainty is declared\
  \ to represent manifested and unintended future cost variance with a probability\
  \ of 100% and an unknown quantity and innovative starting conditions considered\
  \ to exist when no verified and accurate cost model is available. The shape of data\
  \ is used as an organising principle and the attribute of geometrical symmetry of\
  \ cost variance point clouds used for the quantification of cost uncertainty. The\
  \ results of the investigation suggest that the uncertainty of a cost estimate at\
  \ any future point in time may be determined by the geometric symmetry of the cost\
  \ variance data in its point cloud form at the time of estimation. Recommendations\
  \ for future research include using the framework to determine the \u201Cmost likely\
  \ values\u201D of estimates in Monte Carlo simulations and generalising the dependency\
  \ model introduced. Future work is also recommended to reduce the framework limitations\
  \ noted."
author: Oliver Schwabe and Essam Shehab and John Erkoyuncu
categories: Aerospace Engineering (Q1); Mechanical Engineering (Q1); Mechanics of
  Materials (Q1)
citable_docs._(3years): 109.0
cites_/_doc._(2years): 1213.0
country: United Kingdom
coverage: 1961-1962, 1964-1968, 1970, 1972-1979, 1981, 1983-1992, 1994-2020
doi: 10.1016/j.paerosci.2016.05.001
eigenfactor_score: 0.00471
h_index: 113.0
isbn: null
issn: '03760421'
issn1: '03760421'
issn2: '03760421'
issn3: '03760421'
jcr_value: '8.653'
keywords: Cost estimate, Geometry, Symmetry, Topology, Uncertainty
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 17071.0
region: Western Europe
scimago_value: 2328.0
sjr_best_quartile: Q1
sourceid: 12274.0
title_bib: A framework for geometric quantification and forecasting of cost uncertainty
  for aerospace innovations
title_csv: Progress in aerospace sciences
total_cites: 4816.0
total_cites_(3years): 1463.0
total_docs._(2020): 35.0
total_docs._(3years): 109.0
total_refs.: 5975.0
type: journal
type_publication: article
year: 2016
---
abstract: Machine learning overfitting caused by data scarcity greatly limits the
  application of chemical artificial intelligence in membrane materials. As the original
  data for thin film polyamide nanofiltration membranes is limited, here we propose
  to extract the natural features of monomer molecular structures and rationally distort
  them to augment the data availability. This few-shot learning method allows a chemical
  engineering project to leverage the powerful fit of deep learning without big data
  at the outset, which is advantageous over traditional machine learning models. The
  rejection and flux predictions of polyamide nanofiltration membranes are practiced
  by the molecular augmentation in deep learning. Convergence of loss function indicates
  that the model is effectively optimized. Correlation coefficients over 0.80 and
  the mean relative error below 5% prove an accurate prediction of nanofiltration
  performance. The success of predicting nanofiltration membrane performances is widely
  instructive for molecule and material science.
author: Ziyang Zhang and Yingtao Luo and Huawen Peng and Yu Chen and Rong-Zhen Liao
  and Qiang Zhao
categories: Biochemistry (Q1); Filtration and Separation (Q1); Materials Science (miscellaneous)
  (Q1); Physical and Theoretical Chemistry (Q1)
citable_docs._(3years): 2621.0
cites_/_doc._(2years): 854.0
country: Netherlands
coverage: 1976-2021
doi: 10.1016/j.memsci.2020.118910
eigenfactor_score: 0.048510000000000005
h_index: 249.0
isbn: null
issn: 03767388
issn1: 03767388
issn2: 03767388
issn3: 03767388
jcr_value: '8.742'
keywords: Nanofiltration, Thin film composite membranes, Feature engineering, Machine
  learning, Data augmentation, Molecular vibration
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5452.0
region: Western Europe
scimago_value: 1929.0
sjr_best_quartile: Q1
sourceid: 26953.0
title_bib: Deep spatial representation learning of polyamide nanofiltration membranes
title_csv: Journal of membrane science
total_cites: 87097.0
total_cites_(3years): 22599.0
total_docs._(2020): 984.0
total_docs._(3years): 2622.0
total_refs.: 53650.0
type: journal
type_publication: article
year: 2021
---
abstract: "Yield gaps and water productivity are key indicators to monitor the progress\
  \ towards more sustainable and productive cropping systems. Individual farmers are\
  \ collecting increasing amounts of data (\u2018big data\u2019), which can help monitor\
  \ the process of sustainable intensification at local level. In this study, we build\
  \ upon such data to quantify the magnitude and identify the biophysical and management\
  \ determinants of on-farm yield gaps and water productivity for the main arable\
  \ crops cultivated in the Netherlands. The analysis focused on ware, seed and starch\
  \ potatoes, sugar beet, spring onion, winter wheat and spring barley and covered\
  \ the period 2015\u20132017. A crop modelling approach based on crop coefficients\
  \ (kc) and daily weather data was used to estimate the potential yield (Yp), radiation\
  \ intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were\
  \ estimated to be ca. 10% of Yp for sugar beet, 25\u201330% of Yp for ware, seed\
  \ and starch potato and spring barley, and 35\u201340% of Yp for spring onion and\
  \ winter wheat. Variation in actual yields was associated with water availability\
  \ in key periods of the growing season as well as with sowing and harvest dates.\
  \ However, the R2 of the fitted regressions was rather low (20\u201349%). Current\
  \ levels of crop water productivity ranged between 13\u202Fkg\u202FDM\u202Fha\u2212\
  1\u202Fmm\u22121 for spring barley, ca. 15 kg\u202FDM\u202Fha\u22121\u202Fmm\u2212\
  1 for seed potato, spring onion and winter wheat, 23\u202Fkg\u202FDM\u202Fha\u2212\
  1\u202Fmm\u22121 for ware potato and ca. 25\u202Fkg\u202FDM\u202Fha\u22121\u202F\
  mm\u22121 for starch potato and sugar beet. These values are about half of their\
  \ potential, but increasing actual water productivity further is restricted by rainfall\
  \ amount and distribution. However, doing so should not be prioritized over reducing\
  \ environmental impacts of these intensive cropping systems in the short-term and\
  \ may require large investments from farm to regional levels in the long-term. Although\
  \ these findings are most relevant to similar cropping systems in NW Europe, the\
  \ underlying methods are generic and can be used to benchmark crop performance in\
  \ other cropping systems. Based on this work, we argue that \u2018big data\u2019\
  \ are currently most useful to describe cropping systems at regional scale and derive\
  \ benchmarks of farm performance but not as much to predict and explain crop yield\
  \ variability in time and space."
author: "Jo\xE3o Vasco Silva and Tom\xE1s R. Tenreiro and L\xE9on Sp\xE4tjens and\
  \ Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma"
categories: Agronomy and Crop Science (Q1); Soil Science (Q1)
citable_docs._(3years): 795.0
cites_/_doc._(2years): 539.0
country: Netherlands
coverage: 1978-2020
doi: 10.1016/j.fcr.2020.107828
eigenfactor_score: 0.016069999999999997
h_index: 150.0
isbn: null
issn: 03784290
issn1: '18726852'
issn2: 03784290
issn3: '18726852'
jcr_value: '5.224'
keywords: Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5586.0
region: Western Europe
scimago_value: 1951.0
sjr_best_quartile: Q1
sourceid: 78796.0
title_bib: Can big data explain yield variability and water productivity in intensive
  cropping systems?
title_csv: Field crops research
total_cites: 24118.0
total_cites_(3years): 4752.0
total_docs._(2020): 191.0
total_docs._(3years): 798.0
total_refs.: 10669.0
type: journal
type_publication: article
year: 2020
---
abstract: 'In the big data era, search engine data (SED) have presented new opportunities
  for improving crude oil price prediction; however, the existing research were confined
  to single-language (mostly English) search keywords in SED collection. To address
  such a language bias and grasp worldwide investor attention, this study proposes
  a novel multilingual SED-driven forecasting methodology from a global perspective.
  The proposed methodology includes three main steps: (1) multilingual index construction,
  based on multilingual SED; (2) relationship investigation, between the multilingual
  index and crude oil price; and (3) oil price prediction, with the multilingual index
  as an informative predictor. With WTI spot price as studying samples, the empirical
  results indicate that SED have a powerful predictive power for crude oil price;
  nevertheless, multilingual SED statistically demonstrate better performance than
  single-language SED, in terms of enhancing prediction accuracy and model robustness.'
author: Jingjing Li and Ling Tang and Shouyang Wang
categories: Condensed Matter Physics (Q2); Statistics and Probability (Q2)
citable_docs._(3years): 4288.0
cites_/_doc._(2years): 360.0
country: Netherlands
coverage: 1975-2021
doi: 10.1016/j.physa.2020.124178
eigenfactor_score: .nan
h_index: 166.0
isbn: null
issn: 03784371
issn1: 03784371
issn2: 03784371
issn3: 03784371
jcr_value: null
keywords: Big data, Multilingual search engine index, Crude oil price forecasting,
  Google Trends, Artificial intelligence
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4277.0
region: Western Europe
scimago_value: 640.0
sjr_best_quartile: Q2
sourceid: 29115.0
title_bib: Forecasting crude oil price with multilingual search engine data
title_csv: 'Physica a: statistical mechanics and its applications'
total_cites: .nan
total_cites_(3years): 15067.0
total_docs._(2020): 1210.0
total_docs._(3years): 4300.0
total_refs.: 51750.0
type: journal
type_publication: article
year: 2020
---
abstract: This paper provides a survey of big data analytics applications and associated
  implementation issues. The emphasis is placed on applications that are novel and
  have demonstrated value to the industry, as illustrated using field data and practical
  applications. The paper reflects on the lessons learned from initial implementations,
  as well as ideas that are yet to be explored. The various data science trends treated
  in the literature are outlined, while experiences from applying them in the electricity
  grid setting are emphasized to pave the way for future applications. The paper ends
  with opportunities and challenges, as well as implementation goals and strategies
  for achieving impactful outcomes.
author: Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva
  and Tao Hong and Ricardo Bessa
categories: Electrical and Electronic Engineering (Q1); Energy Engineering and Power
  Technology (Q1)
citable_docs._(3years): 1158.0
cites_/_doc._(2years): 425.0
country: Netherlands
coverage: 1977-2021
doi: 10.1016/j.epsr.2020.106788
eigenfactor_score: 0.01287
h_index: 122.0
isbn: null
issn: 03787796
issn1: 03787796
issn2: 03787796
issn3: 03787796
jcr_value: '3.414'
keywords: Electricity grids, Analytics, Big data, Decision-making
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 3258.0
region: Western Europe
scimago_value: 845.0
sjr_best_quartile: Q1
sourceid: 16044.0
title_bib: Big data analytics for future electricity grids
title_csv: Electric power systems research
total_cites: 13115.0
total_cites_(3years): 5369.0
total_docs._(2020): 643.0
total_docs._(3years): 1165.0
total_refs.: 20949.0
type: journal
type_publication: article
year: 2020
---
abstract: "Wildfires, whether natural or caused by humans, are considered among the\
  \ most dangerous and devastating disasters around the world. Their complexity comes\
  \ from the fact that they are hard to predict, hard to extinguish and cause enormous\
  \ financial losses. To address this issue, many research efforts have been conducted\
  \ in order to monitor, predict and prevent wildfires using several Artificial Intelligence\
  \ techniques and strategies such as Big Data, Machine Learning, and Remote Sensing.\
  \ The latter offers a rich source of satellite images, from which we can retrieve\
  \ a huge amount of data that can be used to monitor wildfires. The method used in\
  \ this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial\
  \ Neural Network and SVM) to process data collected from satellite images over large\
  \ areas and extract insights from them to predict the occurrence of wildfires and\
  \ avoid such disasters. For this reason, we implemented a methodology that serves\
  \ this purpose by building a dataset based on Remote Sensing data related to the\
  \ state of the crops (NDVI), meteorological conditions (LST), as well as the fire\
  \ indicator \u201CThermal Anomalies\u201D, these data, were acquired from \u201C\
  MODIS\u201D (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard\
  \ the Terra and Aqua satellites. This dataset is available on GitHub via this link\
  \ (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the\
  \ big data platform \u201CDatabricks\u201D. Experimental results gave high prediction\
  \ accuracy (98.32%). These results were assessed using several validation strategies\
  \ (e.g., classification metrics, cross-validation, and regularization) as well as\
  \ a comparison with some wildfire early warning systems."
author: Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}
categories: Chemistry (miscellaneous) (Q1); Materials Science (miscellaneous) (Q1);
  Physics and Astronomy (miscellaneous) (Q1); Safety, Risk, Reliability and Quality
  (Q1)
citable_docs._(3years): 437.0
cites_/_doc._(2years): 313.0
country: United Kingdom
coverage: 1977-2020
doi: 10.1016/j.firesaf.2019.01.006
eigenfactor_score: 0.00398
h_index: 78.0
isbn: null
issn: 03797112
issn1: 03797112
issn2: 03797112
issn3: 03797112
jcr_value: '2.764'
keywords: Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining,
  Artificial intelligence
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 3225.0
region: Western Europe
scimago_value: 958.0
sjr_best_quartile: Q1
sourceid: 28425.0
title_bib: 'Predictive modeling of wildfires: a new dataset and machine learning approach'
title_csv: Fire safety journal
total_cites: 5861.0
total_cites_(3years): 1566.0
total_docs._(2020): 260.0
total_docs._(3years): 442.0
total_refs.: 8386.0
type: journal
type_publication: article
year: 2019
---
abstract: The Great Lakes are a vital resource for drinking water and recreation and
  provide a major fishery for millions of people. As part of the Great Lakes Water
  Quality Agreement, the US and Canadian governments have been charged with the protection
  of this system. Persistent, bioaccumulative, and toxic (PBTs) contaminants were
  found to be affecting the lake water quality as early as the late 1960s, and various
  programs sponsored by the US and Canada have been created to monitor PBTs such as
  polychlorinated biphenyls (PCBs) and organochlorine pesticides (OCPs). These programs
  have refined measurement techniques to quantify trace level contaminants using a
  targeted analytical approach. However, new PBTs are being detected in the environment,
  and the traditional targeted methodology is inadequate for understanding the complex
  chemical mixture affecting Great Lakes wildlife. Fortunately, new analytical technologies
  are emerging that allow for comprehensive screening of PBTs beyond targeted methods.
  The current commentary presents an outline of a new framework for contemporary monitoring
  programs. The goal is to facilitate the compilation of legacy, emerging PBT, and
  archive PBT signatures by utilizing the basic practices of traditional targeted
  analysis. This example focuses on fish monitoring programs, and how they are ideally
  suited for legacy monitoring as well as data-driven discovery of new chemicals of
  concern.
author: Bernard S. Crimmins and Harry B. McCarty and Sujan Fernando and Michael S.
  Milligan and James J. Pagano and Thomas M. Holsen and Philip K. Hopke
categories: Aquatic Science (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics
  (Q2)
citable_docs._(3years): 375.0
cites_/_doc._(2years): 255.0
country: United States
coverage: 1970, 1975-2020
doi: 10.1016/j.jglr.2018.07.011
eigenfactor_score: 0.004079999999999999
h_index: 78.0
isbn: null
issn: 03801330
issn1: 03801330
issn2: 03801330
issn3: 03801330
jcr_value: '2.480'
keywords: Great Lakes, Monitoring, Fish, Non-targeted, Persistent bioacccumulative
  toxics (PBTs), Emerging chemicals of concern
publisher_x: International Association of Great Lakes Research
publisher_y: null
ref._/_doc.: 6581.0
region: Northern America
scimago_value: 720.0
sjr_best_quartile: Q2
sourceid: 17510.0
title_bib: 'Commentary: integrating non-targeted and targeted chemical screening in
  great lakes fish monitoring programs'
title_csv: Journal of great lakes research
total_cites: 5567.0
total_cites_(3years): 924.0
total_docs._(2020): 204.0
total_docs._(3years): 390.0
total_refs.: 13425.0
type: journal
type_publication: article
year: 2018
---
abstract: Rapid advancements in biotechnologies such as -omic (genomics, proteomics,
  metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies,
  molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine
  and health wellness. Multiple nations around the world have been seeking novel effective
  ways to make sense of "big data" for evidence-based, outcome-driven, and affordable
  5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare.
  My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular,
  whole body, individual, and population) biomedical data analytics for discovery,
  development, and delivery, including translational bioinformatics in biomarker discovery
  for personalized care; imaging informatics in histopathology for clinical diagnosis
  decision support; bionanoinformatics for minimally-invasive image-guided surgery;
  critical care informatics in ICU for real-time evidence-based decision making; and
  chronic care informatics for patient-centric health. In this talk, first, I will
  highlight major challenges in biomedical and health informatics pipeline consisting
  of data quality control, information feature extraction, advanced knowledge modeling,
  decision making, and proper action taking through feedback. Second, I will present
  informatics methodological research in (i) data integrity and integration; (ii)
  case-based reasoning for individualized care; and (iii) streaming data analytics
  for real-time decision support using a few mobile health case studies (e.g. Sickle
  Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there
  is big shortage of data scientists and engineers who are capable of handling Big
  Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e.
  patients, physicians, payers, and hospitals) on how to tackle these grant challenges.
  I will discuss efforts such as patient-centric educational intervention, community-based
  crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has
  been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition,
  Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta,
  Atlanta Clinical and Translational Science Institute, and industrial partners such
  as Microsoft Research and HP.
author: Wang, May D.
categories: Computer Science Applications; Software
citable_docs._(3years): 834.0
cites_/_doc._(2years): 123.0
country: United States
coverage: 1979-2019
doi: 10.1109/COMPSAC.2015.343
eigenfactor_score: .nan
h_index: 47.0
isbn: null
issn: '07303157'
issn1: '07303157'
issn2: '07306512'
issn3: '07303157'
jcr_value: null
keywords: Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 216.0
sjr_best_quartile: '-'
sourceid: 18705.0
title_bib: Biomedical big data analytics for patient-centric and outcome-driven precision
  health
title_csv: Proceedings - ieee computer society's international computer software and
  applications conference
total_cites: .nan
total_cites_(3years): 1110.0
total_docs._(2020): 0.0
total_docs._(3years): 908.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2015
---
abstract: "The ability to evaluate empirical diffusion MRI acquisitions for quality\
  \ and to correct the resulting imaging metrics allows for improved inference and\
  \ increased replicability. Previous work has shown promise for estimation of bias\
  \ and variance of generalized fractional anisotropy (GFA) but comes at the price\
  \ of computational complexity. This paper aims to provide methods for estimating\
  \ GFA, bias of GFA and standard deviation of GFA quickly and accurately. In order\
  \ to provide a method for bias and variance estimation that can return results faster\
  \ than the previously studied statistical techniques, three deep, fully-connected\
  \ neural networks are developed for GFA, bias of GFA, and standard deviation of\
  \ GFA. The results of these networks are compared to the observed values of the\
  \ metrics as well as those fit from the statistical techniques (i.e. Simulation\
  \ Extrapolation (SIMEX) for bias estimation and wild bootstrap for variance estimation).\
  \ Our GFA network provides predictions that are closer to the true GFA values than\
  \ a Q-ball fit of the observed data (root-mean-square error (RMSE) 0.0077 vs 0.0082,\
  \ p\u202F<\u202F.001). The bias network also shows statistically significant improvement\
  \ in comparison to the SIMEX-estimated error of GFA (RMSE 0.0071 vs. 0.01, p\u202F\
  <\u202F.001)."
author: Allison E. Hainline and Vishwesh Nath and Prasanna Parvathaneni and Kurt G.
  Schilling and Justin A. Blaber and Adam W. Anderson and Hakmook Kang and Bennett
  A. Landman
categories: Biomedical Engineering (Q2); Biophysics (Q2); Radiology, Nuclear Medicine
  and Imaging (Q2)
citable_docs._(3years): 682.0
cites_/_doc._(2years): 250.0
country: United States
coverage: 1982, 1984-2020
doi: 10.1016/j.mri.2019.03.021
eigenfactor_score: 0.00787
h_index: 111.0
isbn: null
issn: 0730725X
issn1: '18735894'
issn2: 0730725X
issn3: '18735894'
jcr_value: '2.546'
keywords: HARDI, Q-ball, Bias correction, GFA, Measurement error, Neural network
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 3885.0
region: Northern America
scimago_value: 723.0
sjr_best_quartile: Q2
sourceid: 17264.0
title_bib: A deep learning approach to estimation of subject-level bias and variance
  in high angular resolution diffusion imaging
title_csv: Magnetic resonance imaging
total_cites: 8690.0
total_cites_(3years): 1720.0
total_docs._(2020): 211.0
total_docs._(3years): 692.0
total_refs.: 8197.0
type: journal
type_publication: article
year: 2019
---
abstract: "Social Networking Services (SNSs) connect people worldwide, where they\
  \ communicate through sharing contents, photos, videos, posting their first-hand\
  \ opinions, comments, and following their friends. Social networks are characterized\
  \ by velocity, volume, value, variety, and veracity, the 5\_V\u2019s of big data.\
  \ Hence, big data analytic techniques and frameworks are commonly exploited in Social\
  \ Network Analysis (SNA). By the ever-increasing growth of social networks, the\
  \ analysis of social data, to describe and find communication patterns among users\
  \ and understand their behaviors, has attracted much attention. In this paper, we\
  \ demonstrate how big data analytics meets social media, and a comprehensive review\
  \ is provided on big data analytic approaches in social networks to search published\
  \ studies between 2013 and August 2020, with 74 identified papers. The findings\
  \ of this paper are presented in terms of main journals/conferences, yearly distributions,\
  \ and the distribution of studies among publishers. Furthermore, the big data analytic\
  \ approaches are classified into two main categories: Content-oriented approaches\
  \ and network-oriented approaches. The main ideas, evaluation parameters, tools,\
  \ evaluation methods, advantages, and disadvantages are also discussed in detail.\
  \ Finally, the open challenges and future directions that are worth further investigating\
  \ are discussed."
author: Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour
  and Seyed Mahdi Jameii
categories: Communication (Q1); Computer Networks and Communications (Q1); Electrical
  and Electronic Engineering (Q1); Law (Q1)
citable_docs._(3years): 438.0
cites_/_doc._(2years): 745.0
country: United Kingdom
coverage: 1984-2020
doi: 10.1016/j.tele.2020.101517
eigenfactor_score: 0.00899
h_index: 66.0
isbn: null
issn: 07365853
issn1: 07365853
issn2: 07365853
issn3: 07365853
jcr_value: '6.182'
keywords: Social networks, Big data, Content analysis, Sentiment analysis, Systematic
  literature review
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 7228.0
region: Western Europe
scimago_value: 1567.0
sjr_best_quartile: Q1
sourceid: 20896.0
title_bib: 'Big data analytics meets social media: a systematic review of techniques,
  open issues, and future directions'
title_csv: Telematics and informatics
total_cites: 5351.0
total_cites_(3years): 3832.0
total_docs._(2020): 96.0
total_docs._(3years): 483.0
total_refs.: 6939.0
type: journal
type_publication: article
year: 2021
---
abstract: "A group of researchers, consultants, software developers, and transit agencies\
  \ convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled\
  \ \u201CHarnessing Big Data\u201D, to present their recent research and discuss\
  \ the state of practice, state of the art, and future directions of big data in\
  \ public transportation. This report documents their discussion. The key conclusion\
  \ of the workshop is that, although much progress has been made in utilizing big\
  \ data to improve transportation planning and operations, much remains to be done,\
  \ both in terms of developing further analysis tools and use cases of big data,\
  \ and of disseminating best practices so that they are adopted across the industry."
author: "Gabriel E. S\xE1nchez-Mart\xEDnez and Marcela Munizaga"
categories: Economics, Econometrics and Finance (miscellaneous) (Q1); Transportation
  (Q1)
citable_docs._(3years): 205.0
cites_/_doc._(2years): 279.0
country: United States
coverage: 1994, 1996, 1999, 2001, 2004-2020
doi: 10.1016/j.retrec.2016.10.008
eigenfactor_score: 0.00218
h_index: 46.0
isbn: null
issn: 07398859
issn1: 07398859
issn2: 07398859
issn3: 07398859
jcr_value: '2.627'
keywords: Big data, Measurement, Implementation challenges, Analysis tools, Transit
  best practices
publisher_x: JAI Press
publisher_y: null
ref._/_doc.: 4648.0
region: Northern America
scimago_value: 1019.0
sjr_best_quartile: Q1
sourceid: 4100151536.0
title_bib: 'Workshop 5 report: harnessing big data'
title_csv: Research in transportation economics
total_cites: 2095.0
total_cites_(3years): 694.0
total_docs._(2020): 160.0
total_docs._(3years): 220.0
total_refs.: 7437.0
type: journal
type_publication: article
year: 2016
---
abstract: "The myriad potential benefits of digital farming hinge on the promise of\
  \ increased accuracy, which allows \u2018doing more with less\u2019 through precise,\
  \ data-driven operations. Yet, precision farming's foundational claim of increased\
  \ accuracy has hardly been the subject of comprehensive examination. Drawing on\
  \ social science studies of big data, this article examines digital agriculture's\
  \ (in)accuracies and their repercussions. Based on an examination of the daily functioning\
  \ of the various components of yield mapping, it finds that digital farming is often\
  \ \u2018precisely inaccurate\u2019, with the high volume and granularity of big\
  \ data erroneously equated with high accuracy. The prevailing discourse of \u2018\
  ultra-precise\u2019 digital technologies ignores farmers' essential efforts in making\
  \ these technologies more accurate, via calibration, corroboration and interpretation.\
  \ We suggest that there is the danger of a \u2018precision trap\u2019. Namely, an\
  \ exaggerated belief in the precision of big data that over time leads to an erosion\
  \ of checks and balances (analogue data, farmer observation et cetera) on farms.\
  \ The danger of \u2018precision traps\u2019 increases with the opacity of algorithms,\
  \ with shifts from real-time measurement and advice towards forecasting, and with\
  \ farmers' increased remoteness from field operations. Furthermore, we identify\
  \ an emerging \u2018precision divide\u2019: unequally distributed precision benefits\
  \ resulting from the growing algorithmic divide between farmers focusing on staple\
  \ crops, catered well by technological innovation on the one hand, and farmers cultivating\
  \ other crops, who have to make do with much less advanced or applicable algorithms\
  \ on the other. Consequently, for the latter farms digital farming may feel more\
  \ like \u2018imprecision farming\u2019."
author: Oane Visser and Sarah Ruth Sippel and Louis Thiemann
categories: Development (Q1); Forestry (Q1); Geography, Planning and Development (Q1);
  Sociology and Political Science (Q1)
citable_docs._(3years): 557.0
cites_/_doc._(2years): 443.0
country: United Kingdom
coverage: 1985-2020
doi: 10.1016/j.jrurstud.2021.07.024
eigenfactor_score: 0.01005
h_index: 104.0
isbn: null
issn: '07430167'
issn1: '07430167'
issn2: '07430167'
issn3: '07430167'
jcr_value: '4.849'
keywords: Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big
  data
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 7409.0
region: Western Europe
scimago_value: 1497.0
sjr_best_quartile: Q1
sourceid: 15673.0
title_bib: Imprecision farming? examining the (in)accuracy and risks of digital agriculture
title_csv: Journal of rural studies
total_cites: 9142.0
total_cites_(3years): 2862.0
total_docs._(2020): 280.0
total_docs._(3years): 573.0
total_refs.: 20744.0
type: journal
type_publication: article
year: 2021
---
abstract: Real-time monitoring of cloud resources is crucial for a variety of tasks
  such as performance analysis, workload management, capacity planning and fault detection.
  Applications producing big data make the monitoring task very difficult at high
  sampling frequencies because of high computational and communication overheads in
  collecting, storing, and managing information. We present an adaptive algorithm
  for monitoring big data applications that adapts the intervals of sampling and frequency
  of updates to data characteristics and administrator needs. Adaptivity allows us
  to limit computational and communication costs and to guarantee high reliability
  in capturing relevant load changes. Experimental evaluations performed on a large
  testbed show the ability of the proposed adaptive algorithm to reduce resource utilization
  and communication overhead of big data monitoring without penalizing the quality
  of data, and demonstrate our improvements to the state of the art.
author: Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi
categories: Computer Networks and Communications (Q1); Hardware and Architecture (Q1);
  Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)
citable_docs._(3years): 568.0
cites_/_doc._(2years): 472.0
country: United States
coverage: 1984-2021
doi: 10.1016/j.jpdc.2014.08.007
eigenfactor_score: 0.00477
h_index: 87.0
isbn: null
issn: '07437315'
issn1: '07437315'
issn2: '10960848'
issn3: '07437315'
jcr_value: '3.734'
keywords: Adaptivity, Monitoring, Cloud computing, Big data, Scalability
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 4240.0
region: Northern America
scimago_value: 638.0
sjr_best_quartile: Q1
sourceid: 25621.0
title_bib: Adaptive, scalable and reliable monitoring of big data on clouds
title_csv: Journal of parallel and distributed computing
total_cites: 4371.0
total_cites_(3years): 2473.0
total_docs._(2020): 169.0
total_docs._(3years): 592.0
total_refs.: 7166.0
type: journal
type_publication: article
year: 2015
---
abstract: 'ABSTRACT

  Objectives

  Predictive risk models are advocated in psychosocial oncology practice to provide
  timely and appropriate support to those likely to experience the emotional and psychological
  consequences of cancer and its treatments. New digital technologies mean that large
  scale and routine data collection are becoming part of everyday clinical practice.
  Using these data to try to identify those at greatest risk for late psychosocial
  effects of cancer is an attractive proposition in a climate of unmet need and limited
  resource. In this paper, we present a framework to support the development of high-quality
  predictive risk models in psychosocial and supportive oncology. The aim is to provide
  awareness and increase accessibility of best practice literature to support researchers
  in psychosocial and supportive care to undertake a structured evidence-based approach.

  Data Sources

  Statistical prediction risk model publications.

  Conclusion

  In statistical modeling and data science different approaches are needed if the
  goal is to predict rather than explain. The deployment of a poorly developed and
  tested predictive risk model has the potential to do great harm. Recommendations
  for best practice to develop predictive risk models have been developed but there
  appears to be little application within psychosocial and supportive oncology care.

  Implications for Nursing Practice

  Use of best practice evidence will ensure the development and validation of predictive
  models that are robust as these are currently lacking. These models have the potential
  to enhance supportive oncology care through harnessing routine digital collection
  of patient-reported outcomes and the targeting of interventions according to risk
  characteristics.'
author: Jenny Harris and Edward Purssell and Emma Ream and Anne Jones and Jo Armes
  and Victoria Cornelius
categories: Oncology (nursing) (Q2)
citable_docs._(3years): 178.0
cites_/_doc._(2years): 186.0
country: United Kingdom
coverage: 1985-2020
doi: 10.1016/j.soncn.2020.151089
eigenfactor_score: 0.0016899999999999999
h_index: 45.0
isbn: null
issn: 07492081
issn1: '18783449'
issn2: 07492081
issn3: '18783449'
jcr_value: '2.315'
keywords: Predictive risk models, Regression models, Cancer, Psychosocial, Supportive
  care, Psychological, Distress
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 4950.0
region: Western Europe
scimago_value: 596.0
sjr_best_quartile: Q2
sourceid: 29925.0
title_bib: How to develop statistical predictive risk models in oncology nursing to
  enhance psychosocial and supportive care
title_csv: Seminars in oncology nursing
total_cites: 1412.0
total_cites_(3years): 396.0
total_docs._(2020): 66.0
total_docs._(3years): 195.0
total_refs.: 3267.0
type: journal
type_publication: article
year: 2020
---
abstract: "Machine learning has seen slow but steady uptake in diagnostic pathology\
  \ over the past decade to assess digital whole-slide images. Machine learning tools\
  \ have incredible potential to standardise, and likely even improve, histopathologic\
  \ diagnoses, but they are not yet widely used in clinical practice. We describe\
  \ the principles of these tools and technologies and some successful preclinical\
  \ and pretranslational efforts in cardiovascular pathology, as well as a roadmap\
  \ for moving forward. In nonhuman animal models, one proof-of-principle application\
  \ is in rodent progressive cardiomyopathy, which is of particular significance to\
  \ drug toxicity studies. Basic science successes include screening the quality of\
  \ differentiated stem cells and characterising cardiomyocyte developmental stages,\
  \ with potential applications for research and toxicology/drug safety screening\
  \ using derived or native human pluripotent stem cells differentiated into cardiomyocytes.\
  \ Translational studies of particular note include those with success in diagnosing\
  \ the various forms of heart allograft rejection. For fully realising the value\
  \ of these tools in clinical cardiovascular pathology, we identify 3 essential challenges.\
  \ First is image quality standardisation to ensure that algorithms can be developed\
  \ and implemented on robust, consistent data. The second is consensus diagnosis;\
  \ experts don\u2019t always agree, and thus \u201Ctruth\u201D may be difficult to\
  \ establish, but the algorithms themselves may provide a solution. The third is\
  \ the need for large-enough data sets to facilitate robust algorithm development,\
  \ necessitating large cross-institutional shared image databases. The power of histopathology-based\
  \ machine learning technologies is tremendous, and we outline the next steps needed\
  \ to capitalise on this power.\nR\xE9sum\xE9\nAu cours de la derni\xE8re d\xE9cennie,\
  \ l\u2019apprentissage automatique a connu une adoption lente, mais constante, dans\
  \ l\u2019analyse des images num\xE9ris\xE9es de lamelle enti\xE8re pour le diagnostic\
  \ de maladies. Les outils d\u2019apprentissage automatique ont un fabuleux potentiel\
  \ de standardisation, voire d\u2019am\xE9lioration, des diagnostics histopathologiques;\
  \ ils ne sont toutefois pas encore largement utilis\xE9s en pratique clinique. Cet\
  \ article vise \xE0 expliquer les principes de ces outils et technologies et \xE0\
  \ pr\xE9senter certaines exp\xE9riences pr\xE9cliniques et pr\xE9translationnelles\
  \ fructueuses en pathologie cardiovasculaire, de m\xEAme qu\u2019une feuille de\
  \ route pour aller de l\u2019avant. Dans les mod\xE8les animaux non humains, la\
  \ cardiomyopathie \xE9volutive chez le rongeur constitue une d\xE9monstration de\
  \ principe qui rev\xEAt une importance particuli\xE8re pour le succ\xE8s des \xE9\
  tudes toxicologiques. Les r\xE9ussites en sciences fondamentales comprennent la\
  \ s\xE9lection de cellules souches diff\xE9renci\xE9es de qualit\xE9 et la caract\xE9\
  risation des \xE9tapes du d\xE9veloppement des cardiomyocytes, de m\xEAme que leurs\
  \ applications potentielles en recherche et en analyse toxicologique/de l\u2019\
  innocuit\xE9 des m\xE9dicaments \xE0 l\u2019aide de cellules souches pluripotentes\
  \ humaines ou d\xE9riv\xE9es se diff\xE9renciant en cardiomyocytes. Les \xE9tudes\
  \ translationnelles notables comprennent celles ayant permis de diagnostiquer les\
  \ diverses formes de rejet d\u2019allogreffe cardiaque. Pour pleinement comprendre\
  \ la valeur de ces outils en pathologie clinique cardiovasculaire, nous avons rep\xE9\
  r\xE9 trois d\xE9fis essentiels. Le premier est la standardisation de la qualit\xE9\
  \ des images pour garantir que les algorithmes soient \xE9labor\xE9s et mis en \u0153\
  uvre \xE0 partir de donn\xE9es solides et coh\xE9rentes. Le second est le diagnostic\
  \ consensuel; les experts ne sont pas toujours d\u2019accord, et par cons\xE9quent,\
  \ il peut \xEAtre difficile d\u2019\xE9tablir la \xAB v\xE9rit\xE9 \xBB, mais les\
  \ algorithmes pourraient eux-m\xEAmes fournir une solution. Le troisi\xE8me est\
  \ la n\xE9cessit\xE9 de constituer des ensembles de donn\xE9es suffisantes pour\
  \ favoriser l\u2019\xE9laboration d\u2019algorithmes solides, ce qui n\xE9cessite\
  \ de vastes bases de donn\xE9es d\u2019images partag\xE9es entre les \xE9tablissements.\
  \ La puissance des technologies d\u2019apprentissage automatique fond\xE9es sur\
  \ l\u2019histopathologie est immense; nous d\xE9crirons les prochaines \xE9tapes\
  \ qui nous permettront de tirer profit de leur puissance."
author: Carolyn Glass and Kyle J. Lafata and William Jeck and Roarke Horstmeyer and
  Colin Cooke and Jeffrey Everitt and Matthew Glass and David Dov and Michael A. Seidman
categories: Cardiology and Cardiovascular Medicine (Q1)
citable_docs._(3years): 691.0
cites_/_doc._(2years): 249.0
country: Canada
coverage: 1985-2020
doi: 10.1016/j.cjca.2021.11.008
eigenfactor_score: 0.015090000000000001
h_index: 90.0
isbn: null
issn: 0828282X
issn1: '19167075'
issn2: 0828282X
issn3: '19167075'
jcr_value: '5.223'
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2647.0
region: Northern America
scimago_value: 1395.0
sjr_best_quartile: Q1
sourceid: 22504.0
title_bib: The role of machine learning in cardiovascular pathology
title_csv: Canadian journal of cardiology
total_cites: 8782.0
total_cites_(3years): 2482.0
total_docs._(2020): 342.0
total_docs._(3years): 921.0
total_refs.: 9052.0
type: journal
type_publication: article
year: 2022
---
abstract: "Tunnel fire is one of the most severe global fire hazards and causes a\
  \ significant amount of economic losses and casualties every year. Over the last\
  \ 50\_years, numerous full-scale and reduced-scale tunnel fire tests, as well as\
  \ numerical simulations have been conducted to quantify the critical fire events\
  \ and key parameters to guide the fire safety design of the tunnel. In light of\
  \ the recent advances in big data and artificial intelligence, this paper aims to\
  \ establish a database that contains all existing experimental data of tunnel fire,\
  \ based on an extensive literature review on tunnel fire tests. This tunnel-fire\
  \ database summarizes seven key parameters of flame, ventilation, and smoke in that\
  \ is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database.\
  \ The test conditions, experimental phenomena, and data of each literature work\
  \ were organized and categorized in a standard format that could be conveniently\
  \ accessed and continuously updated. Based on this database, machine learning is\
  \ applied to predict the critical ventilation velocity of a tunnel fire as a demonstration.\
  \ The review of the current database not only reveals more valuable information\
  \ and hidden problems in the conventional collection of test data, but also provides\
  \ new directions in future tunnel fire research. The established database and methodology\
  \ help promote the application of artificial intelligence and smart firefighting\
  \ in tunnel fire safety."
author: Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan
  Huang and Fu Xiao and Asif Usmani
categories: Building and Construction (Q1); Geotechnical Engineering and Engineering
  Geology (Q1)
citable_docs._(3years): 942.0
cites_/_doc._(2years): 653.0
country: United Kingdom
coverage: 1986-2020
doi: 10.1016/j.tust.2020.103691
eigenfactor_score: 0.0149
h_index: 98.0
isbn: null
issn: 08867798
issn1: 08867798
issn2: 08867798
issn3: 08867798
jcr_value: '5.915'
keywords: Big data, Empirical model, Deep learning, Critical event, Smart firefighting
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4400.0
region: Western Europe
scimago_value: 2172.0
sjr_best_quartile: Q1
sourceid: 14642.0
title_bib: Perspectives of big experimental database and artificial intelligence in
  tunnel fire research
title_csv: Tunnelling and underground space technology
total_cites: 16336.0
total_cites_(3years): 6440.0
total_docs._(2020): 375.0
total_docs._(3years): 946.0
total_refs.: 16500.0
type: journal
type_publication: article
year: 2021
---
abstract: The onset of the Internet of Things enables machines to be outfitted with
  always-on sensors that can provide health information to cloud-based monitoring
  systems for prognostics and health management (PHM), which greatly improves reliability
  and avoids downtime of machines and processes on the shop floor. On the other hand,
  real-time monitoring produces large amounts of data, leading to significant challenges
  for efficient and effective data transmission (from the shop floor to the cloud)
  and analysis (in the cloud). Restricted by industrial hardware capability, especially
  Internet bandwidth, most solutions approach data transmission from the perspective
  of data compression (before transmission, at local computing devices) coupled with
  data reconstruction (after transmission, in the cloud). However, existing data compression
  techniques may not adapt to domain-specific characteristics of data, and hence have
  limitations in addressing high compression ratios where full restoration of signal
  details is important for revealing machine conditions. This study integrates Deep
  Convolutional Autoencoders (DCAE) with local structure and physics-informed loss
  terms that incorporate PHM domain knowledge such as the importance of frequency
  content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing
  (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating
  conditions on reconstruction fidelity. The proposed methods are evaluated on two
  case studies, and autocorrelation-based noise analysis provides insight into the
  relative performance across machine health and operating conditions. Results indicate
  that physically-informed DCAE compression outperforms prevalent data compression
  approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete
  Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further
  improve the data reconstruction quality for certain machine conditions.
author: Matthew Russell and Peng Wang
categories: Aerospace Engineering (Q1); Civil and Structural Engineering (Q1); Computer
  Science Applications (Q1); Control and Systems Engineering (Q1); Mechanical Engineering
  (Q1); Signal Processing (Q1)
citable_docs._(3years): 2017.0
cites_/_doc._(2years): 795.0
country: United States
coverage: 1987-2021
doi: 10.1016/j.ymssp.2021.108709
eigenfactor_score: 0.03775
h_index: 167.0
isbn: null
issn: 08883270
issn1: 08883270
issn2: '10961216'
issn3: 08883270
jcr_value: '6.823'
keywords: Physics-informed deep learning, Prognostics and health management, Data
  compression, Big data
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4336.0
region: Northern America
scimago_value: 2275.0
sjr_best_quartile: Q1
sourceid: 21080.0
title_bib: Physics-informed deep learning for signal compression and reconstruction
  of big data in industrial condition monitoring
title_csv: Mechanical systems and signal processing
total_cites: 30686.0
total_cites_(3years): 16068.0
total_docs._(2020): 538.0
total_docs._(3years): 2031.0
total_refs.: 23327.0
type: journal
type_publication: article
year: 2022
---
abstract: Tool condition monitoring (TCM) in machining operations is crucial to maximise
  the useful tool life while reducing the risks associated with tool breakage. Unlike
  progressive tool wear, tool breakage occurs randomly, with more severe implications
  for workpiece quality, machining system stiffness, and even operator safety. Existing
  literature reviews on TCM focus on tool wear monitoring, including wear state recognition
  and remaining useful life prediction. However, a comprehensive review of tool breakage
  monitoring (TBM) techniques is lacking. Generic signal processing and intelligent
  decision-making methods cannot fully satisfy the practical requirements of the TBM.
  In addition, developing and evaluating TBM models using imbalanced data is more
  challenging. Herein, we present the first systematic review on TBM to bridge these
  limitations, and provide adequate guidance for avoiding catastrophic tool failures
  during cutting processes. Signal acquisition, feature extraction, and decision-making
  methodologies for the TBM are outlined and compared with related techniques for
  tool wear monitoring. The effects of data imbalance on TBM models are considered,
  and feasible solutions are provided at the data and algorithm levels. Finally, the
  challenges faced by the TBM are discussed, and potential research directions are
  suggested. The research and application of TBM techniques will certainly better
  empower various machining operations in response to intelligent manufacturing demands.
author: Xuebing Li and Xianli Liu and Caixu Yue and Steven Y. Liang and Lihui Wang
categories: Industrial and Manufacturing Engineering (Q1); Mechanical Engineering
  (Q1)
citable_docs._(3years): 241.0
cites_/_doc._(2years): 824.0
country: United Kingdom
coverage: 1987-2020
doi: 10.1016/j.ijmachtools.2022.103882
eigenfactor_score: .nan
h_index: 156.0
isbn: null
issn: 08906955
issn1: 08906955
issn2: 08906955
issn3: 08906955
jcr_value: null
keywords: Cutting tools, Tool breakage monitoring, Signal acquisition, Feature extraction,
  Intelligent decision-making, Imbalanced data
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4738.0
region: Western Europe
scimago_value: 3418.0
sjr_best_quartile: Q1
sourceid: 19164.0
title_bib: Systematic review on tool breakage monitoring techniques in machining operations
title_csv: International journal of machine tools and manufacture
total_cites: .nan
total_cites_(3years): 2146.0
total_docs._(2020): 65.0
total_docs._(3years): 250.0
total_refs.: 3080.0
type: journal
type_publication: article
year: 2022
---
abstract: This paper reviews the accounting literature that focuses on four Internet-related
  technologies that have the potential to dramatically change and disrupt the work
  of accountants and accounting researchers in the near future. These include cloud,
  big data, blockchain, and artificial intelligence (AI). For instance, access to
  distributed ledgers (blockchain) and big data supported by cloud-based analytics
  tools and AI will automate decision making to a large extent. These technologies
  may significantly improve financial visibility and allow more timely intervention
  due to the perpetual nature of accounting. However, given the number of tasks technology
  has relieved of accountants, these technologies may also lead to concerns about
  the profession's legitimacy. The findings suggest that scholars have not given sufficient
  attention to these technologies and how these technologies affect the everyday work
  of accountants. Research is urgently needed to understand the new kinds of accounting
  required to manage firms in the changing digital economy and to determine the new
  skills and competencies accountants may need to master to remain relevant and add
  value. The paper outlines a set of questions to guide future research.
author: Jodie Moll and Ogan Yigitbasioglu
categories: Accounting (Q1)
citable_docs._(3years): 112.0
cites_/_doc._(2years): 516.0
country: United States
coverage: 1988-2020
doi: 10.1016/j.bar.2019.04.002
eigenfactor_score: 0.0018899999999999998
h_index: 67.0
isbn: null
issn: 08908389
issn1: '10958347'
issn2: 08908389
issn3: '10958347'
jcr_value: '5.577'
keywords: Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 8102.0
region: Northern America
scimago_value: 1223.0
sjr_best_quartile: Q1
sourceid: 28634.0
title_bib: 'The role of internet-related technologies in shaping the work of accountants:
  new directions for accounting research'
title_csv: British accounting review
total_cites: 2836.0
total_cites_(3years): 648.0
total_docs._(2020): 44.0
total_docs._(3years): 118.0
total_refs.: 3565.0
type: journal
type_publication: article
year: 2019
---
abstract: "Preclinical imaging studies of posttraumatic epileptogenesis (PTE) have\
  \ largely been proof-of-concept studies with limited animal numbers, and thus lack\
  \ the statistical power for biomarker discovery. Epilepsy Bioinformatics Study for\
  \ Antiepileptogenic Therapy (EpiBioS4Rx) is a pioneering multicenter trial investigating\
  \ preclinical imaging biomarkers of PTE. EpiBios4Rx faced the issue of harmonizing\
  \ the magnetic resonance imaging (MRI) procedures and imaging data metrics prior\
  \ to its execution. We present here the harmonization process between three preclinical\
  \ MRI facilities at the University of Eastern Finland (UEF), the University of Melbourne\
  \ (Melbourne), and the University of California, Los Angeles (UCLA), and evaluate\
  \ the uniformity of the obtained MRI data. Adult, male rats underwent a lateral\
  \ fluid percussion injury (FPI) and were followed by MRI 2 days, 9 days, 1 month,\
  \ and 5 months post-injury. Ex vivo scans of fixed brains were conducted 7 months\
  \ post-injury as an end point follow-up. Four MRI modalities were used: T2-weighted\
  \ imaging, multi-gradient-echo imaging, diffusion-weighted imaging, and magnetization\
  \ transfer imaging, and acquisition parameters for each modality were tailored to\
  \ account for the different field strengths (4.7\u2009T and 7\u2009T) and different\
  \ MR hardwares used at the three participating centers. Pilot data collection resulted\
  \ in comparable image quality across sites. In interim analysis (of data obtained\
  \ by April 30, 2018), the within-site variation of the quantified signal properties\
  \ was low, while some differences between sites remained. In T2-weighted images\
  \ the signal-to-noise ratios were high at each site, being 35 at UEF, 48 at Melbourne,\
  \ and 32 at UCLA (p\u2009<\u20090.05). The contrast-to-noise ratios were similar\
  \ between the sites (9, 10, and 8, respectively). Magnetization transfer ratio maps\
  \ had identical white matter/ gray matter contrast between the sites, with white\
  \ matter showing 15% higher MTR than gray matter despite different absolute MTR\
  \ values (MTR both in white and gray matter was 3% lower in Melbourne than at UEF,\
  \ p\u2009<\u20090.05). Diffusion-weighting yielded different degrees of signal attenuation\
  \ across sites, being 83% at UEF, 76% in Melbourne, and 80% at UCLA (p\u2009<\u2009\
  0.05). Fractional anisotropy values differed as well, being 0.81 at UEF, 0.73 in\
  \ Melbourne, and 0.84 at UCLA (p\u2009<\u20090.05). The obtained values in sham\
  \ animals showed low variation within each site and no change over time, suggesting\
  \ high repeatability of the measurements. Quality control scans with phantoms demonstrated\
  \ stable hardware performance over time. Timing of post-TBI scans was designed to\
  \ target specific phases of the dynamic pathology, and the execution at different\
  \ centers was highly accurate. Besides a few outliers, the 2-day scans were done\
  \ within an hour from the target time point. At day 9, most animals were scanned\
  \ within an hour from the target time point, and all but 2 outliers within 24\u2009\
  h from the target. The 1-month post-TBI scans were done within 31\u2009\xB1\u2009\
  3 days. MRI procedures and animal physiology during scans were similar between the\
  \ sites. Taken together, the 10% inter-site difference in FA and 3% difference in\
  \ MTR values should be included into analysis as a covariate or balanced out in\
  \ post-processing in order to detect disease-related effects on brain structure\
  \ at the same scale. However, for a MRI biomarker for post-traumatic epileptogenesis\
  \ to have realistic chance of being successfully translated to validation in clinical\
  \ trials, it would need to be a robust TBI-induced structural change which tolerates\
  \ the inter-site methodological variability described here."
author: "Riikka Immonen and Gregory Smith and Rhys D. Brady and David Wright and Leigh\
  \ Johnston and Neil G. Harris and Eppu Manninen and Raimo Salo and Craig Branch\
  \ and Dominique Duncan and Ryan Cabeen and Xavier Ekolle Ndode-Ekane and Cesar Santana\
  \ Gomez and Pablo M. Casillas-Espinosa and Idrish Ali and Sandy R. Shultz and Pedro\
  \ Andrade and Noora Puhakka and Richard J. Staba and Terence J. O\u2019Brien and\
  \ Arthur W. Toga and Asla Pitk\xE4nen and Olli Gr\xF6hn"
categories: Neurology (clinical) (Q2); Neurology (Q3)
citable_docs._(3years): 542.0
cites_/_doc._(2years): 274.0
country: Netherlands
coverage: 1987-2020
doi: 10.1016/j.eplepsyres.2019.01.001
eigenfactor_score: 0.00773
h_index: 113.0
isbn: null
issn: 09201211
issn1: 09201211
issn2: '18726844'
issn3: 09201211
jcr_value: '3.045'
keywords: Common data element, Diffusion tensor imaging, Magnetization transfer imaging,
  Multi-site harmonization, Post-traumatic epilepsy, Traumatic brain injury
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4132.0
region: Western Europe
scimago_value: 863.0
sjr_best_quartile: Q2
sourceid: 15514.0
title_bib: Harmonization of pipeline for preclinical multicenter mri biomarker discovery
  in a rat model of post-traumatic epileptogenesis
title_csv: Epilepsy research
total_cites: 8587.0
total_cites_(3years): 1514.0
total_docs._(2020): 180.0
total_docs._(3years): 565.0
total_refs.: 7437.0
type: journal
type_publication: article
year: 2019
---
abstract: Most organisations using Open Data currently focus on data processing and
  analysis. However, although Open Data may be available online, these data are generally
  of poor quality, thus discouraging others from contributing to and reusing them.
  This paper describes an approach to publish statistical data from public repositories
  by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in
  order to facilitate the analysis of multidimensional models. We have defined a framework
  based on the entire lifecycle of data publication including a novel step of Linked
  Open Data assessment and the use of external repositories as knowledge base for
  data enrichment. As a result, users are able to interact with the data generated
  according to the RDF Data Cube vocabulary, which makes it possible for general users
  to avoid the complexity of SPARQL when analysing data. The use case was applied
  to the Barcelona Open Data platform and revealed the benefits of the application
  of our approach, such as helping in the decision-making process.
author: "Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such\
  \ and Jes\xFAs Peral"
categories: Law (Q1); Hardware and Architecture (Q2); Software (Q2)
citable_docs._(3years): 243.0
cites_/_doc._(2years): 371.0
country: Netherlands
coverage: 1985-2020
doi: 10.1016/j.csi.2019.103378
eigenfactor_score: .nan
h_index: 63.0
isbn: null
issn: 09205489
issn1: 09205489
issn2: 09205489
issn3: 09205489
jcr_value: null
keywords: Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF
  Data Cube vocabulary, Semantic web, Big data
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6394.0
region: Western Europe
scimago_value: 556.0
sjr_best_quartile: Q1
sourceid: 24303.0
title_bib: Adding value to linked open data using a multidimensional model approach
  based on the rdf data cube vocabulary
title_csv: Computer standards and interfaces
total_cites: .nan
total_cites_(3years): 1013.0
total_docs._(2020): 36.0
total_docs._(3years): 246.0
total_refs.: 2302.0
type: journal
type_publication: article
year: 2020
---
abstract: The assessment of the criticality of raw materials allows the identification
  of the likelihood of a supply disruption of a material and the vulnerability of
  a system (e.g. a national economy, technology, or company) to this disruption. Inconclusive
  outcomes of various studies suggest that criticality assessments would benefit from
  the identification of best practices. To prepare the field for such guidance, this
  paper aims to clarify the mechanisms that affect methodological choices which influence
  the results of a study. This is achieved via literature review and round table discussions
  among international experts. The paper demonstrates that criticality studies are
  divergent in the system under study, the anticipated risk, the purpose of the study,
  and material selection. These differences in goal and scope naturally result in
  different choices regarding indicator selection, the required level of aggregation
  as well as the subsequent choice of aggregation method, and the need for a threshold
  value. However, this link is often weak, which suggests a lack of understanding
  of cause-and-effect mechanisms of indicators and outcomes. Data availability is
  a key factor that limits the evaluation of criticality. Furthermore, data quality,
  including both data uncertainty and data representativeness, is rarely addressed
  in the interpretation and communication of results. Clear guidance in the formulation
  of goals and scopes of criticality studies, the selection of adequate indicators
  and aggregation methods, and the interpretation of the outcomes, are important initial
  steps in improving the quality of criticality assessments.
author: "Dieuwertje Schrijvers and Alessandra Hool and Gian Andrea Blengini and Wei-Qiang\
  \ Chen and Jo Dewulf and Roderick Eggert and Layla {van Ellen} and Roland Gauss\
  \ and James Goddin and Komal Habib and Christian Hagel\xFCken and Atsufumi Hirohata\
  \ and Margarethe Hofmann-Amtenbrink and Jan Kosmol and Ma\xEFt\xE9 {Le Gleuher}\
  \ and Milan Grohol and Anthony Ku and Min-Ha Lee and Gang Liu and Keisuke Nansai\
  \ and Philip Nuss and David Peck and Armin Reller and Guido Sonnemann and Luis Tercero\
  \ and Andrea Thorenz and Patrick A. W\xE4ger"
categories: Economics and Econometrics (Q1); Waste Management and Disposal (Q1)
citable_docs._(3years): 1167.0
cites_/_doc._(2years): 993.0
country: Netherlands
coverage: 1988-2021
doi: 10.1016/j.resconrec.2019.104617
eigenfactor_score: .nan
h_index: 130.0
isbn: null
issn: 09213449
issn1: '18790658'
issn2: 09213449
issn3: '18790658'
jcr_value: null
keywords: Critical raw materials, Material criticality, Critical resources, Strategic
  raw materials, Criticality assessment
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6273.0
region: Western Europe
scimago_value: 2468.0
sjr_best_quartile: Q1
sourceid: 26424.0
title_bib: A review of methods and data to determine raw material criticality
title_csv: Resources, conservation and recycling
total_cites: .nan
total_cites_(3years): 12067.0
total_docs._(2020): 484.0
total_docs._(3years): 1190.0
total_refs.: 30363.0
type: journal
type_publication: article
year: 2020
---
abstract: Today's manufacturing processes are pushed to their limits to generate products
  with ever-increasing quality at low costs. A prominent hurdle on this path arises
  from the multiscale, multiphysics, dynamic, and stochastic nature of many manufacturing
  systems, which motivated many innovations at the intersection of artificial intelligence
  (AI), data analytics, and manufacturing sciences. This study reviews recent advances
  in Mechanistic-AI, defined as a methodology that combines the raw mathematical power
  of AI methods with mechanism-driven principles and engineering insights. Mechanistic-AI
  solutions are systematically analyzed for three aspects of manufacturing processes,
  i.e., modeling, design, and control, with a focus on approaches that can improve
  data requirements, generalizability, explainability, and capability to handle challenging
  and heterogeneous manufacturing data. Additionally, we introduce a corpus of cutting-edge
  Mechanistic-AI methods that have shown to be very promising in other scientific
  fields but yet to be applied in manufacturing. Finally, gaps in the knowledge and
  under-explored research directions are identified, such as lack of incorporating
  manufacturing constraints into AI methods, lack of uncertainty analysis, and limited
  reproducibility and established benchmarks. This paper shows the immense potential
  of the Mechanistic-AI to address new problems in manufacturing systems and is expected
  to drive further advancements in manufacturing and related fields.
author: Mojtaba Mozaffar and Shuheng Liao and Xiaoyu Xie and Sourav Saha and Chanwook
  Park and Jian Cao and Wing Kam Liu and Zhengtao Gan
categories: Ceramics and Composites (Q1); Computer Science Applications (Q1); Industrial
  and Manufacturing Engineering (Q1); Metals and Alloys (Q1); Modeling and Simulation
  (Q1)
citable_docs._(3years): 1367.0
cites_/_doc._(2years): 589.0
country: Netherlands
coverage: 1990-2021
doi: 10.1016/j.jmatprotec.2021.117485
eigenfactor_score: 0.02301
h_index: 190.0
isbn: null
issn: 09240136
issn1: 09240136
issn2: '18734774'
issn3: 09240136
jcr_value: '5.551'
keywords: Scientific data science, Deep learning, Additive manufacturing, Physics-informed
  machine learning, Data-driven discovery, Data-driven design
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 2824.0
region: Western Europe
scimago_value: 1736.0
sjr_best_quartile: Q1
sourceid: 20972.0
title_bib: 'Mechanistic artificial intelligence (mechanistic-ai) for modeling, design,
  and control of advanced manufacturing processes: current state and perspectives'
title_csv: Journal of materials processing technology
total_cites: 41944.0
total_cites_(3years): 8479.0
total_docs._(2020): 408.0
total_docs._(3years): 1369.0
total_refs.: 11521.0
type: journal
type_publication: article
year: 2022
---
abstract: 'Background

  Meat packaging and intelligent evaluation and monitoring of key parameters not only
  are important technologies to ensure meat quality and safety but also form the key
  foundation for optimizing packaging materials and improving the efficiency of cold
  chain operations. In recent years, numerous studies have focused on comprehensive
  (or multi-functional) packaging materials, multiple parameter evaluation methods,
  quality intelligent monitoring technology, and optimization of the control of various
  links in cold chain logistics (CCL). Such research has significant practical application
  value for extending meat shelf-life and reducing the risk of foodborne diseases.

  Scope and approach

  This paper reviews the current research status, existing problems, and future evolution
  of CCL by focusing on meat packaging, meat quality evaluation and monitoring, and
  meat quality prediction and control. We also elaborate in detail the challenges
  faced in researching these topics and discuss the focal points of future research
  aiming to improve the quality and efficiency of CCL.

  Key findings and conclusions

  Packaging material optimization and dynamic quality perception are vital for achieving
  meat quality and safety over the entire CCL and demand the digital and intelligent
  development of the meat cold chain. A key finding of this review is that the comprehensive
  (or composite) packaging and intelligent quality assessment and monitoring are important
  forces promoting the transformation of traditional meat CCL to smart, green, and
  efficient CLL involving the intelligent management and control of all links therein.'
author: Qing-Shan Ren and Kui Fang and Xin-Ting Yang and Jia-Wei Han
categories: Biotechnology (Q1); Food Science (Q1)
citable_docs._(3years): 670.0
cites_/_doc._(2years): 1190.0
country: United Kingdom
coverage: 1990-2020
doi: 10.1016/j.tifs.2021.12.006
eigenfactor_score: .nan
h_index: 188.0
isbn: null
issn: 09242244
issn1: 09242244
issn2: 09242244
issn3: 09242244
jcr_value: null
keywords: Meat, Cold chain logistics, Packaging, Quality perception, Intelligent development
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 10657.0
region: Western Europe
scimago_value: 2676.0
sjr_best_quartile: Q1
sourceid: 22475.0
title_bib: 'Ensuring the quality of meat in cold chain logistics: a comprehensive
  review'
title_csv: Trends in food science and technology
total_cites: .nan
total_cites_(3years): 9617.0
total_docs._(2020): 363.0
total_docs._(3years): 730.0
total_refs.: 38684.0
type: journal
type_publication: article
year: 2022
---
abstract: "The Near Surface Concentrations (NSC) of O3, CO, and NO2 are crucial worldwide\
  \ indicators of air quality. However, current frameworks devised for the estimation\
  \ of the NSC of O3, CO, and NO2 have defects, such as coarse spatial resolution\
  \ and large missing coverage. To address this issue, this study aims to estimate\
  \ the daily (~13:30 local time) full-coverage NSC of O3, CO, and NO2 at a high spatial\
  \ resolution (0.05\xB0 for O3 and NO2; 0.07\xB0 for CO) over China by using datasets\
  \ from S5P-TROPOMI and GEOS-FP. In specific, the light gradient boosting machine\
  \ is employed to train the estimation models. Validation results show that the NSC\
  \ of O3, CO, and NO2 are well estimated, with the R2s of 0.91, 0.71, and 0.83 for\
  \ the sample-based cross validation, respectively. Meanwhile, the proposed framework\
  \ achieves a satisfactory performance in comparison to the latest related works,\
  \ as reflected by the estimation accuracy and spatial resolution. As for the mapping,\
  \ the estimated results show coherent spatial distribution and can accurately grasp\
  \ the seasonal characteristics of each air pollutant. Finally, the estimated results\
  \ are utilized to analyze the temporal variations of O3, CO, and NO2 during the\
  \ COrona VIrus Disease 2019 (COVID-19) lockdown in China, which is an extend application\
  \ for adopting the proposed framework in air quality monitoring. Results show that\
  \ the estimated NSC of O3, CO, and NO2 in 2020 present significant variations during\
  \ different periods of the COVID-19 lockdown in China compared to last year. In\
  \ addition, the variations in the NSC of O3, CO, and NO2 during the COVID-19 lockdown\
  \ in China possibly result from restrictions in the anthropogenic activities."
author: Yuan Wang and Qiangqiang Yuan and Tongwen Li and Liye Zhu and Liangpei Zhang
categories: Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications
  (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography,
  Planning and Development (Q1)
citable_docs._(3years): 668.0
cites_/_doc._(2years): 1056.0
country: Netherlands
coverage: 1989-2020
doi: 10.1016/j.isprsjprs.2021.03.018
eigenfactor_score: 0.02145
h_index: 138.0
isbn: null
issn: 09242716
issn1: 09242716
issn2: 09242716
issn3: 09242716
jcr_value: '8.979'
keywords: Full-coverage, Near surface concentrations, Air quality, S5P-TROPOMI, GEOS-FP,
  COVID-19
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6104.0
region: Western Europe
scimago_value: 2960.0
sjr_best_quartile: Q1
sourceid: 29161.0
title_bib: Estimating daily full-coverage near surface o3, co, and no2 concentrations
  at a high spatial resolution over china based on s5p-tropomi and geos-fp
title_csv: Isprs journal of photogrammetry and remote sensing
total_cites: 18026.0
total_cites_(3years): 7306.0
total_docs._(2020): 264.0
total_docs._(3years): 677.0
total_refs.: 16114.0
type: journal
type_publication: article
year: 2021
---
abstract: Current limitations impeding on data reproducibility are often poor statistical
  design, underpowered studies, lack of robust data, lack of methodological detail,
  biased reporting and lack of open data sharing, coupled with wrong research incentives.
  To improve data reproducibility, robustness and quality for brain disease research,
  a Preclinical Data Forum Network was formed under the umbrella of the European College
  of Neuropsychopharmacology (ECNP). The goal of this network, members of which met
  for the first time in October 2014, is to establish a forum to collaborate in precompetitive
  space, to exchange and develop best practices, and to bring together the members
  from academia, pharmaceutical industry, publishers, journal editors, funding organizations,
  public/private partnerships and non-profit advocacy organizations. To address the
  most pertinent issues identified by the Network, it was decided to establish a data
  sharing platform that allows open exchange of information in the area of preclinical
  neuroscience and to develop an educational scientific program. It is also planned
  to reach out to other organizations to align initiatives to enhance efficiency,
  and to initiate activities to improve the clinical relevance of preclinical data.
  Those Network activities should contribute to scientific rigor and lead to robust
  and relevant translational data. Here we provide a synopsis of the proceedings from
  the inaugural meeting.
author: Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena
  Koustova and Anton Bespalov
categories: Neurology (Q1); Neurology (clinical) (Q1); Pharmacology (Q1); Pharmacology
  (medical) (Q1); Psychiatry and Mental Health (Q1); Biological Psychiatry (Q2)
citable_docs._(3years): 371.0
cites_/_doc._(2years): 385.0
country: Netherlands
coverage: 1990-2020
doi: 10.1016/j.euroneuro.2015.05.011
eigenfactor_score: 0.01119
h_index: 112.0
isbn: null
issn: 0924977X
issn1: 0924977X
issn2: '18737862'
issn3: 0924977X
jcr_value: '4.600'
keywords: Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience,
  Pre-clinical
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6310.0
region: Western Europe
scimago_value: 1603.0
sjr_best_quartile: Q1
sourceid: 15576.0
title_bib: 'The preclinical data forum network: a new ecnp initiative to improve data
  quality and robustness for (preclinical) neuroscience'
title_csv: European neuropsychopharmacology
total_cites: 8999.0
total_cites_(3years): 1610.0
total_docs._(2020): 154.0
total_docs._(3years): 383.0
total_refs.: 9717.0
type: journal
type_publication: article
year: 2015
---
abstract: 'A meta-learning algorithm, conventionally used for visual recognition,
  was applied to the recognition and classification of aroma oils. A printable chemiresistive
  sensor array was fabricated, based on composites of carbon black with various active
  materials. Standard aromatherapy kits with 30 types of essential oils were used
  as targets in an odor sensing experiment. Benefiting from the pattern recognition
  ability of the fabricated sensor array, a high-quality dataset was obtained with
  30 aroma oil classes, in which each class had nine replicate samples. A deep metric
  learning model, based on a Siamese neural network and a multilayer perceptron, was
  used to perform the N-way k-shot meta-learning. A test accuracy of over 98.7% was
  obtained for 31-way 9-shot learning, on discriminating whether the input pair samples
  were taken from similar or dissimilar classes. The model was effective in extracting
  meta-features of the aroma oils; this was proved by the improved clustering effect
  of samples in the spaces of principal components analysis and t-distributed stochastic
  neighbor embedding. The 30 aroma oils were divided into two datasets according to
  6-fold cross-validation: 25 aroma oil classes (plus one blank class) as seen classes
  for constructing 26-way 9-shot learning models and the remaining five aroma oils
  as unseen classes for prediction. Average accuracies of 93.5% and 93.9% were achieved
  for recognition of the unseen aroma oils from the seen classes and classification
  of the unseen aroma oils themselves, respectively, demonstrating the effectiveness
  of the developed sensor and model for odor recognition and classification.'
author: Chuanjun Liu and Hitoshi Miyauchi and Kenshi Hayashi
categories: Condensed Matter Physics (Q1); Electrical and Electronic Engineering (Q1);
  Electronic, Optical and Magnetic Materials (Q1); Instrumentation (Q1); Materials
  Chemistry (Q1); Metals and Alloys (Q1); Surfaces, Coatings and Films (Q1)
citable_docs._(3years): 6556.0
cites_/_doc._(2years): 734.0
country: Netherlands
coverage: 1970, 1990-2021
doi: 10.1016/j.snb.2021.130960
eigenfactor_score: .nan
h_index: 197.0
isbn: null
issn: 09254005
issn1: 09254005
issn2: 09254005
issn3: 09254005
jcr_value: null
keywords: Chemiresistive sensor array, Meta learning, Deep metric learning, Siamese
  network, Aroma oil, Recognition and classification
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4627.0
region: Western Europe
scimago_value: 1601.0
sjr_best_quartile: Q1
sourceid: 25236.0
title_bib: 'Deepsniffer: a meta-learning-based chemiresistive odor sensor for recognition
  and classification of aroma oils'
title_csv: 'Sensors and actuators, b: chemical'
total_cites: .nan
total_cites_(3years): 46369.0
total_docs._(2020): 1444.0
total_docs._(3years): 6558.0
total_refs.: 66816.0
type: journal
type_publication: article
year: 2022
---
abstract: Big data and analytics have shown promise in predicting safety incidents
  and identifying preventative measures directed towards specific risk variables.
  However, the safety industry is lagging in big data utilization due to various obstacles,
  which may include lack of data readiness (e.g., disparate databases, missing data,
  low validity) and personnel competencies. This paper provides a primer on the application
  of big data to safety. We then describe a safety analytics readiness assessment
  framework that highlights system requirements and the challenges that safety professionals
  may encounter in meeting these requirements. The proposed framework suggests that
  safety analytics readiness depends on (a) the quality of the data available, (b)
  organizational norms around data collection, scaling, and nomenclature, (c) foundational
  infrastructure, including technological platforms and skills required for data collection,
  storage, and analysis of health and safety metrics, and (d) measurement culture,
  or the emergent social patterns between employees, data acquisition, and analytic
  processes. A safety-analytics readiness assessment can assist organizations with
  understanding current capabilities so measurement systems can be matured to accommodate
  more advanced analytics for the ultimate purpose of improving decisions that mitigate
  injury and incidents.
author: "Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman\
  \ and Yal\xE7\u0131n A\xE7\u0131kg\xF6z"
categories: Public Health, Environmental and Occupational Health (Q1); Safety Research
  (Q1); Safety, Risk, Reliability and Quality (Q1)
citable_docs._(3years): 1020.0
cites_/_doc._(2years): 549.0
country: Netherlands
coverage: 1991-2020
doi: 10.1016/j.ssci.2021.105569
eigenfactor_score: 0.01488
h_index: 111.0
isbn: null
issn: 09257535
issn1: 09257535
issn2: 09257535
issn3: 09257535
jcr_value: '4.877'
keywords: Safety analytics, Data analytics, Readiness assessment, Occupational health
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5826.0
region: Western Europe
scimago_value: 1178.0
sjr_best_quartile: Q1
sourceid: 12332.0
title_bib: 'Advancing safety analytics: a diagnostic framework for assessing system
  readiness within occupational safety and health'
title_csv: Safety science
total_cites: 17184.0
total_cites_(3years): 5909.0
total_docs._(2020): 451.0
total_docs._(3years): 1047.0
total_refs.: 26276.0
type: journal
type_publication: article
year: 2022
---
abstract: The ambient noise in controlled-source electromagnetic (CSEM) data seriously
  affects the accuracy and reliability of the exploration result. Traditional correlation-based
  data selection method requires manually setting the threshold. To overcome the deficiency,
  we analyze the typical noises in CSEM data and find that normalized cross-correlation
  (NCC), absolute maximum value of the amplitude (Max), and detrend fluctuation analysis
  (DFA) can be used to accurately identify high-quality time series. Based on this
  discovery, we replace traditional manually intervention with unsupervised machine
  learning and propose a novel CSEM data processing method. We applied the newly proposed
  method to synthetic and measured CSEM data to verify the feasibility and effectiveness.
  Experimental results demonstrate that the newly proposed method is superior to the
  conventional data selection method because it accurately selects the best data fragments
  from noisy data automatically. The newly proposed method requires no human intervention
  which makes the results obtained free of subjective distortion caused by the operator.
author: Guang Li and Zhushi He and Juzhi Deng and Jingtian Tang and Youyao Fu and
  Xiaoqiong Liu and Changming Shen
categories: Geophysics (Q2)
citable_docs._(3years): 750.0
cites_/_doc._(2years): 209.0
country: Netherlands
coverage: 1992-2020
doi: 10.1016/j.jappgeo.2021.104262
eigenfactor_score: 0.00758
h_index: 82.0
isbn: null
issn: 09269851
issn1: 09269851
issn2: 09269851
issn3: 09269851
jcr_value: '2.121'
keywords: CSEM data processing, Periodic signal de-noising, Signal-noise identification,
  Fuzzy -means clustering (FCM), Machine Learning, Correlation Analysis
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 3955.0
region: Western Europe
scimago_value: 627.0
sjr_best_quartile: Q2
sourceid: 28435.0
title_bib: Robust csem data processing by unsupervised machine learning
title_csv: Journal of applied geophysics
total_cites: 6990.0
total_cites_(3years): 1705.0
total_docs._(2020): 243.0
total_docs._(3years): 751.0
total_refs.: 9610.0
type: journal
type_publication: article
year: 2021
---
abstract: This study explores perceptions of the impact of Ball and Brown (1968) (BB68)
  upon accounting research, education, standards-setting, and practice. These perceptions
  are gathered from an extensive literature review and from 27 interviewees drawn
  from academe, practice and standards-setting from around the globe. The study reports
  upon the genesis of BB68, it's disruptive effect on accounting research methodology
  and methods, and its broader impact on accounting education, practice and standards-setting.
  Reasons are provided for the popularity of BB68 and an exploration is undertaken
  of both the positive and negative impacts of the research methodology inspired by
  BB68. The characteristics of the Ball and Brown research team are also discussed
  and the paper finishes with perceptions regarding the future relevance of BB68.
  It is concluded that BB68 stands as an exemplar for innovation and for quality in
  the execution of accounting and finance research but that much of that innovative
  spirit has been lost as the BB68 methodology became mainstream.
author: Bryan A. Howieson
categories: Economics and Econometrics (Q2); Finance (Q2)
citable_docs._(3years): 334.0
cites_/_doc._(2years): 271.0
country: Netherlands
coverage: 1993-2020
doi: 10.1016/j.pacfin.2019.05.003
eigenfactor_score: .nan
h_index: 58.0
isbn: null
issn: 0927538X
issn1: 0927538X
issn2: 0927538X
issn3: 0927538X
jcr_value: null
keywords: Accounting practices, Accounting education, Ball and Brown 1968, Finance
  practices, Normative research, Positive research, Research impact, Research methodology,
  Research paradigms, Standards-setting
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5248.0
region: Western Europe
scimago_value: 697.0
sjr_best_quartile: Q2
sourceid: 15465.0
title_bib: Frankenstein's monster or the birth of venus? perceptions of the impact
  and contributions of ball and brown 1968
title_csv: Pacific basin finance journal
total_cites: .nan
total_cites_(3years): 1055.0
total_docs._(2020): 182.0
total_docs._(3years): 336.0
total_refs.: 9551.0
type: journal
type_publication: article
year: 2019
---
abstract: Clinical trials are the basis of Evidence-Based Medicine. Trial results
  are reviewed by experts and consensus panels for producing meta-analyses and clinical
  practice guidelines. However, reviewing these results is a long and tedious task,
  hence the meta-analyses and guidelines are not updated each time a new trial is
  published. Moreover, the independence of experts may be difficult to appraise. On
  the contrary, in many other domains, including medical risk analysis, the advent
  of data science, big data and visual analytics allowed moving from expert-based
  to fact-based knowledge. Since 12 years, many trial results are publicly available
  online in trial registries. Nevertheless, data science methods have not yet been
  applied widely to trial data. In this paper, we present a platform for analyzing
  the safety events reported during clinical trials and published in trial registries.
  This platform is based on an ontological model including 582 trials on pain treatments,
  and uses semantic web technologies for querying this dataset at various levels of
  granularity. It also relies on a 26-dimensional flower glyph for the visualization
  of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness.
  We illustrate the interest of this platform through several use cases and we were
  able to find back conclusions that were initially found during meta-analyses. The
  platform was presented to four experts in drug safety, and is publicly available
  online, with the ontology of pain treatment ADE.
author: Jean-Baptiste Lamy
categories: Artificial Intelligence (Q1); Medicine (miscellaneous) (Q1)
citable_docs._(3years): 237.0
cites_/_doc._(2years): 669.0
country: Netherlands
coverage: 1989-2020
doi: 10.1016/j.artmed.2021.102074
eigenfactor_score: 0.004220000000000001
h_index: 87.0
isbn: null
issn: 09333657
issn1: 09333657
issn2: '18732860'
issn3: 09333657
jcr_value: '5.326'
keywords: Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug
  events, Pain treatments, Painkillers
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5494.0
region: Western Europe
scimago_value: 980.0
sjr_best_quartile: Q1
sourceid: 24140.0
title_bib: 'A data science approach to drug safety: semantic and visual mining of
  adverse drug events from clinical trials of pain treatments'
title_csv: Artificial intelligence in medicine
total_cites: 4245.0
total_cites_(3years): 1746.0
total_docs._(2020): 165.0
total_docs._(3years): 245.0
total_refs.: 9065.0
type: journal
type_publication: article
year: 2021
---
abstract: Modern artificial intelligence techniques have solved some previously intractable
  problems and produced impressive results in selected medical domains. One of their
  drawbacks is that they often need very large amounts of data. Pre-existing datasets
  in the form of national cancer registries, image/genetic depositories and clinical
  datasets already exist and have been used for research. In theory, the combination
  of healthcare Big Data with modern, data-hungry artificial intelligence techniques
  should offer significant opportunities for artificial intelligence development,
  but this has not yet happened. Here we discuss some of the structural reasons for
  this, barriers preventing artificial intelligence from making full use of existing
  datasets, and make suggestions as to enable progress. To do this, we use the framework
  of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability,
  Accessibility, Interoperability, and Reuse). We share our experience in navigating
  these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported
  initiative to integrate fragmented patient data into an enriched dataset. We conclude
  with some comments as to the limits of such approaches.
author: J.W. Wang and M. Williams
categories: Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)
citable_docs._(3years): 304.0
cites_/_doc._(2years): 247.0
country: United Kingdom
coverage: 1975, 1984-1985, 1989-2020
doi: 10.1016/j.clon.2021.11.040
eigenfactor_score: 0.005379999999999999
h_index: 76.0
isbn: null
issn: 09366555
issn1: '14332981'
issn2: 09366555
issn3: '14332981'
jcr_value: '4.126'
keywords: Artificial intelligence, Big Data, database, deep learning, registries,
  repository
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 3226.0
region: Western Europe
scimago_value: 1037.0
sjr_best_quartile: Q1
sourceid: 29271.0
title_bib: Registries, databases and repositories for developing artificial intelligence
  in cancer care
title_csv: Clinical oncology
total_cites: 4553.0
total_cites_(3years): 1127.0
total_docs._(2020): 180.0
total_docs._(3years): 455.0
total_refs.: 5806.0
type: journal
type_publication: article
year: 2022
---
abstract: OrBiTo was a precompetitive collaboration focused on the development of
  the next generation of Oral Biopharmaceutics Tools. The consortium included world
  leading scientists from nine universities, one regulatory agency, one non-profit
  research organisation, three small/medium sized specialist technology companies
  together with thirteen pharmaceutical companies. The goal of the OrBiTo project
  was to deliver a framework for rational application of predictive biopharmaceutics
  tools for oral drug delivery. This goal was achieved through novel prospective investigations
  to define new methodologies or refinement of existing tools. Extensive validation
  has been performed of novel and existing biopharmaceutics tools using historical
  datasets supplied by industry partners as well as laboratory ring studies. A combination
  of high quality in vitro and in vivo characterizations of active drugs and formulations
  have been integrated into physiologically based in silico biopharmaceutics models
  capturing the full complexity of gastrointestinal drug absorption and some of the
  best practices has been highlighted. This approach has given an unparalleled opportunity
  to deliver transformational change in European industrial research and development
  towards model based pharmaceutical product development in accordance with the vision
  of model-informed drug development.
author: "B. Abrahamsson and M. McAllister and P. Augustijns and P. Zane and J. Butler\
  \ and R. Holm and P. Langguth and A. Lindahl and A. M\xFCllertz and X. Pepin and\
  \ A. Rostami-Hodjegan and E. Sj\xF6gren and M. Berntsson and H. Lennern\xE4s"
categories: Biotechnology (Q1); Medicine (miscellaneous) (Q1); Pharmaceutical Science
  (Q1)
citable_docs._(3years): 844.0
cites_/_doc._(2years): 535.0
country: Netherlands
coverage: 1991-2020
doi: 10.1016/j.ejpb.2020.05.008
eigenfactor_score: 0.01324
h_index: 158.0
isbn: null
issn: 09396411
issn1: '18733441'
issn2: 09396411
issn3: '18733441'
jcr_value: '5.571'
keywords: Biopharmaceutics, PBPK, IVIVC, Dissolution, Drug absorption, Permeability
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5607.0
region: Western Europe
scimago_value: 1103.0
sjr_best_quartile: Q1
sourceid: 21332.0
title_bib: "Six years of progress in the oral biopharmaceutics area \u2013 a summary\
  \ from the imi orbito project"
title_csv: European journal of pharmaceutics and biopharmaceutics
total_cites: 20326.0
total_cites_(3years): 4755.0
total_docs._(2020): 259.0
total_docs._(3years): 849.0
total_refs.: 14522.0
type: journal
type_publication: article
year: 2020
---
abstract: 'Context:

  Over the last decade, Agile methods have changed the software development process
  in an unparalleled way and with the increasing popularity of Big Data, optimizing
  development cycles through data analytics is becoming a commodity.

  Objective:

  Although a myriad of research exists on software analytics as well as on Agile software
  development (ASD) practice on itself, there exists no systematic overview of the
  research done on ASD from a data analytics perspective. Therefore, the objective
  of this work is to make progress by linking ASD with Big Data analytics (BDA).

  Method:

  As the primary method to find relevant literature on the topic, we performed manual
  search and snowballing on papers published between 2011 and 2019.

  Results:

  In total, 88 primary studies were selected and analyzed. Our results show that BDA
  is employed throughout the whole ASD lifecycle. The results reveal that data-driven
  software development is focused on the following areas: code repository analytics,
  defects/bug fixing, testing, project management analytics, and application usage
  analytics.

  Conclusions:

  As BDA and ASD are fast-developing areas, improving the productivity of software
  development teams is one of the most important objectives BDA is facing in the industry.
  This study provides scholars with information about the state of software analytics
  research and the current trends as well as applications in the business environment.
  Whereas, thanks to this literature review, practitioners should be able to understand
  better how to obtain actionable insights from their software artifacts and on which
  aspects of data analytics to focus when investing in such initiatives.'
author: "Katarzyna Biesialska and Xavier Franch and Victor Munt\xE9s-Mulero"
categories: Computer Science Applications (Q2); Information Systems (Q2); Software
  (Q2)
citable_docs._(3years): 403.0
cites_/_doc._(2years): 512.0
country: Netherlands
coverage: 1970, 1987-2020
doi: 10.1016/j.infsof.2020.106448
eigenfactor_score: 0.00554
h_index: 103.0
isbn: null
issn: 09505849
issn1: 09505849
issn2: 09505849
issn3: 09505849
jcr_value: '2.730'
keywords: Agile software development, Software analytics, Data analytics, Machine
  learning, Artificial intelligence, Literature review
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5664.0
region: Western Europe
scimago_value: 606.0
sjr_best_quartile: Q2
sourceid: 18732.0
title_bib: 'Big data analytics in agile software development: a systematic mapping
  study'
title_csv: Information and software technology
total_cites: 5172.0
total_cites_(3years): 2316.0
total_docs._(2020): 121.0
total_docs._(3years): 420.0
total_refs.: 6853.0
type: journal
type_publication: article
year: 2021
---
abstract: "The big data approach offers a powerful alternative to Evidence-based medicine.\
  \ This approach could guide cancer management thanks to machine learning application\
  \ to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data\
  \ Analysis) project is to develop a standardized web data collection system, focused\
  \ on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association\
  \ of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed\
  \ to thyroid cancer management and data collection. The agreement conditions, the\
  \ ontology of the collected data and the related software services were defined\
  \ by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly\
  \ started the project, defined and signed the Thyroid COBRA consortium agreement.\
  \ Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage\
  \ System (C-SS) appeared to be not time-consuming and to be privacy respecting,\
  \ as data can be extracted directly from the single centre's storage platforms through\
  \ a secured connection that ensures reliable encryption of sensible data. Automatic\
  \ data archiving could be directly performed from Image Hospital Storage System\
  \ or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow\
  \ \u201CCloud storage way\u201D or \u201Cdistributed learning\u201D approaches for\
  \ predictive model definition and further clinical decision support tools development.\
  \ The development of the Thyroid COBRA data Storage System C-SS through a multicentre\
  \ consortium approach appeared to be a feasible tool in the setup of complex and\
  \ privacy saving data sharing system oriented to the management of thyroid cancer\
  \ and in the near future every cancer type."
author: Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca
  Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello
  and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi
  and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito
  Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni
categories: Internal Medicine (Q2)
citable_docs._(3years): 557.0
cites_/_doc._(2years): 214.0
country: Netherlands
coverage: 1989-2020
doi: 10.1016/j.ejim.2018.02.012
eigenfactor_score: 0.00933
h_index: 71.0
isbn: null
issn: 09536205
issn1: 09536205
issn2: '18790828'
issn3: 09536205
jcr_value: '4.487'
keywords: Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer
  management
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 2330.0
region: Western Europe
scimago_value: 894.0
sjr_best_quartile: Q2
sourceid: 26617.0
title_bib: 'A new standardized data collection system for interdisciplinary thyroid
  cancer management: thyroid cobra'
title_csv: European journal of internal medicine
total_cites: 7083.0
total_cites_(3years): 1904.0
total_docs._(2020): 384.0
total_docs._(3years): 907.0
total_refs.: 8947.0
type: journal
type_publication: article
year: 2018
---
abstract: Artificial intelligence (AI) and wearable sensors are two essential fields
  to realize the goal of tailoring the best precision medicine treatment for individual
  patients. Integration of these two fields enables better acquisition of patient
  data and improved design of wearable sensors for monitoring the wearers' health,
  fitness and their surroundings. Currently, as the Internet of Things (IoT), big
  data and big health move from concept to implementation, AI-biosensors with appropriate
  technical characteristics are facing new opportunities and challenges. In this paper,
  the most advanced progress made in the key phases for future wearable and implantable
  technology from biosensing, wearable biosensing to AI-biosensing is summarized.
  Without a doubt, material innovation, biorecognition element, signal acquisition
  and transportation, data processing and intelligence decision system are the most
  important parts, which are the main focus of the discussion. The challenges and
  opportunities of AI-biosensors moving forward toward future medicine devices are
  also discussed.
author: Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang
categories: Biomedical Engineering (Q1); Biophysics (Q1); Biotechnology (Q1); Electrochemistry
  (Q1); Medicine (miscellaneous) (Q1); Nanoscience and Nanotechnology (Q1)
citable_docs._(3years): 2718.0
cites_/_doc._(2years): 1020.0
country: United Kingdom
coverage: 1990-2020
doi: 10.1016/j.bios.2020.112412
eigenfactor_score: .nan
h_index: 192.0
isbn: null
issn: 09565663
issn1: '18734235'
issn2: 09565663
issn3: '18734235'
jcr_value: null
keywords: Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication,
  Machine learning, Healthcare
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5468.0
region: Western Europe
scimago_value: 2546.0
sjr_best_quartile: Q1
sourceid: 15437.0
title_bib: 'Artificial intelligence biosensors: challenges and prospects'
title_csv: Biosensors and bioelectronics
total_cites: .nan
total_cites_(3years): 27420.0
total_docs._(2020): 701.0
total_docs._(3years): 2720.0
total_refs.: 38330.0
type: journal
type_publication: article
year: 2020
---
abstract: The time overhead is huge and the clustering quality is unstable when running
  the K-means algorithm on massive raw data. To solve these problems, the concept
  of the influence space is introduced, and on this basis, a new clustering algorithm
  named ISBFK-means based on the influence space is proposed in this paper. First,
  the influence space divides the given data set into multiple small regions. Then,
  the representative data objects in each region are obtained to form a new data set,
  in which the class labels of representative data objects are those of all the data
  objects in the correlation influence space. Next, the K-means clustering is performed
  on the new data set, thereby obtaining the final clustering result. Theoretical
  analysis and experimental results show that this approach effectively reduces the
  amount of data in the clustering process and improves the stability of clustering
  quality. As a major feature of this work, the celestial spectral data observed by
  the LAMOST survey are especially employed to verify the algorithm ISBFK-means. The
  experimental results indicate that this algorithm has higher performance than other
  similar algorithms on the correctness, efficiency and sensitivity to the quality
  of spectral data.
author: Yuqing Yang and Jianghui Cai and Haifeng Yang and Yating Li and Xujun Zhao
categories: Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering
  (miscellaneous) (Q1)
citable_docs._(3years): 1943.0
cites_/_doc._(2years): 867.0
country: United Kingdom
coverage: 1990-2021
doi: 10.1016/j.eswa.2022.117018
eigenfactor_score: 0.040530000000000004
h_index: 207.0
isbn: null
issn: 09574174
issn1: 09574174
issn2: 09574174
issn3: 09574174
jcr_value: '6.954'
keywords: Clustering, Influence space, Region partition, Representative data objects
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5495.0
region: Western Europe
scimago_value: 1368.0
sjr_best_quartile: Q1
sourceid: 24201.0
title_bib: 'Isbfk-means: a new clustering algorithm based on influence space'
title_csv: Expert systems with applications
total_cites: 55444.0
total_cites_(3years): 17345.0
total_docs._(2020): 770.0
total_docs._(3years): 1945.0
total_refs.: 42314.0
type: journal
type_publication: article
year: 2022
---
abstract: "In the age of big data, intelligence, and Industry 4.0, intelligence plays\
  \ an increasingly significant role in management or, more specifically, decision\
  \ making; thus, it becomes a popular topic and is recognised as an important discipline.\
  \ Hence, safety intelligence (SI) as a new safety concept and term was proposed.\
  \ SI aims to transform raw safety data and information into meaningful and actionable\
  \ information for safety management; it is considered an essential perspective for\
  \ safety management in the era of Safety 4.0 (computational safety science\u2014\
  a new paradigm for safety science in the age of big data, intelligence, and Industry\
  \ 4.0). However, thus far, no existing research provides a framework that comprehensively\
  \ describes SI and guides the implementation of SI practices in organisations. To\
  \ address this research gap and to provide a framework for SI and its practice in\
  \ the context of safety management, based on a systematic and comprehensive explanation\
  \ on SI from different perspectives, this study attempts to propose a theoretical\
  \ framework for SI from a safety management perspective and then presents an SI\
  \ practice model aimed at supporting safety management in organisations."
author: Bing Wang
categories: Chemical Engineering (miscellaneous) (Q1); Environmental Engineering (Q1);
  Safety, Risk, Reliability and Quality (Q1); Environmental Chemistry (Q2)
citable_docs._(3years): 1071.0
cites_/_doc._(2years): 641.0
country: United Kingdom
coverage: 1990-2021
doi: 10.1016/j.psep.2020.10.008
eigenfactor_score: 0.01335
h_index: 76.0
isbn: null
issn: 09575820
issn1: '17443598'
issn2: 09575820
issn3: '17443598'
jcr_value: '6.158'
keywords: Safety intelligence (SI), Safety big data, Safety 4.0, Safety management,
  Safety decision-making
publisher_x: Institution of Chemical Engineers
publisher_y: null
ref._/_doc.: 4979.0
region: Western Europe
scimago_value: 1173.0
sjr_best_quartile: Q1
sourceid: 13754.0
title_bib: 'Safety intelligence as an essential perspective for safety management
  in the era of safety 4.0: from a theoretical to a practical framework'
title_csv: Process safety and environmental protection
total_cites: 12452.0
total_cites_(3years): 6888.0
total_docs._(2020): 417.0
total_docs._(3years): 1079.0
total_refs.: 20761.0
type: journal
type_publication: article
year: 2021
---
abstract: "Modern agriculture and food production systems are facing increasing pressures\
  \ from climate change, land and water availability, and, more recently, a pandemic.\
  \ These factors are threatening the environmental and economic sustainability of\
  \ current and future food supply systems. Scientific and technological innovations\
  \ are needed more than ever to secure enough food for a fast-growing global population.\
  \ Scientific advances have led to a better understanding of how various components\
  \ of the agricultural system interact, from the cell to the field level. Despite\
  \ incredible advances in genetic tools over the past few decades, our ability to\
  \ accurately assess crop status in the field, at scale, has been severely lacking\
  \ until recently. Thanks to recent advances in remote sensing and Artificial Intelligence\
  \ (AI), we can now quantify field scale phenotypic information accurately and integrate\
  \ the big data into predictive and prescriptive management tools. This review focuses\
  \ on the use of recent technological advances in remote sensing and AI to improve\
  \ the resilience of agricultural systems, and we will present a unique opportunity\
  \ for the development of prescriptive tools needed to address the next decade\u2019\
  s agricultural and human nutrition challenges."
author: Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash
  Ashapure and Juan Landivar-Bowles
categories: Bioengineering (Q1); Biomedical Engineering (Q1); Biotechnology (Q1)
citable_docs._(3years): 444.0
cites_/_doc._(2years): 914.0
country: United Kingdom
coverage: 1990-2020
doi: 10.1016/j.copbio.2020.09.003
eigenfactor_score: 0.0197
h_index: 202.0
isbn: null
issn: 09581669
issn1: '18790429'
issn2: 09581669
issn3: '18790429'
jcr_value: '9.740'
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6160.0
region: Western Europe
scimago_value: 2843.0
sjr_best_quartile: Q1
sourceid: 15579.0
title_bib: The potential of remote sensing and artificial intelligence as tools to
  improve the resilience of agriculture production systems
title_csv: Current opinion in biotechnology
total_cites: 18835.0
total_cites_(3years): 4236.0
total_docs._(2020): 193.0
total_docs._(3years): 475.0
total_refs.: 11888.0
type: journal
type_publication: article
year: 2021
---
abstract: Electrophysiology has long been the workhorse of neuroscience, allowing
  scientists to record with millisecond precision the action potentials generated
  by neurons in vivo. Recently, calcium imaging of fluorescent indicators has emerged
  as a powerful alternative. This technique has its own strengths and weaknesses and
  unique data processing problems and interpretation confounds. Here we review the
  computational methods that convert raw calcium movies to estimates of single neuron
  spike times with minimal human supervision. By computationally addressing the weaknesses
  of calcium imaging, these methods hold the promise of significantly improving data
  quality. We also introduce a new metric to evaluate the output of these processing
  pipelines, which is based on the cluster isolation distance routinely used in electrophysiology.
author: Carsen Stringer and Marius Pachitariu
categories: Neuroscience (miscellaneous) (Q1)
citable_docs._(3years): 469.0
cites_/_doc._(2years): 646.0
country: United Kingdom
coverage: 1991-2021
doi: 10.1016/j.conb.2018.11.005
eigenfactor_score: 0.02518
h_index: 229.0
isbn: null
issn: 09594388
issn1: 09594388
issn2: '18736882'
issn3: 09594388
jcr_value: '6.627'
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6518.0
region: Western Europe
scimago_value: 4229.0
sjr_best_quartile: Q1
sourceid: 14992.0
title_bib: Computational processing of neural recordings from calcium imaging data
title_csv: Current opinion in neurobiology
total_cites: 17009.0
total_cites_(3years): 3279.0
total_docs._(2020): 128.0
total_docs._(3years): 487.0
total_refs.: 8343.0
type: journal
type_publication: article
year: 2019
---
abstract: "Background\nSignificant advances in the management of patients with lymphoid\
  \ and myeloid malignancies entered clinical practice in the early 2000\u2019s. The\
  \ EUROCARE-5 study database provides an opportunity to assess the impact of these\
  \ changes at the population level by country in Europe. We provide survival estimates\
  \ for clinically relevant haematological malignancies (HM), using the International\
  \ Classification of Diseases for Oncology 3, by country, gender and age in Europe.\n\
  Methods\nWe estimated age-standardised relative survival using the complete cohort\
  \ approach for 625,000 adult patients diagnosed in 2000\u20132007 and followed up\
  \ to 2008. Survival information was provided by 89 participating cancer registries\
  \ from 29 European countries. Mean survival in Europe was calculated as the population\
  \ weighted average of country-specific estimates.\nResults\nOn average in Europe,\
  \ 5-year relative survival was highest for Hodgkin lymphoma (81%; 40,625 cases),\
  \ poorest for acute myeloid leukaemia (17%; 57,026 cases), and intermediate for\
  \ non-Hodgkin lymphoma (59%; 329,204 cases), chronic myeloid leukaemia (53%; 17,713\
  \ cases) and plasma cell neoplasms (39%; 94,024 cases). Survival was generally lower\
  \ in Eastern Europe and highest in Central and Northern Europe. Wider between country\
  \ differences (>10%) were observed for malignancies that benefited from therapeutic\
  \ advances, such as chronic myeloid leukaemia, chronic lymphocytic leukaemia, follicular\
  \ lymphoma, diffuse large B-cell lymphoma and multiple myeloma. Lower differences\
  \ (<10%) were observed for Hodgkin lymphoma.\nConclusions\nDelayed or reduced access\
  \ to innovative and appropriate therapies could plausibly have contributed to the\
  \ observed geographical disparities between European regions and countries. Population\
  \ based survival by morphological sub-type is important for measuring outcomes of\
  \ HM management. To better inform quality of care research, the collection of detailed\
  \ clinical information at the population level should be prioritised."
author: "Roberta {De Angelis} and Pamela Minicozzi and Milena Sant and Luigino {Dal\
  \ Maso} and David H. Brewster and Gemma Osca-Gelis and Otto Visser and Marc Maynadi\xE9\
  \ and Rafael Marcos-Gragera and Xavier Troussard and Dominic Agius and Paolo Roazzi\
  \ and Elisabetta Meneghini and Alain Monnereau and M. Hackl and N. Zielonke and\
  \ W. Oberaigner and E. {Van Eycken} and K. Henau and Z. Valerianova and N. Dimitrova\
  \ and M. Sekerija and M. Zvolsk\xFD and L. Du\u0161ek and H. Storm and G. Engholm\
  \ and M. M\xE4gi and T. Aareleid and N. Malila and K. Sepp\xE4 and M. Velten and\
  \ X. Troussard and V. Bouvier and G. Launoy and A.V. Guizard and J. Faivre and A.M.\
  \ Bouvier and P. Arveux and M. Maynadi\xE9 and A.S. Woronoff and M. Robaszkiewicz\
  \ and I. Baldi and A. Monnereau and B. Tretarre and N. Bossard and A. Belot and\
  \ M. Colonna and F. Molini\xE9 and S. Bara and C. Schvartz and B. Lap\xF4tre-Ledoux\
  \ and P. Grosclaude and M. Meyer and R. Stabenow and S. Luttmann and A. Eberle and\
  \ H. Brenner and A. Nennecke and J. Engel and G. Schubert-Fritschle and J. Kieschke\
  \ and J. Heidrich and B. Holleczek and A. Katalinic and J.G. J\xF3nasson and L.\
  \ Tryggvad\xF3ttir and H. Comber and G. Mazzoleni and A. Bulatko and C. Buzzoni\
  \ and A. Giacomin and A. {Sutera Sardo} and P. Mancuso and S. Ferretti and E. Crocetti\
  \ and A. Caldarella and G. Gatta and M. Sant and H. Amash and C. Amati and P. Baili\
  \ and F. Berrino and S. Bonfarnuzzo and L. Botta and F. {Di Salvo} and R. Foschi\
  \ and C. Margutti and E. Meneghini and P. Minicozzi and A. Trama and D. Serraino\
  \ and L. {Dal Maso} and R. {De Angelis} and M. Caldora and R. Capocaccia and E.\
  \ Carrani and S. Francisci and S. Mallone and D. Pierannunzio and P. Roazzi and\
  \ S. Rossi and M. Santaquilani and A. Tavilla and F. Pannozzo and S. Busco and L.\
  \ Bonelli and M. Vercelli and V. Gennaro and P. Ricci and M. Autelitano and G. Randi\
  \ and M. {Ponz De Leon} and C. Marchesi and C. Cirilli and M. Fusco and M.F. Vitale\
  \ and M. Usala and A. Traina and R. Staiti and F. Vitale and B. Ravazzolo and M.\
  \ Michiara and R. Tumino and P. {Giorgi Rossi} and E. {Di Felice} and F. Falcini\
  \ and A. Iannelli and O. Sechi and R. Cesaraccio and S. Piffer and A. Madeddu and\
  \ F. Tisano and S. Maspero and A.C. Fanetti and R. Zanetti and S. Rosso and P. Candela\
  \ and T. Scuderi and F. Stracci and F. Bianconi and G. Tagliabue and P. Contiero\
  \ and A.P. {Dei Tos} and S. Guzzinati and S. Pildava and G. Smailyte and N. Calleja\
  \ and D. Agius and T.B. Johannesen and J. Rachtan and S. G\xF3zdz and R. Mezyk and\
  \ J. Blaszczyk and M. Bebenek and M. Bielska-Lasota and G. {Forjaz de Lacerda} and\
  \ M.J. Bento and C. Castro and A. Miranda and A. Mayer-da-Silva and F. Nicula and\
  \ D. Coza and C. {Safaei Diba} and M. Primic-Zakelj and E. Almar and C. Ram\xED\
  rez and M. Errezola and J. Bidaurrazaga and A. Torrella-Ramos and J.M. {D\xEDaz\
  \ Garc\xEDa} and R. Jimenez-Chillaron and R. Marcos-Gragera and A. {Izquierdo Font}\
  \ and M.J. Sanchez and D.Y.L. Chang and C. Navarro and M.D. Chirlaque and C. Moreno-Iribas\
  \ and E. Ardanaz and J. Galceran and M. Carulla and M. Lambe and S. Khan and M.\
  \ Mousavi and C. Bouchardy and M. Usel and S.M. Ess and H. Frick and M. Lorez and\
  \ S.M. Ess and C. Herrmann and A. Bordoni and A. Spitale and I. Konzelmann and O.\
  \ Visser and V. Lemmens and M. Coleman and C. Allemani and B. Rachet and J. Verne\
  \ and N. Easey and G. Lawrence and T. Moran and J. Rashbass and M. Roche and J.\
  \ Wilkinson and A. Gavin and C. Donnelly and D.H. Brewster and D.W. Huws and C.\
  \ White and R. Otter"
categories: Cancer Research (Q1); Oncology (Q1)
citable_docs._(3years): 961.0
cites_/_doc._(2years): 651.0
country: United Kingdom
coverage: 1990-2020
doi: 10.1016/j.ejca.2015.08.003
eigenfactor_score: 0.04649
h_index: 214.0
isbn: null
issn: 09598049
issn1: 09598049
issn2: '18790852'
issn3: 09598049
jcr_value: '9.162'
keywords: Relative survival, Europe, Cancer registry, Lymphoma, Leukaemia, Hodgkin
  lymphoma, Non-Hodgkin lymphoma, Multiple myeloma
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 1342.0
region: Western Europe
scimago_value: 3354.0
sjr_best_quartile: Q1
sourceid: 29761.0
title_bib: "Survival variations by country and age for lymphoid and myeloid malignancies\
  \ in europe 2000\u20132007: results of eurocare-5 population-based study"
title_csv: European journal of cancer
total_cites: 40294.0
total_cites_(3years): 7985.0
total_docs._(2020): 935.0
total_docs._(3years): 1157.0
total_refs.: 12549.0
type: journal
type_publication: article
year: 2015
---
abstract: "As a novel means of researching China's Belt and Road Initiative (BRI),\
  \ this article advances a critical remote sensing agenda that connects the view\
  \ from above provided by satellite imagery with the grounded, qualitative methodologies\
  \ more typical of political geography such as ethnographic fieldwork. Satellite\
  \ imagery is widely used to produce empirics relating to the BRI, and the Chinese\
  \ state is showing increasing interest in applying Earth observation data to governance.\
  \ A more critical approach attentive to the politics of remote sensing, especially\
  \ in light of China's emergence as a space and satellite power and its embrace of\
  \ big data, is needed to more precisely reveal what changing pixels represent on\
  \ the ground and expose the potential issues with data captured from high above\
  \ the planet. This paper offers three theoretical and methodological objectives\
  \ for critical remote sensing. First, I reflect on the geopolitics involved in the\
  \ production and analysis of satellite imagery. Second, through analysis of night\
  \ light imagery, which captures illuminated anthropogenic activities, I interrogate\
  \ metanarratives of development. Third, I engage with qualitative methods by \u201C\
  ground-truthing\u201D remote sensing with ethnographic observations along China's\
  \ borders. I also seek to avoid the methodological nationalism often present in\
  \ remote sensing research by situating these mixed-methods case studies at scales\
  \ above and below the nation-state. As one of the largest development interventions\
  \ in history materializes, pursuing critical remote sensing can create opportunities\
  \ for social scientists to leverage quantitative and geospatial methods in support\
  \ of more equitable and sustainable futures."
author: Mia M. Bennett
categories: Geography, Planning and Development (Q1); History (Q1); Sociology and
  Political Science (Q1)
citable_docs._(3years): 294.0
cites_/_doc._(2years): 305.0
country: United Kingdom
coverage: 1983, 1992-2020
doi: 10.1016/j.polgeo.2019.102127
eigenfactor_score: 0.006940000000000001
h_index: 97.0
isbn: null
issn: 09626298
issn1: 09626298
issn2: 09626298
issn3: 09626298
jcr_value: '3.660'
keywords: Critical remote sensing, Development, Night lights, Infrastructure, China,
  Belt and Road Initiative
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 7548.0
region: Western Europe
scimago_value: 1527.0
sjr_best_quartile: Q1
sourceid: 22857.0
title_bib: Is a pixel worth 1000 words? critical remote sensing and china's belt and
  road initiative
title_csv: Political geography
total_cites: 5342.0
total_cites_(3years): 1175.0
total_docs._(2020): 136.0
total_docs._(3years): 342.0
total_refs.: 10265.0
type: journal
type_publication: article
year: 2020
---
abstract: This study develops and validates the concept of Data Analytics Competency
  as a five multidimensional formative index (i.e., data quality, bigness of data,
  analytical skills, domain knowledge, and tools sophistication) and empirically examines
  its impact on firm decision making performance (i.e., decision quality and decision
  efficiency). The findings based on an empirical analysis of survey data from 151
  Information Technology managers and data analysts demonstrate a large, significant,
  positive relationship between data analytics competency and firm decision making
  performance. The results reveal that all dimensions of data analytics competency
  significantly improve decision quality. Furthermore, interestingly, all dimensions,
  except bigness of data, significantly increase decision efficiency. This is the
  first known empirical study to conceptualize, operationalize and validate the concept
  of data analytics competency and to study its impact on decision making performance.
  The validity of the data analytics competency construct as conceived and operationalized,
  suggests the potential for future research evaluating its relationships with possible
  antecedents and consequences. For practitioners, the results provide important guidelines
  for increasing firm decision making performance through the use of data analytics.
author: Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein
categories: Information Systems (Q1); Information Systems and Management (Q1); Management
  Information Systems (Q1)
citable_docs._(3years): 61.0
cites_/_doc._(2years): 1245.0
country: Netherlands
coverage: 1991-2020
doi: 10.1016/j.jsis.2017.10.001
eigenfactor_score: 0.00233
h_index: 88.0
isbn: null
issn: 09638687
issn1: 09638687
issn2: 09638687
issn3: 09638687
jcr_value: '11.022'
keywords: Data analytics competency, Data quality, Bigness of data, Analytical skills,
  Domain knowledge, Tools sophistication, Decision making performance
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 8796.0
region: Western Europe
scimago_value: 3133.0
sjr_best_quartile: Q1
sourceid: 12396.0
title_bib: Data analytics competency for improving firm decision making performance
title_csv: Journal of strategic information systems
total_cites: 2945.0
total_cites_(3years): 965.0
total_docs._(2020): 24.0
total_docs._(3years): 75.0
total_refs.: 2111.0
type: journal
type_publication: article
year: 2018
---
abstract: In this paper, a novel robust Bayesian network is proposed for process modeling
  with low-quality data. Since unreliable data can cause model parameters to deviate
  from the real distributions and make network structures unable to characterize the
  true causalities, data quality feature is utilized to improve the process modeling
  and monitoring performance. With a predetermined trustworthy center, the data quality
  measurement results can be evaluated through an exponential function with Mahalanobis
  distances. The conventional Bayesian network learning algorithms including structure
  learning and parameter learning are modified by the quality feature in a weighting
  form, intending to extract useful information and make a reasonable model. The effectiveness
  of the proposed method is demonstrated through TE benchmark process and a real industrial
  process.
author: Guangjie Chen and Zhiqiang Ge
categories: Applied Mathematics (Q1); Computer Science Applications (Q1); Control
  and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1)
citable_docs._(3years): 611.0
cites_/_doc._(2years): 442.0
country: United Kingdom
coverage: 1993-2020
doi: 10.1016/j.conengprac.2020.104344
eigenfactor_score: 0.00717
h_index: 119.0
isbn: null
issn: 09670661
issn1: 09670661
issn2: 09670661
issn3: 09670661
jcr_value: '3.475'
keywords: Robust Bayesian network, Data quality feature, Process monitoring, Fault
  diagnosis
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4087.0
region: Western Europe
scimago_value: 1175.0
sjr_best_quartile: Q1
sourceid: 18174.0
title_bib: Robust bayesian networks for low-quality data modeling and process monitoring
  applications
title_csv: Control engineering practice
total_cites: 8368.0
total_cites_(3years): 2779.0
total_docs._(2020): 196.0
total_docs._(3years): 615.0
total_refs.: 8010.0
type: journal
type_publication: article
year: 2020
---
abstract: "Summary\nMore than 70% of the experimentally determined macromolecular\
  \ structures in the Protein Data Bank (PDB) contain small-molecule ligands. Quality\
  \ indicators of \u223C643,000 ligands present in \u223C106,000 PDB X-ray crystal\
  \ structures have been analyzed. Ligand quality varies greatly with regard to goodness\
  \ of fit between ligand structure and experimental data, deviations in bond lengths\
  \ and angles from known chemical structures, and inappropriate interatomic clashes\
  \ between the ligand and its surroundings. Based on principal component analysis,\
  \ correlated quality indicators of ligand structure have been aggregated into two\
  \ largely orthogonal composite indicators measuring goodness of fit to experimental\
  \ data and deviation from ideal chemical structure. Ranking of the composite quality\
  \ indicators across the PDB archive enabled construction of uniformly distributed\
  \ composite ranking score. This score is implemented at RCSB.org to compare chemically\
  \ identical ligands in distinct PDB structures with easy-to-interpret two-dimensional\
  \ ligand quality plots, allowing PDB users to quickly assess ligand structure quality\
  \ and select the best exemplars."
author: Chenghua Shao and John D. Westbrook and Changpeng Lu and Charmi Bhikadiya
  and Ezra Peisach and Jasmine Y. Young and Jose M. Duarte and Robert Lowe and Sijian
  Wang and Yana Rose and Zukang Feng and Stephen K. Burley
categories: Molecular Biology (Q1); Structural Biology (Q1)
citable_docs._(3years): 574.0
cites_/_doc._(2years): 408.0
country: United States
coverage: 1993-2020
doi: 10.1016/j.str.2021.10.003
eigenfactor_score: 0.022940000000000002
h_index: 182.0
isbn: null
issn: 09692126
issn1: 09692126
issn2: '18784186'
issn3: 09692126
jcr_value: '5.006'
keywords: ligand structure quality, composite ranking score, ligand structure, small-molecule
  ligand, ligand quality indicator, multivariate analysis, principal component analysis,
  Protein Data Bank, PDB, RCSB PDB
publisher_x: Cell Press
publisher_y: null
ref._/_doc.: 4983.0
region: Northern America
scimago_value: 2907.0
sjr_best_quartile: Q1
sourceid: 14274.0
title_bib: Simplified quality assessment for small-molecule ligands in the protein
  data bank
title_csv: Structure
total_cites: 17041.0
total_cites_(3years): 2494.0
total_docs._(2020): 150.0
total_docs._(3years): 575.0
total_refs.: 7474.0
type: journal
type_publication: article
year: 2022
---
abstract: "Big data analytics (BDA) has emerged as a significant area of research\
  \ for both researchers and practitioners in the retail industry, indicating the\
  \ importance and influence of solving data-related problems in contemporary business\
  \ organization. The present study utilised a quantitative-methods approach to investigate\
  \ factors affecting retailers' adoption of BDA across three countries. A survey\
  \ questionnaire was used to collect data from managers and decision-makers in the\
  \ retail industry. Data of 2278 respondents were analysed through structural equation\
  \ modelling. The findings revealed that security concerns, external support, top\
  \ management support, and rational decision making culture have a greater effect\
  \ on BDA adoption in developed countries UK than in UAE and Egypt. However, competition\
  \ intensity and firm size have a greater effect on BDA adoption in UAE and Egypt\
  \ than in UK. Finally, human variables (competence of information system's staff\
  \ and staff's information system knowledge) have a greater effect on BDA adoption\
  \ in Egypt than UK and UAE. The findings indicate that a \u201Cone-size-fits-all\u201D\
  \ approach is insufficient in capturing the heterogeneity of managers across countries.\
  \ Implications for practice and theory were demonstrated."
author: Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag
categories: Marketing (Q1)
citable_docs._(3years): 550.0
cites_/_doc._(2years): 777.0
country: United Kingdom
coverage: 1994-2021
doi: 10.1016/j.jretconser.2021.102827
eigenfactor_score: 0.008759999999999999
h_index: 89.0
isbn: null
issn: 09696989
issn1: 09696989
issn2: 09696989
issn3: 09696989
jcr_value: '7.135'
keywords: Big data analytics, Technology adoption, Diffusion of innovations model,
  Cross-national differences, Retail industry
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 8030.0
region: Western Europe
scimago_value: 1568.0
sjr_best_quartile: Q1
sourceid: 22992.0
title_bib: Cross-national differences in big data analytics adoption in the retail
  industry
title_csv: Journal of retailing and consumer services
total_cites: 10506.0
total_cites_(3years): 4555.0
total_docs._(2020): 346.0
total_docs._(3years): 555.0
total_refs.: 27784.0
type: journal
type_publication: article
year: 2022
---
abstract: The evaluation, acquisition and use of newly available big data sources
  has become a major strategic and organizational challenge for airline network planners.
  We address this challenge by developing a maturity model for big data readiness
  for airline network planning. The development of the maturity model is grounded
  in literature, expert interviews and case study research involving nine airlines.
  Four airline business models are represented, namely full-service carriers, low-cost
  airlines, scheduled charter airlines and cargo airlines. The maturity model has
  been well received with seven change requests in the model development phase. The
  revised version has been evaluated as exhaustive and useful by airline network planners.
  The self-assessment of airlines revealed low to medium maturity for most domains.
  Organizational factors show the lowest average maturity, IT architecture the highest.
  Full-service carriers seem to be more mature than airlines with different business
  models.
author: Iris Hausladen and Maximilian Schosser
categories: Law (Q1); Management, Monitoring, Policy and Law (Q1); Strategy and Management
  (Q1); Transportation (Q1)
citable_docs._(3years): 349.0
cites_/_doc._(2years): 468.0
country: United Kingdom
coverage: 1994-1995, 1997-2020
doi: 10.1016/j.jairtraman.2019.101721
eigenfactor_score: 0.00394
h_index: 75.0
isbn: null
issn: 09696997
issn1: 09696997
issn2: 09696997
issn3: 09696997
jcr_value: '4.134'
keywords: Maturity model, Network planning, Big data analytics, Airlines, Case study
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5195.0
region: Western Europe
scimago_value: 1220.0
sjr_best_quartile: Q1
sourceid: 20532.0
title_bib: Towards a maturity model for big data analytics in airline network planning
title_csv: Journal of air transport management
total_cites: 4929.0
total_cites_(3years): 1612.0
total_docs._(2020): 142.0
total_docs._(3years): 356.0
total_refs.: 7377.0
type: journal
type_publication: article
year: 2020
---
abstract: The Basel III framework, whose main thrust has been enhancing the banking
  sector's safety and stability, emphasises the need to improve the quality and quantity
  of capital components, leverage ratio, liquidity standards, and enhanced disclosures.
  This article first lays the context of Basel III and then incorporates the views
  of senior executives of Indian banks and risk management experts on addressing the
  challenges of implementing the Basel III framework, especially in areas such as
  augmentation of capital resources, growth versus financial stability, challenges
  for enhanced profitability, deposit pricing, cost of credit, maintenance of liquidity
  standards, and strengthening of risk architecture.
author: M. Jayadev
categories: Business, Management and Accounting (miscellaneous) (Q2); Economics and
  Econometrics (Q3)
citable_docs._(3years): 82.0
cites_/_doc._(2years): 241.0
country: United Kingdom
coverage: 2010-2020
doi: 10.1016/j.iimb.2013.03.010
eigenfactor_score: .nan
h_index: 20.0
isbn: null
issn: 09703896
issn1: 09703896
issn2: 09703896
issn3: 09703896
jcr_value: null
keywords: Basel III, Capital regulation, Capital management, Indian banking
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4644.0
region: Western Europe
scimago_value: 425.0
sjr_best_quartile: Q2
sourceid: 19700175280.0
title_bib: 'Basel iii implementation: issues and challenges for indian banks'
title_csv: Iimb management review
total_cites: .nan
total_cites_(3years): 220.0
total_docs._(2020): 36.0
total_docs._(3years): 95.0
total_refs.: 1672.0
type: journal
type_publication: article
year: 2013
---
abstract: "Traditional air quality data have a spatial resolution of 1\_km or above,\
  \ making it challenging to resolve detailed air pollution exposure in complex urban\
  \ areas. Combining urban morphology, dynamic traffic emission, regional and local\
  \ meteorology, physicochemical transformations in air quality models using big data\
  \ fusion technology, an ultra-fine resolution modeling system was developed to provide\
  \ air quality data down to street level. Based on one-year ultra-fine resolution\
  \ data, this study investigated the effects of pollution heterogeneity on the individual\
  \ and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide\
  \ (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized\
  \ cities. Sharp fine-scale variabilities in air pollution were revealed within individual\
  \ city blocks. Using traditional 1\_km average to represent individual exposure\
  \ resulted in a positively skewed deviation of up to 200% for high-end exposure\
  \ individuals. Citizens were disproportionally affected by air pollution, with annual\
  \ pollutant concentrations varied by factors of 2 to 5 among 452 District Council\
  \ Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities\
  \ among the population. Unfavorable city planning resulted in a positive spatial\
  \ coincidence between pollution and population, which increased public exposure\
  \ to air pollutants by as large as 46% among districts in Hong Kong. Our results\
  \ highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity\
  \ in pollution exposure in the dense urban area and the critical role of smart urban\
  \ planning in reducing exposure inequities."
author: Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H.
  Fung and Alexis K.H. Lau
categories: Environmental Chemistry (Q1); Environmental Engineering (Q1); Environmental
  Science (miscellaneous) (Q1); Medicine (miscellaneous) (Q1)
citable_docs._(3years): 1001.0
cites_/_doc._(2years): 583.0
country: China
coverage: 1970, 1972-1973, 1978-1985, 1993, 1995-2021
doi: 10.1016/j.jes.2022.02.041
eigenfactor_score: 0.013340000000000001
h_index: 99.0
isbn: null
issn: '10010742'
issn1: '10010742'
issn2: '18787320'
issn3: '10010742'
jcr_value: '5.565'
keywords: Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban
  area
publisher_x: Chinese Academy of Sciences
publisher_y: null
ref._/_doc.: 5341.0
region: Asiatic Region
scimago_value: 1316.0
sjr_best_quartile: Q1
sourceid: 23393.0
title_bib: Impacts of pollution heterogeneity on population exposure in dense urban
  areas using ultra-fine resolution air quality data
title_csv: Journal of environmental sciences
total_cites: 17274.0
total_cites_(3years): 5975.0
total_docs._(2020): 329.0
total_docs._(3years): 1018.0
total_refs.: 17571.0
type: journal
type_publication: article
year: 2023
---
abstract: Regional healthcare platforms collect clinical data from hospitals in specific
  areas for the purpose of healthcare management. It is a common requirement to reuse
  the data for clinical research. However, we have to face challenges like the inconsistence
  of terminology in electronic health records (EHR) and the complexities in data quality
  and data formats in regional healthcare platform. In this paper, we propose methodology
  and process on constructing large scale cohorts which forms the basis of causality
  and comparative effectiveness relationship in epidemiology. We firstly constructed
  a Chinese terminology knowledge graph to deal with the diversity of vocabularies
  on regional platform. Secondly, we built special disease case repositories (i.e.,
  heart failure repository) that utilize the graph to search the related patients
  and to normalize the data. Based on the requirements of the clinical research which
  aimed to explore the effectiveness of taking statin on 180-days readmission in patients
  with heart failure, we built a large-scale retrospective cohort with 29647 cases
  of heart failure patients from the heart failure repository. After the propensity
  score matching, the study group (n=6346) and the control group (n=6346) with parallel
  clinical characteristics were acquired. Logistic regression analysis showed that
  taking statins had a negative correlation with 180-days readmission in heart failure
  patients. This paper presents the workflow and application example of big data mining
  based on regional EHR data.
author: Daowen Liu and Liqi Lei and Tong Ruan and Ping He
categories: Medicine (miscellaneous) (Q4)
citable_docs._(3years): 125.0
cites_/_doc._(2years): 73.0
country: United Kingdom
coverage: 1991-2020
doi: 10.24920/003579
eigenfactor_score: .nan
h_index: 21.0
isbn: null
issn: '10019294'
issn1: '10019294'
issn2: '10019294'
issn3: '10019294'
jcr_value: null
keywords: electronic health records, clinical terminology knowledge graph, clinical
  special disease case repository, evaluation of data quality, large scale cohort
  study
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 2920.0
region: Western Europe
scimago_value: 215.0
sjr_best_quartile: Q4
sourceid: 29227.0
title_bib: 'Constructing large scale cohort for clinical study on heart failure with
  electronic health record in regional healthcare platform: challenges and strategies
  in data reuse'
title_csv: Chinese medical sciences journal
total_cites: .nan
total_cites_(3years): 97.0
total_docs._(2020): 51.0
total_docs._(3years): 128.0
total_refs.: 1489.0
type: journal
type_publication: article
year: 2019
---
abstract: 'Owing to wide applications of automatic control systems in the process
  industries, the impacts of controller performance on industrial processes are becoming
  increasingly significant. Consequently, controller maintenance is critical to guarantee
  routine operations of industrial processes. The workflow of controller maintenance
  generally involves the following steps: monitor operating controller performance
  and detect performance degradation, diagnose probable root causes of control system
  malfunctions, and take specific actions to resolve associated problems. In this
  article, a comprehensive overview of the mainstream of control loop monitoring and
  diagnosis is provided, and some existing problems are also analyzed and discussed.
  From the viewpoint of synthesizing abundant information in the context of big data,
  some prospective ideas and promising methods are outlined to potentially solve problems
  in industrial applications.'
author: Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang
categories: Chemical Engineering (miscellaneous) (Q2); Chemistry (miscellaneous) (Q2);
  Environmental Engineering (Q2); Biochemistry (Q3)
citable_docs._(3years): 871.0
cites_/_doc._(2years): 316.0
country: China
coverage: 1993-2020
doi: 10.1016/j.cjche.2016.05.039
eigenfactor_score: 0.00619
h_index: 54.0
isbn: null
issn: '10049541'
issn1: '10049541'
issn2: '10049541'
issn3: '10049541'
jcr_value: '3.171'
keywords: Control loop performance assessment, Industrial alarm system, Process knowledge,
  Root cause diagnosis, Big data
publisher_x: Chemical Industry Press
publisher_y: null
ref._/_doc.: 4399.0
region: Asiatic Region
scimago_value: 595.0
sjr_best_quartile: Q2
sourceid: 12888.0
title_bib: 'A review of control loop monitoring and diagnosis: prospects of controller
  maintenance in big data era'
title_csv: Chinese journal of chemical engineering
total_cites: 6469.0
total_cites_(3years): 2755.0
total_docs._(2020): 355.0
total_docs._(3years): 874.0
total_refs.: 15618.0
type: journal
type_publication: article
year: 2016
---
abstract: Physical metallurgical (PM) and data-driven approaches can be independently
  applied to alloy design. Steel technology is a field of physical metallurgy around
  which some of the most comprehensive understanding has been developed, with vast
  models on the relationship between composition, processing, microstructure and properties.
  They have been applied to the design of new steel alloys in the pursuit of grades
  of improved properties. With the advent of rapid computing and low-cost data storage,
  a wealth of data has become available to a suite of modelling techniques referred
  to as machine learning (ML). ML is being emergingly applied in materials discovery
  while it requires data mining with its adoption being limited by insufficient high-quality
  datasets, often leading to unrealistic materials design predictions outside the
  boundaries of the intended properties. It is therefore required to appraise the
  strength and weaknesses of PM and ML approach, to assess the real design power of
  each towards designing novel steel grades. This work incorporates models and datasets
  from well-established literature on marageing steels. Combining genetic algorithm
  (GA) with PM models to optimise the parameters adopted for each dataset to maximise
  the prediction accuracy of PM models, and the results were compared with ML models.
  The results indicate that PM approaches provide a clearer picture of the overall
  composition-microstructure-properties relationship but are highly sensitive to the
  alloy system and hence lack on exploration ability of new domains. ML conversely
  provides little explicit physical insight whilst yielding a stronger prediction
  accuracy for large-scale data. Hybrid PM/ML approaches provide solutions maximising
  accuracy, while leading to a clearer physical picture and the desired properties.
author: "Chunguang Shen and Chenchong Wang and Pedro E.J. Rivera-D\xEDaz-del-Castillo\
  \ and Dake Xu and Qian Zhang and Chi Zhang and Wei Xu"
categories: Ceramics and Composites (Q1); Materials Chemistry (Q1); Mechanical Engineering
  (Q1); Mechanics of Materials (Q1); Metals and Alloys (Q1); Polymers and Plastics
  (Q1)
citable_docs._(3years): 883.0
cites_/_doc._(2years): 814.0
country: Netherlands
coverage: 1993-2021
doi: 10.1016/j.jmst.2021.02.017
eigenfactor_score: .nan
h_index: 68.0
isbn: null
issn: '10050302'
issn1: '10050302'
issn2: '10050302'
issn3: '10050302'
jcr_value: null
keywords: Machine learning, Physical metallurgy, Small sample problem, Marageing steel
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5264.0
region: Western Europe
scimago_value: 1743.0
sjr_best_quartile: Q1
sourceid: 12330.0
title_bib: 'Discovery of marageing steels: machine learning vs. physical metallurgical
  modelling'
title_csv: Journal of materials science and technology
total_cites: .nan
total_cites_(3years): 6659.0
total_docs._(2020): 572.0
total_docs._(3years): 890.0
total_refs.: 30108.0
type: journal
type_publication: article
year: 2021
---
abstract: Data quality is an important aspect in data application and management,
  and currency is one of the major dimensions influencing its quality. In real applications,
  datasets timestamps are often incomplete and unavailable, or even absent. With the
  increasing requirements to update real-time data, existing methods can fail to adequately
  determine the currency of entities. In consideration of the velocity of big data,
  we propose a series of efficient algorithms for determining the currency of dynamic
  datasets, which we divide into two steps. In the preprocessing step, to better determine
  data currency and accelerate dataset updating, we propose the use of a topological
  graph of the processing order of the entity attributes. Then, we construct an Entity
  Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL)
  to improve the querying and updating processes of both the data currency graph and
  currency scores. In the currency determination step, we propose definitions of the
  currency score and currency information for tuples referring to the same entity
  and use examples to discuss methods and algorithms for their computation. Based
  on our experimental results with both real and synthetic data, we verify that our
  methods can efficiently update data in the correct order of currency.
author: Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao,
  Hong
categories: Multidisciplinary (Q1)
citable_docs._(3years): 199.0
cites_/_doc._(2years): 279.0
country: China
coverage: 2003-2021
doi: 10.23919/TST.2017.7914196
eigenfactor_score: 0.00116
h_index: 43.0
isbn: null
issn: '10070214'
issn1: '10070214'
issn2: '10070214'
issn3: '10070214'
jcr_value: '2.016'
keywords: Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data
  quality management; data currency; dynamic determining
publisher_x: Tsing Hua University
publisher_y: null
ref._/_doc.: 3143.0
region: Asiatic Region
scimago_value: 428.0
sjr_best_quartile: Q1
sourceid: 25522.0
title_bib: Efficient currency determination algorithms for dynamic data
title_csv: Tsinghua science and technology
total_cites: 1474.0
total_cites_(3years): 564.0
total_docs._(2020): 69.0
total_docs._(3years): 200.0
total_refs.: 2169.0
type: journal
type_publication: article
year: 2017
---
abstract: High-quality data are the foundation to monitor the progress and evaluate
  the effects of road traffic injury prevention measures. Unfortunately, official
  road traffic injury statistics delivered by governments worldwide, are often believed
  somewhat unreliable and invalid. We summarized the reported problems concerning
  the road traffic injury statistics through systematically searching and reviewing
  the literature. The problems include absence of regular data, under-reporting, low
  specificity, distorted cause spectrum of road traffic injury, inconsistency, inaccessibility,
  and delay of data release. We also explored the mechanisms behind the problematic
  data and proposed the solutions to the addressed challenges for road traffic statistics.
author: Fang-Rong Chang and He-Lai Huang and David C. Schwebel and Alan H.S. Chan
  and Guo-Qing Hu
categories: Orthopedics and Sports Medicine (Q3); Surgery (Q3)
citable_docs._(3years): 222.0
cites_/_doc._(2years): 156.0
country: Netherlands
coverage: 2001-2020
doi: 10.1016/j.cjtee.2020.06.001
eigenfactor_score: .nan
h_index: 26.0
isbn: null
issn: '10081275'
issn1: '10081275'
issn2: '10081275'
issn3: '10081275'
jcr_value: null
keywords: Traffic injury data, Reported problems, Mechanisms behind the data
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 2900.0
region: Western Europe
scimago_value: 442.0
sjr_best_quartile: Q3
sourceid: 29582.0
title_bib: 'Global road traffic injury statistics: challenges, mechanisms and solutions'
title_csv: Chinese journal of traumatology - english edition
total_cites: .nan
total_cites_(3years): 423.0
total_docs._(2020): 78.0
total_docs._(3years): 227.0
total_refs.: 2262.0
type: journal
type_publication: article
year: 2020
---
abstract: "Objective\nDatabases and softwares are important to manage modern high-throughput\
  \ laboratories and store clinical and genomic information for quality assurance.\
  \ Commercial softwares are expensive with proprietary code issue while academic\
  \ versions have adaptation issue. Our aim was to develop an adaptable in-house software\
  \ that can stores specimen and disease-associated genetic information in biobank\
  \ to facilitate translational research.\nMethods\nPrototype was designed as per\
  \ the research requirements and computational tools were used to develop software\
  \ under three tiers; Visual Basic and ASP.net for presentation tier, SQL server\
  \ for data tier, and Ajax and JavaScript for business tier. We retrieved specimens\
  \ from biobank using this software and performed microarray based transcriptomic\
  \ analysis to detect differentialy expressed genes (DEGs) with FC \xB12 and P-value\
  \ <0.05 in triple negative breast cancer cases. Ingenuity pathway analysis tool\
  \ was used to predict canonical molecular pathways associated with disease. Overall\
  \ performance and utility of software was evaluated by JMeter software, CRUD function\
  \ test and set of feedback questioners.\nResults\nWe developed \u201CBiosearch System\u201D\
  , a web-based software enabling management of biobank samples (tissue, blood, FTTP\
  \ slides) and their extracts (DNA, RNA and proteins) with clinical and experimental\
  \ details. The client satisfaction feedback was excellent with score 4.7/5. We identified\
  \ a total of 1181 DEGs including both upregulated (IFI6, LEF1, FANCI, CASC5, PLXNA3\
  \ etc.) and down-regulated (ADH1B, LYVE1, ADH1C, ADH1B, ADIPOQ, PLIN1, LYVE1 etc.)\
  \ genes in triple negative breast cancer. Pathway analysis of DEGs revealed significant\
  \ activation of interferon signaling (z-score 2.646) and kinetochore metaphase signaling\
  \ pathway (z-score 2.138) in cancer.\nConclusion\nBiosearch System is a user friendly\
  \ LIMS for collection, storage and retrieval of specimen and clinical information.\
  \ It is secure, efficient, and very convenient in sample tracking and data analysis.\
  \ We illustrated its utility in transcriptomic study of breast cancer. Additionally,\
  \ it can facilitate and speed up any genomic study and translational research publications."
author: Sajjad Karim and Mona Al-Kharraz and Zeenat Mirza and Hend Noureldin and Heba
  Abusamara and Nofe Alganmi and Adnan Merdad and Saddig Jastaniah and Sudhir Kumar
  and Mahmood Rasool and Adel Abuzenadah and Mohammed Al-Qahtani
categories: Multidisciplinary (Q1)
citable_docs._(3years): 342.0
cites_/_doc._(2years): 374.0
country: Saudi Arabia
coverage: 1994, 2009-2020
doi: 10.1016/j.jksus.2021.101760
eigenfactor_score: .nan
h_index: 38.0
isbn: null
issn: '10183647'
issn1: '10183647'
issn2: '10183647'
issn3: '10183647'
jcr_value: null
keywords: Biosearch system, LIMS database, Biobank, Genomics, Microarray, Bioinformatics
publisher_x: King Saud University
publisher_y: null
ref._/_doc.: 3350.0
region: Middle East
scimago_value: 574.0
sjr_best_quartile: Q1
sourceid: 86891.0
title_bib: "Development of \u201Cbiosearch system\u201D for biobank management and\
  \ storage of disease associated genetic information"
title_csv: Journal of king saud university - science
total_cites: .nan
total_cites_(3years): 1344.0
total_docs._(2020): 498.0
total_docs._(3years): 343.0
total_refs.: 16685.0
type: journal
type_publication: article
year: 2022
---
abstract: "Reliable data is needed to understand financial relationships in the power\
  \ sector. However, relevant data acquisition and visualization can be a challenge\
  \ due to the fragmented nature of the power sector. The US DOE and ORNL leveraged\
  \ a Sankey prototype to elucidate the \u2018big picture\u2019 of financial flows\
  \ to understand the complex relationships between specific actors within the power\
  \ sector. The continued incorporation of high quality data can improve the fidelity\
  \ of such an approach and lead to an increasingly detailed understanding of financial\
  \ relationships in the power sector and their implications for policymakers."
author: Claire Zeng and Stephen Hendrickson and Sangkeun Matt Lee and Supriya Chinthavali
  and Jessica Lin and Eric Hsieh and Mallikarjun Shankar
categories: Business and International Management (Q1); Law (Q1); Energy (miscellaneous)
  (Q2); Management of Technology and Innovation (Q2)
citable_docs._(3years): 276.0
cites_/_doc._(2years): 205.0
country: United States
coverage: 1988-2020
doi: 10.1016/j.tej.2017.03.001
eigenfactor_score: .nan
h_index: 47.0
isbn: null
issn: '10406190'
issn1: '10406190'
issn2: '10406190'
issn3: '10406190'
jcr_value: null
keywords: Energy, Financial data, Power sector revenue, Sankey, Visualization
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2527.0
region: Northern America
scimago_value: 750.0
sjr_best_quartile: Q1
sourceid: 28802.0
title_bib: 'Energy finance data warehouse: tracking revenues through the power sector'
title_csv: Electricity journal
total_cites: .nan
total_cites_(3years): 632.0
total_docs._(2020): 113.0
total_docs._(3years): 311.0
total_refs.: 2855.0
type: journal
type_publication: article
year: 2017
---
abstract: "Based on microdata from China's listed companies and macrodata for broadband\
  \ internet access in prefecture-level cities, this paper explores the relationship\
  \ between broadband internet and enterprise innovation. Using the change in market\
  \ concentration caused by the North\u2013South separation reform of China Telecom\
  \ in 2002 as an instrumental variable, the results show that in general, a 1% increase\
  \ in broadband internet access results in a 1.395% increase in the number of corporate\
  \ patents. Specifically, the number of valid patents, patent citations and valid\
  \ patent citations, reflecting patent quality, increases by 1.499%, 0.920% and 0.763%,\
  \ respectively. The mechanistic analysis shows that broadband internet access contributes\
  \ to increasing the number of R&D personnel and personal innovation efficiency,\
  \ enhancing enterprises' willingness to innovate, and easing financing constraints.\
  \ Further analysis suggests that broadband internet access mainly promotes invention\
  \ patents rather than design patents. The innovation effect is more evident among\
  \ high-tech, inventor-intensive, state-owned enterprises and enterprises located\
  \ in the non-southeastern coastal region of China."
author: Mengjun Yang and Shilin Zheng and Lin Zhou
categories: Economics and Econometrics (Q1); Finance (Q1)
citable_docs._(3years): 311.0
cites_/_doc._(2years): 396.0
country: Netherlands
coverage: 1989-2020
doi: 10.1016/j.chieco.2022.101802
eigenfactor_score: 0.00605
h_index: 76.0
isbn: null
issn: 1043951X
issn1: 1043951X
issn2: 1043951X
issn3: 1043951X
jcr_value: '4.227'
keywords: Broadband internet, Innovation, Instrumental variable, Knowledge spillover,
  Financing constraints
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4520.0
region: Western Europe
scimago_value: 1361.0
sjr_best_quartile: Q1
sourceid: 20566.0
title_bib: Broadband internet and enterprise innovation
title_csv: China economic review
total_cites: 5722.0
total_cites_(3years): 1318.0
total_docs._(2020): 184.0
total_docs._(3years): 319.0
total_refs.: 8316.0
type: journal
type_publication: article
year: 2022
---
abstract: Radiotherapy is a discipline closely integrated with computer science. Artificial
  intelligence (AI) has developed rapidly over the past few years. With the explosive
  growth of medical big data, AI promises to revolutionize the field of radiotherapy
  through highly automated workflow, enhanced quality assurance, improved regional
  balances of expert experiences, and individualized treatment guided by multi-omics.
  In addition to independent researchers, the increasing number of large databases,
  biobanks, and open challenges significantly facilitated AI studies on radiation
  oncology. This article reviews the latest research, clinical applications, and challenges
  of AI in each part of radiotherapy including image processing, contouring, planning,
  quality assurance, motion management, and outcome prediction. By summarizing cutting-edge
  findings and challenges, we aim to inspire researchers to explore more future possibilities
  and accelerate the arrival of AI radiotherapy.
author: Guangqi Li and Xin Wu and Xuelei Ma
categories: Cancer Research (Q1)
citable_docs._(3years): 286.0
cites_/_doc._(2years): 1139.0
country: United States
coverage: 1990-2020
doi: 10.1016/j.semcancer.2022.08.005
eigenfactor_score: 0.01211
h_index: 148.0
isbn: null
issn: 1044579X
issn1: '10963650'
issn2: 1044579X
issn3: '10963650'
jcr_value: '15.707'
keywords: Artificial intelligence, Radiotherapy, Auto-segmentation, Auto-planning,
  Quality assurance
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 16697.0
region: Northern America
scimago_value: 3908.0
sjr_best_quartile: Q1
sourceid: 24046.0
title_bib: Artificial intelligence in radiotherapy
title_csv: Seminars in cancer biology
total_cites: 11552.0
total_cites_(3years): 3555.0
total_docs._(2020): 287.0
total_docs._(3years): 310.0
total_refs.: 47920.0
type: journal
type_publication: article
year: 2022
---
abstract: The use of clarifying questions (CQs) is a fairly new and useful technique
  to aid systems in recognizing the intent, context, and preferences behind user queries.
  Yet, understanding the extent of the effect of CQs on user behavior and the ability
  to identify relevant information remains relatively unexplored. In this work, we
  conduct a large user study to understand the interaction of users with CQs in various
  quality categories, and the effect of CQ quality on user search performance in terms
  of finding relevant information, search behavior, and user satisfaction. Analysis
  of implicit interaction data and explicit user feedback demonstrates that high-quality
  CQs improve user performance and satisfaction. By contrast, low- and mid-quality
  CQs are harmful, and thus allowing the users to complete their tasks without CQ
  support may be preferred in this case. We also observe that user engagement, and
  therefore the need for CQ support, is affected by several factors, such as search
  result quality or perceived task difficulty. The findings of this study can help
  researchers and system designers realize why, when, and how users interact with
  CQs, leading to a better understanding and design of search clarification systems.
author: Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Pera, Maria
  Soledad and Liu, Yiqun
categories: Business, Management and Accounting (miscellaneous) (Q1); Information
  Systems (Q1); Computer Science Applications (Q2)
citable_docs._(3years): 126.0
cites_/_doc._(2years): 710.0
country: United States
coverage: 1983-2020
doi: 10.1145/3524110
eigenfactor_score: 0.00183
h_index: 83.0
isbn: null
issn: '10468188'
issn1: '10468188'
issn2: '15582868'
issn3: '10468188'
jcr_value: '4.797'
keywords: User Study; Information Seeking Systems; Clarifying Questions
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 7002.0
region: Northern America
scimago_value: 672.0
sjr_best_quartile: Q1
sourceid: 18997.0
title_bib: 'Users meet clarifying questions: toward a better understanding of user
  interactions for search clarification'
title_csv: Acm transactions on information systems
total_cites: 2193.0
total_cites_(3years): 850.0
total_docs._(2020): 44.0
total_docs._(3years): 126.0
total_refs.: 3081.0
type: journal
type_publication: article
year: 2022
---
abstract: This research examines for the first time the relationship between Big data
  and Smart data among French automotive distributors. Many low-tech firms engage
  in these data policies to improve their decisions and performance through the predictive
  capacities of their data. A discussion emerges in the literature according to which
  an effective policy lies in the conversion of a mass of raw data into so-called
  intelligent data. In order to understand better this digital transition, we question
  the transformation of data policies practiced in low-tech firms through the founding
  model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical
  study of 112 French automotive distributors develops the existing literature by
  proposing an original and detailed typology of the data policies practiced (Low
  data, Big data and Smart data). Secondly, after specifying the elements of the differences
  between the quantitative nature of Big data and the qualitative nature of Smart
  data, our results reveal and analyse for the first time the existence of their synergistic
  relationship. Companies transform their Big data approach into Smart data when they
  move from massive exploitation to intelligent exploitation of their data. The phenomenon
  is part of a high-end loop data exploitation. Initially, the exploitation of intelligent
  data can only be done by extracting a sample from a large raw data pool previously
  made by a Big data policy. Secondly, the organization's raw data pool is in turn
  enriched by the repayment of contributions made by the Smart data approach. Thus,
  this study develops three important ways. First off, we identify, detail and compare
  the current data policies of a traditional industry. Secondly, we reveal and explain
  the evolution of digital practices within organizations that now combine both quantitative
  and qualitative data exploitation. Finally, our results guide decision-makers towards
  the synergistic and the legitimate association of different forms of data management
  for better performance.
author: "Jean-S\xE9bastien Lacam and David Salvetat"
categories: Computer Science Applications (Q2); Information Systems and Management
  (Q2); Management of Technology and Innovation (Q2); Marketing (Q2); Strategy and
  Management (Q2)
citable_docs._(3years): 56.0
cites_/_doc._(2years): 476.0
country: United Kingdom
coverage: 1990-2020
doi: 10.1016/j.hitech.2021.100406
eigenfactor_score: .nan
h_index: 46.0
isbn: null
issn: '10478310'
issn1: '10478310'
issn2: '10478310'
issn3: '10478310'
jcr_value: null
keywords: Big data, Smart data, Volume, Velocity, Variety, Automotive distribution
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6772.0
region: Western Europe
scimago_value: 684.0
sjr_best_quartile: Q2
sourceid: 19713.0
title_bib: 'Big data and smart data: two interdependent and synergistic digital policies
  within a virtuous data exploitation loop'
title_csv: Journal of high technology management research
total_cites: .nan
total_cites_(3years): 243.0
total_docs._(2020): 18.0
total_docs._(3years): 56.0
total_refs.: 1219.0
type: journal
type_publication: article
year: 2021
---
abstract: "Observational data research studying access, utilization, cost, and outcomes\
  \ of image-guided interventions using publicly available \u201Cbig data\u201D sets\
  \ is growing in the interventional radiology (IR) literature. Publicly available\
  \ data sets offer insight into real-world care and represent an important pillar\
  \ of IR research moving forward. They offer insights into how IR procedures are\
  \ being used nationally and whether they are working as intended. On the other hand,\
  \ large data sources are aggregated using complex sampling frames, and their strengths\
  \ and weaknesses only become apparent after extensive use. Unintentional misuse\
  \ of large data sets can result in misleading or sometimes erroneous conclusions.\
  \ This review introduces the most commonly used databases relevant to IR research,\
  \ highlights their strengths and limitations, and provides recommendations for use.\
  \ In addition, it summarizes methodologic best practices pertinent to all data sets\
  \ for planning and executing scientifically rigorous and clinically relevant observational\
  \ research."
author: Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria
  M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed
categories: Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging
  (Q1); Cardiology and Cardiovascular Medicine (Q2)
citable_docs._(3years): 715.0
cites_/_doc._(2years): 186.0
country: United States
coverage: 1990-2020
doi: 10.1016/j.jvir.2022.08.003
eigenfactor_score: 0.01078
h_index: 133.0
isbn: null
issn: '10510443'
issn1: '10510443'
issn2: '15357732'
issn3: '10510443'
jcr_value: '3.464'
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 1689.0
region: Northern America
scimago_value: 979.0
sjr_best_quartile: Q1
sourceid: 17243.0
title_bib: A practical guide to use of publicly available data sets for observational
  research in interventional radiology
title_csv: Journal of vascular and interventional radiology
total_cites: 11063.0
total_cites_(3years): 2111.0
total_docs._(2020): 401.0
total_docs._(3years): 1086.0
total_refs.: 6771.0
type: journal
type_publication: article
year: 2022
---
abstract: "Assessing support for molecular phylogenies is difficult because the data\
  \ is heterogeneous in quality and overwhelming in quantity. Traditionally, node\
  \ support values (bootstrap frequency, Bayesian posterior probability) are used\
  \ to assess confidence in tree topologies. Other analyses to assess the quality\
  \ of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and\
  \ the resulting phylogenetic trees (e.g. internode certainty, parameter permutation\
  \ tests, topological tests) exist but are rarely applied. Here we argue that a single\
  \ qualitative analysis is insufficient to assess support of a phylogenetic hypothesis\
  \ and relate data quality to tree quality. We use six molecular markers to infer\
  \ the phylogeny of Blattodea and apply various tests to assess relationship support,\
  \ locus quality, and the relationship between the two. We use internode-certainty\
  \ calculations in conjunction with bootstrap scores, alignment permutations, and\
  \ an approximately unbiased (AU) test to assess if the molecular data unambiguously\
  \ support the phylogenetic relationships found. Our results show higher support\
  \ for the position of Lamproblattidae, high support for the termite phylogeny, and\
  \ low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea.\
  \ We use Lento plots in conjunction with mutation-saturation plots, calculations\
  \ of locus homoplasy to assess locus quality, identify long branch attraction, and\
  \ decide if the tree\u2019s relationships are the result of data biases. We conclude\
  \ that multiple tests and metrics need to be taken into account to assess tree support\
  \ and data robustness."
author: "Dominic Evangelista and France Thouz\xE9 and Manpreet Kaur Kohli and Philippe\
  \ Lopez and Fr\xE9d\xE9ric Legendre"
categories: Ecology, Evolution, Behavior and Systematics (Q1); Genetics (Q1); Molecular
  Biology (Q2)
citable_docs._(3years): 968.0
cites_/_doc._(2years): 389.0
country: United States
coverage: 1992-2020
doi: 10.1016/j.ympev.2018.05.007
eigenfactor_score: 0.02216
h_index: 159.0
isbn: null
issn: '10557903'
issn1: '10959513'
issn2: '10557903'
issn3: '10959513'
jcr_value: '4.286'
keywords: Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long
  branch attraction, Signal analysis
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 8166.0
region: Northern America
scimago_value: 1612.0
sjr_best_quartile: Q1
sourceid: 18965.0
title_bib: Topological support and data quality can only be assessed through multiple
  tests in reviewing blattodea phylogeny
title_csv: Molecular phylogenetics and evolution
total_cites: 22497.0
total_cites_(3years): 3802.0
total_docs._(2020): 208.0
total_docs._(3years): 978.0
total_refs.: 16985.0
type: journal
type_publication: article
year: 2018
---
abstract: "The role of the healthcare organization is shifting and must overcome the\
  \ challenges of fragmented, costly care, and lack of evidence in practice, to reduce\
  \ cost, ensure quality, and deliver high-value care. Notable gaps exist within the\
  \ expected quality and delivery of pediatric healthcare, necessitating a change\
  \ in the role of the healthcare organization. To realize these goals, the use of\
  \ collaborative networks that leverage massive datasets to provide information for\
  \ the development of learning healthcare systems will become increasingly necessary\
  \ as efforts are made to narrow the gap in healthcare quality for children. By building\
  \ upon the lessons learned from early collaborative efforts and other industries,\
  \ operationalizing new technologies, encouraging clinical\u2013community partnerships,\
  \ and improving performance through transparent pursuit of meaningful goals, pediatric\
  \ surgery can increase the adoption of best practices by developing collaborative\
  \ networks that provide evidence-based clinical decision support and accelerate\
  \ progress toward a new culture of delivering high-quality, high-value, and evidenced-based\
  \ pediatric surgical care."
author: Grace E. Hsiung and Fizan Abdullah
categories: Pediatrics, Perinatology and Child Health (Q1); Surgery (Q1)
citable_docs._(3years): 174.0
cites_/_doc._(2years): 264.0
country: United Kingdom
coverage: 1992-2020
doi: 10.1053/j.sempedsurg.2015.08.008
eigenfactor_score: 0.00282
h_index: 65.0
isbn: null
issn: '10558586'
issn1: '10558586'
issn2: '15329453'
issn3: '10558586'
jcr_value: '2.754'
keywords: Quality improvement, Health services research, Pediatric collaborative networks,
  Pediatric health, Learning healthcare systems
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 4803.0
region: Western Europe
scimago_value: 848.0
sjr_best_quartile: Q1
sourceid: 22283.0
title_bib: Improving surgical care for children through multicenter registries and
  qi collaboratives
title_csv: Seminars in pediatric surgery
total_cites: 2233.0
total_cites_(3years): 598.0
total_docs._(2020): 76.0
total_docs._(3years): 191.0
total_refs.: 3650.0
type: journal
type_publication: article
year: 2015
---
abstract: "Introduction: Computational methods have been widely applied to toxicology\
  \ across pharmaceutical, consumer product and environmental fields over the past\
  \ decade. Progress in computational toxicology is now reviewed. Methods: A literature\
  \ review was performed on computational models for hepatotoxicity (e.g. for drug-induced\
  \ liver injury (DILI)), cardiotoxicity, renal toxicity and genotoxicity. In addition\
  \ various publications have been highlighted that use machine learning methods.\
  \ Several computational toxicology model datasets from past publications were used\
  \ to compare Bayesian and Support Vector Machine (SVM) learning methods. Results:\
  \ The increasing amounts of data for defined toxicology endpoints have enabled machine\
  \ learning models that have been increasingly used for predictions. It is shown\
  \ that across many different models Bayesian and SVM perform similarly based on\
  \ cross validation data. Discussion: Considerable progress has been made in computational\
  \ toxicology in a decade in both model development and availability of larger scale\
  \ or \u2018big data\u2019 models. The future efforts in toxicology data generation\
  \ will likely provide us with hundreds of thousands of compounds that are readily\
  \ accessible for machine learning models. These models will cover relevant chemistry\
  \ space for pharmaceutical, consumer product and environmental applications."
author: Sean Ekins
categories: Pharmacology (Q3); Toxicology (Q4)
citable_docs._(3years): 413.0
cites_/_doc._(2years): 75.0
country: United States
coverage: 1992-2002, 2004-2020
doi: 10.1016/j.vascn.2013.12.003
eigenfactor_score: 0.00214
h_index: 71.0
isbn: null
issn: '10568719'
issn1: '10568719'
issn2: '10568719'
issn3: '10568719'
jcr_value: '1.950'
keywords: Bayesian, Computational toxicology, Machine learning, Support Vector Machine
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 3978.0
region: Northern America
scimago_value: 353.0
sjr_best_quartile: Q3
sourceid: 25146.0
title_bib: Progress in computational toxicology
title_csv: Journal of pharmacological and toxicological methods
total_cites: 2757.0
total_cites_(3years): 451.0
total_docs._(2020): 69.0
total_docs._(3years): 416.0
total_refs.: 2745.0
type: journal
type_publication: article
year: 2014
---
abstract: To explore the construction of a big data indicator system is conducive
  to a comprehensive, scientific, timely and accurate grasp of the quality of life
  of our residents and its evolutionary trends. This paper systematically sorts out
  the performance dimensions of the residents' quality of life, and integrates two
  types of methods of objective observation and subjective evaluation commercial POI(Point
  of Interest) data. From the aspects of life, entertainment, transportation, etc.,
  preliminary development has been made including 8 first-level indicators, 16 second-level
  indicators, and 27 third-level indicators Big data indicator system, and measure
  the "clogging point" of the improvement of residents' quality of life, with a view
  to providing a scientific and feasible decision-making reference for "meeting the
  people's increasing needs for a better life".
author: Yang Wang and Hong Zhang and Libing Liu
categories: Economics and Econometrics (Q2); Finance (Q2)
citable_docs._(3years): 524.0
cites_/_doc._(2years): 245.0
country: United States
coverage: 1992-2021
doi: 10.1016/j.iref.2022.01.004
eigenfactor_score: .nan
h_index: 54.0
isbn: null
issn: '10590560'
issn1: '10590560'
issn2: '10590560'
issn3: '10590560'
jcr_value: null
keywords: Quality of life, Point of interest, Happiness
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 5112.0
region: Northern America
scimago_value: 781.0
sjr_best_quartile: Q2
sourceid: 22707.0
title_bib: Does city construction improve life quality?-evidence from poi data of
  china
title_csv: International review of economics and finance
total_cites: .nan
total_cites_(3years): 1462.0
total_docs._(2020): 186.0
total_docs._(3years): 529.0
total_refs.: 9509.0
type: journal
type_publication: article
year: 2022
---
abstract: 'Purpose

  Status epilepticus is an often apparently randomly occurring, life-threatening medical
  emergency which affects the quality of life in patients with epilepsy and their
  families. The purpose of this review is to summarize information on ambulatory seizure
  detection, seizure prediction, and status epilepticus prevention.

  Method

  Narrative review.

  Results

  Seizure detection devices are currently under investigation with regards to utility
  and feasibility in the detection of isolated seizures, mainly in adult patients
  with generalized tonic-clonic seizures, in long-term epilepsy monitoring units,
  and occasionally in the outpatient setting. Detection modalities include accelerometry,
  electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors,
  surface electromyography, video detection systems, gyroscope, peripheral temperature,
  photoplethysmography, and respiratory sensors, among others. Initial detection results
  are promising, and improve even further, when several modalities are combined. Some
  portable devices have already been U.S. FDA approved to detect specific seizures.
  Improved seizure prediction may be attainable in the future given that epileptic
  seizure occurrence follows complex patient-specific non-random patterns. The combination
  of multimodal monitoring devices, big data sets, and machine learning may enhance
  patient-specific detection and predictive algorithms. The integration of these technological
  advances and novel approaches into closed-loop warning and treatment systems in
  the ambulatory setting may help detect seizures sooner, and tentatively prevent
  status epilepticus in the future.

  Conclusions

  Ambulatory monitoring systems are being developed to improve seizure detection and
  the quality of life in patients with epilepsy and their families.'
author: Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper
categories: Medicine (miscellaneous) (Q1); Neurology (Q2); Neurology (clinical) (Q2)
citable_docs._(3years): 718.0
cites_/_doc._(2years): 272.0
country: United Kingdom
coverage: 1992-2020
doi: 10.1016/j.seizure.2018.09.013
eigenfactor_score: .nan
h_index: 85.0
isbn: null
issn: '10591311'
issn1: '10591311'
issn2: '15322688'
issn3: '10591311'
jcr_value: null
keywords: Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure
  detection sensors, Automated seizure detection
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 3479.0
region: Western Europe
scimago_value: 1158.0
sjr_best_quartile: Q1
sourceid: 19380.0
title_bib: Status epilepticus prevention, ambulatory monitoring, early seizure detection
  and prediction in at-risk patients
title_csv: 'Seizure : the journal of the british epilepsy association'
total_cites: .nan
total_cites_(3years): 2457.0
total_docs._(2020): 317.0
total_docs._(3years): 752.0
total_refs.: 11029.0
type: journal
type_publication: article
year: 2019
---
abstract: "In the big data era, internal audit functions (IAFs) should innovate their\
  \ techniques so as to add value to their organizations. The use of data analytics\
  \ (DA) increases IAFs\u2019 ability to extract value from big data, helping IAFs\
  \ to enhance their activities\u2019 efficiency and effectiveness. We use responses\
  \ from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates\
  \ of IAFs\u2019 DA usage. From the literature, we identify five main variables expected\
  \ to be associated with IAFs\u2019 DA use. We find a positive and significant association\
  \ between DA use and (i) the IAF reporting to the audit committee (AC) and (ii)\
  \ CAEs\u2019 ability to build positive relationships with managers. These findings\
  \ suggest that IAF independence and CAEs\u2019 soft skills are important to innovate\
  \ IAF techniques favoring DA use. We also find a positive association between DA\
  \ use and IAFs\u2019 involvement in the assurance of enterprise risk management,\
  \ fraud detection, and IT risk audit activities. Our findings contribute to the\
  \ internal auditing and DA literatures, and should be of interest to CAEs, ACs,\
  \ corporate boards, and professional associations."
author: Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza
categories: Accounting (Q2); Finance (Q2)
citable_docs._(3years): 59.0
cites_/_doc._(2years): 247.0
country: United Kingdom
coverage: 1992-2020
doi: 10.1016/j.intaccaudtax.2020.100357
eigenfactor_score: .nan
h_index: 41.0
isbn: null
issn: '10619518'
issn1: '10619518'
issn2: '10619518'
issn3: '10619518'
jcr_value: null
keywords: Internal audit, Data analytics, Big data, Soft skills, Fraud detection,
  IT audit
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6547.0
region: Western Europe
scimago_value: 444.0
sjr_best_quartile: Q2
sourceid: 29884.0
title_bib: "Correlates of the internal audit function\u2019s use of data analytics\
  \ in the big data era: global evidence"
title_csv: Journal of international accounting, auditing and taxation
total_cites: .nan
total_cites_(3years): 178.0
total_docs._(2020): 32.0
total_docs._(3years): 66.0
total_refs.: 2095.0
type: journal
type_publication: article
year: 2021
---
abstract: "Female entrepreneurship is important for business and economic development.\
  \ However, women face greater obstacles than men in accessing financing and information,\
  \ making it more difficult for them to engage in entrepreneurship. This paper examines\
  \ the impact of digital financial inclusion on female entrepreneurship by using\
  \ a national sample consisting of matched data from a digital financial inclusion\
  \ index and a nationally representative survey. The results show that digital financial\
  \ inclusion significantly promotes women\u2019s entrepreneurial behavior. We find\
  \ that digital financial inclusion can ease women\u2019s financing constraints and\
  \ provide business information to alleviate their information constraints. Furthermore,\
  \ the development of digital financial inclusion improves women\u2019s work flexibility,\
  \ inspiring them to engage in entrepreneurship. In addition, digital financial inclusion\
  \ has a greater effect on entrepreneurship among vulnerable women, such as those\
  \ with less education or a lack of financial autonomy and those living in areas\
  \ with high gender inequality, which supports the idea that digital financial inclusion\
  \ can empower women."
author: Xiaolan Yang and Yidong Huang and Mei Gao
categories: Economics and Econometrics (Q2); Finance (Q2)
citable_docs._(3years): 346.0
cites_/_doc._(2years): 272.0
country: United States
coverage: 1992-2020
doi: 10.1016/j.najef.2022.101800
eigenfactor_score: 0.00234
h_index: 37.0
isbn: null
issn: '10629408'
issn1: '10629408'
issn2: '10629408'
issn3: '10629408'
jcr_value: '2.772'
keywords: Digital financial inclusion, Female entrepreneurship, CFPS
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 4826.0
region: Northern America
scimago_value: 607.0
sjr_best_quartile: Q2
sourceid: 24089.0
title_bib: Can digital financial inclusion promote female entrepreneurship? evidence
  and mechanisms
title_csv: North american journal of economics and finance
total_cites: 2130.0
total_cites_(3years): 911.0
total_docs._(2020): 268.0
total_docs._(3years): 348.0
total_refs.: 12933.0
type: journal
type_publication: article
year: 2022
---
abstract: The use of electronic case report forms (CRF) to gather data in randomized
  clinical trials has grown to progressively replace paper-based forms. Computerized
  form designs must ensure the same data quality expected of paper CRF, by following
  Good Clinical Practice rules. Electronic data capture (EDC) tools must also comply
  with applicable statutory and regulatory requirements. Here the authors focus on
  the development of computerized systems for clinical trials implementing FDA and
  EU recommendations and regulations, and describe a laptop-based electronic CRF used
  in a randomized, multicenter clinical trial.
author: Bogdan Ene-Iordache and Sergio Carminati and Luca Antiga and Nadia Rubis and
  Piero Ruggenenti and Giuseppe Remuzzi and Andrea Remuzzi
categories: Health Informatics (Q1)
citable_docs._(3years): 580.0
cites_/_doc._(2years): 469.0
country: United States
coverage: 1994-2020
doi: 10.1197/jamia.M2787
eigenfactor_score: .nan
h_index: 150.0
isbn: null
issn: '10675027'
issn1: 1527974X
issn2: '10675027'
issn3: 1527974X
jcr_value: null
keywords: null
publisher_x: Oxford University Press
publisher_y: null
ref._/_doc.: 3581.0
region: Northern America
scimago_value: 1614.0
sjr_best_quartile: Q1
sourceid: 23600.0
title_bib: 'Developing regulatory-compliant electronic case report forms for clinical
  trials: experience with the demand trial'
title_csv: 'Journal of the american medical informatics association : jamia'
total_cites: .nan
total_cites_(3years): 3360.0
total_docs._(2020): 264.0
total_docs._(3years): 632.0
total_refs.: 9454.0
type: journal
type_publication: article
year: 2009
---
abstract: This paper introduces a novel transit data analytics platform for public
  transit planning, assessing service quality and revealing service problems in high
  spatiotemporal resolution for public transit systems based on Automatic Passenger
  Counting (APC) and Automatic Vehicle Location (AVL) technologies. The platform offers
  a systematic way for users and decision makers to understand system performance
  from many aspects of service quality, including passenger waiting time, stop-skipping
  frequency, bus bunching level, bus travel time, on-time performance, and bus fullness.
  The AVL-APC data from September 2012 to March 2016 were archived in a database to
  support the development of a user-friendly web application that allows both users
  and managers to interactively query bus performance metrics for any bus routes,
  stops, or trips for any time period. This paper demonstrates a case study using
  the platform to examine bus bunching in a transit system operated by the Port Authority
  of Allegheny County (PAAC) in Pittsburgh. It is found that the incidence of bus
  bunching is heavily impacted by the location on the route as well as the time of
  day, and the bunching problem is more severe for bus routes operating in mixed traffic
  than for bus rapid transit, which operates along a dedicated busway. Furthermore,
  a second case study is presented with a comprehensive analysis on a representative
  route in Pittsburgh under schedule changes. Suggestions for operation of this route
  to improve service quality are proposed based on the data analytics results.
author: Xidong Pi and Mark Egge and Jackson Whitmore and Zhen (Sean) Qian and Amy
  Silbermann
categories: Geography, Planning and Development (Q1); Urban Studies (Q1); Transportation
  (Q2)
citable_docs._(3years): 30.0
cites_/_doc._(2years): 230.0
country: United States
coverage: 2007, 2009, 2011-2018, 2020
doi: 10.5038/2375-0901.21.2.2
eigenfactor_score: 0.00054
h_index: 25.0
isbn: null
issn: 1077291X
issn1: 1077291X
issn2: 1077291X
issn3: 1077291X
jcr_value: '2.529'
keywords: Transit system, Automatic Vehicle Location, Automatic Passenger Counting,
  data analytics platform, performance metrics, bus bunching, service quality
publisher_x: National Center for Transit Research
publisher_y: null
ref._/_doc.: 3250.0
region: Northern America
scimago_value: 721.0
sjr_best_quartile: Q1
sourceid: 21100205751.0
title_bib: 'Understanding transit system performance using avl-apc data: an analytics
  platform with case studies for the pittsburgh region'
title_csv: Journal of public transportation
total_cites: 1013.0
total_cites_(3years): 78.0
total_docs._(2020): 4.0
total_docs._(3years): 36.0
total_refs.: 130.0
type: journal
type_publication: article
year: 2018
---
abstract: "Objectives\nThis study aimed to showcase the potential and key concerns\
  \ and risks of artificial intelligence (AI) in the health sector, illustrating its\
  \ application with current examples, and to provide policy guidance for the development,\
  \ assessment, and adoption of AI technologies to advance policy objectives.\nMethods\n\
  Nonsystematic scan and analysis of peer-reviewed and gray literature on AI in the\
  \ health sector, focusing on key insights for policy and governance.\nResults\n\
  The application of AI in the health sector is currently in the early stages. Most\
  \ applications have not been scaled beyond the research setting. The use in real-world\
  \ clinical settings is especially nascent, with more evidence in public health,\
  \ biomedical research, and \u201Cback office\u201D administration. Deploying AI\
  \ in the health sector carries risks and hazards that must be managed proactively\
  \ by policy makers. For AI to produce positive health and policy outcomes, 5 key\
  \ areas for policy are proposed, including health data governance, operationalizing\
  \ AI principles, flexible regulation, skills among health workers and patients,\
  \ and strategic public investment.\nConclusions\nAI is not a panacea, but a tool\
  \ to address specific problems. Its successful development and adoption require\
  \ data governance that ensures high-quality data are available and secure; relevant\
  \ actors can access technical infrastructure and resources; regulatory frameworks\
  \ promote trustworthy AI products; and health workers and patients have the information\
  \ and skills to use AI products and services safely, effectively, and efficiently.\
  \ All of this requires considerable investment and international collaboration."
author: Tiago Cravo Oliveira Hashiguchi and Jillian Oderkirk and Luke Slawomirski
categories: Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental
  and Occupational Health (Q1)
citable_docs._(3years): 532.0
cites_/_doc._(2years): 367.0
country: United Kingdom
coverage: 1998-2020
doi: 10.1016/j.jval.2021.11.1369
eigenfactor_score: 0.01786
h_index: 103.0
isbn: null
issn: '10983015'
issn1: '10983015'
issn2: '15244733'
issn3: '10983015'
jcr_value: '5.725'
keywords: artificial intelligence, governance, machine learning, policy
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 3876.0
region: Western Europe
scimago_value: 1859.0
sjr_best_quartile: Q1
sourceid: 22377.0
title_bib: "Fulfilling the promise of artificial intelligence in the health sector:\
  \ let\u2019s get real"
title_csv: Value in health
total_cites: 12642.0
total_cites_(3years): 2291.0
total_docs._(2020): 211.0
total_docs._(3years): 572.0
total_refs.: 8179.0
type: journal
type_publication: article
year: 2022
---
abstract: The present study evaluates the performance of PNN models for porosity prediction
  using seismic attributes. Four seismic datasets and more than 20 wells from different
  sedimentary basins located in Libya, Iraq, Egypt and USA are employed to characterize
  the effective attributes for porosity prediction. Verification and testing error
  analysis is adopted for evaluating the prediction performance. Results indicated
  that the porosity prediction models are primarily dependent to the propagation related
  attributes with frequency attributes as the most effective parameters in porosity
  prediction. In addition, the data quality and processing history strongly control
  the prediction model performance with relatively limited effects to dataset dimensionality
  (2D versus 3D) and the number of wells utilized in model construction. Such results
  are important to better understand and evaluate the performance of PNN porosity
  prediction models using various seismic attributes.
author: Abdulaziz M. Abdulaziz
categories: Fuel Technology (Q1); Organic Chemistry (Q1); Process Chemistry and Technology
  (Q1); Catalysis (Q2); Geochemistry and Petrology (Q2); Renewable Energy, Sustainability
  and the Environment (Q2)
citable_docs._(3years): 288.0
cites_/_doc._(2years): 525.0
country: Egypt
coverage: 2011-2020
doi: 10.1016/j.ejpe.2019.12.001
eigenfactor_score: .nan
h_index: 38.0
isbn: null
issn: '11100621'
issn1: '11100621'
issn2: '20902468'
issn3: '11100621'
jcr_value: null
keywords: Porosity prediction, Seismic attributes, PNN, Al Ghani field, Sooner field,
  Kifl field, Baltim field
publisher_x: Egyptian Petroleum Research Institute
publisher_y: null
ref._/_doc.: 3891.0
region: Africa/Middle East
scimago_value: 942.0
sjr_best_quartile: Q1
sourceid: 21100819607.0
title_bib: 'The effective seismic attributes in porosity prediction for different
  rock types: some implications from four case studies'
title_csv: Egyptian journal of petroleum
total_cites: .nan
total_cites_(3years): 1586.0
total_docs._(2020): 33.0
total_docs._(3years): 288.0
total_refs.: 1284.0
type: journal
type_publication: article
year: 2020
---
abstract: Internet of Things (IoT) is a fundamental concept of a new technology that
  will be promising and significant in various fields. IoT is a vision that allows
  things or objects equipped with sensors, actuators, and processors to talk and communicate
  with each other over the internet to achieve a meaningful goal. Unfortunately, one
  of the major challenges that affect IoT is data quality and uncertainty, as data
  volume increases noise, inconsistency and redundancy increases within data and causes
  paramount issues for IoT technologies. And since IoT is considered to be a massive
  quantity of heterogeneous networked embedded devices that generate big data, then
  it is very complex to compute and analyze such massive data. So this paper introduces
  a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed
  datasets (RDDs) to detect outliers that affect the data quality of IoT technologies.
  NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D,
  and 25-D) and the results were promising. Finally, comparisons have been made between
  NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and
  these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of
  RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT
  data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers
  that exist in the datasets of N-dimensions by using resilient distributed datasets
  (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications
  and technologies.
author: Haitham Ghallab and Hanan Fahmy and Mona Nasr
categories: Information Systems (Q1); Computer Science Applications (Q2); Management
  Science and Operations Research (Q2)
citable_docs._(3years): 59.0
cites_/_doc._(2years): 694.0
country: Egypt
coverage: 2010-2020
doi: 10.1016/j.eij.2019.12.001
eigenfactor_score: 0.00093
h_index: 34.0
isbn: null
issn: '11108665'
issn1: '11108665'
issn2: '11108665'
issn3: '11108665'
jcr_value: '3.943'
keywords: Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN,
  RDDs
publisher_x: Faculty of Computers and Information, Cairo University
publisher_y: null
ref._/_doc.: 3521.0
region: Africa/Middle East
scimago_value: 728.0
sjr_best_quartile: Q1
sourceid: 19700182731.0
title_bib: Detection outliers on internet of things using big data technology
title_csv: Egyptian informatics journal
total_cites: 820.0
total_cites_(3years): 415.0
total_docs._(2020): 47.0
total_docs._(3years): 59.0
total_refs.: 1655.0
type: journal
type_publication: article
year: 2020
---
abstract: "Purpose\nFixed-field intensity modulated radiation therapy (FF-IMRT) or\
  \ volumetric modulated arc therapy (VMAT) beams complexity is due to fluence fluctuation.\
  \ Pre-treatment Quality Assurance (PTQA) failure could be linked to it. Several\
  \ plan complexity metrics (PCM) have been published to quantify this complexity\
  \ but in a heterogeneous formalism. This review proposes to gather different PCM\
  \ and to discuss their eventual PTQA failure identifier abilities.\nMethods and\
  \ materials\nA systematic literature search and outcome extraction from MEDLINE/PubMed\
  \ (National Center for Biotechnology Information, NCBI) was performed. First, a\
  \ list and a synthesis of available PCM is made in a homogeneous formalism. Second,\
  \ main results relying on the link between PCM and PTQA results but also on other\
  \ uses are listed.\nResults\nA total of 163 studies were identified and n\u202F\
  =\u202F19 were selected after inclusion and exclusion criteria application. Difference\
  \ is made between fluence and degree of freedom (DOF)-based PCM. Results about the\
  \ PCM potential as PTQA failure identifier are described and synthesized. Others\
  \ uses are also found in quality, big data, machine learning and audit procedure.\n\
  Conclusions\nA state of the art is made thanks to this homogeneous PCM classification.\
  \ For now, PCM should be seen as a planning procedure quality indicator although\
  \ PTQA failure identifier results are mitigated. However limited clinical use seems\
  \ possible for some cases. Yet, addressing the general PTQA failure prediction case\
  \ could be possible with the big data or machine learning help."
author: "Mika\xEBl Antoine and Flavien Ralite and Charles Soustiel and Thomas Marsac\
  \ and Paul Sargos and Audrey Cugny and J\xE9r\xF4me Caron"
categories: Physics and Astronomy (miscellaneous) (Q1); Radiology, Nuclear Medicine
  and Imaging (Q1); Biophysics (Q2); Medicine (miscellaneous) (Q2)
citable_docs._(3years): 779.0
cites_/_doc._(2years): 256.0
country: Italy
coverage: 1989-2020
doi: 10.1016/j.ejmp.2019.05.024
eigenfactor_score: .nan
h_index: 44.0
isbn: null
issn: '11201797'
issn1: '11201797'
issn2: 1724191X
issn3: '11201797'
jcr_value: null
keywords: Modulation indices, Plan complexity, Volumetric modulated arc therapy
publisher_x: Associazione Italiana di Fisica Medica
publisher_y: null
ref._/_doc.: 3856.0
region: Western Europe
scimago_value: 883.0
sjr_best_quartile: Q1
sourceid: 17037.0
title_bib: 'Use of metrics to quantify imrt and vmat treatment plan complexity: a
  systematic review and perspectives'
title_csv: Physica medica
total_cites: .nan
total_cites_(3years): 2126.0
total_docs._(2020): 307.0
total_docs._(3years): 792.0
total_refs.: 11838.0
type: journal
type_publication: article
year: 2019
---
abstract: 'The objective of this quality evaluation was to evaluate the changes in
  public health nursing (PHN) interventions after the implementation of an evidence-based
  family home visiting (EB-FHV) guideline encoded using the Omaha System.

  Design and sample

  This quality improvement evaluation was conducted using a secondary dataset of 27,910
  PHN family home visiting interventions from visits to 129 adult clients enrolled
  in EB-FHV programs in a Midwestern PHN agency. The interventions were documented
  12 months before and 14 months after EB-FHV Guideline implementation. The EB-FHV
  consisted of 94 PHN interventions for 10 Omaha System problems, with electronic
  health record (EHR) data generated by PHNs during routine clinical documentation.
  Standard descriptive and inferential statistics were employed in the analysis.

  Measures

  The Omaha System was used to compare PHN practice before and after the guideline
  implementation.

  Results

  Documentation patterns revealed that PHNs tailored interventions while also shifting
  toward the use of the EB-FHV guideline interventions. Ten EB-FHV problems accounted
  for 96.3% of interventions documented before and 98.5% of interventions documented
  after implementation. The proportion of interventions before and after EB-FHV by
  problem differed significantly for all problems except Substance use. Fewer interventions
  were provided after EB-FHV for the primary problems of Pregnancy and Postpartum,
  with a shift to more interventions for Caretaking/parenting.

  Conclusion

  The PHN documentation after guideline implementation demonstrated adherence to the
  EB-FHV guideline, while tailoring the evidence-based interventions differentially
  by problem. Further research is needed to extend this quality improvement approach
  to other guidelines and populations.'
author: Karen A. Monsen and Sadie M. Swenson and Lisa V. Klotzbach and Michelle A.
  Mathiason and Karen E. Johnson
categories: Philosophy (Q2); Nursing (miscellaneous) (Q4); Public Health, Environmental
  and Occupational Health (Q4)
citable_docs._(3years): 156.0
cites_/_doc._(2years): 42.0
country: Czech Republic
coverage: 2014-2020
doi: 10.1016/j.kontakt.2017.03.002
eigenfactor_score: .nan
h_index: 8.0
isbn: null
issn: '12124117'
issn1: '18047122'
issn2: '12124117'
issn3: '18047122'
jcr_value: null
keywords: Family home visiting, Omaha System, Intervention, Guideline, Evaluation
publisher_x: University of South Bohemia
publisher_y: null
ref._/_doc.: 2933.0
region: Eastern Europe
scimago_value: 167.0
sjr_best_quartile: Q2
sourceid: 21100456164.0
title_bib: Empirical evaluation of the changes in public health nursing interventions
  after the implementation of an evidence-based family home visiting guideline
title_csv: Kontakt
total_cites: .nan
total_cites_(3years): 75.0
total_docs._(2020): 43.0
total_docs._(3years): 168.0
total_refs.: 1261.0
type: journal
type_publication: article
year: 2017
---
abstract: "Thermal fluid processes are inherently multi-physics and multi-scale, involving\
  \ mass-momentum-energy transport phenomena at multiple scales. Thermal fluid simulation\
  \ (TFS) is based on solving conservative equations, for which \u2013 except for\
  \ \u201Cfirst-principles\u201D direct numerical simulation \u2013 closure relations\
  \ (CRs) are required to provide microscopic interactions or so-called sub-grid-scale\
  \ physics. In practice, TFS is realized through reduced-order modeling, and its\
  \ CRs as low-fidelity models can be informed by observations and data from relevant\
  \ and adequately evaluated experiments and high-fidelity simulations. This paper\
  \ is focused on data-driven TFS models, specifically on their development using\
  \ machine learning (ML). Five ML frameworks are introduced including physics-separated\
  \ ML (PSML or Type I ML), physics-evaluated ML (PEML or Type II ML), physics-integrated\
  \ ML (PIML or Type III ML), physics-recovered (PRML or Type IV ML), and physics-discovered\
  \ ML (PDML or Type V ML). The frameworks vary in their performance for different\
  \ applications depending on the level of knowledge of governing physics, source,\
  \ type, amount and quality of available data for training. Notably, outlined for\
  \ the first time in this paper, Type III models present stringent requirements on\
  \ modeling, substantial computing resources for training, and high potential in\
  \ extracting value from \u201Cbig data\u201D in thermal fluid research. The current\
  \ paper demonstrates and investigates ML frameworks in three examples. First, we\
  \ utilize the heat diffusion equation with a nonlinear conductivity model formulated\
  \ by convolutional neural networks (CNNs) and feedforward neural networks (FNNs)\
  \ to illustrate the applications of Type I, Type II, Type III, and Type V ML. The\
  \ results indicate a preference for Type II ML under deficient data support. Type\
  \ III ML can effectively utilize field data, potentially generating more robust\
  \ predictions than Type I and Type II ML. CNN-based closures exhibit more predictability\
  \ than FNN-based closures, but CNN-based closures require more training data to\
  \ obtain accurate predictions. Second, we illustrate how to employ Type I ML and\
  \ Type II ML frameworks for data-driven turbulence modeling using reference works.\
  \ Third, we demonstrate Type I ML by building a deep FNN-based slip closure for\
  \ two-phase flow modeling. The results show that deep FNN-based closures exhibit\
  \ a bounded error in the prediction domain."
author: Chih-Wei Chang and Nam T. Dinh
categories: Condensed Matter Physics (Q1); Engineering (miscellaneous) (Q1)
citable_docs._(3years): 1267.0
cites_/_doc._(2years): 391.0
country: France
coverage: 1973-1978, 1987, 1999-2021
doi: 10.1016/j.ijthermalsci.2018.09.002
eigenfactor_score: 0.01547
h_index: 119.0
isbn: null
issn: '12900729'
issn1: '12900729'
issn2: '12900729'
issn3: '12900729'
jcr_value: '3.744'
keywords: Thermal fluid simulation, Closure relations, Multiscale modeling, Machine
  learning framework, Deep learning, Data driven, Convolutional neural networks, Classification
publisher_x: Elsevier Masson SAS
publisher_y: null
ref._/_doc.: 4081.0
region: Western Europe
scimago_value: 1208.0
sjr_best_quartile: Q1
sourceid: 13761.0
title_bib: Classification of machine learning frameworks for data-driven thermal fluid
  models
title_csv: International journal of thermal sciences
total_cites: 16386.0
total_cites_(3years): 4983.0
total_docs._(2020): 331.0
total_docs._(3years): 1268.0
total_refs.: 13507.0
type: journal
type_publication: article
year: 2019
---
abstract: "In this study, highly accurate particulate matter (PM10 and PM2.5) predictions\
  \ were obtained using meteorological prediction data from the local data assimilation\
  \ and prediction system (LDAPS) and tree-based machine learning (ML). The study\
  \ area was Seoul, South Korea, and data from July 2018 to June 2021 as well as LDAPS\
  \ 36-h predictions with 1-h intervals 4 times a day were used. The predicted PM\
  \ values were then compared with the observed PM measurements to evaluate the prediction\
  \ accuracy. The PM prediction performance of the Community Multi-Scale Air Quality\
  \ (CMAQ)-based chemical transport model (CTM) was compared with that reported by\
  \ this study. The experimental results report that, among tree-based ML algorithms,\
  \ light gradient boosting (LGB) is the most suitable for PM prediction. The PM prediction\
  \ results of the LGB algorithm for the hourly test data were: bias\_=\_\u22120.10\_\
  \u03BCg/m3, root mean square error (RMSE)\_=\_13.15\_\u03BCg/m3, and R2\_=\_0.86\
  \ for PM10 and bias\_=\_\u22120.02\_\u03BCg/m3, RMSE\_=\_7.48\_\u03BCg/m3, and R2\_\
  =\_0.83 for PM2.5, and for daily mean were: RMSE \u22641.16\_\u03BCg/m3 and R2\_\
  =\_0.996. The relative RMSE (%RMSE) is 21% lower than the results of the CTM model,\
  \ and R2 is 0.20 higher. Even in the high PM concentration case prediction results,\
  \ the algorithm showed good predictive performance with %RMSE\_=\_8.91%\u201320.43%\
  \ and R2\_=\_0.89\u20130.97. Therefore, in addition to the CTM, high-accuracy PM\
  \ prediction results using ML can also be used for air quality monitoring and improvement."
author: Bu-Yo Kim and Yun-Kyu Lim and Joo Wan Cha
categories: Pollution (Q1); Waste Management and Disposal (Q1); Atmospheric Science
  (Q2)
citable_docs._(3years): 455.0
cites_/_doc._(2years): 450.0
country: Netherlands
coverage: 2010-2020
doi: 10.1016/j.apr.2022.101547
eigenfactor_score: 0.00526
h_index: 45.0
isbn: null
issn: '13091042'
issn1: '13091042'
issn2: '13091042'
issn3: '13091042'
jcr_value: '4.352'
keywords: Particulate matter prediction, PM, PM, Tree-based machine learning, Air
  quality monitoring, Light gradient boosting algorithm
publisher_x: Turkish National Committee for Air Pollution Research (TUNCAP)
publisher_y: null
ref._/_doc.: 5666.0
region: Western Europe
scimago_value: 984.0
sjr_best_quartile: Q1
sourceid: 21100254615.0
title_bib: Short-term prediction of particulate matter (pm10 and pm2.5) in seoul,
  south korea using tree-based machine learning algorithms
title_csv: Atmospheric pollution research
total_cites: 4254.0
total_cites_(3years): 2090.0
total_docs._(2020): 249.0
total_docs._(3years): 455.0
total_refs.: 14109.0
type: journal
type_publication: article
year: 2022
---
abstract: For many years, data quality is among the most commonly discussed issue
  in Linked Open Data (LOD) due to the huge volume of integrated datasets that are
  usually heterogeneous. Several ontology-based approaches dealing with quality problems
  have been proposed. However, when datasets lack a well-defined schema, these approaches
  become ineffective because of the lack of metadata. Moreover, the detection of quality
  problems based on an analysis between RDF (Resource Description Framework) triples
  without requiring ontology statistical and semantical information is not addressed.
  Keeping in mind that ontologies are not always available and they may be incomplete
  or misused. In this paper, a novel free-ontology process called LODQuMa is proposed
  to assess and improve the quality of LOD. It is mainly based on profiling statistics,
  synonym relationships between predicates, QVCs (Quality Verification Cases), and
  SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on
  the DBpedia dataset demonstrate that the proposed process is effective for increasing
  the intrinsic quality dimensions, resulting in correct and compact datasets.
author: Samah Salem and Fouzia Benchikha
categories: Computer Science (miscellaneous) (Q1)
citable_docs._(3years): 267.0
cites_/_doc._(2years): 489.0
country: Saudi Arabia
coverage: 2014-2020
doi: 10.1016/j.jksuci.2021.06.001
eigenfactor_score: .nan
h_index: 33.0
isbn: null
issn: '13191578'
issn1: '22131248'
issn2: '13191578'
issn3: '22131248'
jcr_value: null
keywords: Linked Open Data, Quality assessment, Quality improvement, Synonym predicates,
  Profiling statistics, DBpedia
publisher_x: King Saud University
publisher_y: null
ref._/_doc.: 3987.0
region: Middle East
scimago_value: 617.0
sjr_best_quartile: Q1
sourceid: 21100389724.0
title_bib: 'Lodquma: a free-ontology process for linked (open) data quality management'
title_csv: Journal of king saud university - computer and information sciences
total_cites: .nan
total_cites_(3years): 1739.0
total_docs._(2020): 314.0
total_docs._(3years): 341.0
total_refs.: 12519.0
type: journal
type_publication: article
year: 2022
---
abstract: Hyperspectral imaging technology has evolved for over thirty years and is
  widely used for geologic mapping, environmental monitoring, vegetation analysis,
  atmospheric characterization, biological and chemical detection, etc. With advances
  in technology, hyperspectral imagery not only determines the presence of materials
  and objects, but more importantly, also quantifies the variability and abundance
  of the identified materials or objects. Airborne hyperspectral imagers still perform
  a vital role in remote sensing fields due to advantages of spatial resolution, performance
  capabilities in a cloudy atmosphere, and onboard maintenance as compared to similar
  imagers aboard spaceborne platforms. To date, hundreds of airborne hyperspectral
  systems have been designed, built, and operated. Here, a review of key technologies
  for airborne hyperspectral imaging technology during past three decades is presented.
  First discussed will be high throughput imaging modes, high quality spectroscopic
  subsystems, and high sensitivity detector technology used on current airborne hyperspectral
  imagers. Particularly, the importance of data-processing such as calibration, geometric
  rectification, and atmospheric correction are discussed. Next, several new and novel
  applications are presented on the basis of state-of-the-art airborne hyperspectral
  technology. Finally, an outlook of challenges and future technology directions is
  presented along with general advice for designing and realizing novel high-performance
  airborne hyperspectral systems in this rapidly evolving field. By illustrating the
  status and prospects of typical airborne hyperspectral imagers, this overview provides
  a comparison of the technologies employed in previous hyperspectral imaging systems,
  current imaging technology research programs and prospects for innovative technology
  in future airborne hyperspectral imaging platforms.
author: Jianxin Jia and Yueming Wang and Jinsong Chen and Ran Guo and Rong Shu and
  Jianyu Wang
categories: Atomic and Molecular Physics, and Optics (Q2); Condensed Matter Physics
  (Q2); Electronic, Optical and Magnetic Materials (Q2)
citable_docs._(3years): 914.0
cites_/_doc._(2years): 292.0
country: Netherlands
coverage: 1994-2020
doi: 10.1016/j.infrared.2019.103115
eigenfactor_score: .nan
h_index: 65.0
isbn: null
issn: '13504495'
issn1: '13504495'
issn2: '13504495'
issn3: '13504495'
jcr_value: null
keywords: Airborne, Hyperspectral, Key technology, Surface reflectance, Application
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 3501.0
region: Western Europe
scimago_value: 542.0
sjr_best_quartile: Q2
sourceid: 12121.0
title_bib: 'Status and application of advanced airborne hyperspectral imaging technology:
  a review'
title_csv: Infrared physics and technology
total_cites: .nan
total_cites_(3years): 2807.0
total_docs._(2020): 391.0
total_docs._(3years): 918.0
total_refs.: 13688.0
type: journal
type_publication: article
year: 2020
---
abstract: The last decade has seen an explosion in data sources available for monitoring
  and prediction of environmental phenomena. While several inferential methods have
  been developed to make predictions on the underlying process by combining these
  data, an optimal sampling design for additional data collection in the presence
  of multiple heterogeneous sources has not yet been developed. Here, we provide an
  adaptive spatial design strategy based on a utility function that combines both
  prediction uncertainty and risk-factor criteria. Prediction uncertainty is obtained
  through a spatial data fusion approach based on fixed rank kriging that can tackle
  data with differing spatial supports and signal-to-noise ratios. We focus on the
  application of low-cost portable sensors, which tend to be relatively noisy, for
  air pollution monitoring, where data from regulatory stations as well as numeric
  modeling systems are also available. Although we find that spatial adaptive sampling
  designs can help to improve predictions and reduce prediction uncertainty, low-cost
  portable sensors are only likely to be beneficial if they are sufficient in number
  and quality. Our conclusions are based on a multi-factorial simulation experiment,
  and on a realistic simulation of pollutants in the Erie and Niagara counties in
  Western New York.
author: Eun-Hye Yoo and Andrew Zammit-Mangion and Michael G. Chipeta
categories: Atmospheric Science (Q1); Environmental Science (miscellaneous) (Q1)
citable_docs._(3years): 2034.0
cites_/_doc._(2years): 493.0
country: United Kingdom
coverage: 1968-1989, 1994-2021
doi: 10.1016/j.atmosenv.2019.117091
eigenfactor_score: 0.03998
h_index: 240.0
isbn: null
issn: '13522310'
issn1: '13522310'
issn2: '18732844'
issn3: '13522310'
jcr_value: '4.798'
keywords: Adaptive spatial sampling design, Change-of-support problem, Fixed rank
  kriging, Low-cost portable air sensors, Measurement uncertainty
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5833.0
region: Western Europe
scimago_value: 1400.0
sjr_best_quartile: Q1
sourceid: 23357.0
title_bib: Adaptive spatial sampling design for environmental field prediction using
  low-cost sensing technologies
title_csv: Atmospheric environment
total_cites: 68466.0
total_cites_(3years): 10479.0
total_docs._(2020): 609.0
total_docs._(3years): 2046.0
total_refs.: 35523.0
type: journal
type_publication: article
year: 2020
---
abstract: Competence lack, inadequate social support at work leads to the inability
  of workers since they are suffering from occupational stress. This will cause distress,
  burnout or psychosomatic difficulties, decreases in quality of life and service
  provision. Some of them may connect to work in an individual's personal life, both
  as managers, recognize stressors in their department, and respond on a departmental
  basis or individually. Many workers say that their employee utilization monitoring
  is not sufficient until computer counting involves. In addition, the systems are
  associated with higher stress, health hazards, and work unhappiness among supervised
  personnel. Monitoring these problems can increase employee awareness of personal
  productivity, providing performance information more promptly and frequently. Interventions
  are based on an examination of the variables that impact the performance of health
  workers. The article for employee stress management and health monitoring using
  information technology (SMHM-IT) gives better working conditions, motivation, retention,
  etc. Evaluation of occupational risks is a framework introduced to manage health
  and safety implications associated with preventative measures for improving and
  protecting the highest physical, social, or emotional working skills. Statistical
  data analysis is introduced to compare a medical specialty which includes analysis
  of employee's details. Results are compared with assessments shows that architecture
  offers successful in-time accessibility of performance 98.12% is achieved.
author: Ming Chen and Bin Ran and Xiaoying Gao and Guilan Yu and Jing Wang and J.
  Jagannathan
categories: Clinical Psychology (Q1); Pathology and Forensic Medicine (Q1); Psychiatry
  and Mental Health (Q1)
citable_docs._(3years): 281.0
cites_/_doc._(2years): 432.0
country: United Kingdom
coverage: 1996-2020
doi: 10.1016/j.avb.2021.101713
eigenfactor_score: 0.0069900000000000006
h_index: 102.0
isbn: null
issn: '13591789'
issn1: '13591789'
issn2: '13591789'
issn3: '13591789'
jcr_value: '4.382'
keywords: Occupational stress, Improving performance, Productivity, Stress, Health,
  Information technology
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 8092.0
region: Western Europe
scimago_value: 1586.0
sjr_best_quartile: Q1
sourceid: 29451.0
title_bib: Evaluation of occupational stress management for improving performance
  and productivity at workplaces by monitoring the health, well-being of workers
title_csv: Aggression and violent behavior
total_cites: 6869.0
total_cites_(3years): 1410.0
total_docs._(2020): 120.0
total_docs._(3years): 286.0
total_refs.: 9710.0
type: journal
type_publication: article
year: 2021
---
abstract: Recent developments in data science in general and machine learning in particular
  have transformed the way experts envision the future of surgery. Surgical Data Science
  (SDS) is a new research field that aims to improve the quality of interventional
  healthcare through the capture, organization, analysis and modeling of data. While
  an increasing number of data-driven approaches and clinical applications have been
  studied in the fields of radiological and clinical data science, translational success
  stories are still lacking in surgery. In this publication, we shed light on the
  underlying reasons and provide a roadmap for future advances in the field. Based
  on an international workshop involving leading researchers in the field of SDS,
  we review current practice, key achievements and initiatives as well as available
  standards and tools for a number of topics relevant to the field, namely (1) infrastructure
  for data acquisition, storage and access in the presence of regulatory constraints,
  (2) data annotation and sharing and (3) data analytics. We further complement this
  technical perspective with (4) a review of currently available SDS products and
  the translational progress from academia and (5) a roadmap for faster clinical translation
  and exploitation of the full potential of SDS, based on an international multi-round
  Delphi process.
author: "Lena Maier-Hein and Matthias Eisenmann and Duygu Sarikaya and Keno M\xE4\
  rz and Toby Collins and Anand Malpani and Johannes Fallert and Hubertus Feussner\
  \ and Stamatia Giannarou and Pietro Mascagni and Hirenkumar Nakawala and Adrian\
  \ Park and Carla Pugh and Danail Stoyanov and Swaroop S. Vedula and Kevin Cleary\
  \ and Gabor Fichtinger and Germain Forestier and Bernard Gibaud and Teodor Grantcharov\
  \ and Makoto Hashizume and Doreen Heckmann-N\xF6tzel and Hannes G. Kenngott and\
  \ Ron Kikinis and Lars M\xFCndermann and Nassir Navab and Sinan Onogur and Tobias\
  \ Ro\xDF and Raphael Sznitman and Russell H. Taylor and Minu D. Tizabi and Martin\
  \ Wagner and Gregory D. Hager and Thomas Neumuth and Nicolas Padoy and Justin Collins\
  \ and Ines Gockel and Jan Goedeke and Daniel A. Hashimoto and Luc Joyeux and Kyle\
  \ Lam and Daniel R. Leff and Amin Madani and Hani J. Marcus and Ozanan Meireles\
  \ and Alexander Seitel and Dogu Teber and Frank \xDCckert and Beat P. M\xFCller-Stich\
  \ and Pierre Jannin and Stefanie Speidel"
categories: Computer Graphics and Computer-Aided Design (Q1); Computer Vision and
  Pattern Recognition (Q1); Health Informatics (Q1); Radiological and Ultrasound Technology
  (Q1); Radiology, Nuclear Medicine and Imaging (Q1)
citable_docs._(3years): 411.0
cites_/_doc._(2years): 1328.0
country: Netherlands
coverage: 1996-2020
doi: 10.1016/j.media.2021.102306
eigenfactor_score: 0.018359999999999998
h_index: 135.0
isbn: null
issn: '13618415'
issn1: '13618415'
issn2: '13618423'
issn3: '13618415'
jcr_value: '8.545'
keywords: Surgical data science, Artificial intelligence, Deep learning, Computer
  aided surgery, Clinical translation
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5729.0
region: Western Europe
scimago_value: 2887.0
sjr_best_quartile: Q1
sourceid: 17271.0
title_bib: "Surgical data science \u2013 from concepts toward clinical translation"
title_csv: Medical image analysis
total_cites: 11568.0
total_cites_(3years): 7895.0
total_docs._(2020): 161.0
total_docs._(3years): 413.0
total_refs.: 9223.0
type: journal
type_publication: article
year: 2022
---
abstract: "Starting from a set of 6190 meteorological stations we are choosing 6130\
  \ of them and only for Northern Hemisphere we are computing average values for absolute\
  \ annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard\
  \ deviations for years 1800\u20132013, while we use 4887 stations and 389467 rows\
  \ of complete yearly data. The data quality and the seasonal bias indices are defined\
  \ and used in order to evaluate our dataset. After the year 1969 the data quality\
  \ is monotonically decreasing while the seasonal bias is positive in most of the\
  \ cases. An Extreme Value Distribution estimation is performed for minimum and maximum\
  \ values, giving some upper bounds for both of them and indicating a big magnitude\
  \ for temperature changes. Finally suggestions for improving the quality of meteorological\
  \ data are presented."
author: Demetris T. Christopoulos
categories: Geophysics (Q2); Atmospheric Science (Q3); Space and Planetary Science
  (Q3)
citable_docs._(3years): 620.0
cites_/_doc._(2years): 189.0
country: United Kingdom
coverage: 1997-2020
doi: 10.1016/j.jastp.2015.03.009
eigenfactor_score: 0.005229999999999999
h_index: 89.0
isbn: null
issn: '13646826'
issn1: '13646826'
issn2: '13646826'
issn3: '13646826'
jcr_value: '1.735'
keywords: Absolute temperature, Northern Hemisphere, Valid station, Data quality,
  Seasonal bias, Extreme values distribution, Missing records, Big data analysis
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4974.0
region: Western Europe
scimago_value: 515.0
sjr_best_quartile: Q2
sourceid: 28436.0
title_bib: Extraction of the global absolute temperature for northern hemisphere using
  a set of 6190 meteorological stations from 1800 to 2013
title_csv: Journal of atmospheric and solar-terrestrial physics
total_cites: 7321.0
total_cites_(3years): 1256.0
total_docs._(2020): 180.0
total_docs._(3years): 628.0
total_refs.: 8954.0
type: journal
type_publication: article
year: 2015
---
abstract: "Seismic tomography has arrived at the threshold of the era of big data.\
  \ However, how to extract information optimally from every available time-series\
  \ remains a challenge; one that is directly related to the objective function chosen\
  \ as a distance metric between observed and synthetic data. Time-domain cross-correlation\
  \ and frequency-dependent multitaper traveltime measurements are generally tied\
  \ to window selection algorithms in order to balance the amplitude differences between\
  \ seismic phases. Even then, such measurements naturally favour the dominant signals\
  \ within the chosen windows. Hence, it is difficult to select all usable portions\
  \ of seismograms with any sort of optimality. As a consequence, information ends\
  \ up being lost, in particular from scattered waves. In contrast, measurements based\
  \ on instantaneous phase allow extracting information uniformly over the seismic\
  \ records without requiring their segmentation. And yet, measuring instantaneous\
  \ phase, like any other phase measurement, is impeded by phase wrapping. In this\
  \ paper, we address this limitation by using a complex-valued phase representation\
  \ that we call \u2018exponentiated phase\u2019. We demonstrate that the exponentiated\
  \ phase is a good substitute for instantaneous-phase measurements. To assimilate\
  \ as much information as possible from every seismogram while tackling the non-linearity\
  \ of inversion problems, we discuss a flexible hybrid approach to combine various\
  \ objective functions in adjoint seismic tomography. We focus on those based on\
  \ the exponentiated phase, to take into account relatively small-magnitude scattered\
  \ waves; on multitaper measurements of selected surface waves; and on cross-correlation\
  \ measurements on specific windows to select distinct body-wave arrivals. Guided\
  \ by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation\
  \ measurements, and their hybridization, affect tomographic results. Despite their\
  \ use of multiple measurements, the computational cost to evaluate gradient kernels\
  \ for the objective functions is scarcely affected, allowing for issues with data\
  \ quality and measurement challenges to be simultaneously addressed efficiently."
author: "Yuan, Yanhua O and Bozda\u011F, Ebru and Ciardelli, Caio and Gao, Fuchun\
  \ and Simons, F J"
categories: Geochemistry and Petrology (Q1); Geophysics (Q1)
citable_docs._(3years): 1517.0
cites_/_doc._(2years): 277.0
country: United Kingdom
coverage: 1922-1943, 1945, 1947-1957, 1988-2020
doi: 10.1093/gji/ggaa063
eigenfactor_score: 0.03134
h_index: 168.0
isbn: null
issn: 1365246X
issn1: 0956540X
issn2: 1365246X
issn3: 0956540X
jcr_value: '2.934'
keywords: Inverse theory;Time-series analysis;Seismic tomography
publisher_x: Oxford University Press
publisher_y: null
ref._/_doc.: 5695.0
region: Western Europe
scimago_value: 1302.0
sjr_best_quartile: Q1
sourceid: 27947.0
title_bib: The exponentiated phase measurement, and objective-function hybridization
  for adjoint waveform tomography
title_csv: Geophysical journal international
total_cites: 32388.0
total_cites_(3years): 4356.0
total_docs._(2020): 531.0
total_docs._(3years): 1529.0
total_refs.: 30241.0
type: journal
type_publication: article
year: 2019
---
abstract: Data science and analytics are attracting more and more attention from researchers
  and practitioners in recent years. Due to the rapid development of advanced technologies
  nowadays, a massive amount of real time data regarding flight information, flight
  performance, airport conditions, air traffic conditions, weather, ticket prices,
  passengers comments, crew comments, etc., are all available from a diverse set of
  sources, including flight performance monitoring systems, operational systems of
  airlines and airports, and social media platforms. Development of data analytics
  in aviation and related applications is also growing rapidly. This paper concisely
  examines data science and analytics in aviation studies in several critical areas,
  namely big data analysis, air transport network management, forecasting, and machine
  learning. The papers featured in this special issue are also introduced and reviewed,
  and future directions for data science and analytics in aviation are discussed.
author: Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi
categories: Business and International Management (Q1); Civil and Structural Engineering
  (Q1); Management Science and Operations Research (Q1); Transportation (Q1)
citable_docs._(3years): 499.0
cites_/_doc._(2years): 763.0
country: United Kingdom
coverage: 1997-2020
doi: 10.1016/j.tre.2020.101837
eigenfactor_score: .nan
h_index: 110.0
isbn: null
issn: '13665545'
issn1: '13665545'
issn2: '13665545'
issn3: '13665545'
jcr_value: null
keywords: Data science, Aviation, Analytics, Flight, Air logistics
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5633.0
region: Western Europe
scimago_value: 2042.0
sjr_best_quartile: Q1
sourceid: 20909.0
title_bib: Data science and analytics in aviation
title_csv: 'Transportation research, part e: logistics and transportation review'
total_cites: .nan
total_cites_(3years): 4018.0
total_docs._(2020): 233.0
total_docs._(3years): 507.0
total_refs.: 13126.0
type: journal
type_publication: article
year: 2020
---
abstract: Wireless Rechargeable Sensor Network (WRSN) is largely used in monitoring
  of environment and traffic, video surveillance and medical care, etc., and helps
  to improve the quality of urban life. However, it is challenging to provide the
  sustainable energy for sensors deployed in buildings, soil or other places, where
  it is hard to harvest the energy from environment. To address this issue, we design
  a new wireless charging system, which levers the bus network assisted drone in urban
  areas. We formulate the drone scheduling problem based on this new wireless charging
  system to minimize the total time cost of drone subject to all sensors can be charged
  under the energy constraint of drone. Then, we propose an approximation algorithm
  DSA for the energy tightened drone scheduling problem. To make the tasks of WRSN
  sustainable, we further formulate the drone scheduling problem with deadlines of
  sensors, and present the approximation algorithm DDSA to find the drone schedule
  with the maximal number of sensors charged by the drone before deadlines. Through
  the extensive simulations, we demonstrate that DSA can reduce the total time cost
  by 84.83% compared with Greedy Replenished Energy algorithm, and uses at most 5.98
  times of the total time cost of optimal solution on average. Then, we also demonstrate
  that DDSA can increase the survival rate of sensors by 51.95% compared with Deadline
  Greedy Replenished Energy algorithm, and can obtain 77.54% survival rate of optimal
  solution on average.
author: Yong Jin and Jia Xu and Sixu Wu and Lijie Xu and Dejun Yang and Kaijian Xia
categories: Hardware and Architecture (Q1); Software (Q2)
citable_docs._(3years): 262.0
cites_/_doc._(2years): 497.0
country: Netherlands
coverage: 1996-2020
doi: 10.1016/j.sysarc.2021.102059
eigenfactor_score: 0.00137
h_index: 51.0
isbn: null
issn: '13837621'
issn1: '13837621'
issn2: '13837621'
issn3: '13837621'
jcr_value: '3.777'
keywords: Wireless rechargeable sensor network, Bus network, Drone scheduling, Traveling
  salesman path problem, Submodular orienteering problem
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4339.0
region: Western Europe
scimago_value: 598.0
sjr_best_quartile: Q1
sourceid: 12398.0
title_bib: Bus network assisted drone scheduling for sustainable charging of wireless
  rechargeable sensor network
title_csv: Journal of systems architecture
total_cites: 1570.0
total_cites_(3years): 1137.0
total_docs._(2020): 111.0
total_docs._(3years): 272.0
total_refs.: 4816.0
type: journal
type_publication: article
year: 2021
---
abstract: "Across a wide range of disciplines, mounting evidence points to solutions\
  \ for addressing the global biodiversity and climate crisis through sustainable\
  \ land use development. Managing ecosystem services offers promising potential of\
  \ combining environmental, economic, and social interests in this process. Achieving\
  \ sustainability, however, requires collaboration across disciplines, or in short\
  \ \u201Ccross-disciplinary\u201D approaches. Multi-, inter- and transdisciplinary\
  \ approaches are often used as synonyms, although they are defined by different\
  \ levels of integrating results and perspectives. We highlight challenges and opportunities\
  \ related to these cross-disciplinary approaches by using research on bird- and\
  \ bat-mediated ecosystem services as a case - with a focus on sustainable agricultural\
  \ development. Examples from transdisciplinary collaborations show how more integrative\
  \ and inclusive approaches promote the implementation of basic and applied ecological\
  \ research into land use practices. Realizing this opportunity requires strong partnerships\
  \ between science, practice and policy, as well as integration of diverse skills\
  \ and perspectives. If appropriately funded and guided, this effort is rewarded\
  \ by improved data quality, more targeted concepts, as well as improvement implementation\
  \ and impact of sustainability research and practice. We outline a stepwise approach\
  \ for developing these processes and highlight case studies from bird and bat research\
  \ to inspire cross-disciplinary approaches within and beyond ecology."
author: Bea Maas and Carolina Ocampo-Ariza and Christopher J. Whelan
categories: Ecology, Evolution, Behavior and Systematics (Q1)
citable_docs._(3years): 190.0
cites_/_doc._(2years): 310.0
country: Germany
coverage: 2000-2020
doi: 10.1016/j.baae.2021.06.010
eigenfactor_score: 0.0035
h_index: 79.0
isbn: null
issn: '14391791'
issn1: '16180089'
issn2: '14391791'
issn3: '16180089'
jcr_value: '3.414'
keywords: Agricultural biodiversity, Collaborative conservation, Ecosystem functions,
  Ecosystem services, Knowledge co-production, Sustainable agriculture, Transdisciplinary
  research
publisher_x: Urban und Fischer Verlag Jena
publisher_y: null
ref._/_doc.: 6126.0
region: Western Europe
scimago_value: 1372.0
sjr_best_quartile: Q1
sourceid: 22800.0
title_bib: 'Cross-disciplinary approaches for better research: the case of birds and
  bats'
title_csv: Basic and applied ecology
total_cites: 4102.0
total_cites_(3years): 725.0
total_docs._(2020): 65.0
total_docs._(3years): 195.0
total_refs.: 3982.0
type: journal
type_publication: article
year: 2021
---
abstract: Cardiovascular diseases (CVD) are leading causes of death and morbidity
  in Australia and worldwide. Despite improvements in treatment, there remain large
  gaps in our understanding to prevent, treat and manage CVD events and associated
  morbidities. This article lays out a vision for enhancing CVD research in Australia
  through the development of a Big Data system, bringing together the multitude of
  rich administrative and health datasets available. The article describes the different
  types of Big Data available for CVD research in Australia and presents an overview
  of the potential benefits of a Big Data system for CVD research and some of the
  major challenges in establishing the system for Australia. The steps for progressing
  this vision are outlined.
author: Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping
  Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and
  Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree
categories: Cardiology and Cardiovascular Medicine (Q2); Pulmonary and Respiratory
  Medicine (Q2)
citable_docs._(3years): 618.0
cites_/_doc._(2years): 193.0
country: United Kingdom
coverage: 2000-2020
doi: 10.1016/j.hlc.2021.04.023
eigenfactor_score: 0.0073
h_index: 46.0
isbn: null
issn: '14439506'
issn1: '14442892'
issn2: '14439506'
issn3: '14442892'
jcr_value: '2.975'
keywords: Big data, Datasets, Cardiovascular disease, National platform
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 3057.0
region: Western Europe
scimago_value: 770.0
sjr_best_quartile: Q2
sourceid: 23235.0
title_bib: 'A versatile big data health system for australia: driving improvements
  in cardiovascular health'
title_csv: Heart lung and circulation
total_cites: 4050.0
total_cites_(3years): 1363.0
total_docs._(2020): 327.0
total_docs._(3years): 744.0
total_refs.: 9995.0
type: journal
type_publication: article
year: 2021
---
abstract: 'AI and big data technologies have been increasingly deployed to process
  complex, heterogeneous, high-resolution environmental data, and generate results
  at greater speeds and higher accuracies to facilitate environmental decision-making.
  However, current attempts to develop reliable AI and big data technologies for environmental
  decision-making are still inadequate. In this special issue, AI for Social Good:
  AI and Big Data Approaches for Environmental Decision-Making, we attempt to address
  the following important questions: What are the conditions for AI and big data technologies
  to facilitate environmental decision-making? How can AI and big data be used to
  facilitate environmental decision-making? Do AI and big data serve those most at
  risk of environmental pollution? Who should own and govern AI and big data? This
  special issue brings together researchers in relevant fields of AI and environmental
  science to address these pertinent questions. First, we will review the existing
  works which attempt to address these four questions. Second, we summarize the significance
  and novelty of six articles included in our special issue in addressing these four
  questions. Finally, we highlight the important principles of AI for Social Good,
  which can help distinguish good from bad environmental decisions based on AI and
  big data technologies.'
author: Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui
categories: Geography, Planning and Development (Q1); Management, Monitoring, Policy
  and Law (Q1)
citable_docs._(3years): 624.0
cites_/_doc._(2years): 552.0
country: Netherlands
coverage: 1998-2020
doi: 10.1016/j.envsci.2021.09.001
eigenfactor_score: .nan
h_index: 115.0
isbn: null
issn: '14629011'
issn1: '18736416'
issn2: '14629011'
issn3: '18736416'
jcr_value: null
keywords: null
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6395.0
region: Western Europe
scimago_value: 1716.0
sjr_best_quartile: Q1
sourceid: 21536.0
title_bib: 'Ai for social good: ai and big data approaches for environmental decision-making'
title_csv: Environmental science and policy
total_cites: .nan
total_cites_(3years): 3655.0
total_docs._(2020): 262.0
total_docs._(3years): 635.0
total_refs.: 16755.0
type: journal
type_publication: article
year: 2021
---
abstract: 'Summary

  Diarrhoea is an important cause of morbidity and mortality in children from low-income
  and middle-income countries (LMICs), despite advances in the management of this
  condition. Understanding of the causes of diarrhoea in children in LMICs has advanced
  owing to large multinational studies and big data analytics computing the disease
  burden, identifying the important variables that have contributed to reducing this
  burden. The advent of the mobile phone has further enabled the management of childhood
  diarrhoea by providing both clinical support to health-care workers (such as diagnosis
  and management) and communicating preventive measures to carers (such as breastfeeding
  and vaccination reminders) in some settings. There are still challenges in addressing
  the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented
  geographical areas, concerns about patient confidentiality, unequal partnerships
  between study investigators, and the reactive approach to outbreaks. A transparent
  approach to promote the inclusion of researchers in LMICs could address partnership
  imbalances. A big data umbrella encompassing cloud-based centralised databases to
  analyse interlinked human, animal, agricultural, social, and climate data would
  provide an informative solution to the development of appropriate management protocols
  in LMICs.'
author: Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and
  Farah Naz Qamar and Zoya Haq and Iruka N Okeke
categories: Infectious Diseases (Q1)
citable_docs._(3years): 512.0
cites_/_doc._(2years): 660.0
country: United Kingdom
coverage: 2001-2020
doi: 10.1016/S1473-3099(21)00585-5
eigenfactor_score: .nan
h_index: 235.0
isbn: null
issn: '14733099'
issn1: '14744457'
issn2: '14733099'
issn3: '14744457'
jcr_value: null
keywords: null
publisher_x: Lancet Publishing Group
publisher_y: null
ref._/_doc.: 1427.0
region: Western Europe
scimago_value: 7475.0
sjr_best_quartile: Q1
sourceid: 22471.0
title_bib: 'Using big data and mobile health to manage diarrhoeal disease in children
  in low-income and middle-income countries: societal barriers and ethical implications'
title_csv: Lancet infectious diseases, the
total_cites: .nan
total_cites_(3years): 9339.0
total_docs._(2020): 533.0
total_docs._(3years): 1381.0
total_refs.: 7607.0
type: journal
type_publication: article
year: 2022
---
abstract: Rapid urbanization, population increase, emerging contaminants and increasing
  water scarcity have put a major constraint on the wastewater treatment system. Scarcity
  of water is steering current way of water recycle, and the drive focus towards resource
  recovery. Zero waste pathway in circular bioeconomy can bring transformation of
  wastewater commercialization by adding value with resource recovery. The complex
  biological reactions, unforeseen microbial behaviours, lack of reliable on-line
  instrumentation, complex modelling, lack of visualize techniques, low-quality industrial
  measurements and highly time-varying intensive data-driven operations call for the
  intelligence techniques and operations. The study is a review of sustainable circularity
  and intelligent data-driven operations and control of the wastewater treatment plant.
  Water surveillance and monitoring, circular economy and sustainability, automation
  pyramid, digital transformation, artificial intelligence, data pipeline, digital
  twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health
  management were reviewed. The deployment of the digital systems has evidently proven
  to bridges the gap between the data-driven soft sensor, operation and control systems
  in WWTP. Accurate prediction of the WWTP variables can support process design and
  control, reduce operation cost, improve system reliability, predictive maintenance
  and troubleshooting, increase water quality, increase stakeholder's engagement and
  endorse optimization of the plant performance. This procures the best compliance
  with international standards and diversification. The inclusion of life cycle environmental
  or cost management technologies in optimization models is an interesting pathway
  towards sustainable water treatment in-line with sustainable development goals,
  circular bioeconomy and industry 4.0.
author: Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda
  and Jane Catherine Ngila
categories: Geochemistry and Petrology (Q2); Geophysics (Q2)
citable_docs._(3years): 293.0
cites_/_doc._(2years): 287.0
country: United Kingdom
coverage: 1982, 1991-1992, 1995, 2002-2020
doi: 10.1016/j.pce.2022.103152
eigenfactor_score: 0.00279
h_index: 82.0
isbn: null
issn: '14747065'
issn1: '14747065'
issn2: '14747065'
issn3: '14747065'
jcr_value: '2.712'
keywords: Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor,
  Wastewater treatment
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4539.0
region: Western Europe
scimago_value: 724.0
sjr_best_quartile: Q2
sourceid: 23315.0
title_bib: Sustainable circularity and intelligent data-driven operations and control
  of the wastewater treatment plant
title_csv: Physics and chemistry of the earth
total_cites: 5794.0
total_cites_(3years): 923.0
total_docs._(2020): 111.0
total_docs._(3years): 303.0
total_refs.: 5038.0
type: journal
type_publication: article
year: 2022
---
abstract: The main problem of ecological data modeling is their interpretation and
  its correct understanding. This problem cannot be solved solely by a big data collection.
  To sufficiently understand ecosystems we need to know how these processes behave
  and how they respond to internal and external factors. Similarly, we need to know
  the behavior of processes that are involved in the climate system and the biosphere
  of the earth. In order to characterize precisely the behavior of individual elements
  and ecosystems we need to use deterministic, stochastic and chaotic behavior. Unfortunately,
  the chaotic part of systems is typically completely ignored in almost all approaches.
  Ignoring of chaotical part leads to many biased outcomes. To overcome this gap we
  model chaotic system behavior by random iterated function system which provides
  a generic guideline for such data management. This also allows to replicate a complexity
  and chaos of ecosystem.
author: "M. Stehl\xEDk and J. Du\u0161ek and J. Kise\u013E\xE1k"
categories: Ecology, Evolution, Behavior and Systematics (Q2); Ecological Modeling
  (Q3)
citable_docs._(3years): 172.0
cites_/_doc._(2years): 178.0
country: Netherlands
coverage: 2004-2020
doi: 10.1016/j.ecocom.2015.12.003
eigenfactor_score: 0.0017399999999999998
h_index: 57.0
isbn: null
issn: 1476945X
issn1: 1476945X
issn2: 1476945X
issn3: 1476945X
jcr_value: '1.882'
keywords: "Stochasticity, Determinism, Entropy, Chaos, Wetland ecosystem, Kullback\u2013\
  Leibler (KL) divergence"
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5435.0
region: Western Europe
scimago_value: 537.0
sjr_best_quartile: Q2
sourceid: 20289.0
title_bib: Missing chaos in global climate change data interpreting?
title_csv: Ecological complexity
total_cites: 2150.0
total_cites_(3years): 395.0
total_docs._(2020): 48.0
total_docs._(3years): 180.0
total_refs.: 2609.0
type: journal
type_publication: article
year: 2016
---
abstract: "Robotic Process Automation (RPA) has received growing attention within\
  \ the digital transformation as this cutting-edge technology automates human behavior\
  \ and promises high potentials. However, the adoption in purchasing and supply management\
  \ (PSM) is still in its infancy and has hardly been explored, particularly in the\
  \ public sector. Based on a multiple case study including 19 organizations of the\
  \ public and private sector, this paper narrows that gap and presents comprehensive\
  \ insights into potentials, barriers, suitable processes, and best practices and\
  \ components for RPA implementation. The findings indicate that adoption depends\
  \ on the organizations\u2019 digital procurement readiness and maturity. Application\
  \ areas of RPA enlarge with increasing experience and range from transactional and\
  \ operative tasks within the procure-to-pay process to more strategic use cases\
  \ in sourcing and supply relationship management. Potentials mainly comprise employee\
  \ reliefs, cost savings, and increased operational efficiency and quality. We uncover\
  \ multiple technical, organizational, and environmental barriers related to IT infrastructure\
  \ and human resources, internal communication, financial resources, top management\
  \ support, organizational structures, supplier-related issues, and government regulations.\
  \ Furthermore, our study indicates several differences between the private and public\
  \ sectors for RPA implementation. We outline implications for the emerging research\
  \ on RPA and pivotal directions for organizational practice."
author: Christian Flechsig and Franziska Anslinger and Rainer Lasch
categories: Marketing (Q1); Strategy and Management (Q1)
citable_docs._(3years): 101.0
cites_/_doc._(2years): 576.0
country: United Kingdom
coverage: 2003-2020
doi: 10.1016/j.pursup.2021.100718
eigenfactor_score: 0.0016
h_index: 85.0
isbn: null
issn: '14784092'
issn1: '14784092'
issn2: '14784092'
issn3: '14784092'
jcr_value: '5.500'
keywords: Robotic process automation, Digital procurement, Implementation, Barriers,
  Digital readiness, Public sector
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 8779.0
region: Western Europe
scimago_value: 1708.0
sjr_best_quartile: Q1
sourceid: 22972.0
title_bib: 'Robotic process automation in purchasing and supply management: a multiple
  case study on potentials, barriers, and implementation'
title_csv: Journal of purchasing and supply management
total_cites: 2586.0
total_cites_(3years): 639.0
total_docs._(2020): 34.0
total_docs._(3years): 109.0
total_refs.: 2985.0
type: journal
type_publication: article
year: 2022
---
abstract: Recent rapid development of Internet-based computer technologies has made
  possible many novel applications in radiation dose delivery. However, translational
  speed of applying these new technologies in radiotherapy could hardly catch up due
  to the complex commissioning process and quality assurance protocol. Implementing
  novel Internet-based technology in radiotherapy requires corresponding design of
  algorithm and infrastructure of the application, set up of related clinical policies,
  purchase and development of software and hardware, computer programming and debugging,
  and national to international collaboration. Although such implementation processes
  are time consuming, some recent computer advancements in the radiation dose delivery
  are still noticeable. In this review, we will present the background and concept
  of some recent Internet-based computer technologies such as cloud computing, big
  data processing and machine learning, followed by their potential applications in
  radiotherapy, such as treatment planning and dose delivery. We will also discuss
  the current progress of these applications and their impacts on radiotherapy. We
  will explore and evaluate the expected benefits and challenges in implementation
  as well.
author: James C.L. Chow
categories: Oncology (Q3); Radiology, Nuclear Medicine and Imaging (Q3); Cancer Research
  (Q4)
citable_docs._(3years): 248.0
cites_/_doc._(2years): 131.0
country: Poland
coverage: 1998-2020
doi: 10.1016/j.rpor.2017.08.005
eigenfactor_score: .nan
h_index: 23.0
isbn: null
issn: '15071367'
issn1: '15071367'
issn2: '15071367'
issn3: '15071367'
jcr_value: null
keywords: Radiotherapy, Computer technology, Cloud computing, Machine learning, Big
  data
publisher_x: Elsevier Sp. z o.o.
publisher_y: null
ref._/_doc.: 3138.0
region: Eastern Europe
scimago_value: 367.0
sjr_best_quartile: Q3
sourceid: 54509.0
title_bib: Internet-based computer technology on radiotherapy
title_csv: Reports of practical oncology and radiotherapy
total_cites: .nan
total_cites_(3years): 338.0
total_docs._(2020): 166.0
total_docs._(3years): 252.0
total_refs.: 5209.0
type: journal
type_publication: article
year: 2017
---
abstract: "Background: The use of electronic health record (EHR) systems encourages\
  \ and facilitates the use of data for the development and surveillance of quality\
  \ indicators, including pain management. Aim: to conduct an integrative review on\
  \ pain management research using data extracted from EHR in order to synthesize\
  \ and analyze the following elements: pain management (assessments, interventions,\
  \ and outcomes) and study results with potential clinical implications, data source,\
  \ clinical sample characteristics, and method description. Design: An integrative\
  \ review of the literature was undertaken to identify exemplars of scientific research\
  \ studies that explore pain management using data from EHR, using Cooper\u2019s\
  \ framework. Results: Our search of 1,061 records from PubMed, Scopus, and Cinahl\
  \ was narrowed down to 28 eligible articles to be analyzed. Conclusion: Results\
  \ of this integrative review will make a critical contribution, assisting others\
  \ in developing research proposals and sound research methods, as well as providing\
  \ an overview of such studies over the past 10 years. Through this review it is\
  \ therefore possible to guide new research on clinical pain management using EHR."
author: Aline Tsuma Gaedke Nomura and Lisiane Pruinelli and Luciana Nabinger Menna
  Barreto and Murilo dos Santos Graeff and Elizabeth A. Swanson and Thamiris Silveira
  and Miriam de Abreu Almeida
categories: Advanced and Specialized Nursing (Q1)
citable_docs._(3years): 197.0
cites_/_doc._(2years): 181.0
country: United Kingdom
coverage: 2000-2020
doi: 10.1016/j.pmn.2021.01.016
eigenfactor_score: 0.0017699999999999999
h_index: 48.0
isbn: null
issn: '15249042'
issn1: '15249042'
issn2: '15328635'
issn3: '15249042'
jcr_value: '1.929'
keywords: null
publisher_x: W.B. Saunders Ltd
publisher_y: null
ref._/_doc.: 3895.0
region: Western Europe
scimago_value: 557.0
sjr_best_quartile: Q1
sourceid: 21937.0
title_bib: Pain management in clinical practice research using electronic health records
title_csv: Pain management nursing
total_cites: 1835.0
total_cites_(3years): 399.0
total_docs._(2020): 117.0
total_docs._(3years): 216.0
total_refs.: 4557.0
type: journal
type_publication: article
year: 2021
---
abstract: Predicting the popularity of web contents in online social networks is essential
  for many applications. However, existing works are usually under non-incremental
  settings. In other words, they have to rebuild models from scratch when new data
  occurs, which are inefficient in big data environments. It leads to an urgent need
  for incremental prediction, which can update previous results with new data and
  conduct prediction incrementally. Moreover, the promising direction of group-level
  popularity prediction has not been well treated, which explores fine-grained information
  while keeping a low cost. To this end, we identify the problem of incremental group-level
  popularity prediction, and propose a novel model IGPP to address it. We first predict
  the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC
  (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental
  prediction, we propose three strategies to restart the CP decomposition. To the
  best of our knowledge, this is the first work that identifies and solves the problem
  of incremental group-level popularity prediction. Extensive experimental results
  show significant improvements of the IGPP method over other works both in the prediction
  accuracy and the efficiency.
author: Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin
categories: Computer Networks and Communications (Q1)
citable_docs._(3years): 140.0
cites_/_doc._(2years): 414.0
country: United States
coverage: 2001-2020
doi: 10.1145/3461839
eigenfactor_score: 0.00144
h_index: 56.0
isbn: null
issn: '15335399'
issn1: '15576051'
issn2: '15335399'
issn3: '15576051'
jcr_value: '3.135'
keywords: information diffusion, tensor analysis, popularity prediction, Group level,
  online social networks, incremental approach
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 5044.0
region: Northern America
scimago_value: 667.0
sjr_best_quartile: Q1
sourceid: 15773.0
title_bib: Incremental group-level popularity prediction in online social networks
title_csv: Acm transactions on internet technology
total_cites: 940.0
total_cites_(3years): 612.0
total_docs._(2020): 45.0
total_docs._(3years): 149.0
total_refs.: 2270.0
type: journal
type_publication: article
year: 2021
---
abstract: "Long-term monitoring of animal activity can yield key information for both\
  \ researchers in ethology and engineers in charge of developing precision livestock\
  \ farming tools. First, a barn is segmented into delimited areas (e.g. cubicles)\
  \ with which an activity can be associated (e.g. resting), then a real-time location\
  \ system (RTLS) can be used to automatically convert cow position into behaviour.\
  \ Working within the EU-PLF project, we tested a system already able to determine\
  \ basic activities (resting, moving, eating\u2026) and logged a \u201Cbig data\u201D\
  \ set of billions of data points (123 days\_\xD7\_190 cows\_\xD7\_1 location-per-second\
  \ readings). We then focused on integrating image analysis techniques to help visualise\
  \ and analyse the dataset, first to validate the data and then to enrich the information\
  \ extracted. The algorithm developed using freely available tools quickly confirmed\
  \ the ability of the system to determine cows' main activities (except drinking\
  \ behaviour), even with 11% of positions missing. The good localisation precision\
  \ (16\_cm) made it possible to enrich the time-budget with new activities such as\
  \ using brushes and licking mineral blocks. For both activities, using visual observations\
  \ as gold standard, activity profiles with excellent sensitivity (nearly 80%) were\
  \ extracted. This validation procedure is both necessary and generalisable to other\
  \ situations. The improvement of biological information contained in such data holds\
  \ promise for people designing alarm devices and health and welfare indicators for\
  \ farmers and/or vets."
author: "Bruno Meunier and Philippe Pradel and Karen H. Sloth and Carole Ciri\xE9\
  \ and Eric Delval and Marie M. Mialon and Isabelle Veissier"
categories: Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Control
  and Systems Engineering (Q1); Food Science (Q1); Soil Science (Q1)
citable_docs._(3years): 581.0
cites_/_doc._(2years): 507.0
country: United States
coverage: 2002-2020
doi: 10.1016/j.biosystemseng.2017.08.019
eigenfactor_score: 0.007370000000000001
h_index: 110.0
isbn: null
issn: '15375110'
issn1: '15375129'
issn2: '15375110'
issn3: '15375129'
jcr_value: '4.123'
keywords: RTLS, Image analysis, Dairy cows, Behaviour, Precision livestock farming,
  Time-budget
publisher_x: Academic Press Inc.
publisher_y: null
ref._/_doc.: 4150.0
region: Northern America
scimago_value: 894.0
sjr_best_quartile: Q1
sourceid: 61490.0
title_bib: Image analysis to refine measurements of dairy cow behaviour from a real-time
  location system
title_csv: Biosystems engineering
total_cites: 9924.0
total_cites_(3years): 3137.0
total_docs._(2020): 236.0
total_docs._(3years): 588.0
total_refs.: 9794.0
type: journal
type_publication: article
year: 2018
---
abstract: Summary form only given. Instrumentation of processes and an organization's
  environment provides vast amounts of data that can be used to drivedecisions. Next
  to setting up data collection, supervising data quality, and applying proper methods
  of analysis, organizationsface the challenge to set up an infrastructure and architecture
  to do so efficiently and cost-effectively. Virtualized platformssuch as private
  or public clouds are the method of choice for deployment, in particular for data
  analyses not occurringconstantly. A cloud provider, either a commercial Cloud company
  or an IT organization within an enterprise, will like to set upa cloud platform
  such that clients can run big data workloads effectively on. Cloud customers would
  like to set up big dataapplications in a cost-effective and performant way on their
  platform.This keynote will walk through a few real life big data analysis scenarios
  from different industries and discuss thechallenges Cloud providers face making
  trade-offs. Understanding those challenges and solutions help cloud users choose
  theright match between their algorithm, big data system and cloud platform.
author: Ludwig, Heiko
categories: Computer Networks and Communications; Computer Science Applications; Hardware
  and Architecture; Software; Theoretical Computer Science
citable_docs._(3years): 86.0
cites_/_doc._(2years): 77.0
country: United States
coverage: 1998, 2014, 2019
doi: 10.1109/EDOC.2014.21
eigenfactor_score: .nan
h_index: 16.0
isbn: null
issn: '15417719'
issn1: '15417719'
issn2: '15417719'
issn3: '15417719'
jcr_value: null
keywords: Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments
publisher_x: null
publisher_y: null
ref._/_doc.: 2183.0
region: Northern America
scimago_value: 232.0
sjr_best_quartile: '-'
sourceid: 21100384313.0
title_bib: Managing big data effectively - a cloud provider and a cloud consumer perspective
title_csv: Proceedings - ieee international enterprise distributed object computing
  workshop, edocw
total_cites: .nan
total_cites_(3years): 87.0
total_docs._(2020): 18.0
total_docs._(3years): 95.0
total_refs.: 393.0
type: conference and proceedings
type_publication: inproceedings
year: 2014
---
abstract: Disease prediction has the potential to benefit stakeholders such as the
  government and health insurance companies. It can identify patients at risk of disease
  or health conditions. Clinicians can then take appropriate measures to avoid or
  minimize the risk and in turn, improve quality of care and avoid potential hospital
  admissions. Due to the recent advancement of tools and techniques for data analytics,
  disease risk prediction can leverage large amounts of semantic information, such
  as demographics, clinical diagnosis and measurements, health behaviours, laboratory
  results, prescriptions and care utilisation. In this regard, electronic health data
  can be a potential choice for developing disease prediction models. A significant
  number of such disease prediction models have been proposed in the literature over
  time utilizing large-scale electronic health databases, different methods, and healthcare
  variables. The goal of this comprehensive literature review was to discuss different
  risk prediction models that have been proposed based on electronic health data.
  Search terms were designed to find relevant research articles that utilized electronic
  health data to predict disease risks. Online scholarly databases were searched to
  retrieve results, which were then reviewed and compared in terms of the method used,
  disease type, and prediction accuracy. This paper provides a comprehensive review
  of the use of electronic health data for risk prediction models. A comparison of
  the results from different techniques for three frequently modelled diseases using
  electronic health data was also discussed in this study. In addition, the advantages
  and disadvantages of different risk prediction models, as well as their performance,
  were presented. Electronic health data have been widely used for disease prediction.
  A few modelling approaches show very high accuracy in predicting different diseases
  using such data. These modelling approaches have been used to inform the clinical
  decision process to achieve better outcomes.
author: Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat
categories: Applied Mathematics (Q2); Biotechnology (Q2); Genetics (Q3)
citable_docs._(3years): 534.0
cites_/_doc._(2years): 303.0
country: United States
coverage: 2004-2020
doi: 10.1109/TCBB.2019.2937862
eigenfactor_score: .nan
h_index: 71.0
isbn: null
issn: '15455963'
issn1: '15579964'
issn2: '15455963'
issn3: '15579964'
jcr_value: null
keywords: null
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: IEEE Computer Society Press
ref._/_doc.: 2919.0
region: Northern America
scimago_value: 745.0
sjr_best_quartile: Q2
sourceid: 17971.0
title_bib: 'Use of electronic health data for disease prediction: a comprehensive
  literature review'
title_csv: Ieee/acm transactions on computational biology and bioinformatics
total_cites: .nan
total_cites_(3years): 1935.0
total_docs._(2020): 273.0
total_docs._(3years): 572.0
total_refs.: 7970.0
type: journal
type_publication: article
year: 2021
---
abstract: 'Taxi service is one of the most important modes for urban transportation.
  In recent years, many taxi companies have been routinely collecting data to track
  the movement of each taxi for improving security, coordination, and service performance.
  This paper is intended to use the GPS vehicle positioning data to assess the route
  choice behavior of taxi drivers and explore if the routes selected by taxi drivers
  can be incorporated into a traveler information system. It is often perceived that
  taxi drivers have the ability to select quality routes assuming that: (1) they tend
  to be more knowledgeable about alternative routes and time-dependent traffic conditions
  than general public, including some publicly available route guidance systems due
  to the nature of their profession; and (2) they are typically more motivated to
  incorporate their knowledge about traffic conditions into their route choice decisions.
  An experimental study is conducted to examine the validity of these two assumptions.
  We have developed a framework that can effectively process the data into information
  about routes selected by taxi drivers and their associated travel times. The performance
  of the routes selected by taxi drivers is compared with the performance of those
  recommended by e-maps. Our results indicate that the routes selected by taxi drivers
  are generally more efficient than the routes recommended by some major e-maps, suggesting
  that taxi drivers are more active in selecting routes to avoid congestion.'
author: Zheng Wang and Wei-Hua Lin and Wangtu Xu
categories: Aerospace Engineering (Q1); Applied Mathematics (Q1); Automotive Engineering
  (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1);
  Information Systems (Q1); Software (Q1)
citable_docs._(3years): 129.0
cites_/_doc._(2years): 438.0
country: United Kingdom
coverage: 2004-2020
doi: 10.1080/15472450.2019.1617142
eigenfactor_score: 0.0016899999999999999
h_index: 47.0
isbn: null
issn: '15472450'
issn1: '15472450'
issn2: '15472450'
issn3: '15472450'
jcr_value: '4.277'
keywords: Data driven approach, route choice, taxi service, travel time estimation
publisher_x: Taylor and Francis Ltd.
publisher_y: null
ref._/_doc.: 4347.0
region: Western Europe
scimago_value: 1321.0
sjr_best_quartile: Q1
sourceid: 144889.0
title_bib: A data driven approach to assessing the reliability of using taxicab as
  probes for real-time route selections
title_csv: Journal of intelligent transportation systems
total_cites: 1524.0
total_cites_(3years): 632.0
total_docs._(2020): 77.0
total_docs._(3years): 133.0
total_refs.: 3347.0
type: journal
type_publication: article
year: 2021
---
abstract: "Summary\nIn this Minireview, we provide an epidemiologist\u2019s perspective\
  \ on the debate and recent advances in determining the relationship between diet\
  \ and cardiovascular health. We conclude that, in order to reduce the global burden\
  \ of cardiovascular disease, there should be a greater emphasis on improving overall\
  \ diet quality and food sources of macronutrients, such as dietary fats and carbohydrates.\
  \ In addition, building a\_strong evidence base through high-quality intervention\
  \ and observational studies is crucial for effective policy changes, which can greatly\
  \ improve the food environment and population health."
author: An Pan and Xu Lin and Elena Hemler and Frank B. Hu
categories: Cell Biology (Q1); Molecular Biology (Q1); Physiology (Q1)
citable_docs._(3years): 661.0
cites_/_doc._(2years): 1738.0
country: United States
coverage: 2005-2020
doi: 10.1016/j.cmet.2018.02.017
eigenfactor_score: 0.091
h_index: 266.0
isbn: null
issn: '15504131'
issn1: '15504131'
issn2: '15504131'
issn3: '15504131'
jcr_value: '27.287'
keywords: null
publisher_x: Cell Press
publisher_y: null
ref._/_doc.: 5589.0
region: Northern America
scimago_value: 10326.0
sjr_best_quartile: Q1
sourceid: 146172.0
title_bib: 'Diet and cardiovascular disease: advances and challenges in population-based
  studies'
title_csv: Cell metabolism
total_cites: 52192.0
total_cites_(3years): 12515.0
total_docs._(2020): 215.0
total_docs._(3years): 693.0
total_refs.: 12016.0
type: journal
type_publication: article
year: 2018
---
abstract: Body Area Networks (BANs) are becoming increasingly popular and have shown
  great potential in real-time monitoring of the human body. With the promise of being
  cost-effective and unobtrusive and facilitating continuous monitoring, BANs have
  attracted a wide range of monitoring applications, including medical and healthcare,
  sports, and rehabilitation systems. Most of these applications are real time and
  life critical and require a strict guarantee of Quality of Service (QoS) in terms
  of timeliness, reliability, and so on. Recently, there has been a number of proposals
  describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different
  layers or tiers and different protocols). This survey put these individual efforts
  into perspective and presents a more holistic view of the area. In this regard,
  this article identifies a set of QoS requirements for BAN applications and shows
  how these requirements are linked in a three-tier BAN system and presents a comprehensive
  review of the existing proposals against those requirements. In addition, open research
  issues, challenges, and future research directions in achieving these QoS in BANs
  are highlighted.
author: Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta
categories: Computer Networks and Communications (Q2)
citable_docs._(3years): 117.0
cites_/_doc._(2years): 352.0
country: United States
coverage: 2005-2020
doi: 10.1145/3085580
eigenfactor_score: 0.00099
h_index: 67.0
isbn: null
issn: '15504859'
issn1: '15504859'
issn2: '15504859'
issn3: '15504859'
jcr_value: '2.253'
keywords: QoS, medical care, Body area networks, cloud computing, healthcare
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 5345.0
region: Northern America
scimago_value: 598.0
sjr_best_quartile: Q2
sourceid: 4700152843.0
title_bib: 'Qos in body area networks: a survey'
title_csv: Acm transactions on sensor networks
total_cites: 1365.0
total_cites_(3years): 447.0
total_docs._(2020): 44.0
total_docs._(3years): 118.0
total_refs.: 2352.0
type: journal
type_publication: article
year: 2017
---
abstract: 'Participatory sensing is a promising sensing paradigm that enables collection,
  processing, dissemination and analysis of the phenomena of interest by ordinary
  citizens through their handheld sensing devices. Participatory sensing has huge
  potential in many applications, such as smart transportation and air quality monitoring.
  However, participants may submit low-quality, misleading, inaccurate, or even malicious
  data if a participatory sensing campaign is not launched effectively. Therefore,
  it has become a significant issue to establish an efficient participatory sensing
  campaign for improving the data quality. This article proposes a novel five-tier
  framework of participatory sensing and addresses several technical challenges in
  this proposed framework including: (1) optimized deployment of data collection points
  (DC-points); and (2) efficient recruitment strategy of participants. Toward this
  end, the deployment of DC-points is formulated as an optimization problem with maximum
  utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm
  is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing
  data collection and communication, a trajectory-based strategy for participant recruitment
  is proposed to enable campaign organizers to identify well-suited participants for
  data sensing based on a joint consideration of temporal availability, trust, and
  energy. Extensive experiments and performance analysis of the proposed framework
  and associated algorithms are conducted. The results demonstrate that the proposed
  algorithm can achieve a good sensing coverage with a smaller number of DC-points,
  and the participants that are termed as social sensors are easily selected, to evaluate
  the feasibility and extensibility of the proposed recruitment strategies.'
author: Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.
categories: Hardware and Architecture (Q1); Computer Networks and Communications (Q2)
citable_docs._(3years): 240.0
cites_/_doc._(2years): 398.0
country: United States
coverage: 2005-2020
doi: 10.1145/2808198
eigenfactor_score: .nan
h_index: 49.0
isbn: null
issn: '15516857'
issn1: '15516865'
issn2: '15516857'
issn3: '15516865'
jcr_value: null
keywords: recruitment, DTA, deployment, trajectory, Participatory sensing, tensor
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 5326.0
region: Northern America
scimago_value: 558.0
sjr_best_quartile: Q1
sourceid: 4700151918.0
title_bib: 'Launching an efficient participatory sensing campaign: a smart mobile
  device-based approach'
title_csv: Acm transactions on multimedia computing, communications and applications
total_cites: .nan
total_cites_(3years): 1130.0
total_docs._(2020): 91.0
total_docs._(3years): 252.0
total_refs.: 4847.0
type: journal
type_publication: article
year: 2015
---
abstract: "Background\nData from large electronic databases are increasingly used\
  \ in epidemiological research, but golden standards for database validation remain\
  \ elusive. The Prescription Registry (IPR) and the National Health Service (NHS)\
  \ databases in Iceland have not undergone formal validation, and gross errors have\
  \ repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new\
  \ amphetamine tablets entered the Icelandic market, but were withdrawn half a year\
  \ later due to being substandard. Return of unused stocks provided knowledge of\
  \ the exact number of tablets used and hence a case where quality of the data could\
  \ be assessed.\nObjective\nA case study of the quality of statistics in a national\
  \ database on pharmaceuticals.\nMethods\nData on the sales of the substandard amphetamine\
  \ were obtained from the Prescription Registry and the pharmaceuticals statistics\
  \ database. Upon the revelation of discrepancies, explanations were sought from\
  \ the respective institutions, the producer, and dose dispensing companies.\nResults\n\
  The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According\
  \ to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially\
  \ stated 82860 tablets to have been sold, correcting to 74796 upon being notified\
  \ about errors. The producer stated 72811 tablets to have been sold, and agreed\
  \ with the dose dispensing companies on sales to those. The producer\u2019s numbers\
  \ were confirmed by the Medicines Agency.\nConclusion\nOver-registration in the\
  \ IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This\
  \ case provided a unique opportunity for external validation of sales data for pharmaceuticals\
  \ in Iceland, revealing enormous quality problems. The case has implications regarding\
  \ database integrity beyond Iceland."
author: "Ingunn Bj\xF6rnsdottir and Guri Birgitte Verne"
categories: Pharmacy (Q1); Pharmaceutical Science (Q2)
citable_docs._(3years): 404.0
cites_/_doc._(2years): 257.0
country: United States
coverage: 2005-2020
doi: 10.1016/j.sapharm.2018.02.009
eigenfactor_score: .nan
h_index: 47.0
isbn: null
issn: '15517411'
issn1: '15517411'
issn2: '15517411'
issn3: '15517411'
jcr_value: null
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 4248.0
region: Northern America
scimago_value: 710.0
sjr_best_quartile: Q1
sourceid: 4700151922.0
title_bib: 'Exhibiting caution with use of big data: the case of amphetamine in iceland''s
  prescription registry'
title_csv: Research in social and administrative pharmacy
total_cites: .nan
total_cites_(3years): 1395.0
total_docs._(2020): 306.0
total_docs._(3years): 514.0
total_refs.: 12998.0
type: journal
type_publication: article
year: 2018
---
abstract: Cognitive function is an important end point of treatments in dementia clinical
  trials. Measuring cognitive function by standardized tests, however, is biased toward
  highly constrained environments (such as hospitals) in selected samples. Patient-powered
  real-world evidence using information and communication technology devices, including
  environmental and wearable sensors, may help to overcome these limitations. This
  position paper describes current and novel information and communication technology
  devices and algorithms to monitor behavior and function in people with prodromal
  and manifest stages of dementia continuously, and discusses clinical, technological,
  ethical, regulatory, and user-centered requirements for collecting real-world evidence
  in future randomized controlled trials. Challenges of data safety, quality, and
  privacy and regulatory requirements need to be addressed by future smart sensor
  technologies. When these requirements are satisfied, these technologies will provide
  access to truly user relevant outcomes and broader cohorts of participants than
  currently sampled in clinical trials.
author: "Stefan Teipel and Alexandra K\xF6nig and Jesse Hoey and Jeff Kaye and Frank\
  \ Kr\xFCger and Julie M. Robillard and Thomas Kirste and Claudio Babiloni"
categories: Cellular and Molecular Neuroscience (Q1); Developmental Neuroscience (Q1);
  Epidemiology (Q1); Geriatrics and Gerontology (Q1); Health Policy (Q1); Neurology
  (clinical) (Q1); Psychiatry and Mental Health (Q1)
citable_docs._(3years): 412.0
cites_/_doc._(2years): 1368.0
country: United States
coverage: 2005-2020
doi: 10.1016/j.jalz.2018.05.003
eigenfactor_score: .nan
h_index: 118.0
isbn: null
issn: '15525260'
issn1: '15525279'
issn2: '15525260'
issn3: '15525279'
jcr_value: null
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 5213.0
region: Northern America
scimago_value: 6713.0
sjr_best_quartile: Q1
sourceid: 3600148102.0
title_bib: Use of nonintrusive sensor-based information and communication technology
  for real-world evidence for clinical trials in dementia
title_csv: Alzheimer's and dementia
total_cites: .nan
total_cites_(3years): 5971.0
total_docs._(2020): 182.0
total_docs._(3years): 448.0
total_refs.: 9488.0
type: journal
type_publication: article
year: 2018
---
abstract: The impact of the Internet of Things (IoT) on the advancement of the healthcare
  industry is immense. The ushering of the Medicine 4.0 has resulted in an increased
  effort to develop platforms, both at the hardware level as well as the underlying
  software level. This vision has led to the development of Healthcare IoT (H-IoT)
  systems. The basic enabling technologies include the communication systems between
  the sensing nodes and the processors; and the processing algorithms for generating
  an output from the data collected by the sensors. However, at present, these enabling
  technologies are also supported by several new technologies. The use of Artificial
  Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge
  paradigm is bringing the computing power close to the deployed network and hence
  mitigating many challenges in the process. While the big data allows handling an
  enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring
  flexibility to the system while the blockchains are finding the most novel use cases
  in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are
  driving the innovation in the H-IoT applications. This paper delves into the ways
  these technologies are transforming the H-IoT systems and also identifies the future
  course for improving the Quality of Service (QoS) using these new technologies.
author: Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos,
  Athanasios V. and Kim, Sung Won
categories: Electrical and Electronic Engineering (Q1)
citable_docs._(3years): 360.0
cites_/_doc._(2years): 3755.0
country: United States
coverage: 2005-2020
doi: 10.1109/COMST.2020.2973314
eigenfactor_score: 0.03684
h_index: 197.0
isbn: null
issn: 1553877X
issn1: 1553877X
issn2: 1553877X
issn3: 1553877X
jcr_value: '25.249'
keywords: Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality
  of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software
  defined networks
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 19942.0
region: Northern America
scimago_value: 6605.0
sjr_best_quartile: Q1
sourceid: 17900156715.0
title_bib: 'The future of healthcare internet of things: a survey of emerging technologies'
title_csv: Ieee communications surveys and tutorials
total_cites: 22146.0
total_cites_(3years): 15073.0
total_docs._(2020): 91.0
total_docs._(3years): 368.0
total_refs.: 18147.0
type: journal
type_publication: article
year: 2020
---
abstract: The article presents a textual Big Data analytics solution developed in
  a real setting as a part of a high-capacity document digitization and storage system.
  A software based on machine learning techniques performs automated extraction and
  processing of textual contents. The work focuses on performance and data confidence
  evaluation and describes the approach to computing a set of indicators for textual
  data quality. It then presents experimental results.
author: Fugini, Mariagrazia and Finocchi, Jacopo
categories: Conservation (Q1); Computer Graphics and Computer-Aided Design (Q2); Computer
  Science Applications (Q3); Information Systems (Q3)
citable_docs._(3years): 74.0
cites_/_doc._(2years): 279.0
country: United States
coverage: 2008-2020
doi: 10.1145/3461015
eigenfactor_score: .nan
h_index: 25.0
isbn: null
issn: '15564673'
issn1: '15564711'
issn2: '15564673'
issn3: '15564711'
jcr_value: null
keywords: machine learning, Big Data analytics, unstructured Big Data, data quality,
  content management, text analytics
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 4817.0
region: Northern America
scimago_value: 371.0
sjr_best_quartile: Q1
sourceid: 19400157014.0
title_bib: Data and process quality evaluation in a textual big data archiving system
title_csv: Journal on computing and cultural heritage
total_cites: .nan
total_cites_(3years): 224.0
total_docs._(2020): 36.0
total_docs._(3years): 79.0
total_refs.: 1734.0
type: journal
type_publication: article
year: 2022
---
abstract: Light detection and ranging (LiDAR) provides a 3-D understanding of environment
  and plays an important role in autonomous driving. To study the influence of 3-D
  data quality on the environment perception and provide a theoretical basis for optimizing
  system design, a multi-beam LiDAR perception assessment model has been established
  to reveal the relationship between data quality and multi-parameters, including
  system and motion parameters. A novel ground segmentation algorithm was proposed
  with a combination of the grid elevation and the neighbor relationship, which was
  used to validate how the data quality influences the results of environment perception.
  By the way of down-sampling based on the Karlsruhe Institute of Technology and Toyota
  Technological Institute (KITTI) dataset, the experimental results showed that the
  proposed ground segmentation with combination of grid-elevation and neighbor-relationship
  (GSCGN) method was superior than other general ground segmentation methods in terms
  of accuracy and efficiency. It should be noted that the mean vertical angular resolution
  (MVAR), laser repetition frequency, and beam numbers were the dominant influencing
  parameters on the point density and the accuracy of ground segmentation. Based on
  the experimental results, the lower limits of system parameters were determined
  as 16-beam and 4-kHz repetition frequency, with the acceptable recall of 92.2% for
  ground and 93.5% for object, the accuracy of 92.9% and the runtime of 0.036 s, which
  can not only provide a reliable environment perception effect, but also reducing
  the computational burden to satisfy the real-time autonomous driving. This study
  offers a meaningful investigation to guide LiDAR system design with balancing the
  contradiction between the optimized system design and the high-degree environment
  perception.
author: Li, Xiaolu and Zhou, Yier and Hua, Baocheng
categories: Electrical and Electronic Engineering (Q1); Instrumentation (Q1)
citable_docs._(3years): 1120.0
cites_/_doc._(2years): 447.0
country: United States
coverage: 1963-2020
doi: 10.1109/TIM.2021.3094230
eigenfactor_score: 0.013269999999999999
h_index: 119.0
isbn: null
issn: '15579662'
issn1: 00189456
issn2: '15579662'
issn3: 00189456
jcr_value: '4.016'
keywords: Laser radar;Three-dimensional displays;Solid modeling;System analysis and
  design;Laser modes;Laser beams;Autonomous vehicles;Evaluation indicator;lower limit;multi-beam
  light detection and ranging (LiDAR);perception assessment model;system design
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3425.0
region: Northern America
scimago_value: 820.0
sjr_best_quartile: Q1
sourceid: 15361.0
title_bib: Study of a multi-beam lidar perception assessment model for real-time autonomous
  driving
title_csv: Ieee transactions on instrumentation and measurement
total_cites: 18199.0
total_cites_(3years): 5589.0
total_docs._(2020): 962.0
total_docs._(3years): 1170.0
total_refs.: 32944.0
type: journal
type_publication: article
year: 2021
---
abstract: Intelligent Transportation Systems (ITS) is a smart-transportation system
  for road-side assistance and data exchange support by integrating cloud and wireless
  networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data
  exchanges for satisfying user demands. The rate of big data granting to the vehicular
  users is interrupted by the fundamental attributes such as mobility and link instability
  of the vehicles. To address the issues in vehicular data exchange big data, this
  article introduces displacement-aware service endowment scheme with the benefits
  of data offloading. Displacement-aware big data endowment ensures responsive availability
  of vehicle request information despite unfavorable location and density factors.
  The time congruency in V2V and V2X data exchanges are adopted for minimizing data
  exchange dropouts. In the data offloading phase, extraneous information and big
  data responses are detained based on data exchange relevance to improve congestion
  free big data endowment. The distinct methods work in a co-operative manner to improve
  big data quality of fast configuring smart vehicles to provide reliable big data
  in smart city environments.
author: Manogaran, Gunasekaran and Nguyen, Tu N.
categories: Automotive Engineering (Q1); Computer Science Applications (Q1); Mechanical
  Engineering (Q1)
citable_docs._(3years): 1015.0
cites_/_doc._(2years): 841.0
country: United States
coverage: 2000-2020
doi: 10.1109/TITS.2021.3078753
eigenfactor_score: 0.02555
h_index: 153.0
isbn: null
issn: '15580016'
issn1: '15580016'
issn2: '15249050'
issn3: '15580016'
jcr_value: '6.492'
keywords: Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility
  prediction;time synchronized data exchanges;data offloading;V2X data exchange.
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3484.0
region: Northern America
scimago_value: 1591.0
sjr_best_quartile: Q1
sourceid: 18378.0
title_bib: Displacement-aware service endowment scheme for improving intelligent transportation
  systems data exchange
title_csv: Ieee transactions on intelligent transportation systems
total_cites: 20072.0
total_cites_(3years): 9523.0
total_docs._(2020): 595.0
total_docs._(3years): 1026.0
total_refs.: 20731.0
type: journal
type_publication: article
year: 2021
---
abstract: The recent advances in Internet of Things (IoT), computational analytics,
  processing power, and assimilation of Big Data (BD) are playing an important role
  in revolutionizing maintenance and operations regimes within the wider facilities
  management (FM) sector. The BD offers the potential for the FM to obtain valuable
  insights from a large amount of heterogeneous data collected through various sources
  and IoT allows for the integration of sensors. The aim of this article is to extend
  the exploratory studies conducted on Big Data analytics (BDA) implementation and
  empirically test and categorize the associated drivers and challenges. Using exploratory
  factor analysis (EFA), the researchers aim to bridge the current knowledge gap and
  highlight the principal factors affecting the BDA implementation. Questionnaires
  detailing 26 variables are sent to the FM organization in the U.K. who are in the
  process or have already implemented BDA initiatives within their FM operations.
  Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that
  driven by market competition and ambitious sustainability goals, the industry is
  moving to holistically integrate analytics into its decision making. However, data
  quality, technological barriers, inadequate preparedness, data management, and governance
  issues and skill gaps are posing to be significant barriers to the fulfillment of
  expected opportunities. The findings of this study have important implications for
  FM businesses that are evaluating the potential of the BDA and IoT applications
  for their operations. Most importantly, it addresses the role of the BD maturity
  in FM organizations and its implications for perception of drivers.
author: Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon
categories: Electrical and Electronic Engineering (Q1); Strategy and Management (Q2)
citable_docs._(3years): 212.0
cites_/_doc._(2years): 281.0
country: United States
coverage: 1969-2020
doi: 10.1109/TEM.2019.2959914
eigenfactor_score: 0.00209
h_index: 92.0
isbn: null
issn: '15580040'
issn1: 00189391
issn2: '15580040'
issn3: 00189391
jcr_value: '6.146'
keywords: Frequency modulation;Organizations;Big Data;Maintenance engineering;Data
  mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology
  implementation
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 2685.0
region: Northern America
scimago_value: 702.0
sjr_best_quartile: Q1
sourceid: 17359.0
title_bib: 'Drivers and challenges associated with the implementation of big data
  within u.k. facilities management sector: an exploratory factor analysis approach'
title_csv: Ieee transactions on engineering management
total_cites: 4148.0
total_cites_(3years): 668.0
total_docs._(2020): 252.0
total_docs._(3years): 223.0
total_refs.: 6766.0
type: journal
type_publication: article
year: 2022
---
abstract: "Recorded seismograms are usually distorted by statics owing to complex\
  \ geological conditions, such as lateral variations in sediment thickness or complex\
  \ topographies. These distorted and discontinuous signals usually exist in either\
  \ arrival times or amplitudes of waves, and they are most likely to be smeared as\
  \ velocity perturbations along their associated raypaths. Therefore, statics may\
  \ blur images of the target bodies or, even worse, introduce unexpected and false\
  \ anomalies into subsurface structures. To partly resolve this problem, we develop\
  \ a weighted statics correction method to estimate unwanted temporal shifts of traces\
  \ using the closure-phase technique, which is utilized in astronomical imaging.\
  \ In the proposed method, the source and receiver statics are regarded as independent\
  \ quantities contributing to the waveform shifts based on their acquisition geometries.\
  \ Numerical tests on both the synthetic and field cases show noticeable, although\
  \ gradual, improvements in data quality compared to the conventional plus\u2013\
  minus (PM) method. In general, this method provides a straightforward strategy to\
  \ reedit the travel times in seismic profiles without inverting for a near-surface\
  \ velocity model. Moreover, it can be extended to any interferometrical methods\
  \ in seismic data processing that satisfies the closure-phase conditions."
author: Yu, Han and Hanafy, Sherif M. and Liu, Lulu
categories: Earth and Planetary Sciences (miscellaneous) (Q1); Electrical and Electronic
  Engineering (Q1)
citable_docs._(3years): 1908.0
cites_/_doc._(2years): 687.0
country: United States
coverage: 1980-2020
doi: 10.1109/TGRS.2022.3169519
eigenfactor_score: 0.043789999999999996
h_index: 254.0
isbn: null
issn: '15580644'
issn1: '15580644'
issn2: 01962892
issn3: '15580644'
jcr_value: '5.600'
keywords: Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational
  modeling;Closure phase;first arrivals;interferometry;statics
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3747.0
region: Northern America
scimago_value: 2141.0
sjr_best_quartile: Q1
sourceid: 17360.0
title_bib: 'A weighted closure-phase statics correction method: synthetic and field
  data examples'
title_csv: Ieee transactions on geoscience and remote sensing
total_cites: 48898.0
total_cites_(3years): 13865.0
total_docs._(2020): 823.0
total_docs._(3years): 1908.0
total_refs.: 30841.0
type: journal
type_publication: article
year: 2022
---
abstract: The heterogeneous network is the foundation of next-generation networks.
  It aims to explore the existing network resources effectively, and providing better
  QoS for every kind of traffic flow as far as possible. However, the diversity and
  dynamic nature of heterogeneous networks will bring a huge burden and big data to
  the network traffic control. Therefore, how to achieve efficient and intelligent
  network traffic control becomes the key problem of heterogeneous networks. In this
  article, an AI-inspired traffic control scheme is proposed. In order to realize
  fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e.,
  inter-layer, intra-layer, and caching and pushing) network traffic control is introduced.
  It is worth noting that backpropagation in deep recurrent neural networks is applied
  in the intra-layer such that an intelligent traffic control scheme can be derived
  efficiently when facing the huge traffic load in heterogeneous networks. Moreover,
  DBSCAN is adopted in the inter-layer, which supports efficient classification in
  the inter-layer. In addition, caching and pushing is adopted to make full use of
  network resources and provide better QoS. Simulation results demonstrate the effectiveness
  and practicability of the proposed scheme.
author: Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li
categories: Computer Networks and Communications (Q1); Hardware and Architecture (Q1);
  Information Systems (Q1); Software (Q1)
citable_docs._(3years): 375.0
cites_/_doc._(2years): 1358.0
country: United States
coverage: 1986-2020
doi: 10.1109/MNET.2018.1800120
eigenfactor_score: 0.011890000000000001
h_index: 129.0
isbn: null
issn: 1558156X
issn1: 08908044
issn2: 1558156X
issn3: 08908044
jcr_value: '10.693'
keywords: Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural
  networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked
  control systems;Recurrent neural networks
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 1340.0
region: Northern America
scimago_value: 2546.0
sjr_best_quartile: Q1
sourceid: 27239.0
title_bib: Artificial intelligence inspired multi-dimensional traffic control for
  heterogeneous networks
title_csv: Ieee network
total_cites: 6526.0
total_cites_(3years): 4922.0
total_docs._(2020): 263.0
total_docs._(3years): 401.0
total_refs.: 3525.0
type: journal
type_publication: article
year: 2018
---
abstract: The past decade has seen tremendous development in digital health, including
  in innovative new technologies such as Electronic Health Records, telemedicine,
  virtual visits, wearable technology and sophisticated analytical tools such as artificial
  intelligence (AI) and machine learning for the deep-integration of big data. In
  the field of rare connective tissue diseases (rCTDs), these opportunities include
  increased access to scarce and remote expertise, improved patient monitoring, increased
  participation and therapeutic adherence, better patient outcomes and patient empowerment.
  In this review, we discuss opportunities and key-barriers to improve application
  of digital health technologies in the field of autoimmune diseases. We also describe
  what could be the fully digital pathway of rCTD patients. Smart technologies can
  be used to provide real-world evidence about the natural history of rCTDs, to determine
  real-life drug utilization, advanced efficacy and safety data for rare diseases
  and highlight significant unmet needs. Yet, digitalization remains one of the most
  challenging issues faced by rCTD patients, their physicians and healthcare systems.
  Digital health technologies offer enormous potential to improve autoimmune rCTD
  care but this potential has so far been largely unrealized due to those significant
  obstacles. The need for robust assessments of the efficacy, affordability and scalability
  of AI in the context of digital health is crucial to improve the care of patients
  with rare autoimmune diseases.
author: "Hugo Bergier and Lo\xEFc Duron and Christelle Sordet and Lou Kawka and Aur\xE9\
  lien Schlencker and Fran\xE7ois Chasset and Laurent Arnaud"
categories: Immunology (Q1); Immunology and Allergy (Q1)
citable_docs._(3years): 408.0
cites_/_doc._(2years): 766.0
country: Netherlands
coverage: 2002-2020
doi: 10.1016/j.autrev.2021.102864
eigenfactor_score: 0.01481
h_index: 122.0
isbn: null
issn: '15689972'
issn1: '15689972'
issn2: '18730183'
issn3: '15689972'
jcr_value: '9.754'
keywords: Autoimmune diseases, Digital technology, Big data, Delivery of health care,
  Telemedicine
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 6852.0
region: Western Europe
scimago_value: 2621.0
sjr_best_quartile: Q1
sourceid: 20689.0
title_bib: 'Digital health, big data and smart technologies for the care of patients
  with systemic autoimmune diseases: where do we stand?'
title_csv: Autoimmunity reviews
total_cites: 13493.0
total_cites_(3years): 3509.0
total_docs._(2020): 177.0
total_docs._(3years): 465.0
total_refs.: 12128.0
type: journal
type_publication: article
year: 2021
---
abstract: "The goal of Internet of Things (IoT) is to bring any object online, thereby\
  \ creating a massive volume of data that can overwhelm the existing computing and\
  \ networking technologies. Therefore, centered cloud isn't ideal for rapidly expanding\
  \ IoT environmental requirements. Fog computing (FC) moves some portion of the computing\
  \ load (related to real-time services) from the cloud into edge fog devices. FC\
  \ is expected to become the subsequent major computing transition and this one has\
  \ ability to overcome existing cloud limitations. However the key obstacles facing\
  \ FC are: wide distribution, isolated coupling, quality-of-service (QoS) regulation,\
  \ adaptability to conditions, and particularly the standardization and normalization\
  \ is still in phase of development. Software defined networking (SDN) will help\
  \ fog to solve these obstacles. SDN means unified network control plane (which is\
  \ separated from data plane), allowing the introduction for advanced traffic control\
  \ and the orchestration mechanisms of networks and resources. On the grounds of\
  \ SDN concept, and then combining it with FC, the network type can be modified to\
  \ resolve all those cloud drawbacks and improve IoT system's QoS. Within this paper,\
  \ architecture is developed through the combination of independently researched\
  \ areas of SDN and FC to enhance the QoS in an IoT system. An algorithm (which is\
  \ dependent\_on partition the SDN virtually) is presented to support the architecture\
  \ whose purpose is to select the optimal access point and optimal place to process\
  \ the data. The main objective of this algorithm is to provide improved QoS by partitioning\
  \ the corresponding fog devices through the SDN controller. A use case dependent\
  \ on the presented architecture and algorithm is then provided and assessed this\
  \ use case's QoS parameter values (network usage, cost, latency and power consumption)\
  \ using the iFogSim simulator. In contrast to cloud-only deployment, the result\
  \ indicates a major enhancement of the mentioned QoS parameter values in the deployment\_\
  of fog with SDN. In addition, once compared to a relative former\_identical\_use\
  \ case; the findings of this paper show improved results for power consumption,\
  \ network usage and latency. In fact, when compared to a former identical use case,\
  \ the outcome of this paper shows around 3 times less latency and 2 times less network\
  \ usage. Finally the ground (IoMT, Industry 4.0, Green IoT, and 5G) that is influenced\
  \ by this QoS improvement is broadly illustrated in this paper."
author: Ishtiaq Ahammad and Md. Ashikur Rahman Khan and Zayed Us Salehin
categories: Hardware and Architecture (Q2); Modeling and Simulation (Q2); Software
  (Q2)
citable_docs._(3years): 312.0
cites_/_doc._(2years): 402.0
country: Netherlands
coverage: 2002-2021
doi: 10.1016/j.simpat.2021.102292
eigenfactor_score: 0.00315
h_index: 69.0
isbn: null
issn: 1569190X
issn1: 1569190X
issn2: 1569190X
issn3: 1569190X
jcr_value: '3.272'
keywords: Internet of Things, Software-Defined Networking, Fog Computing, Quality
  of Service, Modeling and Simulation, iFogSim Simulator
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4398.0
region: Western Europe
scimago_value: 554.0
sjr_best_quartile: Q2
sourceid: 12189.0
title_bib: Qos performance enhancement policy through combining fog and sdn
title_csv: Simulation modelling practice and theory
total_cites: 3547.0
total_cites_(3years): 1227.0
total_docs._(2020): 132.0
total_docs._(3years): 316.0
total_refs.: 5805.0
type: journal
type_publication: article
year: 2021
---
abstract: "The previous research of clinical big data mining showed that stir-baking\
  \ Semen Cuscuta with salt solution (YP) ranked the first in the usage rate of treating\
  \ abortion caused by kidney deficiency. At the same time, pharmacodynamic studies\
  \ also showed that YP has better effect on improving recurrent spontaneous abortion\
  \ (RSA) compared to raw products of Semen Cuscuta (SP). However, there were few\
  \ studies on the biomarkers of YP improving RSA. In this study, the chemical and\
  \ metabonomic profiling were used to screen the quality markers of YP on improving\
  \ RSA. Firstly, a metabolomics study was carried out to select representative biomarkers\
  \ of RSA. The ultra-high performance liquid chromatography coupled with electrospray\
  \ ionization-quadrupole-time of flight-mass spectrometry (UPLC-ESI-Q-TOF-MS) technique\
  \ was used to investigate the components of exogenous and endogenous in serum of\
  \ rats after administrated with YP and SP. As a result, 14 differential compounds\
  \ were identified between the serum of rats administrated SP and YP. Compared to\
  \ SP, there was an upward trend in YP of the compounds including kaempferol-3-glucuronide,\
  \ iso-kaempferol-3-glucuronide, (1S) \u221211-hydroxyhexadecanoic acid and 3-phenylpropionic\
  \ acid. Meanwhile, there was a reducing trend in YP of the compounds including kaempferol\
  \ 3-arabinofuranoside, apigenin-3-O-glucoside, hyperoside, caffeic acid-\u03B2-D\
  \ glucoside, dicaffeoylquinic acid, linoleic acid, 3,4-dicaffeoylquinic acid, caffeic\
  \ acid, palmitic acid and methyl myristate. 12 biomarkers for RSA indication were\
  \ identified. SP and YP have a certain effect on the endogenous biomarker. The regulation\
  \ effect of YP was higher than that of SP. The main metabolic pathways included\
  \ phenylalanine, tyrosine and tryptophan biosynthesis, glycerophospholipid metabolism,\
  \ fatty acid biosynthesis, sphingolipid metabolism, biosynthesis of unsaturated\
  \ fatty acids. This study demonstrated a promising way to elucidate the active chemical\
  \ and endogenous material basis of TCM."
author: Xiaoli Wang and Haiyan Gao and Song Tan and Chao Xu and Fengqing Xu and Tongsheng
  Wang and Jijun Chu and Yanquan Han and Deling Wu and Chuanshan Jin
categories: Analytical Chemistry (Q2); Clinical Biochemistry (Q2); Medicine (miscellaneous)
  (Q2); Biochemistry (Q3); Cell Biology (Q3)
citable_docs._(3years): 1525.0
cites_/_doc._(2years): 299.0
country: Netherlands
coverage: 2002-2020
doi: 10.1016/j.jchromb.2021.122727
eigenfactor_score: .nan
h_index: 149.0
isbn: null
issn: '15700232'
issn1: 1873376X
issn2: '15700232'
issn3: 1873376X
jcr_value: null
keywords: ', Stir-baking with salt solution, Metabolomics, UHPLC-Q-TOF-MS'
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 3566.0
region: Western Europe
scimago_value: 729.0
sjr_best_quartile: Q2
sourceid: 24172.0
title_bib: An integrated approach to uncover quality markers of stir-baking semen
  cuscuta with salt solution preventing recurrent spontaneous abortion based on chemical
  and metabolomic profiling
title_csv: 'Journal of chromatography b: analytical technologies in the biomedical
  and life sciences'
total_cites: .nan
total_cites_(3years): 4732.0
total_docs._(2020): 354.0
total_docs._(3years): 1537.0
total_refs.: 12622.0
type: journal
type_publication: article
year: 2021
---
abstract: In recent years, the just-in-time (JIT) predictive models have attracted
  considerable attention due to their ability to prevent degradation of prediction
  accuracy. However, one of their practical limitations is expensive computation,
  which becomes a major factor that prevents them from being used for big data quality
  prediction. This is because the JIT modeling methods need to update the local regression
  model using the relevant samples that are searched through the lineal scan of the
  database during online operation. To solve this issue, the present work proposes
  a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data
  quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly
  used to hash big data into a set of buckets, in which similar samples are grouped
  on themselves. During online prediction, HbJIT looks up multiple buckets that have
  a high probability of containing similar samples of a query object through the intelligent
  probing scheme, uses the data objects in the buckets as the candidate set of the
  results, and then filters the candidate objects using a linear scan. After filtering,
  the most relevant samples are used to construct the local regression model to yield
  the prediction of the query object. By integrating the multi-probe hashing strategy
  into the JIT learning framework, HbJIT can not only deal with process nonlinearity
  and time-varying characteristics but also is applicable to large-scale industrial
  processes. Experimental results on real-world dataset have demonstrated that the
  proposed HbJIT is time-efficient in processing large-scale datasets, and greatly
  reduces the online prediction time without compromising on the prediction accuracy.
author: Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li
categories: Chemical Engineering (miscellaneous); Computer Science Applications
citable_docs._(3years): 0.0
cites_/_doc._(2years): 73.0
country: Netherlands
coverage: 1997, 2000-2019
doi: 10.1016/B978-0-323-85159-6.50280-3
eigenfactor_score: .nan
h_index: 25.0
isbn: null
issn: '15707946'
issn1: '15707946'
issn2: '15707946'
issn3: '15707946'
jcr_value: null
keywords: Virtual sensor, soft-sensor, big data quality prediction, hashing-based
  just-in-time modeling
publisher_x: Elsevier
publisher_y: Elsevier
ref._/_doc.: 974.0
region: Western Europe
scimago_value: .nan
sjr_best_quartile: '-'
sourceid: 11200153545.0
title_bib: Hashing-based just-in-time learning for big data quality prediction
title_csv: Computer aided chemical engineering
total_cites: .nan
total_cites_(3years): 1080.0
total_docs._(2020): 345.0
total_docs._(3years): 1662.0
total_refs.: 3359.0
type: book series
type_publication: incollection
year: 2022
---
abstract: 'Assessing the quality of an evolving knowledge base is a challenging task
  as it often requires to identify correct quality assessment procedures. Since data
  is often derived from autonomous, and increasingly large data sources, it is impractical
  to manually curate the data, and challenging to continuously and automatically assess
  their quality. In this paper, we explore two main areas of quality assessment related
  to evolving knowledge bases: (i) identification of completeness issues using knowledge
  base evolution analysis, and (ii) identification of consistency issues based on
  integrity constraints, such as minimum and maximum cardinality, and range constraints.
  For the completeness analysis, we use data profiling information from consecutive
  knowledge base releases to estimate completeness measures that allow predicting
  quality issues. Then, we perform consistency checks to validate the results of the
  completeness analysis using integrity constraints and learning models. The approach
  has been tested both quantitatively and qualitatively by using a subset of datasets
  from both DBpedia and 3cixty knowledge bases. The performance of the approach is
  evaluated using precision, recall, and F1 score. From completeness analysis, we
  observe a 94% precision for the English DBpedia KB and 95% precision for the 3cixty
  Nice KB. We also assessed the performance of our consistency analysis by using five
  learning models over three sub-tasks, namely minimum cardinality, maximum cardinality,
  and range constraint. We observed that the best performing model in our experimental
  setup is Random Forest, reaching an F1 score greater than 90% for minimum and maximum
  cardinality and 84% for range constraints.'
author: "Mohammad Rifat Ahmmad Rashid and Giuseppe Rizzo and Marco Torchiano and Nandana\
  \ Mihindukulasooriya and Oscar Corcho and Ra\xFAl Garc\xEDa-Castro"
categories: Computer Networks and Communications (Q2); Human-Computer Interaction
  (Q2); Software (Q2)
citable_docs._(3years): 79.0
cites_/_doc._(2years): 387.0
country: Netherlands
coverage: 2003-2020
doi: 10.1016/j.websem.2018.11.004
eigenfactor_score: .nan
h_index: 83.0
isbn: null
issn: '15708268'
issn1: '15708268'
issn2: '15708268'
issn3: '15708268'
jcr_value: null
keywords: Quality assessment, Evolution analysis, Validation, Knowledge base, RDF
  shape, Machine learning
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4154.0
region: Western Europe
scimago_value: 502.0
sjr_best_quartile: Q2
sourceid: 14879.0
title_bib: Completeness and consistency analysis for evolving knowledge bases
title_csv: Web semantics
total_cites: .nan
total_cites_(3years): 341.0
total_docs._(2020): 26.0
total_docs._(3years): 83.0
total_refs.: 1080.0
type: journal
type_publication: article
year: 2019
---
abstract: "With the development of IIoT (Industrial Internet of Things), Artificial\
  \ Intelligence technology is widely used in many research areas, such as image classification,\
  \ speech recognition, and information retrieval. Traditional supervised machine\
  \ learning obtains labels from high-quality oracles, which is high cost and time-consuming\
  \ and does not consider security. Since multi-label active learning becomes a hot\
  \ topic, it is more challenging to train efficient and secure classification models,\
  \ and reduce the label cost in the field of IIoT. To address this issue, this research\
  \ focuses on the secure multi-label active learning for IIoT using an economical\
  \ and efficient strategy called crowdsourcing, which involves querying labels from\
  \ multiple low-cost annotators with various expertise on crowdsourcing platforms\
  \ rather than relying on a high-quality oracle. To eliminate the effects of annotation\
  \ noise caused by imperfect annotators, we propose the Multi-label Active Learning\
  \ from Crowds (MALC) method, which uses a probabilistic model to simultaneously\
  \ compute the annotation consensus and estimate the classifier\u2019s parameters\
  \ while also taking instance similarity into account. Then, to actively choose the\
  \ most informative instances and labels, as well as the most reliable annotators,\
  \ an instance-label-annotator triplets selection technique is proposed. Experimental\
  \ results on two real-world data sets show that the performance of MALC is superior\
  \ to existing methods."
author: Ming Wu and Qianmu Li and Muhammad Bilal and Xiaolong Xu and Jing Zhang and
  Jun Hou
categories: Computer Networks and Communications (Q1); Hardware and Architecture (Q1);
  Software (Q1)
citable_docs._(3years): 481.0
cites_/_doc._(2years): 542.0
country: Netherlands
coverage: 2003-2020
doi: 10.1016/j.adhoc.2021.102594
eigenfactor_score: 0.00533
h_index: 94.0
isbn: null
issn: '15708705'
issn1: '15708705'
issn2: '15708705'
issn3: '15708705'
jcr_value: '4.111'
keywords: Crowdsourcing, Secure IIoT, Annotation consensus, Multi-label learning,
  Active learning
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4177.0
region: Western Europe
scimago_value: 781.0
sjr_best_quartile: Q1
sourceid: 26799.0
title_bib: Multi-label active learning from crowds for secure iiot
title_csv: Ad hoc networks
total_cites: 5667.0
total_cites_(3years): 2621.0
total_docs._(2020): 171.0
total_docs._(3years): 496.0
total_refs.: 7142.0
type: journal
type_publication: article
year: 2021
---
abstract: "Among the reforms to OTC derivative markets since the global financial\
  \ crisis is a commitment to collateralize counterparty exposures and to clear standardized\
  \ contracts via central counterparties (CCPs). The reforms aim to reduce interconnectedness\
  \ and improve counterparty risk management in these important markets. At the same\
  \ time, however, the reforms necessarily concentrate risk in one or a few nodes\
  \ in the financial network and also increase institutions\u2019 demand for high-quality\
  \ assets to meet collateral requirements. This paper looks more closely at the implications\
  \ of increased CCP clearing for both the topology and stability of the financial\
  \ network. Building on Heath et al. (2013) and Markose (2012), the analysis supports\
  \ the view that the concentration of risk in CCPs could generate instability if\
  \ not appropriately managed. Nevertheless, maintaining CCP prefunded financial resources\
  \ in accordance with international standards and dispersing any unfunded losses\
  \ widely through the system can limit the potential for a CCP to transmit stress\
  \ even in very extreme market conditions. The analysis uses the Bank for International\
  \ Settlements Macroeconomic Assessment Group on Derivatives (MAGD) data set on the\
  \ derivatives positions of the 41 largest bank participants in global OTC derivative\
  \ markets in 2012."
author: Alexandra Heath and Gerard Kelly and Mark Manning and Sheri Markose and Ali
  Rais Shaghaghi
categories: Economics, Econometrics and Finance (miscellaneous) (Q1); Finance (Q1)
citable_docs._(3years): 211.0
cites_/_doc._(2years): 396.0
country: Netherlands
coverage: 2004-2020
doi: 10.1016/j.jfs.2015.12.004
eigenfactor_score: 0.005529999999999999
h_index: 50.0
isbn: null
issn: '15723089'
issn1: '15723089'
issn2: '15723089'
issn3: '15723089'
jcr_value: '3.727'
keywords: OTC derivatives reforms, Central counterparty (CCP), Netting efficiency,
  Collateralization
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5284.0
region: Western Europe
scimago_value: 2272.0
sjr_best_quartile: Q1
sourceid: 144987.0
title_bib: Ccps and network stability in otc derivatives markets
title_csv: Journal of financial stability
total_cites: 3013.0
total_cites_(3years): 974.0
total_docs._(2020): 51.0
total_docs._(3years): 219.0
total_refs.: 2695.0
type: journal
type_publication: article
year: 2016
---
abstract: Industry 4.0 is the new industrial revolution. By connecting every machine
  and activity through network sensors to the Internet, a huge amount of data is generated.
  Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence
  (AI), which are used to evaluate the generated data and produce valuable information
  about the manufacturing enterprise, while introducing in parallel the Industrial
  AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by
  giving emphasis to the features, requirements, and challenges behind Industry 4.0.
  In addition, a new architecture for AIA is presented. Furthermore, the most important
  ML and DL algorithms used in Industry 4.0 are presented and compiled in detail.
  Each algorithm is discussed and evaluated in terms of its features, its applications,
  and its efficiency. Then, we focus on one of the most important Industry 4.0 fields,
  namely the smart grid, where ML and DL models are presented and analyzed in terms
  of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges
  in the field of data analysis in the context of the new Industrial era are highlighted
  and discussed such as scalability, cybersecurity, and big data.
author: Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis
  and Dimitrios Tzovaras
categories: Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)
citable_docs._(3years): 61.0
cites_/_doc._(2years): 1249.0
country: Ireland
coverage: 2007-2020
doi: 10.1016/j.cosrev.2020.100341
eigenfactor_score: 0.00195
h_index: 44.0
isbn: null
issn: '15740137'
issn1: '15740137'
issn2: '15740137'
issn3: '15740137'
jcr_value: '7.872'
keywords: Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid
publisher_x: Elsevier Ireland Ltd
publisher_y: null
ref._/_doc.: 14424.0
region: Western Europe
scimago_value: 1646.0
sjr_best_quartile: Q1
sourceid: 8000153138.0
title_bib: 'Machine learning and deep learning in smart manufacturing: the smart grid
  paradigm'
title_csv: Computer science review
total_cites: 1249.0
total_cites_(3years): 891.0
total_docs._(2020): 54.0
total_docs._(3years): 61.0
total_refs.: 7789.0
type: journal
type_publication: article
year: 2021
---
abstract: "We designed and applied interactive visualisation techniques for investigating\
  \ how social networks are embedded in time and space, using data collected from\
  \ smartphone logs. Our interest in spatial aspects of social networks is that they\
  \ may reveal associations between participants missed by simply making contact through\
  \ smartphone devices. Four linked and co-ordinated views of spatial, temporal, individual\
  \ and social network aspects of the data, along with demographic and attitudinal\
  \ variables, helped add context to the behaviours we observed. Using these techniques,\
  \ we were able to characterise spatial and temporal aspects of participants\u2019\
  \ social networks and suggest explanations for some of them. This provides some\
  \ validation of our techniques. Unexpected deficiencies in the data that became\
  \ apparent prompted us to evaluate the dataset in more detail. Contrary to what\
  \ we expected, we found significant gaps in participant records, particularly in\
  \ terms of location, a poorly connected sample of participants and asymmetries in\
  \ reciprocal call logs. Although the data captured are of high quality, deficiencies\
  \ such as these remain and are likely to have a significant impact on interpretations\
  \ relating to spatial aspects of the social network. We argue that appropriately-designed\
  \ interactive visualisation techniques\u2013afforded by our flexible prototyping\
  \ approach\u2013are effective in identifying and characterising data inconsistencies.\
  \ Such deficiencies are likely to exist in other similar datasets, and although\
  \ the visual approaches we discuss for identifying data problems may not be scalable,\
  \ the categories of problems we identify may be used to inform attempts to systematically\
  \ account for errors in larger smartphone datasets."
author: Aidan Slingsby and Roger Beecham and Jo Wood
categories: Computer Networks and Communications (Q1); Computer Science (miscellaneous)
  (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Applied
  Mathematics (Q2); Computer Science Applications (Q2)
citable_docs._(3years): 342.0
cites_/_doc._(2years): 467.0
country: Netherlands
coverage: 2005-2020
doi: 10.1016/j.pmcj.2013.07.002
eigenfactor_score: 0.00335
h_index: 64.0
isbn: null
issn: '15741192'
issn1: '15741192'
issn2: '15741192'
issn3: '15741192'
jcr_value: '3.453'
keywords: Big data, Human behaviour, Spatiotemporal, Social networks, Visual analysis
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4059.0
region: Western Europe
scimago_value: 687.0
sjr_best_quartile: Q1
sourceid: 3200147819.0
title_bib: Visual analysis of social networks in space and time using smartphone logs
title_csv: Pervasive and mobile computing
total_cites: 2540.0
total_cites_(3years): 1558.0
total_docs._(2020): 68.0
total_docs._(3years): 350.0
total_refs.: 2760.0
type: journal
type_publication: article
year: 2013
---
abstract: "Recent advancements in consumer directed personal computing technology\
  \ have led to the generation of biomedically-relevant data streams with potential\
  \ health applications. This has catalyzed international interest in Patient Generated\
  \ Health Data (PGHD), defined as \u201Chealth-related data \u2013 including health\
  \ history, symptoms, biometric data, treatment history, lifestyle choices, and other\
  \ information-created, recorded, gathered, or inferred by or from patients or their\
  \ designees (i.e. care partners or those who assist them) to help address a health\
  \ concern.\u201D(Shapiro et\_al., 2012) PGHD offers several opportunities to improve\
  \ the efficiency and output of clinical trials, particularly within oncology. These\
  \ range from using PGHD to understand mechanisms of action of therapeutic strategies,\
  \ to understanding and predicting treatment-related toxicity, to designing interventions\
  \ to improve adherence and clinical outcomes. To facilitate the optimal use of PGHD,\
  \ methodological research around considerations related to feasibility, validation,\
  \ measure selection, and modeling of PGHD streams is needed. With successful integration,\
  \ PGHD can catalyze the application of \u201Cbig data\u201D to cancer clinical research,\
  \ creating both \u201Cn of 1\u201D and population-level observations, and generating\
  \ new insights into the nature of health and disease."
author: William A. Wood and Antonia V. Bennett and Ethan Basch
categories: Cancer Research (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular
  Medicine (Q1); Oncology (Q1)
citable_docs._(3years): 421.0
cites_/_doc._(2years): 591.0
country: Netherlands
coverage: 2007-2020
doi: 10.1016/j.molonc.2014.08.006
eigenfactor_score: 0.01225
h_index: 88.0
isbn: null
issn: '15747891'
issn1: '15747891'
issn2: '18780261'
issn3: '15747891'
jcr_value: '6.603'
keywords: Information technology, Patient reported outcomes, Quality of care, Clinical
  trials
publisher_x: John Wiley and Sons Ltd
publisher_y: null
ref._/_doc.: 5459.0
region: Western Europe
scimago_value: 2332.0
sjr_best_quartile: Q1
sourceid: 5800207508.0
title_bib: Emerging uses of patient generated health data in clinical research
title_csv: Molecular oncology
total_cites: 8378.0
total_cites_(3years): 2983.0
total_docs._(2020): 239.0
total_docs._(3years): 428.0
total_refs.: 13048.0
type: journal
type_publication: article
year: 2015
---
abstract: "iEcology is used to supplement traditional ecological data by sourcing\
  \ large quantities of media from the internet. Images and their metadata are widely\
  \ available online and can provide information on species occurrence, behaviour\
  \ and visible traits. However, this data is inherently noisy and data quality varies\
  \ significantly between sources. Many iEcology studies utilise data from a single\
  \ source for simplicity and efficiency. Hence, a tool to compare the suitability\
  \ of different media sources in addressing a particular research question is needed.\
  \ We provide a simple, novel way to estimate the fraction of images within multiple\
  \ unverified datasets that potentially depict a specified target fauna. Our method,\
  \ the Sum of Tag Frequency Differences (STFD), uses any pretrained, general-purpose\
  \ image classifier. One of the method's innovations is that it does not require\
  \ training the classifier to recognise the target fauna. Instead, STFD analyses\
  \ the frequency of the generic text-tags returned by a classifier for multiple datasets\
  \ and compares them to the corresponding frequencies of an authoritative image dataset\
  \ that depicts only the target organism. From this comparison, STFD allows us to\
  \ deduce the fraction of images of the target in unverified datasets. To validate\
  \ the STFD approach, we processed images from five sources: Flickr, iNaturalist,\
  \ Instagram, Reddit and Twitter. For each media source, we conducted an STFD analysis\
  \ of three fauna invasive to Australia: Cane toads (Rhinella marina), German wasps\
  \ (Vespula germanica), and the higher-level colloquial taxonomic classification,\
  \ \u201Cwild rabbits\u201D. We found the STFD provided an accurate assessment of\
  \ image source relevance across all data sources and target organisms. This was\
  \ demonstrated by the consistent, very strong correlation (toads r\_\u22650.97,\
  \ wasps r\_\u22650.95, wild rabbits\u2265\_0.95) between STFD predictions, and the\
  \ fraction of target images in a source dataset observed by a human expert. The\
  \ STFD provides a low-cost, simple and accurate comparison of the relevance of online\
  \ image sources to specific fauna for iEcology applications. It does not require\
  \ expertise in machine learning or training neural-network species-specific classifiers.\
  \ The method enables researchers to assess multiple image sources to select those\
  \ warranting detailed investigation for the development of tools for web-scraping,\
  \ citizen science campaigns, further monitoring or analysis."
author: Hannah M. Burke and Reid Tingley and Alan Dorin
categories: Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational
  Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling
  (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)
citable_docs._(3years): 271.0
cites_/_doc._(2years): 343.0
country: Netherlands
coverage: 2006-2020
doi: 10.1016/j.ecoinf.2022.101598
eigenfactor_score: 0.0033200000000000005
h_index: 55.0
isbn: null
issn: '15749541'
issn1: '15749541'
issn2: '15749541'
issn3: '15749541'
jcr_value: '3.142'
keywords: Biodiversity monitoring, Computer vision, Data mining, iEcology, Social
  media
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 5127.0
region: Western Europe
scimago_value: 774.0
sjr_best_quartile: Q1
sourceid: 3100147401.0
title_bib: 'Tag frequency difference: rapid estimation of image set relevance for
  species occurrence data using general-purpose image classifiers'
title_csv: Ecological informatics
total_cites: 2893.0
total_cites_(3years): 964.0
total_docs._(2020): 109.0
total_docs._(3years): 273.0
total_refs.: 5588.0
type: journal
type_publication: article
year: 2022
---
abstract: "The equity of urban park access has received great attention from studies\
  \ on public service provision. However, individuals\u2019 growing demands for recreational\
  \ activities have brought diversity and complexity to park usages, drawing doubts\
  \ on traditional measurements of park accessibility. To fill the gap, this study\
  \ explores park equity issues with a dataset containing 12.03 million mobile phone\
  \ users who accessed one of the 332 parks in Shanghai. We measured community-level\
  \ park accessibility with two traditional place-based indicators \u2013 park area\
  \ proportion and Gaussian-based 2SFCA accessibility, and three innovative activity-based\
  \ indicators \u2013 park activity frequency, park activity trip length, and park\
  \ activity duration. We then explored the geographic and social inequity by calculating\
  \ Gini index and conducting correlation analysis. The results show that place-based\
  \ and activity-based indicators presented citywide differences, indicating a significant\
  \ impact of human activities on urban park accessibility. The geographic inequality\
  \ of park distribution was undermined by people\u2019s actual park usages. However,\
  \ residents in communities with higher quality of built-environment had higher park\
  \ activity frequency while shorter trip length, and social inequity of park access\
  \ among the total population was more obvious than the low-recreation-demand population.\
  \ Therefore, policy-makers should rethink how to provide park resources to address\
  \ the inequity issues brought by human activities. Our study contributes to the\
  \ existing literature in the following ways: (1) compared place-based park accessibility\
  \ and activity-based park accessibility in the same context, and (2) identified\
  \ low-recreation-demand population as a comparison group to explore impacts of recreation\
  \ demand on park equity."
author: Xiyuan Ren and ChengHe Guan
categories: Ecology (Q1); Forestry (Q1); Soil Science (Q1)
citable_docs._(3years): 651.0
cites_/_doc._(2years): 463.0
country: Germany
coverage: 2002-2020
doi: 10.1016/j.ufug.2022.127709
eigenfactor_score: .nan
h_index: 74.0
isbn: null
issn: '16188667'
issn1: '16188667'
issn2: '16188667'
issn3: '16188667'
jcr_value: null
keywords: Geographic and social inequity, Urban park, Mobile phone data, Recreational
  activity, Low-recreation-demand population
publisher_x: Urban und Fischer Verlag GmbH und Co. KG
publisher_y: null
ref._/_doc.: 6431.0
region: Western Europe
scimago_value: 1163.0
sjr_best_quartile: Q1
sourceid: 145301.0
title_bib: Evaluating geographic and social inequity of urban parks in shanghai through
  mobile phone-derived human activities
title_csv: Urban forestry and urban greening
total_cites: .nan
total_cites_(3years): 3276.0
total_docs._(2020): 280.0
total_docs._(3years): 658.0
total_refs.: 18007.0
type: journal
type_publication: article
year: 2022
---
abstract: Population dynamics and health risk factors keep changing in the KSA, requiring
  continuous research and quality data. We aimed to review the current status of population
  health data, outline the available opportunities for data utilization, and provide
  recommendations for population data-related improvement initiatives. We provide
  practical solutions to support the collection, linkage, quality assurance, and governance
  of population health data.
author: Saleh A. Alessy and Maha Alattas and Mahmoud A. Mahmoud and Ali Alqarni and
  Suliman Alghnam
categories: Medicine (miscellaneous) (Q3)
citable_docs._(3years): 253.0
cites_/_doc._(2years): 120.0
country: Netherlands
coverage: 2009-2020
doi: 10.1016/j.jtumed.2022.06.011
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '16583612'
issn1: '16583612'
issn2: '16583612'
issn3: '16583612'
jcr_value: null
keywords: Data, Epidemiology, Health research, KSA, Population health
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 2739.0
region: Western Europe
scimago_value: 269.0
sjr_best_quartile: Q3
sourceid: 19700188101.0
title_bib: 'Population health data in ksa: status, challenges, and opportunities'
title_csv: Journal of taibah university medical sciences
total_cites: .nan
total_cites_(3years): 362.0
total_docs._(2020): 93.0
total_docs._(3years): 262.0
total_refs.: 2547.0
type: journal
type_publication: article
year: 2022
---
abstract: This study integrates different machine learning (ML) methods and 5-fold
  cross-validation (CV) method to estimate the ground maximal surface settlement (MSS)
  induced by tunneling. We further investigate the applicability of artificial intelligent
  (AI) based prediction through a comparative study of two tunnelling datasets with
  different sizes and features. Four different ML approaches, including support vector
  machine (SVM), random forest (RF), back-propagation neural network (BPNN), and deep
  neural network (DNN), are utilized. Two techniques, i.e. particle swarm optimization
  (PSO) and grid search (GS) methods, are adopted for hyperparameter optimization.
  To assess the reliability and efficiency of the predictions, three performance evaluation
  indicators, including the mean absolute error (MAE), root mean square error (RMSE),
  and Pearson correlation coefficient (R), are calculated. Our results indicate that
  proposed models can accurately and efficiently predict the settlement, while the
  RF model outperforms the other three methods on both datasets. The difference in
  model performance on two datasets (Datasets A and B) reveals the importance of data
  quality and quantity. Sensitivity analysis indicates that Dataset A is more significantly
  affected by geological conditions, while geometric characteristics play a more dominant
  role on Dataset B.
author: Libin Tang and SeonHong Na
categories: Geotechnical Engineering and Engineering Geology (Q1)
citable_docs._(3years): 298.0
cites_/_doc._(2years): 462.0
country: China
coverage: 2013-2020
doi: 10.1016/j.jrmge.2021.08.006
eigenfactor_score: 0.00573
h_index: 46.0
isbn: null
issn: '16747755'
issn1: '16747755'
issn2: '16747755'
issn3: '16747755'
jcr_value: '4.338'
keywords: Surface settlement, Tunnel construction, Machine learning (ML), Hyperparameter
  optimization, Cross-validation (CV)
publisher_x: Chinese Academy of Sciences
publisher_y: null
ref._/_doc.: 5061.0
region: Asiatic Region
scimago_value: 1470.0
sjr_best_quartile: Q1
sourceid: 21100381006.0
title_bib: Comparison of machine learning methods for ground settlement prediction
  with different tunneling datasets
title_csv: Journal of rock mechanics and geotechnical engineering
total_cites: 3915.0
total_cites_(3years): 1614.0
total_docs._(2020): 115.0
total_docs._(3years): 306.0
total_refs.: 5820.0
type: journal
type_publication: article
year: 2021
---
abstract: 'To enhance the credibility of human reliability analysis, various kinds
  of data have been recently collected and analyzed. Although it is obvious that the
  quality of data is critical, the practices or considerations for securing data quality
  have not been sufficiently discussed. In this work, based on the experience of the
  recent human reliability data extraction projects, which produced more than fifty
  thousand data-points, we derive a number of issues to be considered for generating
  meaningful data. As a result, thirteen considerations are presented here as pertaining
  to the four different data extraction activities: preparation, collection, analysis,
  and application. Although the lessons were acquired from a single kind of data collection
  framework, it is believed that these results will guide researchers to consider
  important issues in the process of extracting data.'
author: Yochan Kim
categories: Nuclear Energy and Engineering (Q2)
citable_docs._(3years): 602.0
cites_/_doc._(2years): 240.0
country: South Korea
coverage: 2008-2020
doi: 10.1016/j.net.2020.01.034
eigenfactor_score: 0.00525
h_index: 40.0
isbn: null
issn: '17385733'
issn1: 2234358X
issn2: '17385733'
issn3: 2234358X
jcr_value: '2.341'
keywords: Data analytics, Human reliability analysis, HuREX framework, Lesson learned,
  Simulation data
publisher_x: Korean Nuclear Society
publisher_y: null
ref._/_doc.: 2662.0
region: Asiatic Region
scimago_value: 737.0
sjr_best_quartile: Q2
sourceid: 11700154337.0
title_bib: 'Considerations for generating meaningful hra data: lessons learned from
  hurex data collection'
title_csv: Nuclear engineering and technology
total_cites: 3095.0
total_cites_(3years): 1465.0
total_docs._(2020): 404.0
total_docs._(3years): 604.0
total_refs.: 10754.0
type: journal
type_publication: article
year: 2020
---
abstract: In the era of big data medicinal chemists are exposed to an enormous amount
  of bioactivity data. Numerous public data sources allow for querying across medium
  to large data sets mostly compiled from literature. However, the data available
  are still quite incomplete and of mixed quality. This mini review will focus on
  how medicinal chemists might use such resources and how valuable the current data
  sources are for guiding drug discovery.
author: Lars Richter and Gerhard F. Ecker
categories: Biotechnology (Q1); Drug Discovery (Q1); Molecular Medicine (Q1)
citable_docs._(3years): 89.0
cites_/_doc._(2years): 564.0
country: United Kingdom
coverage: 2004-2020
doi: 10.1016/j.ddtec.2015.06.001
eigenfactor_score: .nan
h_index: 51.0
isbn: null
issn: '17406749'
issn1: '17406749'
issn2: '17406749'
issn3: '17406749'
jcr_value: null
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6263.0
region: Western Europe
scimago_value: 1676.0
sjr_best_quartile: Q1
sourceid: 21207.0
title_bib: Medicinal chemistry in the era of big data
title_csv: 'Drug discovery today: technologies'
total_cites: .nan
total_cites_(3years): 521.0
total_docs._(2020): 16.0
total_docs._(3years): 99.0
total_refs.: 1002.0
type: journal
type_publication: article
year: 2015
---
abstract: Staggering statistics regarding the global burden of disease due to lack
  of surgical care worldwide has been gaining attention in the global health literature
  over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9
  million lives were lost due to an absence of surgical care in 2010, equivalent to
  33% of all deaths worldwide. Although data from low- and middle-income countries
  (LMICs) are limited, recent investigations, such as the African Surgical Outcomes
  Study, highlight that despite operating on low risk patients, there is increased
  postoperative mortality in LMICs versus higher-resource settings, a majority of
  which occur secondary to seemingly preventable complications like surgical site
  infections. We propose that implementing creative, low-cost surgical outcomes monitoring
  and select quality improvement systems proven effective in high-income countries,
  such as surgical infection prevention programs and safety checklists, can enhance
  the delivery of safe surgical care in existing LMIC surgical systems. While efforts
  to initiate and expand surgical access and capacity continues to deserve attention
  in the global health community, here we advocate for creative modifications to current
  service structures, such as promoting a culture of safety, employing technology
  and mobile health (mHealth) for patient data collection and follow-up, and harnessing
  partnerships for information sharing, to create a framework for improving morbidity
  and mortality in responsible, scalable, and sustainable ways.
author: Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and
  Michael D. Kluger
categories: Medicine (miscellaneous) (Q1); Surgery (Q1)
citable_docs._(3years): 959.0
cites_/_doc._(2years): 438.0
country: Netherlands
coverage: 2003-2020
doi: 10.1016/j.ijsu.2019.07.036
eigenfactor_score: 0.01876
h_index: 61.0
isbn: null
issn: '17439191'
issn1: '17439191'
issn2: '17439159'
issn3: '17439191'
jcr_value: '6.071'
keywords: Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality
  improvement, Big data
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 1762.0
region: Western Europe
scimago_value: 1315.0
sjr_best_quartile: Q1
sourceid: 130156.0
title_bib: Ensuring safe surgical care across resource settings via surgical outcomes
  data & quality improvement initiatives
title_csv: International journal of surgery
total_cites: 16011.0
total_cites_(3years): 4685.0
total_docs._(2020): 698.0
total_docs._(3years): 1214.0
total_refs.: 12301.0
type: journal
type_publication: article
year: 2019
---
abstract: Cardiovascular disease is a major illness that causes human death, especially
  in the elderly. Timely and accurate diagnosis of arrhythmia types is the key to
  early prevention and diagnosis of cardiovascular diseases. This paper proposed an
  arrhythmia classification algorithm based on multi-head self-attention mechanism
  (ACA-MA). First, an ECG signal preprocessing algorithm based on wavelet transform
  is put forward and implemented using db6 wavelet transform to focus on improving
  the data quality of ECG signals and reduce the noise of ECG signals. Second, a linear
  projection layer for acquiring semantic features of ECG signals is designed using
  the matching relationship between ECG tag and segmented ECG signals. Third, a position
  encoding-based spatiotemporal characterization method of ECG signal sequences is
  designed to integrate time series information into a matrix operation. Fourth, a
  multi-head self-attentive mechanism capable of capturing global contextual information
  is proposed to extract relationships and semantic features between ECG segments
  and achieve semantic association and information stitching of nonadjacent ECG signals.
  Finally, experimental results on the arrhythmia dataset MIT/BIH show that ACA-MA
  outperforms other state-of-the-art methods with an overall classification accuracy
  of 99.4%, a specific rate of 99.41%, and a sensitivity of 97.36%.
author: Yue Wang and Guanci Yang and Shaobo Li and Yang Li and Ling He and Dan Liu
categories: Health Informatics (Q2); Signal Processing (Q2)
citable_docs._(3years): 780.0
cites_/_doc._(2years): 490.0
country: Netherlands
coverage: 2006-2021
doi: 10.1016/j.bspc.2022.104206
eigenfactor_score: 0.00713
h_index: 72.0
isbn: null
issn: '17468094'
issn1: '17468094'
issn2: '17468094'
issn3: '17468094'
jcr_value: '3.880'
keywords: Arrhythmia classification, Electrocardiogram (ECG), Attention mechanism,
  Feature extraction
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4265.0
region: Western Europe
scimago_value: 767.0
sjr_best_quartile: Q2
sourceid: 4700152237.0
title_bib: Arrhythmia classification algorithm based on multi-head self-attention
  mechanism
title_csv: Biomedical signal processing and control
total_cites: 6354.0
total_cites_(3years): 3928.0
total_docs._(2020): 425.0
total_docs._(3years): 783.0
total_refs.: 18127.0
type: journal
type_publication: article
year: 2023
---
abstract: This study presents a unique approach in investigating the knowledge diffusion
  structure for the field of data quality through an analysis of the main paths. We
  study a dataset of 1880 papers to explore the knowledge diffusion path, using citation
  data to build the citation network. The main paths are then investigated and visualized
  via social network analysis. This paper takes three different main path analyses,
  namely local, global, and key-route, to depict the knowledge diffusion path and
  additionally implements the g-index and h-index to evaluate the most important journals
  and researchers in the data quality domain.
author: Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou
categories: Applied Mathematics (Q1); Computer Science Applications (Q1); Library
  and Information Sciences (Q1); Management Science and Operations Research (Q1);
  Modeling and Simulation (Q1); Statistics and Probability (Q1)
citable_docs._(3years): 245.0
cites_/_doc._(2years): 566.0
country: Netherlands
coverage: 2007-2020
doi: 10.1016/j.joi.2014.05.001
eigenfactor_score: 0.00554
h_index: 76.0
isbn: null
issn: '17511577'
issn1: '17511577'
issn2: '17511577'
issn3: '17511577'
jcr_value: '5.107'
keywords: Data quality, Main path analysis, Knowledge diffusion, Citation analysis,
  Social network analysis, Big data
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4792.0
region: Western Europe
scimago_value: 1605.0
sjr_best_quartile: Q1
sourceid: 5100155103.0
title_bib: 'Knowledge diffusion path analysis of data quality literature: a main path
  analysis'
title_csv: Journal of informetrics
total_cites: 4326.0
total_cites_(3years): 1696.0
total_docs._(2020): 79.0
total_docs._(3years): 276.0
total_refs.: 3786.0
type: journal
type_publication: article
year: 2014
---
abstract: "The study of cause-specific mortality data is one of the main sources of\
  \ information for public health monitoring. In most industrialized countries, when\
  \ a death occurs, it is a legal requirement that a medical certificate based on\
  \ the international form recommended by World Health Organization's (WHO) is filled\
  \ in by a physician. The physician reports the causes of death that directly led\
  \ or contributed to the death on the death certificate. The death certificate is\
  \ then forwarded to a coding office, where each cause is coded, and one underlying\
  \ cause is defined, using the rules of the International Classification of Diseases\
  \ and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing\
  \ number of countries have adopted, or have decided to adopt, the coding software\
  \ Iris, developed and maintained by an international consortium1. This whole standardized\
  \ production process results in a high and constantly increasing international comparability\
  \ of cause-specific mortality data. While these data could be used for international\
  \ comparisons and benchmarking of global burden of diseases, quality of care and\
  \ prevention policies, there are also many other ways and methods to explore their\
  \ richness, especially when they are linked with other data sources. Some of these\
  \ methods are potentially referring to the so-called \u201Cbig data\u201D field.\
  \ These methods could be applied both to the production of the data, to the statistical\
  \ processing of the data, and even more to process these data linked to other databases.\
  \ In the present note, we depict the main domains in which this new field of methods\
  \ could be applied. We focus specifically on the context of France, a 65 million\
  \ inhabitants country with a centralized health data system. Finally we will insist\
  \ on the importance of data quality, and the specific problematics related to death\
  \ certification in the forensic medicine domain."
author: "Gr\xE9goire Rey and Karim Bounebache and Claire Rondet"
categories: Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine
  (Q2)
citable_docs._(3years): 407.0
cites_/_doc._(2years): 157.0
country: United Kingdom
coverage: 2007-2020
doi: 10.1016/j.jflm.2016.12.004
eigenfactor_score: 0.00305
h_index: 47.0
isbn: null
issn: 1752928X
issn1: 1752928X
issn2: 1752928X
issn3: 1752928X
jcr_value: '1.614'
keywords: Causes of death data, Data linkages, Big data
publisher_x: Churchill Livingstone
publisher_y: null
ref._/_doc.: 2926.0
region: Western Europe
scimago_value: 569.0
sjr_best_quartile: Q1
sourceid: 5100154502.0
title_bib: Causes of deaths data, linkages and big data perspectives
title_csv: Journal of forensic and legal medicine
total_cites: 2328.0
total_cites_(3years): 668.0
total_docs._(2020): 134.0
total_docs._(3years): 428.0
total_refs.: 3921.0
type: journal
type_publication: article
year: 2018
---
abstract: The use of data has been essential throughout the unfolding COVID-19 pandemic.
  We have needed it to populate our models, inform our understanding, and shape our
  responses to the disease. However, data has not always been easy to find and access,
  it has varied in quality and coverage, been difficult to reuse or repurpose. This
  paper reviews these and other challenges and recommends steps to develop a data
  ecosystem better able to deal with future pandemics by better supporting preparedness,
  prevention, detection and response.
author: Nigel Shadbolt and Alys Brett and Min Chen and Glenn Marion and Iain J. McKendrick
  and Jasmina Panovska-Griffiths and Lorenzo Pellis and Richard Reeve and Ben Swallow
categories: Epidemiology (Q1); Infectious Diseases (Q1); Microbiology (Q1); Parasitology
  (Q1); Public Health, Environmental and Occupational Health (Q1); Virology (Q1)
citable_docs._(3years): 138.0
cites_/_doc._(2years): 593.0
country: Netherlands
coverage: 2009-2020
doi: 10.1016/j.epidem.2022.100612
eigenfactor_score: 0.00442
h_index: 41.0
isbn: null
issn: '17554365'
issn1: '17554365'
issn2: '18780067'
issn3: '17554365'
jcr_value: '4.396'
keywords: Data and models, Data ecosystem, Data lifecycles, FAIR data, Pandemic preparedness,
  COVID-19
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4631.0
region: Western Europe
scimago_value: 2023.0
sjr_best_quartile: Q1
sourceid: 17300154924.0
title_bib: The challenges of data in future pandemics
title_csv: Epidemics
total_cites: 1395.0
total_cites_(3years): 786.0
total_docs._(2020): 48.0
total_docs._(3years): 141.0
total_refs.: 2223.0
type: journal
type_publication: article
year: 2022
---
abstract: "Zusammenfassung\nHintergrund\nIn Deutschland leben etwa 4 Millionen Menschen\
  \ mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe\
  \ von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver\
  \ erforscht werden k\xF6nnen. Deutschlandweit existiert aber bisher kein konkretes,\
  \ umfassendes Konzept f\xFCr den Einsatz von Big Data zur Versorgung von Menschen\
  \ mit seltenen Erkrankungen. Im Rahmen des BMG-gef\xF6rderten Projekts \u201EBIDA-SE\u201C\
  \ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll\
  \ in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einflie\xDFen\
  \ k\xF6nnen.\nMethode\nZiel der vorliegenden Studie war es, das entwickelte Szenario\
  \ hinsichtlich der Akzeptanz, des (klinischen) Nutzens, \xF6konomischer Implikationen\
  \ sowie Grenzen und Barrieren f\xFCr dessen mittelfristige Umsetzung zu evaluieren.\
  \ F\xFCr die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine\
  \ Online-Befragung innerhalb Deutschlands mit insgesamt N\_=\_9 \xC4rzt*innen, N\_\
  =\_69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N\_=\_14\
  \ IT-Expert*innen und N\_=\_21 Versorgungsforscher*innen durchgef\xFChrt. F\xFC\
  r die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen\
  \ bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis\
  \ einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung\
  \ erfolgte prim\xE4r deskriptiv durch eine Analyse von H\xE4ufigkeiten, Mittelwerten\
  \ und Standardabweichungen.\nErgebnisse\nDie Ergebnisse zeigen, dass das entwickelte\
  \ Szenario von allen befragten Gruppen (\xC4rzt*innen, Patient*innen/Patientenvertreter*innen,\
  \ IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erf\xE4\
  hrt. Aus Sicht der \xC4rzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen\
  \ h\xE4tte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung\
  \ zu beschleunigen und die sektoren\xFCbergreifende Behandlung zu verbessern. Investitionen\
  \ in die vorgestellte Anwendung w\xFCrden sich aus Sicht der \xC4rzt*innen und Versorgungsforscher*innen\
  \ rentieren. Zur Finanzierung des vorgestellten Szenarios m\xFCsste jedoch eine\
  \ Anpassung der Verg\xFCtungssituation erfolgen. Die von allen Gruppen benannten\
  \ Grenzen und Barrieren f\xFCr eine mittelfristige Umsetzung des Szenarios lassen\
  \ sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und\
  \ Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualit\xE4\
  t, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um\
  \ Verf\xFCgbarkeit sowie (7) Gewohnheiten und Pr\xE4ferenzen/Arztrolle.\nDiskussion\n\
  Mit der vorliegenden Studie wurde ein erstes fach\xFCbergreifendes, praxisnahes\
  \ Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz,\
  \ Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario\
  \ zuk\xFCnftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario\
  \ erf\xE4hrt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und\
  \ wird mehrheitlich als (klinisch) n\xFCtzlich bewertet, wenngleich noch rechtliche,\
  \ organisatorische und technische Barrieren f\xFCr dessen mittelfristige Umsetzung\
  \ \xFCberwunden werden m\xFCssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen\
  \ abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gew\xE4hrleiten\
  \ und den Zugang zu den Zentren f\xFCr Seltene Erkrankungen zuk\xFCnftig zu kanalisieren.\n\
  Schlussfolgerung\nAuf nationaler Ebene wurden zahlreiche Aktivit\xE4ten angesto\xDF\
  en, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern.\
  \ Das im Rahmen des Projekts \u201EBIDA-SE\u201C entwickelte Szenario erg\xE4nzt\
  \ diese Forschungsaktivit\xE4ten und verdeutlicht, wie Big-Data-basierte Anwendungen\
  \ sinnvoll in der Praxis genutzt werden k\xF6nnen, um die Diagnosestellung und Therapie\
  \ von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu k\xF6nnen.\nIntroduction\n\
  In Germany there are about 4 million people living with a rare disease. Studies\
  \ have shown that big data applications can improve diagnosis of and research on\
  \ rare diseases more effectively. However, no concrete comprehensive concept for\
  \ the use of big data in the care of people with rare diseases has so far been established\
  \ in Germany. As part of the project \u201CBIDA-SE\u201D, which is funded by the\
  \ German Ministry of Health, a first scenario has been designed to show how big\
  \ data applications can be usefully incorporated into the care of people with rare\
  \ diseases.\nMethods\nThe aim of the present study was to evaluate this scenario\
  \ with regard to acceptance, (clinical) benefits, economic aspects, and limitations\
  \ and barriers to its implementation. To evaluate the scenario, an online survey\
  \ was conducted in Germany in October/November 2019 amongst a total of N\_=\_9 physicians,\
  \ N\_=\_69 patients with rare diseases/patient representatives, N\_=\_14 IT experts\
  \ and N\_=\_21 health care researchers. The online questionnaire consisted of both\
  \ standardized, validated questions taken from already tested survey instruments\
  \ and additional questions which were constructed on the basis of a preceding literature\
  \ analysis. The evaluation of the survey was primarily descriptive, with a calculation\
  \ of frequencies, mean values and standard deviations.\nResults\nThe results of\
  \ the evaluation show that the scenario has been accepted by a majority of all groups\
  \ surveyed (physicians, patients/patient representatives, IT experts and health\
  \ care researchers). From the point of view of physicians, patients/patient representatives\
  \ and health care researchers, the scenario has the potential to accelerate the\
  \ diagnosis and initiation of therapy and to improve cross-sectoral treatment. From\
  \ the physician\u2019s and health care researcher\u2019s perspective, investments\
  \ in the application presented in the scenario would be profitable. Financing the\
  \ scenario would, however, require adjusting the reimbursement situation. The limitations\
  \ and barriers identified by all groups for a medium-term implementation of the\
  \ scenario can be grouped into seven thematic areas where action is needed: (1)\
  \ financing and investment, (2) data protection and data security, (3) standards/data\
  \ sources/data quality, (4) acceptance of technology, (5) integration into the daily\
  \ work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's\
  \ role.\nDiscussion\nWith the present study, a first interdisciplinary, practical\
  \ scenario using big data applications was evaluated with regard to acceptance,\
  \ benefits and limitations/barriers. The scenario is widely accepted among the majority\
  \ of all surveyed target groups and is considered (clinically) useful, although\
  \ legal, organisational and technical barriers still need to be overcome for its\
  \ medium-term implementation. The evaluation results contribute to the derivation\
  \ of recommendations for action to ensure the medium-term implementation of the\
  \ scenario and to channel access to the Centres for Rare Diseases in the future.\n\
  Conclusion\nMany activities have been initiated at a national level to improve the\
  \ health care situation of people with rare diseases. The scenario developed in\
  \ the \u201CBIDA-SE\u201D project complements these research activities and illustrates\
  \ how big data applications can be usefully implemented into practice to improve\
  \ the diagnosis and therapy of people with rare diseases in a sustainable way."
author: "Brita Sedlmayr and Andreas Knapp and Mich\xE9le K\xFCmmel and Franziska Bathelt\
  \ and Martin Sedlmayr"
categories: Education (Q2); Health Policy (Q3); Medicine (miscellaneous) (Q3)
citable_docs._(3years): 221.0
cites_/_doc._(2years): 81.0
country: Germany
coverage: 2008-2020
doi: 10.1016/j.zefq.2020.11.002
eigenfactor_score: .nan
h_index: 28.0
isbn: null
issn: '18659217'
issn1: '22120289'
issn2: '18659217'
issn3: '22120289'
jcr_value: null
keywords: Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data,
  Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario,
  Big data, Rare diseases
publisher_x: Urban und Fischer Verlag Jena
publisher_y: null
ref._/_doc.: 2979.0
region: Western Europe
scimago_value: 423.0
sjr_best_quartile: Q2
sourceid: 11300153728.0
title_bib: "Evaluation eines zukunftsszenarios zur nutzung von big-data-anwendungen\
  \ f\xFCr die verbesserung der versorgung von menschen mit seltenen erkrankungen"
title_csv: Zeitschrift fur evidenz, fortbildung und qualitat im gesundheitswesen
total_cites: .nan
total_cites_(3years): 267.0
total_docs._(2020): 73.0
total_docs._(3years): 244.0
total_refs.: 2175.0
type: journal
type_publication: article
year: 2020
---
abstract: "The development of digital farming gives bovine mastitis research and management\
  \ tools access to large datasets. However, the quality of registered data on clinical\
  \ mastitis cases or treatments may be inadequate (e.g. due to missing records).\
  \ In automatic milking systems, the decision to divert milk from the bulk milk tank\
  \ during milking is registered (i.e. milk diversion indicator) for every milking\
  \ and could potentially indicate a clinical mastitis case. This study accordingly\
  \ estimated the diagnostic performance of a milk diversion indicator in relation\
  \ to farmer-recorded clinical mastitis cases in the absence of a \u201Cgold standard\u201D\
  . Data on milk diversion and farmer-reported clinical mastitis from 3,443 lactations\
  \ in 13 herds were analyzed. Each cow lactation was split into 30-DIM periods in\
  \ which it was registered whether milk was diverted and whether clinical mastitis\
  \ was reported. One 30-DIM period was randomly sampled for each lactation and this\
  \ was the unit of analysis, this procedure was repeated 300 times, resulting in\
  \ 300 datasets to create autocorrelation-robust results during analysis. We used\
  \ Bayesian latent class analysis to assess the diagnostic properties of milk diversion\
  \ and farmer-reported clinical status. We analyzed different episode lengths of\
  \ milk diversion of 1 or more milk diversion days until 10 or more milk diversion\
  \ days for two scenarios: farmers with poor-quality (51% sensitivity, 99% specificity)\
  \ and high-quality (90% sensitivity, 99% specificity) mastitis registrations. The\
  \ analysis was done for all 300 datasets. The results showed that for the scenario\
  \ where the quality of clinical mastitis reporting was high, the sensitivity was\
  \ similar for milk-diversion threshold durations of 1\u20134 days (0.843 to 0.793\
  \ versus 0.893). Specificity increased when the number of days of milk diversion\
  \ increased and was \u226598% at a milk-diversion threshold durations of 8 or more\
  \ consecutive milk diversion days. In the scenario where the quality of clinical\
  \ mastitis reporting was low, the sensitivity of milk diversion and reported clinical\
  \ mastitis cases was similar at milk-diversion threshold durations of 1\u20137 days\
  \ (0.687 to 0.448 versus 0.503 to 0.504) while specificity exceeded the 98% at milk-diversion\
  \ threshold durations of 7 or more consecutive milk diversion days. In both scenarios,\
  \ a milk diversion threshold duration of 4\u20137 days achieved the most desirable\
  \ combined sensitivity and specificity. This study concluded that milk diversion\
  \ can be a valid alternative to farmer-reported clinical mastitis as it performs\
  \ similarly in indicating actual clinical mastitis."
author: John Bonestroo and Nils Fall and Mariska {van der Voort} and Ilka Christine
  Klaas and Henk Hogeveen and Ulf Emanuelson
categories: Veterinary (miscellaneous) (Q1); Animal Science and Zoology (Q2)
citable_docs._(3years): 756.0
cites_/_doc._(2years): 191.0
country: Netherlands
coverage: 2006-2020
doi: 10.1016/j.livsci.2021.104698
eigenfactor_score: 0.005520000000000001
h_index: 111.0
isbn: null
issn: '18711413'
issn1: '18711413'
issn2: '18711413'
issn3: '18711413'
jcr_value: '1.943'
keywords: antibiotic treatment, proxy, Automatic milking system, Milk withdrawal,
  Latent class analysis
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 4360.0
region: Western Europe
scimago_value: 622.0
sjr_best_quartile: Q1
sourceid: 3300147807.0
title_bib: Diagnostic properties of milk diversion and farmer-reported mastitis to
  indicate clinical mastitis status in dairy cows using bayesian latent class analysis
title_csv: Livestock science
total_cites: 9037.0
total_cites_(3years): 1646.0
total_docs._(2020): 358.0
total_docs._(3years): 757.0
total_refs.: 15608.0
type: journal
type_publication: article
year: 2021
---
abstract: "Visible light communication (VLC) has emerged as a viable complement to\
  \ traditional radio frequency (RF) based systems and as an enabler for high data\
  \ rate communications for beyond-5G (B5G) indoor communication systems. In particular,\
  \ the emergence of new B5G-based applications with quality of service (QoS) requirements\
  \ and massive connectivity has recently led to research on the required service-levels\
  \ and the development of improved physical (PHY) layer methods. As part of recent\
  \ VLC standards development activities, the IEEE has formed the 802.11bb \u201C\
  Light Communications (LC) for Wireless Local Area Networking\u201D standardization\
  \ group. This paper investigates the network requirements of 5G indoor services\
  \ such as virtual reality (VR) and high-definition (HD) video for residential environments\
  \ using VLC. In this paper, we consider such typical VLC scenarios with additional\
  \ impairments such as light-emitting diode (LED) nonlinearity and imperfect channel\
  \ feedback, and propose hyperparameter-free mitigation techniques using Reproducing\
  \ Kernel Hilbert Space (RKHS) methods. In this context, we also propose using a\
  \ direct current biased optical orthogonal frequency division multiplexing (DCO-OFDM)-based\
  \ adaptive VLC transmission method that uses precomputed bit error rate (BER) expressions\
  \ for these RKHS-based detection methods and performs adaptive BER-based modulation-order\
  \ switching. Simulations of channel impulse responses (CIRs) show that the adaptive\
  \ transmission method provides significantly improved error rate performance, which\
  \ makes it promising for high data rate VLC-based 5G indoor services."
author: Farshad Miramirkhani and Mehdi Karbalayghareh and Engin Zeydan and Rangeet
  Mitra
categories: Electrical and Electronic Engineering (Q2)
citable_docs._(3years): 443.0
cites_/_doc._(2years): 232.0
country: Netherlands
coverage: 2008-2020
doi: 10.1016/j.phycom.2022.101679
eigenfactor_score: 0.0015199999999999999
h_index: 33.0
isbn: null
issn: '18744907'
issn1: '18744907'
issn2: '18744907'
issn3: '18744907'
jcr_value: '1.810'
keywords: Visible light communication (VLC), Ray-tracing, Adaptive transmission, 5G
  services
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 3496.0
region: Western Europe
scimago_value: 354.0
sjr_best_quartile: Q2
sourceid: 11300153720.0
title_bib: Enabling 5g indoor services for residential environment using vlc technology
title_csv: Physical communication
total_cites: 1412.0
total_cites_(3years): 969.0
total_docs._(2020): 197.0
total_docs._(3years): 447.0
total_refs.: 6888.0
type: journal
type_publication: article
year: 2022
---
abstract: Despite a newfound wealth of data and information, the healthcare sector
  is lacking in actionable knowledge. This is largely because healthcare data, though
  plentiful, tends to be inherently complex and fragmented. Health data analytics,
  with an emphasis on predictive analytics, is emerging as a transformative tool that
  can enable more proactive and preventative treatment options. This review considers
  the ways in which predictive analytics has been applied in the for-profit business
  sector to generate well-timed and accurate predictions of key outcomes, with a focus
  on key features that may be applicable to healthcare-specific applications. Published
  medical research presenting assessments of predictive analytics technology in medical
  applications are reviewed, with particular emphasis on how hospitals have integrated
  predictive analytics into their day-to-day healthcare services to improve quality
  of care. This review also highlights the numerous challenges of implementing predictive
  analytics in healthcare settings and concludes with a discussion of current efforts
  to implement healthcare data analytics in the developing country, Saudi Arabia.
author: Hana Alharthi
categories: Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational
  Health (Q1); Infectious Diseases (Q2)
citable_docs._(3years): 441.0
cites_/_doc._(2years): 332.0
country: Netherlands
coverage: 2008-2020
doi: 10.1016/j.jiph.2018.02.005
eigenfactor_score: 0.00603
h_index: 35.0
isbn: null
issn: '18760341'
issn1: '18760341'
issn2: 1876035X
issn3: '18760341'
jcr_value: '3.718'
keywords: Predictive analytics, Healthcare analytics, Data mining, Saudi Arabia
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 3146.0
region: Western Europe
scimago_value: 983.0
sjr_best_quartile: Q1
sourceid: 16800154711.0
title_bib: 'Healthcare predictive analytics: an overview with a focus on saudi arabia'
title_csv: Journal of infection and public health
total_cites: 3870.0
total_cites_(3years): 1684.0
total_docs._(2020): 336.0
total_docs._(3years): 498.0
total_refs.: 10571.0
type: journal
type_publication: article
year: 2018
---
abstract: "While a significant part of the population is concentrated in urban areas,\
  \ the influence of cityscape parameters on human heat stress remain poorly understood.\
  \ Yet we agree to develop urban spaces (street, square, district ...) in a way to\
  \ provide best possible quality of life. In order to do so, quantitative and qualitative\
  \ references are required. To fill this gap the HES-SO\u2020\u2020University of\
  \ Applied Sciences and Arts of Western Switzerland - hepia/leea\u2021\u2021Haute\
  \ \xE9cole du paysage, d\u2019ing\xE9nierie et d\u2019architecture de Gen\xE8ve\
  \ / Laboratory for energy, environment and architecture has developed an innovative\
  \ portable monitoring system that can be easily deployed in various outdoor and\
  \ indoor environments. The monitoring equipment is embedded into a backpack that\
  \ is carried during \u2018climatic urban walks\u2019 that can be reproduced at different\
  \ times of the day or seasons so to yield a detailed and dynamic description of\
  \ the climatic context of a portion of the city from the pedestrian point of view."
author: Peter Gallinelli and Reto Camponovo and Victor Guillot
categories: Energy (miscellaneous)
citable_docs._(3years): 7685.0
cites_/_doc._(2years): 189.0
country: United Kingdom
coverage: 2009-2019
doi: 10.1016/j.egypro.2017.07.427
eigenfactor_score: .nan
h_index: 81.0
isbn: null
issn: '18766102'
issn1: '18766102'
issn2: '18766102'
issn3: '18766102'
jcr_value: null
keywords: climate mitigation, micro climate monitoring, urban design, open data
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 0.0
region: Western Europe
scimago_value: 474.0
sjr_best_quartile: '-'
sourceid: 17700156736.0
title_bib: Cityfeel - micro climate monitoring for climate mitigation and urban design
title_csv: Energy procedia
total_cites: .nan
total_cites_(3years): 15629.0
total_docs._(2020): 0.0
total_docs._(3years): 7789.0
total_refs.: 0.0
type: conference and proceedings
type_publication: article
year: 2017
---
abstract: The enormous amount of data created daily within healthcare has become so
  complex, that it cannot be effectively handled by routine analytical methods. Such
  large data sets can be processed, looking for correlation not otherwise obvious
  in smaller patient samples. Further advances in terms of data processing as well
  as significant infrastructure and personnel investments are required to fully reap
  the benefits of Big Data, in terms of research and financial sense. However, despite
  its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms,
  not only in terms of data input and processing, but particularly with regards to
  analysis of the output, which are explored within the article. Moreover, use of
  Big Data within healthcare carries the ethical question of privacy and consent.
author: Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley
categories: Orthopedics and Sports Medicine (Q4)
citable_docs._(3years): 160.0
cites_/_doc._(2years): 28.0
country: United Kingdom
coverage: 2009-2020
doi: 10.1016/j.mporth.2021.01.004
eigenfactor_score: .nan
h_index: 27.0
isbn: null
issn: '18771327'
issn1: '18771327'
issn2: '18771327'
issn3: '18771327'
jcr_value: null
keywords: Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy
publisher_x: Churchill Livingstone
publisher_y: null
ref._/_doc.: 2119.0
region: Western Europe
scimago_value: 194.0
sjr_best_quartile: Q4
sourceid: 16800154701.0
title_bib: Introduction to big data in trauma and orthopaedics
title_csv: Orthopaedics and trauma
total_cites: .nan
total_cites_(3years): 82.0
total_docs._(2020): 68.0
total_docs._(3years): 208.0
total_refs.: 1441.0
type: journal
type_publication: article
year: 2021
---
abstract: During the last 30years it has become commonplace for epidemiological studies
  to collect locational attributes of disease data. Although this advancement was
  driven largely by the introduction of handheld global positioning systems (GPS),
  and more recently, smartphones and tablets with built-in GPS, the collection of
  georeferenced disease data has moved beyond the use of handheld GPS devices and
  there now exist numerous sources of crowdsourced georeferenced disease data such
  as that available from georeferencing of Google search queries or Twitter messages.
  In addition, cartography has moved beyond the realm of professionals to crowdsourced
  mapping projects that play a crucial role in disease control and surveillance of
  outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive
  review of a range of innovative sources of spatial animal and human health data
  including data warehouses, mHealth, Google Earth, volunteered geographic information
  and mining of internet-based big data sources such as Google and Twitter. We discuss
  the advantages, limitations and applications of each, and highlight studies where
  they have been used effectively.
author: Kim B. Stevens and Dirk U. Pfeiffer
categories: Geography, Planning and Development (Q1); Health, Toxicology and Mutagenesis
  (Q2); Infectious Diseases (Q2); Epidemiology (Q3)
citable_docs._(3years): 88.0
cites_/_doc._(2years): 163.0
country: United Kingdom
coverage: 2009-2020
doi: 10.1016/j.sste.2015.04.003
eigenfactor_score: .nan
h_index: 26.0
isbn: null
issn: '18775845'
issn1: '18775845'
issn2: '18775853'
issn3: '18775845'
jcr_value: null
keywords: Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered
  geographic information
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 3918.0
region: Western Europe
scimago_value: 726.0
sjr_best_quartile: Q1
sourceid: 19700167025.0
title_bib: 'Sources of spatial animal and human health data: casting the net wide
  to deal more effectively with increasingly complex disease problems'
title_csv: Spatial and spatio-temporal epidemiology
total_cites: .nan
total_cites_(3years): 169.0
total_docs._(2020): 40.0
total_docs._(3years): 90.0
total_refs.: 1567.0
type: journal
type_publication: article
year: 2015
---
abstract: "Background\nAfrica and the Caribbean are projected to have greater increases\
  \ in Head and neck cancer (HNC) burden in comparison to North America and Europe.\
  \ The knowledge needed to reinforce prevention in these populations is limited.\
  \ We compared for the first time, incidence rates of HNC in black populations from\
  \ African, the Caribbean and USA.\nMethods\nAnnual age-standardized incidence rates\
  \ (IR) and 95% confidence intervals (95%CI) per 100,000 were calculated for 2013\u2013\
  2015 using population-based cancer registry data for 14,911 HNC cases from the Caribbean\
  \ (Barbados, Guadeloupe, Trinidad & Tobago, N\u202F=\u202F443), Africa (Kenya, Nigeria,\
  \ N\u202F=\u202F772) and the United States (SEER, Florida, N\u202F=\u202F13,696).\
  \ We compared rates by sub-sites and sex among countries using data from registries\
  \ with high quality and completeness.\nResults\nIn 2013\u20132015, compared to other\
  \ countries, HNC incidence was highest among SEER states (IR: 18.2, 95%CI\u202F\
  =\u202F17.6\u201318.8) among men, and highest in Kenya (IR: 7.5, 95%CI\u202F=\u202F\
  6.3\u20138.7) among women. Nasopharyngeal cancer IR was higher in Kenya for men\
  \ (IR: 3.1, 95%CI\u202F=\u202F2.5\u20133.7) and women (IR: 1.5, 95%CI\u202F=\u202F\
  1.0\u20131.9). Female oral cavity cancer was also notably higher in Kenya (IR\u202F\
  =\u202F3.9, 95%CI\u202F=\u202F3.0\u20134.9). Blacks from SEER states had higher\
  \ incidence of laryngeal cancer (IR: 5.5, 95%CI\u202F=\u202F5.2\u20135.8) compared\
  \ to other countries and even Florida blacks (IR: 4.4, 95%CI\u202F=\u202F3.9\u2013\
  5.0).\nConclusion\nWe found heterogeneity in IRs for HNC among these diverse black\
  \ populations; notably, Kenya which had distinctively higher incidence of nasopharyngeal\
  \ and female oral cavity cancer. Targeted etiological investigations are warranted\
  \ considering the low consumption of tobacco and alcohol among Kenyan women. Overall,\
  \ our findings suggest that behavioral and environmental factors are more important\
  \ determinants of HNC than race."
author: "Aviane Auguste and Samuel Gathere and Paulo S. Pinheiro and Clement Adebamowo\
  \ and Adeola Akintola and Kellie Alleyne-Mike and Simon G. Anderson and Kimlin Ashing\
  \ and Fred Kwame Awittor and Baffour Awuah and Bernard Bhakkan and Jacqueline Deloumeaux\
  \ and Maira du\_Plessis and Ima-Obong A. Ekanem and Uwemedimbuk Ekanem and Emmanuel\
  \ Ezeome and Nkese Felix and Andrew K. Gachii and Stanie Gaete and Tracey Gibson\
  \ and Robert Hage and Sharon Harrison and Festus Igbinoba and Kufre Iseh and Evans\
  \ Kiptanui and Ann Korir and Heather-Dawn Lawson-Myers and Adana Llanos and Daniele\
  \ Luce and Dawn McNaughton and Michael Odutola and Abidemi Omonisi and Theresa Otu\
  \ and Jessica Peruvien and Nasiru Raheem and Veronica Roach and Natasha Sobers and\
  \ Nguundja Uamburu and Camille Ragin"
categories: Cancer Research (Q2); Epidemiology (Q2); Oncology (Q2)
citable_docs._(3years): 449.0
cites_/_doc._(2years): 270.0
country: Netherlands
coverage: 2009-2020
doi: 10.1016/j.canep.2021.102053
eigenfactor_score: 0.00808
h_index: 75.0
isbn: null
issn: '18777821'
issn1: '18777821'
issn2: 1877783X
issn3: '18777821'
jcr_value: '2.984'
keywords: Head and neck cancer, Incidence, Blacks, Tobacco smoking, Alcohol drinking,
  HPV, Caribbean, Africa, USA, Population-based cancer registry
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 3385.0
region: Western Europe
scimago_value: 1156.0
sjr_best_quartile: Q2
sourceid: 17700155032.0
title_bib: 'Heterogeneity in head and neck cancer incidence among black populations
  from africa, the caribbean and the usa: analysis of cancer registry data by the
  ac3'
title_csv: Cancer epidemiology
total_cites: 4347.0
total_cites_(3years): 1241.0
total_docs._(2020): 163.0
total_docs._(3years): 465.0
total_refs.: 5517.0
type: journal
type_publication: article
year: 2021
---
abstract: "Objective\nThe National Inpatient Sample (NIS) (the largest all-payer inpatient\
  \ database in the United States) is an important instrument for big data analysis\
  \ of neurosurgical inquiries. However, earlier research has determined that many\
  \ NIS studies are limited by common methodological pitfalls. In this study, we provide\
  \ the first primer of NIS methodological procedures in the setting of neurosurgical\
  \ research and review all reported neurosurgical studies using the NIS.\nMethods\n\
  We designed a protocol for neurosurgical big data research using the NIS, based\
  \ on our subject matter expertise, NIS documentation, and input and verification\
  \ from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive\
  \ search strategy to identify all neurosurgical studies using the NIS in the PubMed\
  \ and MEDLINE, Embase, and Web of Science databases from inception to August 2021.\
  \ Studies underwent qualitative categorization (years of NIS studied, neurosurgical\
  \ subspecialty, age group, and thematic focus of study objective) and analysis of\
  \ longitudinal trends.\nResults\nWe identified a canonical, 4-step protocol for\
  \ NIS analysis: study population selection; defining additional clinical variables;\
  \ identification and coding of outcomes; and statistical analysis. Methodological\
  \ nuances discussed include identifying neurosurgery-specific admissions, addressing\
  \ missing data, calculating additional severity and hospital-specific metrics, coding\
  \ perioperative complications, and applying survey weights to make nationwide estimates.\
  \ Inherent database limitations and common pitfalls of NIS studies discussed include\
  \ lack of disease process\u2013specific variables and data after the index admission,\
  \ inability to calculate certain hospital-specific variables after 2011, performing\
  \ state-level analyses, conflating hospitalization charges and costs, and not following\
  \ proper statistical methodology for performing survey-weighted regression. In a\
  \ systematic review, we identified 647 neurosurgical studies using the NIS. Although\
  \ almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data\
  \ after 2015. The average sample size of studies was 507,352 patients (standard\
  \ deviation\_= 2,739,900). Most studies analyzed cranial procedures (58.1%) and\
  \ adults (68.1%). The most prevalent topic areas analyzed were surgical outcome\
  \ trends (35.7%) and health policy and economics (17.8%), whereas patient disparities\
  \ (9.4%) and surgeon or hospital volume (6.6%) were the least studied.\nConclusions\n\
  We present a standardized methodology to analyze the NIS, systematically review\
  \ the state of the NIS neurosurgical literature, suggest potential future directions\
  \ for neurosurgical big data inquiries, and outline recommendations to improve the\
  \ design of future neurosurgical data instruments."
author: Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera
  Perla} and Robert J. Weil and Steven A. Toms
categories: Neurology (clinical) (Q2); Surgery (Q2)
citable_docs._(3years): 6927.0
cites_/_doc._(2years): 178.0
country: United States
coverage: 2010-2020
doi: 10.1016/j.wneu.2022.02.113
eigenfactor_score: 0.04375
h_index: 95.0
isbn: null
issn: '18788750'
issn1: '18788769'
issn2: '18788750'
issn3: '18788769'
jcr_value: '2.104'
keywords: Big data, Disparities, Health care costs, Health policy, Hospital volume,
  Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2422.0
region: Northern America
scimago_value: 734.0
sjr_best_quartile: Q2
sourceid: 19700171401.0
title_bib: 'The national inpatient sample: a primer for neurosurgical big data research
  and systematic review'
title_csv: World neurosurgery
total_cites: 23506.0
total_cites_(3years): 14126.0
total_docs._(2020): 2675.0
total_docs._(3years): 7491.0
total_refs.: 64788.0
type: journal
type_publication: article
year: 2022
---
abstract: 'Purpose

  A database in which patient data are compiled allows analytic opportunities for
  continuous improvements in treatment quality and comparative effectiveness research.
  We describe the development of a novel, web-based system that supports the collection
  of complex radiation treatment planning information from centers that use diverse
  techniques, software, and hardware for radiation oncology care in a statewide quality
  collaborative, the Michigan Radiation Oncology Quality Consortium (MROQC).

  Methods and materials

  The MROQC database seeks to enable assessment of physician- and patient-reported
  outcomes and quality improvement as a function of treatment planning and delivery
  techniques for breast and lung cancer patients. We created tools to collect anonymized
  data based on all plans.

  Results

  The MROQC system representing 24 institutions has been successfully deployed in
  the state of Michigan. Since 2012, dose-volume histogram and Digital Imaging and
  Communications in Medicine-radiation therapy plan data and information on simulation,
  planning, and delivery techniques have been collected. Audits indicated >90% accurate
  data submission and spurred refinements to data collection methodology.

  Conclusions

  This model web-based system captures detailed, high-quality radiation therapy dosimetry
  data along with patient- and physician-reported outcomes and clinical data for a
  radiation therapy collaborative quality initiative. The collaborative nature of
  the project has been integral to its success. Our methodology can be applied to
  setting up analogous consortiums and databases.'
author: Jean M. Moran and Mary Feng and Lisa A. Benedetti and Robin Marsh and Kent
  A. Griffith and Martha M. Matuszak and Michael Hess and Matthew McMullen and Jennifer
  H. Fisher and Teamour Nurushev and Margaret Grubb and Stephen Gardner and Daniel
  Nielsen and Reshma Jagsi and James A. Hayman and Lori J. Pierce
categories: Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)
citable_docs._(3years): 402.0
cites_/_doc._(2years): 238.0
country: Netherlands
coverage: 2011-2020
doi: 10.1016/j.prro.2016.10.002
eigenfactor_score: 0.00629
h_index: 35.0
isbn: null
issn: '18798500'
issn1: '18798500'
issn2: '18798519'
issn3: '18798500'
jcr_value: '3.539'
keywords: null
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 2387.0
region: Western Europe
scimago_value: 1142.0
sjr_best_quartile: Q1
sourceid: 19900194500.0
title_bib: Development of a model web-based system to support a statewide quality
  consortium in radiation oncology
title_csv: Practical radiation oncology
total_cites: 2610.0
total_cites_(3years): 1179.0
total_docs._(2020): 150.0
total_docs._(3years): 444.0
total_refs.: 3580.0
type: journal
type_publication: article
year: 2017
---
abstract: Recent technologic advancements have enabled the creation of portable, low-cost,
  and unobtrusive sensors with tremendous potential to alter the clinical practice
  of rehabilitation. The application of wearable sensors to track movement has emerged
  as a promising paradigm to enhance the care provided to patients with neurologic
  or musculoskeletal conditions. These sensors enable quantification of motor behavior
  across disparate patient populations and emerging research shows their potential
  for identifying motor biomarkers, differentiating between restitution and compensation
  motor recovery mechanisms, remote monitoring, telerehabilitation, and robotics.
  Moreover, the big data recorded across these applications serve as a pathway to
  personalized and precision medicine. This article presents state-of-the-art and
  next-generation wearable movement sensors, ranging from inertial measurement units
  to soft sensors. An overview of clinical applications is presented across a wide
  spectrum of conditions that have potential to benefit from wearable sensors, including
  stroke, movement disorders, knee osteoarthritis, and running injuries. Complementary
  applications enabled by next-generation sensors that will enable point-of-care monitoring
  of neural activity and muscle dynamics during movement also are discussed.
author: Franchino Porciuncula and Anna Virginia Roto and Deepak Kumar and Irene Davis
  and Serge Roy and Conor J. Walsh and Louis N. Awad
categories: Medicine (miscellaneous) (Q2); Physical Therapy, Sports Therapy and Rehabilitation
  (Q2); Rehabilitation (Q2); Neurology (Q3); Neurology (clinical) (Q3); Sports Science
  (Q3)
citable_docs._(3years): 620.0
cites_/_doc._(2years): 164.0
country: United States
coverage: 2009-2020
doi: 10.1016/j.pmrj.2018.06.013
eigenfactor_score: .nan
h_index: 66.0
isbn: null
issn: '19341482'
issn1: '19341482'
issn2: '19341563'
issn3: '19341482'
jcr_value: null
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 3019.0
region: Northern America
scimago_value: 617.0
sjr_best_quartile: Q2
sourceid: 15800154701.0
title_bib: 'Wearable movement sensors for rehabilitation: a focused review of technological
  and clinical advances'
title_csv: Pm and r
total_cites: .nan
total_cites_(3years): 1273.0
total_docs._(2020): 253.0
total_docs._(3years): 686.0
total_refs.: 7638.0
type: journal
type_publication: article
year: 2018
---
abstract: Service computing is an emerging technology in System of Systems Engineering
  (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing
  a robust and value-added complex system by outsourcing external component systems
  through service composition. The burgeoning Big Service computing just covers the
  significant challenges in constructing and maintaining a stable service-oriented
  SoS. A service-oriented SoS runs under a volatile and uncertain environment. As
  a step toward big service, service fault tolerance (FT) can guarantee the run-time
  quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability
  time series prediction, which aims at predicting the reliability in near future
  for a service-oriented SoS arises as a grand challenge in SoS research. In particular,
  we need to tackle a number of big data related issues given the large and fast increasing
  size of the historical data that will be used for prediction purpose. The decision-making
  of prediction solution space be more complex. To provide highly accurate prediction
  results, we tackle the prediction challenges by identifying the evolution regularities
  of component systems' running states via different machine learning models. We present
  in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform
  one-step-ahead online reliability time series prediction. We also propose a multi-steps
  trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability
  prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach
  is developed to deal with the big data challenges. Extensive experiments conducted
  on real-world Web services demonstrate that our models outperform other well-known
  approaches consistently.
author: Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin
categories: Computer Networks and Communications (Q1); Computer Science Applications
  (Q1); Hardware and Architecture (Q1); Information Systems and Management (Q1)
citable_docs._(3years): 261.0
cites_/_doc._(2years): 491.0
country: United States
coverage: 2008-2020
doi: 10.1109/TSC.2016.2633264
eigenfactor_score: 0.0057799999999999995
h_index: 70.0
isbn: null
issn: '19391374'
issn1: '19391374'
issn2: '19391374'
issn3: '19391374'
jcr_value: '8.216'
keywords: Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big
  data;Quality of service;Temporal evolution regularities;online reliability prediction;big
  service;convolutional neural networks
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 2618.0
region: Northern America
scimago_value: 1207.0
sjr_best_quartile: Q1
sourceid: 18300156728.0
title_bib: Learning the evolution regularities for bigservice-oriented online reliability
  prediction
title_csv: Ieee transactions on services computing
total_cites: 3719.0
total_cites_(3years): 1950.0
total_docs._(2020): 141.0
total_docs._(3years): 364.0
total_refs.: 3692.0
type: journal
type_publication: article
year: 2019
---
abstract: Todays' Intelligent Transportation System (ITS) applications majorly depend
  on either limited neighbouring traffic data or crowd sourced stale traffic data.
  Enabling big traffic data analytics in ITS environments is a step closer towards
  utilizing significant traffic patterns and trends for making more precise and intelligent
  decisions particularly in connected autonomous vehicular environments. Towards this
  end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for
  big traffic data centric ITS applications in connected autonomous vehicular environments.
  Specifically, TRADING balances offloading data traffic among gateways focusing on
  vehicular traffic and network status in the vicinity of gateways. In addition, TRADING
  mitigates the effect of gateway advertisement overhead to liberate the transmission
  channels for traffic big data transmission. The performance of TRADING is comparatively
  evaluated in a realistic simulation environment by considering gateway access overhead,
  load distribution among gateways, data offloading delay, and data offloading success
  ratio. The comparative performance evaluation results show some significant developments
  towards enabling big traffic data centric ITS.
author: Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash
  and Lloret, Jaime
categories: Aerospace Engineering (Q1); Applied Mathematics (Q1); Automotive Engineering
  (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering
  (Q1)
citable_docs._(3years): 3060.0
cites_/_doc._(2years): 780.0
country: United States
coverage: 1967-2020
doi: 10.1109/TVT.2020.2991372
eigenfactor_score: 0.05223
h_index: 178.0
isbn: null
issn: '19399359'
issn1: 00189545
issn2: '19399359'
issn3: 00189545
jcr_value: '5.978'
keywords: Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big
  data;gateway;intelligent transportation systems;VANET;vehicle-to-internet
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3495.0
region: Northern America
scimago_value: 1365.0
sjr_best_quartile: Q1
sourceid: 17393.0
title_bib: 'Trading: traffic aware data offloading for big data enabled intelligent
  transportation system'
title_csv: Ieee transactions on vehicular technology
total_cites: 36492.0
total_cites_(3years): 23276.0
total_docs._(2020): 1427.0
total_docs._(3years): 3074.0
total_refs.: 49867.0
type: journal
type_publication: article
year: 2020
---
abstract: DNA methylation (5mC) and hydroxymethylation (5hmC) are chemical modifications
  of cytosine bases which play a crucial role in epigenetic gene regulation. However,
  cost, data complexity and unavailability of comprehensive analytical tools is one
  of the major challenges in exploring these epigenetic marks. Hydroxymethylation-and
  Methylation-Sensitive Tag sequencing (HMST-seq) is one of the most cost-effective
  techniques that enables simultaneous detection of 5mC and 5hmC at single base pair
  resolution. We present HMST-Seq-Analyzer as a comprehensive and robust method for
  performing simultaneous differential methylation analysis on 5mC and 5hmC data sets.
  HMST-Seq-Analyzer can detect Differentially Methylated Regions (DMRs), annotate
  them, give a visual overview of methylation status and also perform preliminary
  quality check on the data. In addition to HMST-Seq, our tool can be used on whole-genome
  bisulfite sequencing (WGBS) and reduced representation bisulfite sequencing (RRBS)
  data sets as well. The tool is written in Python with capacity to process data in
  parallel and is available at (https://hmst-seq.github.io/hmst/).
author: "Amna Farooq and Sindre Gr\xF8nmyr and Omer Ali and Torbj\xF8rn Rognes and\
  \ Katja Scheffler and Magnar Bj\xF8r\xE5s and Junbai Wang"
categories: Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science
  Applications (Q1); Genetics (Q1); Structural Biology (Q2)
citable_docs._(3years): 255.0
cites_/_doc._(2years): 789.0
country: Sweden
coverage: 2012-2020
doi: 10.1016/j.csbj.2020.09.038
eigenfactor_score: 0.00677
h_index: 45.0
isbn: null
issn: '20010370'
issn1: '20010370'
issn2: '20010370'
issn3: '20010370'
jcr_value: '7.271'
keywords: Methylation analysis, Hydroxy methylation, Differential methylation, Hydroxymethylation-and
  methylation-sensitive tag sequencing, Whole genome bisulfite sequencing
publisher_x: Research Network of Computational and Structural Biotechnology
publisher_y: null
ref._/_doc.: 7869.0
region: Western Europe
scimago_value: 1908.0
sjr_best_quartile: Q1
sourceid: 21100318415.0
title_bib: 'Hmst-seq-analyzer: a new python tool for differential methylation and
  hydroxymethylation analysis in various dna methylation sequencing data'
title_csv: Computational and structural biotechnology journal
total_cites: 3620.0
total_cites_(3years): 2022.0
total_docs._(2020): 357.0
total_docs._(3years): 255.0
total_refs.: 28093.0
type: journal
type_publication: article
year: 2020
---
abstract: "ABSTRACT\nSports injuries, trauma and the globally ageing and obese population\
  \ require increasing levels of knee surgery. Shared decision making has replaced\
  \ the paternalistic approach to patient management. Evidence-based medicine underpins\
  \ surgical treatment strategies, from consenting an individual patient to national\
  \ healthcare system design. The evolution of successful knee-related registries\
  \ starting from specific arthroplasty registries has given rise to ligament reconstruction,\
  \ osteotomy and cartilage surgery registries developing as platforms for surgical\
  \ outcome data collection. Stakeholders include surgeons and their patients, researchers,\
  \ healthcare systems, as well as the funding insurers and governments. Lately, implant\
  \ manufacturers have also been mandated to perform postmarket surveillance with\
  \ some hoping to base that on registry data. Aiming to assess the current status\
  \ of knee-related registries, we performed a comprehensive literature and web search,\
  \ which yielded 23 arthroplasty, 8 ligament, 4 osteotomy and 3 articular cartilage\
  \ registries. Registries were evaluated for their scope, measured variables, impact\
  \ and limitations. Registries have many advantages as they aim to increase awareness\
  \ of outcomes; identify trends in practice over time, early failing implants, outlier\
  \ surgeon or institution performance; and assist postmarketing surveillance. International\
  \ collaborations have highlighted variations in practice. The limitations of registries\
  \ are discussed in detail. Inconsistencies are found in collected data and measured\
  \ variables. Potential measurement and selection biases are outlined. Without mandated\
  \ data collection and with apparent issues such as unverified patient reporting\
  \ of complications, registries are not designed to replace adverse event recording\
  \ in place of a proper safety and efficacy study, as demanded by regulators. Registry\
  \ \u2018big data\u2019 can provide evidence of associations of problems. However,\
  \ registries cannot provide evidence of causation. Hence, without careful consideration\
  \ of the data and its limitations, registry data are at risk of incorrectly drawn\
  \ conclusions and the potential of misuse of the results. That must be guarded against.\
  \ Looking at the future, registry operators benefit from a collective experience\
  \ of running registries as they mature, allowing for improvements across specialties.\
  \ Large-scale registries are not only of merit, improving with stakeholder acceptance,\
  \ but also are critical in furthering our understanding of our patients\u2019 outcomes.\
  \ In doing so, they are a critical element for our future scientific discourse."
author: Eran {Beit Ner} and Norimasa Nakamura and Christian Lattermann and Michael
  James McNicholas
categories: Surgery (Q2); Orthopedics and Sports Medicine (Q3)
citable_docs._(3years): 16.0
cites_/_doc._(2years): 72.0
country: United Kingdom
coverage: 2019-2020
doi: 10.1136/jisakos-2021-000625
eigenfactor_score: .nan
h_index: 3.0
isbn: null
issn: '20597754'
issn1: '20597762'
issn2: '20597754'
issn3: '20597762'
jcr_value: null
keywords: knee injuries, anterior cruciate ligament, arthroplasty, replacement, patient
  outcome assessment, osteotomy, articular cartilage restoration, registry, post marketing
  surveillance
publisher_x: BMJ Publishing Group
publisher_y: null
ref._/_doc.: 3840.0
region: Western Europe
scimago_value: 539.0
sjr_best_quartile: Q2
sourceid: 21100921027.0
title_bib: 'Knee registries: state of the art'
title_csv: Journal of isakos
total_cites: .nan
total_cites_(3years): 13.0
total_docs._(2020): 57.0
total_docs._(3years): 18.0
total_refs.: 2189.0
type: journal
type_publication: article
year: 2022
---
abstract: In order to improve the data quality, the big data cleaning method of distribution
  network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm
  based on DBSCAN clustering was used to detect outliers. However, due to the difficulty
  in determining the LOF threshold, a method of dynamically calculating the threshold
  based on the transformer districts and time was proposed. Besides, the LOF algorithm
  combines the statistical distribution method to reduce the "misjudgment rate". Aiming
  at the diversity and complexity of data missing forms in power big data, this paper
  improved the Random Forest imputation algorithm, which can be applied to various
  forms of missing data, especially the blocked missing data and even some horizontal
  or vertical data completely missing. The data in this paper were from real data
  of 44 transformer districts of a certain 10kV line in distribution network. Experimental
  results showed that outlier detection was accurate and suitable for any shape and
  multidimensional power big data. The improved Random Forest imputation algorithm
  was suitable for all missing forms, with higher imputation accuracy and better model
  stability. By comparing the network loss prediction between the data using this
  data cleaning method and the data removing outliers and missing values, it was found
  that the accuracy of network loss prediction had been improved by nearly 4 percentage
  points using the data cleaning method mentioned in this paper. Additionally, as
  the proportion of bad data increased, the difference between the prediction accuracy
  of cleaned data and that of uncleaned data was greater.
author: Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei
categories: Electrical and Electronic Engineering (Q4); Electronic, Optical and Magnetic
  Materials (Q4); Energy (miscellaneous) (Q4)
citable_docs._(3years): 148.0
cites_/_doc._(2years): 18.0
country: United States
coverage: '2020'
doi: 10.17775/CSEEJPES.2020.04080
eigenfactor_score: 0.0027
h_index: 4.0
isbn: null
issn: '20960042'
issn1: '20960042'
issn2: '20960042'
issn3: '20960042'
jcr_value: '3.938'
keywords: Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering
  algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing
  data imputation;LOF;DBSCAN;Random Forest
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3800.0
region: Northern America
scimago_value: 118.0
sjr_best_quartile: Q4
sourceid: 21101017898.0
title_bib: A big data cleaning method based on improved clof and random forest for
  distribution network
title_csv: Csee journal of power and energy systems
total_cites: 1205.0
total_cites_(3years): 27.0
total_docs._(2020): 43.0
total_docs._(3years): 148.0
total_refs.: 1634.0
type: journal
type_publication: article
year: 2020
---
abstract: Missing data filling is a key step in power big data preprocessing, which
  helps to improve the quality and the utilization of electric power data. Due to
  the limitations of the traditional methods of filling missing data, an improved
  random forest filling algorithm is proposed. As a result of the horizontal and vertical
  directions of the electric power data are based on the characteristics of time series.
  Therefore, the method of improved random forest filling missing data combines the
  methods of linear interpolation, matrix combination and matrix transposition to
  solve the problem of filling large amount of electric power missing data. The filling
  results show that the improved random forest filling algorithm is applicable to
  filling electric power data in various missing forms. What's more, the accuracy
  of the filling results is high and the stability of the model is strong, which is
  beneficial in improving the quality of electric power data.
author: Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu,
  Liang
categories: Control and Systems Engineering; Electrical and Electronic Engineering;
  Energy Engineering and Power Technology
citable_docs._(3years): 0.0
cites_/_doc._(2years): 0.0
country: United States
coverage: '2020'
doi: 10.23919/CJEE.2019.000025
eigenfactor_score: .nan
h_index: 2.0
isbn: null
issn: '20961529'
issn1: '20961529'
issn2: '20961529'
issn3: '20961529'
jcr_value: null
keywords: Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data
  mining;Big data cleaning;missing data filling;data preprocessing;random forest;data
  quality
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3180.0
region: Northern America
scimago_value: .nan
sjr_best_quartile: '-'
sourceid: 21101018551.0
title_bib: A missing power data filling method based on improved random forest algorithm
title_csv: Chinese journal of electrical engineering
total_cites: .nan
total_cites_(3years): 0.0
total_docs._(2020): 35.0
total_docs._(3years): 0.0
total_refs.: 1113.0
type: journal
type_publication: article
year: 2019
---
abstract: Software 2.0 refers to the fundamental shift in software engineering where
  using machine learning becomes the new norm in software with the availability of
  big data and computing infrastructure. As a result, many software engineering practices
  need to be rethought from scratch where data becomes a first-class citizen, on par
  with code. It is well known that 80--90% of the time for machine learning development
  is spent on data preparation. Also, even the best machine learning algorithms cannot
  perform well without good data or at least handling biased and dirty data during
  model training. In this tutorial, we focus on data collection and quality challenges
  that frequently occur in deep learning applications. Compared to traditional machine
  learning, there is less need for feature engineering, but more need for significant
  amounts of data. We thus go through state-of-the-art data collection techniques
  for machine learning. Then, we cover data validation and cleaning techniques for
  improving data quality. Even if the data is still problematic, hope is not lost,
  and we cover fair and robust training techniques for handling data bias and errors.
  We believe that the data management community is well poised to lead the research
  in these directions. The presenters have extensive experience in developing machine
  learning platforms and publishing papers in top-tier database, data mining, and
  machine learning venues.
author: Whang, Steven Euijong and Lee, Jae-Gil
categories: Computer Science (miscellaneous) (Q1)
citable_docs._(3years): 566.0
cites_/_doc._(2years): 550.0
country: United States
coverage: 2008-2020
doi: 10.14778/3415478.3415562
eigenfactor_score: 0.00891
h_index: 134.0
isbn: null
issn: '21508097'
issn1: '21508097'
issn2: '21508097'
issn3: '21508097'
jcr_value: '2.047'
keywords: null
publisher_x: null
publisher_y: VLDB Endowment
ref._/_doc.: 5041.0
region: Northern America
scimago_value: 946.0
sjr_best_quartile: Q1
sourceid: 21100199855.0
title_bib: Data collection and quality challenges for deep learning
title_csv: Proceedings of the vldb endowment
total_cites: 7198.0
total_cites_(3years): 3759.0
total_docs._(2020): 246.0
total_docs._(3years): 598.0
total_refs.: 12401.0
type: journal
type_publication: article
year: 2020
---
abstract: We live in an age where data acquisition is no longer a problem and the
  real challenge is how to determine which information is the right one to take important
  and sometimes difficult decisions. Infoxication (also known as Infobesity or Information
  Overload) is a term used to describe the difficulty of adapting to new situations
  and effectively making decisions when there is too much information to manage. With
  the advent of the Big Data, infoxication is affecting critical domains such as Health
  Sciences, where tough decisions for patient's health is being taken every day based
  on heterogeneous, unconnected and sometimes conflicting information. In order to
  understand the magnitude of the challenge, based on the information publicly available
  about the genetic causes of the disease and using data quality assessment techniques,
  we performed an exhaustive analysis of the DNA variations that have been associated
  to the risk of suffering migraine headache. The same analysis has been repeated
  8 months after, and the results have allowed us to exemplify i) how fragile is the
  information in this domain, ii) the difficulty of finding repositories of contrasted
  and reliable data, and iii) the need to have information systems that, far from
  integrating and storing huge volumes of data, are able to support the decision-making
  process by providing mechanisms agile and flexible enough to be able to adapt to
  the changing user needs.
author: "Palacio, Ana Le\xF3n and L\xF3pez, \xD3scar Pastor"
categories: Computer Science Applications; Information Systems; Software
citable_docs._(3years): 146.0
cites_/_doc._(2years): 81.0
country: United States
coverage: 2011, 2012, 2013, 2014, 2015
doi: 10.1109/RCIS.2019.8877003
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '21511357'
issn1: '21511357'
issn2: '21511349'
issn3: '21511357'
jcr_value: null
keywords: Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information
  Systems;SILE method
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 174.0
sjr_best_quartile: '-'
sourceid: 20300195007.0
title_bib: Infoxication in the genomic data era and implications in the development
  of information systems
title_csv: Proceedings - international conference on research challenges in information
  science
total_cites: .nan
total_cites_(3years): 116.0
total_docs._(2020): 0.0
total_docs._(3years): 150.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2019
---
abstract: Model-driven engineering (MDE) often features quality assurance (QA) techniques
  to help developers creating software that meets reliability, efficiency, and safety
  requirements. In this paper, we consider the question of how quality-aware MDE should
  support data-intensive software systems. This is a difficult challenge, since existing
  models and QA techniques largely ignore properties of data such as volumes, velocities,
  or data location. Furthermore, QA requires the ability to characterize the behavior
  of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which
  are poorly understood from a modeling standpoint. To foster a community response
  to these challenges, we present the research agenda of DICE, a quality-aware MDE
  methodology for data-intensive cloud applications. DICE aims at developing a quality
  engineering tool chain offering simulation, verification, and architectural optimization
  for Big Data applications. We overview some key challenges involved in developing
  these tools and the underpinning models.
author: Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck
  and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe
  and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu,
  Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel
categories: Hardware and Architecture; Software
citable_docs._(3years): 23.0
cites_/_doc._(2years): 304.0
country: United States
coverage: 2012, 2013, 2019
doi: 10.1109/MiSE.2015.21
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '21567891'
issn1: '21567891'
issn2: '21572305'
issn3: '21567891'
jcr_value: null
keywords: Unified modeling language;Big data;Data models;Computational modeling;Analytical
  models;Reliability;Software;Big Data;quality assurance;model-driven engineering
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 197.0
sjr_best_quartile: '-'
sourceid: 21100211110.0
title_bib: 'Dice: quality-driven development of data-intensive cloud applications'
title_csv: Icse workshop on software engineering for adaptive and self-managing systems
total_cites: .nan
total_cites_(3years): 79.0
total_docs._(2020): 0.0
total_docs._(3years): 26.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2015
---
abstract: We are witnessing a rapid growth of electrified vehicles due to the ever-increasing
  concerns on urban air quality and energy security. Compared to other types of electric
  vehicles, electric buses have not yet been prevailingly adopted worldwide due to
  their high owning and operating costs, long charging time, and the uneven spatial
  distribution of charging facilities. Moreover, the highly dynamic environment factors
  such as unpredictable traffic congestion, different passenger demands, and even
  the changing weather can significantly affect electric bus charging efficiency and
  potentially hinder the further promotion of large-scale electric bus fleets. To
  address these issues, in this article, we first analyze a real-world dataset including
  massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then,
  we investigate the electric bus network to understand its operating and charging
  patterns, and further verify the necessity and feasibility of a real-time charging
  scheduling. With such understanding, we design busCharging, a pricing-aware real-time
  charging scheduling system based on Markov Decision Process to reduce the overall
  charging and operating costs for city-scale electric bus fleets, taking the time-variant
  electricity pricing into account. To show the effectiveness of busCharging, we implement
  it with the real-world data from Shenzhen, which includes GPS data of electric buses,
  the metadata of all bus lines and bus stops, combined with data of 376 charging
  stations for electric buses. The evaluation results show that busCharging dramatically
  reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously.
  Finally, we design a scheduling-based charging station expansion strategy to verify
  our busCharging is also effective during the charging station expansion process.
author: Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun
  and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng
categories: Artificial Intelligence (Q1); Theoretical Computer Science (Q1)
citable_docs._(3years): 192.0
cites_/_doc._(2years): 980.0
country: United States
coverage: 2010-2020
doi: 10.1145/3428080
eigenfactor_score: 0.00463
h_index: 63.0
isbn: null
issn: '21576904'
issn1: '21576912'
issn2: '21576904'
issn3: '21576912'
jcr_value: '4.654'
keywords: charging scheduling, charging pattern, MDP, Electric bus, data driven
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 5920.0
region: Northern America
scimago_value: 914.0
sjr_best_quartile: Q1
sourceid: 19700190323.0
title_bib: Pricing-aware real-time charging scheduling and charging station expansion
  for large-scale electric buses
title_csv: Acm transactions on intelligent systems and technology
total_cites: 4104.0
total_cites_(3years): 1489.0
total_docs._(2020): 70.0
total_docs._(3years): 199.0
total_refs.: 4144.0
type: journal
type_publication: article
year: 2020
---
abstract: "The number and volume of remote sensing data and its derived products,\
  \ which are regarded as typical \u201Cbig data\u201D, have grown exponentially.\
  \ How to assess the quality of these big remote sensing products become a challenge.\
  \ As an importance technique, spatial sampling is regarded to be necessary for the\
  \ quality assessment of remote sensing derived products. This paper proposes an\
  \ approach of multiple stratified spatial sampling for assessing the remote sensing\
  \ products, with the aim of resolving the issue of the quality inspection of big\
  \ remote sensing products. The proposed method improves the sampling accuracy without\
  \ increasing the sampling size, and the whole procedure is repeatable and easily\
  \ adopted for the quality inspection of remote sensing derived products."
author: Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong
categories: Computer Vision and Pattern Recognition; Signal Processing
citable_docs._(3years): 246.0
cites_/_doc._(2years): 60.0
country: United States
coverage: 2011, 2012, 2017, 2019
doi: 10.1109/WHISPERS.2015.8075416
eigenfactor_score: .nan
h_index: 16.0
isbn: null
issn: '21586276'
issn1: '21586276'
issn2: '21586276'
issn3: '21586276'
jcr_value: null
keywords: Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple
  stratified;spatial sampling;quality assessment;remote sensing products;big data
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 174.0
sjr_best_quartile: '-'
sourceid: 21000195601.0
title_bib: Multiple stratified sampling strategy for assessing the big remote sensing
  products
title_csv: Workshop on hyperspectral image and signal processing, evolution in remote
  sensing
total_cites: .nan
total_cites_(3years): 126.0
total_docs._(2020): 0.0
total_docs._(3years): 251.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2015
---
abstract: Software-as-a-Service (SaaS) is a model of cloud computing in which software
  functions are delivered to the users as services. The past few years have witnessed
  its global flourishing. In the foreseeable future, SaaS applications will integrate
  with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks,
  and many other computing and communication technologies to deliver customizable
  intelligent services to a vast population. This will give rise to an era of what
  we call Big SaaS systems of unprecedented complexity and scale. They will have huge
  numbers of tenants/users interrelated in complex ways. The code will be complex
  too and require Big Data but provide great value to the customer. With these benefits
  come great societal risks, however, and there are other drawbacks and challenges.
  For example, it is difficult to ensure the quality of data and metadata obtained
  from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS
  applications will also need to evolve continuously. This paper will discuss how
  to address these challenges at all stages of the software lifecycle.
author: Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef,
  Basel and Liu, Dongmei
categories: Artificial Intelligence; Information Systems; Software
citable_docs._(3years): 483.0
cites_/_doc._(2years): 253.0
country: United States
coverage: 2013, 2014, 2019
doi: 10.1109/CLOUD.2015.167
eigenfactor_score: .nan
h_index: 29.0
isbn: null
issn: '21596190'
issn1: '21596182'
issn2: '21596190'
issn3: '21596182'
jcr_value: null
keywords: Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer
  architecture
publisher_x: null
publisher_y: null
ref._/_doc.: 2080.0
region: Northern America
scimago_value: 352.0
sjr_best_quartile: '-'
sourceid: 21100291859.0
title_bib: 'Big saas: the next step beyond big data'
title_csv: Ieee international conference on cloud computing, cloud
total_cites: .nan
total_cites_(3years): 1024.0
total_docs._(2020): 92.0
total_docs._(3years): 496.0
total_refs.: 1914.0
type: conference and proceedings
type_publication: inproceedings
year: 2015
---
abstract: Data quality plays an important role in modern intelligent information system
  and is crucial to any data analysis task. Many imperfection-handling techniques
  avoid overfitting or simply remove offending portions of the data. Data correction
  can help to retain and recover as much information as possible from the original
  data resources. In this paper, we proposed a novel technique based on polynomial
  smooth support vector machine. The quadratic polynomial and the first degree of
  polynomial as the support vector machine smooth functions are investigated. At the
  same time, the function was used as smooth function to calculate compensation values.
  In order to show the procedures of our algorithm, some necessary steps need to be
  considered. Firstly, the original data are normalized, so as to eliminate experimental
  effects of dimensional problems. Secondly, the three different kinds of smooth functions
  need to be analysed mathematically. The difference measure are calculated to make
  sure the results of correction through different data correction models. The results
  of given noised data sets can show that the proposed the data correction method
  based on polynomial smooth support vector machine is effectiveness.
author: Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo
categories: Artificial Intelligence; Computational Theory and Mathematics; Computer
  Networks and Communications; Human-Computer Interaction
citable_docs._(3years): 219.0
cites_/_doc._(2years): 50.0
country: United States
coverage: 2011, 2012, 2013, 2014, 2019
doi: 10.1109/ICMLC.2016.7872993
eigenfactor_score: .nan
h_index: 18.0
isbn: null
issn: '21601348'
issn1: '21601348'
issn2: 2160133X
issn3: '21601348'
jcr_value: null
keywords: Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big
  data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data
  Mining
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 149.0
sjr_best_quartile: '-'
sourceid: 20300195054.0
title_bib: A dynamic data correction algorithm based on polynomial smooth support
  vector machine
title_csv: Proceedings - international conference on machine learning and cybernetics
total_cites: .nan
total_cites_(3years): 114.0
total_docs._(2020): 0.0
total_docs._(3years): 227.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2016
---
abstract: Modern mobile devices are capable of running sophisticated, network-enabled
  applications exploiting a variety of sensors on a single low-cost piece of hardware.
  The electrical industry can benefit from these new platforms to automate existing
  processes and provide engineers and field crew with access to large amounts of complex
  data in real-time, anywhere in the world. The development of a standards-based application
  decouples the mobile client application from a single vendor or existing enterprise
  system, but requires a complex data integration architecture to support the use
  and exploitation of large amounts of data spread across multiple existing systems.
  The integration with a mobile application introduces new challenges when dealing
  with remote devices where data network communications cannot be relied on, especially
  under storm conditions, and the devices themselves are at risk of being lost or
  stolen. Addressing these challenges offers the potential to improve data quality,
  enable access to accurate, up-to-date information in the field and ultimately save
  a utility time and money.
author: McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough,
  N. and Stewart, E. M.
categories: Electrical and Electronic Engineering; Energy (miscellaneous); Engineering
  (miscellaneous)
citable_docs._(3years): 211.0
cites_/_doc._(2years): 86.0
country: United States
coverage: 1994, 1996, 1999-2003, 2005-2006, 2012, 2014, 2016, 2018
doi: 10.1109/TDC.2014.6863306
eigenfactor_score: .nan
h_index: 65.0
isbn: null
issn: '21608563'
issn1: '21608563'
issn2: '21608555'
issn3: '21608563'
jcr_value: null
keywords: Computer integrated manufacturing;Mobile communication;IEC standards;Logic
  gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data
  handling;Data visualization;CIM;Data integration;Big Data
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 1256.0
region: Northern America
scimago_value: 279.0
sjr_best_quartile: '-'
sourceid: 87075.0
title_bib: Data integration challenges for standards-compliant mobile applications
title_csv: Proceedings of the ieee power engineering society transmission and distribution
  conference
total_cites: .nan
total_cites_(3years): 182.0
total_docs._(2020): 160.0
total_docs._(3years): 212.0
total_refs.: 2010.0
type: conference and proceedings
type_publication: inproceedings
year: 2014
---
abstract: 'Along with the development of the sensor system and sensor network, the
  wide applications of sensor networks have arisen at the historic moment. In reality,
  all kinds of sensors monitor every aspect of our life, which provides various services
  and brings the challenge: how to effectively integrate those distributed sensor
  resources and then can be used to find more advanced information or implement the
  sharing of resources are the big problems to be solved. Based on the framework of
  Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined
  with the function of web crawler, we study and find Sensor Observation Service (SOS)
  service which is the core components of the SWE then we design a system based on
  the web crawler technology and the Istituto Scienze della Terra Sensor Observation
  Service (Istsos) architecture. The design of sensor network technology integration
  architecture includes three parts. The layer of data access which is the lowest
  layer encapsulates the access to the database or other source of resources. The
  layer of business logic it provides the core operation of component which was named
  Request Operator, this layer is used for processing various requests from the lowest
  layer in order to return the classes of listening. The layer of web and the client
  is connected, which can provide some thin client of SOS. The published server includes
  the ability of new services creation, addition of new sensors and relative metadata,
  visualization, and manipulation of stored observations, registration of new measures
  and setting of system properties like observable properties and data quality codes.
  In order to get sensor data, web crawler technology is used in our research, which
  can make us get sensor data from the target website, and the standardized sensor
  data is gotten by filtering the original data and then the data is uploaded to the
  database of Istsos with the standardized format. At last, the implementation of
  SOS architecture has been configured. The test''s results show that the integrated
  architecture of services can effectively obtain the required sensor data and display
  them graphically.'
author: Yan Zhou and Haitian Xie
categories: Computer Networks and Communications (Q4); Computer Vision and Pattern
  Recognition (Q4); Electrical and Electronic Engineering (Q4); Geography, Planning
  and Development (Q4); Information Systems (Q4); Software (Q4)
citable_docs._(3years): 209.0
cites_/_doc._(2years): 30.0
country: United States
coverage: 2013, 2016-2018
doi: 10.1109/GEOINFORMATICS.2015.7378670
eigenfactor_score: .nan
h_index: 8.0
isbn: null
issn: 2161024X
issn1: 2161024X
issn2: '21610258'
issn3: 2161024X
jcr_value: null
keywords: Service-oriented architecture;web crawler;sensor network;Sensor Observation
  Service (SOS);Tomact;Istsos
publisher_x: International Association of Chinese Professionals in Geographic Information
  Sciences
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 132.0
sjr_best_quartile: Q4
sourceid: 21100285411.0
title_bib: The integration technology of sensor network based on web crawler
title_csv: International conference on geoinformatics
total_cites: .nan
total_cites_(3years): 76.0
total_docs._(2020): 0.0
total_docs._(3years): 212.0
total_refs.: 0.0
type: journal
type_publication: inproceedings
year: 2015
---
abstract: Data analysis is a demanding task that involves extracting deep insights
  hidden in data. Many businesses enforce data analysis irrespective of the domain,
  as it is crucial in minimizing developmental risks. Raw data cannot be used to perform
  any analysis as poor-quality data leads to erroneous decision-making. This makes
  data quality assessment a necessary function before data analysis. Data quality
  is a multi-dimensional factor that affects the analysis in multiple ways. Among
  all the dimensions, consistency is one of the most critical dimensions to assess.
  Context of data plays an important role in consistency assessment, as the records
  are inherently related within a dataset. Existing studies are computationally expensive
  and do not consider the context of data. In this paper, we propose a comprehensive
  context-aware data consistency assessment tool that uses machine learning to evaluate
  the consistency of data. Our model was developed on Apache Hadoop and Apache Spark
  to support big data, as well as to boost some computationally intensive algorithms.
author: Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.
categories: Computer Networks and Communications; Computer Science Applications; Control
  and Systems Engineering; Electrical and Electronic Engineering; Hardware and Architecture;
  Signal Processing
citable_docs._(3years): 507.0
cites_/_doc._(2years): 62.0
country: United States
coverage: 2001, 2011, 2013, 2019
doi: 10.1109/AICCSA47632.2019.9035250
eigenfactor_score: .nan
h_index: 18.0
isbn: null
issn: '21615330'
issn1: '21615330'
issn2: '21615322'
issn3: '21615330'
jcr_value: null
keywords: Feature extraction;Data analysis;Data integrity;Data models;Machine learning
  algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data
  consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache
  hadoop;apache spark
publisher_x: null
publisher_y: null
ref._/_doc.: 2344.0
region: Northern America
scimago_value: 164.0
sjr_best_quartile: '-'
sourceid: 21100198533.0
title_bib: Assessing context-aware data consistency
title_csv: Proceedings of ieee/acs international conference on computer systems and
  applications, aiccsa
total_cites: .nan
total_cites_(3years): 308.0
total_docs._(2020): 59.0
total_docs._(3years): 517.0
total_refs.: 1383.0
type: conference and proceedings
type_publication: inproceedings
year: 2019
---
abstract: "This paper proposes a concept of \u201Cdata-driven, lean-oriented and closed-loop\u201D\
  \ management for distribution network and explain how to implement this kind of\
  \ management, as shown in fig.1 Firstly, a big data platform is constructed to integrate\
  \ and combine the multi-source data. Secondly, big data analysis technologies such\
  \ as data mining, machine learning and data visualization are applied to solve problems\
  \ in distribution network production. For example, accurate location of the fault\
  \ can be found with help of multisource information from different devices and systems.\
  \ And we can also be aware of the risk points in distribution network through history\
  \ data analysis. Finally, this Paper explains how to promote lean management of\
  \ distribution network in the fields of asset, operation, maintenance and investment\
  \ based on the big data platform and big data analysis methods. In addition, the\
  \ feedback procedure sets up a bridge between application and data collecting, which\
  \ further improve the data quality. Those management measure have been piloted in\
  \ several cities in Jiangsu. The result proves that they can improve power supply\
  \ reliability and reduce operating costs significantly. Two practical cases are\
  \ given to show how they work."
author: Hao, Jiao and Jinming, Chen and Yajuan, Guo
categories: Control and Systems Engineering; Electrical and Electronic Engineering;
  Energy Engineering and Power Technology
citable_docs._(3years): 519.0
cites_/_doc._(2years): 47.0
country: United States
coverage: 2012, 2014
doi: 10.1109/CICED.2018.8592556
eigenfactor_score: .nan
h_index: 10.0
isbn: null
issn: 2161749X
issn1: 2161749X
issn2: '21617481'
issn3: 2161749X
jcr_value: null
keywords: Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles
  and towers;data-driven;lean management;closed-loop;big data analysis
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 133.0
sjr_best_quartile: '-'
sourceid: 21100241614.0
title_bib: Data-driven lean management for distribution network
title_csv: China international conference on electricity distribution, ciced
total_cites: .nan
total_cites_(3years): 245.0
total_docs._(2020): 0.0
total_docs._(3years): 520.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2018
---
abstract: Although the solar energy industry is becoming widespread, it is necessary
  to manage the charging and generating scheduling of solar power generation according
  to the ever-changing climate environment. In order to do this, a judgment criterion
  that can give timely charge / discharge instructions is needed and it needs to be
  actively performed. In this paper, we define a big-data platform for residential
  heat energy consumption. As a technology to secure thermal energy data of apartment
  houses, collect thermal energy data by dividing it into supply/equipment/usage.
  In order to secure standardized thermal energy data from the calorimeter installed.
  Equipped with data classification and processing, LP storage and management, data
  quality measurement and analysis functions. Develop a data adapter, from several
  multiunit dwellings with different calorimeter types. We will collect thermal energy
  data with an integrated big data system.
author: Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon
categories: Computer Networks and Communications; Information Systems
citable_docs._(3years): 0.0
cites_/_doc._(2years): 0.0
country: United States
coverage: 2012-2014
doi: 10.1109/ICTC52510.2021.9620761
eigenfactor_score: .nan
h_index: 18.0
isbn: null
issn: '21621233'
issn1: '21621241'
issn2: '21621233'
issn3: '21621241'
jcr_value: null
keywords: Energy consumption;Temperature distribution;Water storage;Data integrity;Water
  heating;Solar energy;Big Data;energy management;energy big data;energy information
  collection
publisher_x: IEEE Computer Society
publisher_y: null
ref._/_doc.: 1137.0
region: Northern America
scimago_value: .nan
sjr_best_quartile: '-'
sourceid: 21100226431.0
title_bib: Mechanism of a big-data platform for residential heat energy consumption
title_csv: International conference on ict convergence
total_cites: .nan
total_cites_(3years): 0.0
total_docs._(2020): 493.0
total_docs._(3years): 0.0
total_refs.: 5606.0
type: conference and proceedings
type_publication: inproceedings
year: 2021
---
abstract: Product quality prediction, as an important issue of industrial intelligence,
  is a typical task of industrial process analysis, in which product quality will
  be evaluated and improved as feedback for industrial process adjustment. Data-driven
  methods, with predictive model to analyze various industrial data, have been received
  considerable attention in recent years. However, to get an accurate prediction,
  it is an essential issue to extract quality features from industrial data, including
  several variables generated from supply chain and time-variant machining process.
  In this article, a data-driven method based on wide-deep-sequence (WDS) model is
  proposed to provide a reliable quality prediction for industrial process with different
  types of industrial data. To process industrial data of high redundancy, in this
  article, data reduction is first conducted on different variables by different techniques.
  Also, an improved wide-deep (WD) model is proposed to extract quality features from
  key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based
  sequence model is presented for exploring quality information from time-domain features.
  Under the joint training strategy, these models will be combined and optimized by
  a designed penalty mechanism for unreliable predictions, especially on reduction
  of defective products. Finally, experiments on a real-world manufacturing process
  data set are carried out to present the effectiveness of the proposed method in
  product quality prediction.
author: Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence
  T.
categories: Artificial Intelligence (Q1); Computer Networks and Communications (Q1);
  Computer Science Applications (Q1); Software (Q1)
citable_docs._(3years): 1107.0
cites_/_doc._(2years): 1251.0
country: United States
coverage: 2012-2020
doi: 10.1109/TNNLS.2020.3001602
eigenfactor_score: 0.04868
h_index: 212.0
isbn: null
issn: '21622388'
issn1: 2162237X
issn2: '21622388'
issn3: 2162237X
jcr_value: '10.451'
keywords: Feature extraction;Predictive models;Data models;Quality assessment;Product
  design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial
  big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence
  (WDS) model
publisher_x: IEEE Computational Intelligence Society
publisher_y: null
ref._/_doc.: 3746.0
region: Northern America
scimago_value: 2882.0
sjr_best_quartile: Q1
sourceid: 21100235616.0
title_bib: A wide-deep-sequence model-based quality prediction method in industrial
  process analysis
title_csv: Ieee transactions on neural networks and learning systems
total_cites: 36361.0
total_cites_(3years): 14914.0
total_docs._(2020): 609.0
total_docs._(3years): 1117.0
total_refs.: 22815.0
type: journal
type_publication: article
year: 2020
---
abstract: Generating highly accurate predictions for missing quality-of-service (QoS)
  data is an important issue. Latent factor (LF)-based QoS-predictors have proven
  to be effective in dealing with it. However, they are based on first-order solvers
  that cannot well address their target problem that is inherently bilinear and nonconvex,
  thereby leaving a significant opportunity for accuracy improvement. This paper proposes
  to incorporate an efficient second-order solver into them to raise their accuracy.
  To do so, we adopt the principle of Hessian-free optimization and successfully avoid
  the direct manipulation of a Hessian matrix, by employing the efficiently obtainable
  product between its Gauss-Newton approximation and an arbitrary vector. Thus, the
  second-order information is innovatively integrated into them. Experimental results
  on two industrial QoS datasets indicate that compared with the state-of-the-art
  predictors, the newly proposed one achieves significantly higher prediction accuracy
  at the expense of affordable computational burden. Hence, it is especially suitable
  for industrial applications requiring high prediction accuracy of unknown QoS data.
author: Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong
  and Zhu, QingSheng and Leung, Hareton
categories: Computer Science Applications (Q1); Control and Systems Engineering (Q1);
  Electrical and Electronic Engineering (Q1); Human-Computer Interaction (Q1); Information
  Systems (Q1); Software (Q1)
citable_docs._(3years): 997.0
cites_/_doc._(2years): 1119.0
country: United States
coverage: 2013-2020
doi: 10.1109/TCYB.2017.2685521
eigenfactor_score: 0.05214
h_index: 124.0
isbn: null
issn: '21682275'
issn1: '21682275'
issn2: '21682267'
issn3: '21682275'
jcr_value: '11.448'
keywords: Quality of service;Predictive models;Optimization;Computational modeling;Mathematical
  model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service
  (QoS);second-order solver;service computing sparse matrices;Web service
publisher_x: IEEE Advancing Technology for Humanity
publisher_y: null
ref._/_doc.: 3458.0
region: Northern America
scimago_value: 3109.0
sjr_best_quartile: Q1
sourceid: 21100274221.0
title_bib: Incorporation of efficient second-order solvers into latent factor models
  for accurate prediction of missing qos data
title_csv: Ieee transactions on cybernetics
total_cites: 24753.0
total_cites_(3years): 13312.0
total_docs._(2020): 542.0
total_docs._(3years): 1065.0
total_refs.: 18740.0
type: journal
type_publication: article
year: 2018
---
abstract: "Nowadays, the big data paradigm is consolidating its central position in\
  \ the industry, as well as in society at large. Lots of applications, across disparate\
  \ domains, operate on huge amounts of data and offer great advantages both for business\
  \ and research. According to analysts, cloud computing adoption is steadily increasing\
  \ to support big data analyses and Spark is expected to take a prominent market\
  \ position for the next decade. As big data applications gain more and more importance\
  \ over time and given the dynamic nature of cloud resources, it is fundamental to\
  \ develop an intelligent resource management system to provide Quality of Service\
  \ guarantees to end-users. This article presents a set of run-time optimization-based\
  \ resource management policies for advanced big data analytics. Users submit Spark\
  \ applications characterized by a priority and by a hard or soft deadline. Optimization\
  \ policies address two scenarios: i) identification of the minimum capacity to run\
  \ a Spark application within the deadline; ii) re-balance of the cloud resources\
  \ in case of heavy load, minimising the weighted soft deadline application tardiness.\
  \ The solution relies on an initial non-linear programming model formulation and\
  \ a search space exploration based on simulation-optimization procedures. Spark\
  \ application execution times are estimated by relying on a gamut of techniques,\
  \ including machine learning, approximated analyses, and simulation. The benefits\
  \ of the approach are evaluated on Microsoft Azure HDInsight and on a private cloud\
  \ cluster based on POWER8 by considering the TPC-DS industry benchmark and SparkBench.\
  \ The results obtained in the first scenario demonstrate that the percentage error\
  \ of the prediction of the optimal resource usage with respect to system measurement\
  \ and exhaustive search is in the range 4\u201329 percent while literature-based\
  \ techniques present an average error in the range 6\u201363 percent. Moreover,\
  \ in the second scenario, the proposed algorithms can address complex problems like\
  \ computing the optimal redistribution of resources among tens of applications in\
  \ less than a minute with an error of 8 percent on average. On the same considered\
  \ tests, literature-based approaches obtain an average error of about 57 percent."
author: Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna,
  Danilo
categories: Computer Networks and Communications (Q1); Computer Science Applications
  (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1)
citable_docs._(3years): 223.0
cites_/_doc._(2years): 485.0
country: United States
coverage: 2013-2020
doi: 10.1109/TCC.2020.2985682
eigenfactor_score: 0.00415
h_index: 49.0
isbn: null
issn: '21687161'
issn1: '21687161'
issn2: '21687161'
issn3: '21687161'
jcr_value: '5.938'
keywords: Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational
  modeling;Optimization;Big data;quality of service;elastic resource provisioning;cluster
  management
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 2307.0
region: Northern America
scimago_value: 1075.0
sjr_best_quartile: Q1
sourceid: 21100338351.0
title_bib: Optimal resource allocation of cloud-based spark applications
title_csv: Ieee transactions on cloud computing
total_cites: 2658.0
total_cites_(3years): 1406.0
total_docs._(2020): 173.0
total_docs._(3years): 278.0
total_refs.: 3991.0
type: journal
type_publication: article
year: 2022
---
abstract: Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important
  paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is
  a significant problem, which may affect the completion quality of sensing tasks.
  In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive
  perspectives in terms of data quality and sensing cost, which help to better understand
  related objectives and strategies. We first analyze the different characteristics
  of two sensing paradigms, which may lead to difference in task allocation issues
  or strategies. Then, we present some common issues in task allocation with objectives
  in data quality and sensing cost. Furthermore, we provide reviews of unique task
  allocation issues in MCS according to its new characteristics. Finally, we identify
  some potential opportunities for the future research.
author: Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo,
  Bin
categories: Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1);
  Materials Science (miscellaneous) (Q2)
citable_docs._(3years): 24200.0
cites_/_doc._(2years): 448.0
country: United States
coverage: 2013-2020
doi: 10.1109/ACCESS.2019.2896226
eigenfactor_score: 0.15395999999999999
h_index: 127.0
isbn: null
issn: '21693536'
issn1: '21693536'
issn2: '21693536'
issn3: '21693536'
jcr_value: '3.367'
keywords: Sensors;Task analysis;Wireless sensor networks;Resource management;Data
  integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task
  allocation;wireless sensor networks (WSNs)
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 4275.0
region: Northern America
scimago_value: 587.0
sjr_best_quartile: Q1
sourceid: 21100374601.0
title_bib: 'A survey of task allocation: contrastive perspectives from wireless sensor
  networks and mobile crowdsensing'
title_csv: Ieee access
total_cites: 105968.0
total_cites_(3years): 116691.0
total_docs._(2020): 18036.0
total_docs._(3years): 24267.0
total_refs.: 771081.0
type: journal
type_publication: article
year: 2019
---
abstract: 'Current advancements and growth in the arena of the Internet of Things
  (IoT) is providing great potential in the novel epoch of healthcare. The future
  of healthcare is expansively promising, as it advances the excellence of life and
  health of humans, involving several health regulations. Continual increases of multifaceted
  IoT devices in healthcare is beset by challenges, such as powering IoT terminal
  nodes used for health monitoring, data processing, smart decisions, and event management.
  In this paper, we propose a healthcare architecture which is based on an analysis
  of energy harvesting for health monitoring sensors and the realization of Big Data
  analytics in healthcare. The rationale of the proposed architecture is two-fold:
  (1) comprehensive conceptual framework for energy harvesting for health monitoring
  sensors; and (2) data processing and decision management for healthcare. The proposed
  architecture is a three-layered architecture that comprises: (1) energy harvesting
  and data generation; (2) data pre-processing; and (3) data processing and application.
  The proposed scheme highlights the effectiveness of energy-harvesting based IoT
  in healthcare. In addition, it also proposes a solution for smart health monitoring
  and planning. We also utilized consistent datasets on the Hadoop server to validate
  the proposed architecture based on threshold limit values (TLVs). The study demonstrates
  that the proposed architecture offers substantial and immediate value to the field
  of smart health.'
author: Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon
categories: Computer Science (miscellaneous) (Q1); Electrical and Electronic Engineering
  (Q2)
citable_docs._(3years): 169.0
cites_/_doc._(2years): 450.0
country: United States
coverage: 2011-2020
doi: 10.1016/j.suscom.2017.10.009
eigenfactor_score: .nan
h_index: 27.0
isbn: null
issn: '22105379'
issn1: '22105379'
issn2: '22105379'
issn3: '22105379'
jcr_value: null
keywords: Big data analytics, IoT, Energy harvesting
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 4291.0
region: Northern America
scimago_value: 591.0
sjr_best_quartile: Q1
sourceid: 19700201455.0
title_bib: Energy-harvesting based on internet of things and big data analytics for
  smart health monitoring
title_csv: 'Sustainable computing: informatics and systems'
total_cites: .nan
total_cites_(3years): 766.0
total_docs._(2020): 78.0
total_docs._(3years): 178.0
total_refs.: 3347.0
type: journal
type_publication: article
year: 2018
---
abstract: "Smart City and IoT improves the performance of health, transportation,\
  \ energy and reduce the consumption of resources. Among the smart city services,\
  \ Big Data analytics is one of the imperative technologies that have a vast perspective\
  \ to reach sustainability, enhanced resilience, effective quality of life and quick\
  \ management of resources. This paper focuses on the privacy of big data in the\
  \ context of smart health to support smart cities. Furthermore, the trade-off between\
  \ the data privacy and utility in big data analytics is the foremost concern for\
  \ the stakeholders of a smart city. The majority of smart city application databases\
  \ focus on preserving the privacy of individuals with different disease data. In\
  \ this paper, we propose a trust-based hybrid data privacy approach named as \u201C\
  MIDR-Angelization\u201D to assure privacy and utility in big data analytics when\
  \ sharing same disease data of patients in IoT industry. Above all, this study suggests\
  \ that privacy-preserving policies and practices to share disease and health information\
  \ of patients having the same disease should consider detailed disease information\
  \ to enhance data utility. An extensive experimental study performed on a real-world\
  \ dataset to measure instance disclosure risk which shows that the proposed scheme\
  \ outperforms its counterpart in terms of data utility and privacy."
author: Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad
  and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq
categories: Civil and Structural Engineering (Q1); Geography, Planning and Development
  (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation
  (Q1)
citable_docs._(3years): 1284.0
cites_/_doc._(2years): 853.0
country: Netherlands
coverage: 2011-2020
doi: 10.1016/j.scs.2018.04.014
eigenfactor_score: 0.01684
h_index: 61.0
isbn: null
issn: '22106707'
issn1: '22106707'
issn2: '22106707'
issn3: '22106707'
jcr_value: '7.587'
keywords: Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy,
  Re-identification risk, Smart city
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6215.0
region: Western Europe
scimago_value: 1645.0
sjr_best_quartile: Q1
sourceid: 19700194105.0
title_bib: Privacy preserving data by conceptualizing smart cities using midr-angelization
title_csv: Sustainable cities and society
total_cites: 14373.0
total_cites_(3years): 10974.0
total_docs._(2020): 705.0
total_docs._(3years): 1286.0
total_refs.: 43818.0
type: journal
type_publication: article
year: 2018
---
abstract: "Observational studies investigate a wide range of topics in multiple sclerosis\
  \ research. This paper presents an overview of the various observational designs\
  \ and their applications in clinical studies. Observational studies are well suited\
  \ for making discoveries and assessing new explanations of phenomena, but less so\
  \ for establishing causal relationships, due to confounding by indication (selection\
  \ bias), co-morbidity, socio-economic or other factors. Whether observational findings\
  \ are demonstrative, indicative or only suggestive, depends on the research question,\
  \ whether and how the design fits this question, analytical techniques, and the\
  \ quality of data. Observational studies may be cross-sectional vs. longitudinal,\
  \ and prospective vs. retrospective. The term \u2018retrograde\u2019 is proposed\
  \ to explicate that cross-sectional studies may obtain data that cover (long) preceding\
  \ periods. Case reports and case series are usually based on accidental observations\
  \ or routinely collected data. Cross-sectional studies, by simultaneously assessing\
  \ clinical phenomena and external factors, enable the discovery and quantification\
  \ of associations. In ecological studies the unit of analysis is population or group,\
  \ and relationships on patient level cannot be established. A cohort study is a\
  \ longitudinal study that investigates patients with a defining characteristic,\
  \ e.g. diagnosis or specific treatment, by analyzing data acquired at various intervals.\
  \ Prospective cohort studies use (some) data that are not yet available at the time\
  \ the research is conceived, whereas in retrospective studies the data already exist.\
  \ In a case-control study a representative group of patients with a specific clinical\
  \ feature is compared with controls, and the frequencies at which an external factor,\
  \ e.g. infection, has occurred in each group is compared; in a nested case-control\
  \ study controls are drawn from a fully known cohort. Randomized controlled trial\
  \ (RCT)-extension studies are informative because, due to RCT randomization, they\
  \ are free from confounding by indication. Patient or disease registries are organised\
  \ systems for the long-term collection of uniform data on a population that is defined\
  \ by a particular disease, condition or exposure, with the purpose to study changes\
  \ over time. In pharmacotherapeutic research, accidental observations of unexpected\
  \ beneficial effects may lead to further research into a drug's efficacy in other\
  \ conditions. Uncontrolled phase 1 studies investigate safety and dosing aspects.\
  \ Observational studies are alternatives to RCTs when these are not feasible for\
  \ ethical or practical reasons. Phase 4 observational studies play a crucial role\
  \ in the evaluation of the effectiveness of treatments in daily practice, the validation\
  \ of RCT-based side effect profiles, and the discovery of late occurring or rare,\
  \ potentially life-threatening side effects. Combinations of multidisciplinary longitudinal\
  \ data bases into large data sets enable the development of algorithms for personalized\
  \ treatments. To improve the reporting of observational findings on treatment effectiveness,\
  \ it is proposed that abstracts define the research question(s) the study was meant\
  \ to answer, study design and analytical methods, and identify and quantify the\
  \ patient population, treatment of interest, relevant outcomes and the study's strengths\
  \ and limitations. The development of guidelines for Strengthening the Reporting\
  \ of Observational Studies in Effectiveness Research (STROBER), as an extension\
  \ of the guidelines used in epidemiology, is wanted."
author: Peter Joseph Jongen
categories: Medicine (miscellaneous) (Q2); Neurology (Q2); Neurology (clinical) (Q2)
citable_docs._(3years): 838.0
cites_/_doc._(2years): 305.0
country: United States
coverage: 2012-2020
doi: 10.1016/j.msard.2019.07.006
eigenfactor_score: 0.008879999999999999
h_index: 38.0
isbn: null
issn: '22110348'
issn1: '22110356'
issn2: '22110348'
issn3: '22110356'
jcr_value: '4.339'
keywords: Multiple sclerosis, Observational, Effectiveness, Disease-modifying drug,
  Pharmacotherapy
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 2957.0
region: Northern America
scimago_value: 870.0
sjr_best_quartile: Q2
sourceid: 20000195097.0
title_bib: 'Observational designs in clinical multiple sclerosis research: particulars,
  practices and potentialities'
title_csv: Multiple sclerosis and related disorders
total_cites: 5292.0
total_cites_(3years): 2908.0
total_docs._(2020): 701.0
total_docs._(3years): 930.0
total_refs.: 20727.0
type: journal
type_publication: article
year: 2019
---
abstract: Traditional Chinese medicine (TCM) has been an indispensable source of drugs
  for curing various human diseases. However, the inherent chemical diversity and
  complexity of TCM restricted the safety and efficacy of its usage. Over the past
  few decades, the combination of liquid chromatography with mass spectrometry has
  contributed greatly to the TCM qualitative analysis. And novel approaches have been
  continuously introduced to improve the analytical performance, including both the
  data acquisition methods to generate a large and informative dataset, and the data
  post-processing tools to extract the structure-related MS information. Furthermore,
  the fast-developing computer techniques and big data analytics have markedly enriched
  the data processing tools, bringing benefits of high efficiency and accuracy. To
  provide an up-to-date review of the latest techniques on the TCM qualitative analysis,
  multiple data-independent acquisition methods and data-dependent acquisition methods
  (precursor ion list, dynamic exclusion, mass tag, precursor ion scan, neutral loss
  scan, and multiple reaction monitoring) and post-processing techniques (mass defect
  filtering, diagnostic ion filtering, neutral loss filtering, mass spectral trees
  similarity filter, molecular networking, statistical analysis, database matching,
  etc.) were summarized and categorized. Applications of each technique and integrated
  analytical strategies were highlighted, discussion and future perspectives were
  proposed as well.
author: Yang Yu and Changliang Yao and De-an Guo
categories: Pharmacology, Toxicology and Pharmaceutics (miscellaneous) (Q1)
citable_docs._(3years): 262.0
cites_/_doc._(2years): 1062.0
country: Netherlands
coverage: 2012, 2014-2020
doi: 10.1016/j.apsb.2021.02.017
eigenfactor_score: 9.0e-05
h_index: 51.0
isbn: null
issn: '22113835'
issn1: '22113843'
issn2: '22113835'
issn3: '22113843'
jcr_value: '11.413'
keywords: "Liquid chromatography\u2212mass spectrometry, Qualitative analysis, Traditional\
  \ Chinese medicine, Data acquisition, Data post-processing"
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 7633.0
region: Western Europe
scimago_value: 1912.0
sjr_best_quartile: Q1
sourceid: 20700195026.0
title_bib: "Insight into chemical basis of traditional chinese medicine based on the\
  \ state-of-the-art techniques of liquid chromatography\u2212mass spectrometry"
title_csv: Acta pharmaceutica sinica b
total_cites: 6314.0
total_cites_(3years): 2616.0
total_docs._(2020): 202.0
total_docs._(3years): 278.0
total_refs.: 15419.0
type: journal
type_publication: article
year: 2021
---
abstract: The evaluation of the spatial similarity of two observed point patterns
  is an important issue in spatial data quality assessment. In this work we propose
  a formal procedure that takes advantage of the joint use of space-filling curves
  and the multinomial model in order to establish a statistical test to compare spatial
  point patterns. In this mix, the space-filling curves offer a mechanism to order
  the 2D, 3D or n-D space and the multinomial distribution the statistical approach
  for testing homogeneity. A simulation method is proposed in order to analyze the
  applied performance of this idea.
author: "M.V. Alba-Fern\xE1ndez and F.J. Ariza-L\xF3pez and M. Dolores Jim\xE9nez-Gamero\
  \ and J. Rodr\xEDguez-Avi"
categories: Management, Monitoring, Policy and Law (Q1); Computers in Earth Sciences
  (Q2); Statistics and Probability (Q2)
citable_docs._(3years): 168.0
cites_/_doc._(2years): 253.0
country: Netherlands
coverage: 2012-2020
doi: 10.1016/j.spasta.2016.07.004
eigenfactor_score: 0.00244
h_index: 27.0
isbn: null
issn: '22116753'
issn1: '22116753'
issn2: '22116753'
issn3: '22116753'
jcr_value: '2.060'
keywords: Space-filling curve, Multinomial distribution, Simulation
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4025.0
region: Western Europe
scimago_value: 888.0
sjr_best_quartile: Q1
sourceid: 21100206605.0
title_bib: On the similarity analysis of spatial patterns
title_csv: Spatial statistics
total_cites: 923.0
total_cites_(3years): 469.0
total_docs._(2020): 64.0
total_docs._(3years): 172.0
total_refs.: 2576.0
type: journal
type_publication: article
year: 2016
---
abstract: "Utilizing a scientometric review of global trends and structure from 388\
  \ bibliographic records over two decades (1999\u20132018), this study seeks to advance\
  \ the building of comprehensive knowledge maps that draw upon global travel demand\
  \ studies. The study, using the techniques of co-citation analysis, collaboration\
  \ network and emerging trends analysis, identified major disciplines that provide\
  \ knowledge and theories for tourism demand forecasting, many trending research\
  \ topics, the most critical countries, institutions, publications, and articles,\
  \ and the most influential researchers. The increasing interest and output for big\
  \ data and machine learning techniques in the field were visualized via comprehensive\
  \ knowledge maps. This research provides meaningful guidance for researchers, operators\
  \ and decision makers who wish to improve the accuracy of tourism demand forecasting."
author: Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei
categories: Tourism, Leisure and Hospitality Management (Q1)
citable_docs._(3years): 274.0
cites_/_doc._(2years): 677.0
country: United States
coverage: 2012-2020
doi: 10.1016/j.tmp.2020.100715
eigenfactor_score: 0.00445
h_index: 43.0
isbn: null
issn: '22119736'
issn1: '22119736'
issn2: '22119736'
issn3: '22119736'
jcr_value: '6.586'
keywords: Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map,
  CiteSpace, Infographic
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 8644.0
region: Northern America
scimago_value: 1454.0
sjr_best_quartile: Q1
sourceid: 21100202157.0
title_bib: Knowledge mapping of tourism demand forecasting research
title_csv: Tourism management perspectives
total_cites: 3902.0
total_cites_(3years): 1824.0
total_docs._(2020): 140.0
total_docs._(3years): 277.0
total_refs.: 12102.0
type: journal
type_publication: article
year: 2020
---
abstract: The concept of local climate zones (LCZ) has emerged to identify the nature
  of urban climate, air quality, and temperature at local levels. Thus, this study
  reviews the literature on methodologies and data sources used in LCZs empirical
  research and identifies recurrent themes. A systematic review was conducted using
  bibliometric analysis and the PRISMA framework. Web of Science and Scopus databases
  were used to extract relevant datasets, and records were screened and extracted.
  Descriptive analyses reveal that most LCZ empirical research has been done on Chinese
  cities. Numerous data sources and analytical methods have been used, but Landsat
  and WUDAPT methodology is generally favored in the LCZ research due to its simplicity
  and freely available global datasets. Similarly, the review also shows that various
  software and methodologies are available to identify climate-sensitive areas of
  urban settlements with varying functionalities, accuracy, and visualizations. The
  thematic analysis indicates that the LCZ framework and its associated processes
  are being used in crosscutting phenomena such as thermal comfort, urban planning,
  climate change adaptation, and energy use. The review also suggests incorporating
  institutional and social aspects in local climate zones. LCZs can integrate the
  philosophies of climate change adaptation, urban resilience, and sustainability.
author: Ayman Aslam and Irfan Ahmad Rana
categories: Environmental Science (miscellaneous) (Q1); Geography, Planning and Development
  (Q1); Urban Studies (Q1); Atmospheric Science (Q2)
citable_docs._(3years): 264.0
cites_/_doc._(2years): 616.0
country: Netherlands
coverage: 2012-2020
doi: 10.1016/j.uclim.2022.101120
eigenfactor_score: 0.00413
h_index: 43.0
isbn: null
issn: '22120955'
issn1: '22120955'
issn2: '22120955'
issn3: '22120955'
jcr_value: '5.731'
keywords: Climate change adaptation, Climate risk mapping, Sustainable development,
  Urban planning
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5784.0
region: Western Europe
scimago_value: 1151.0
sjr_best_quartile: Q1
sourceid: 21100220478.0
title_bib: 'The use of local climate zones in the urban environment: a systematic
  review of data sources, methods, and themes'
title_csv: Urban climate
total_cites: 3116.0
total_cites_(3years): 1563.0
total_docs._(2020): 165.0
total_docs._(3years): 265.0
total_refs.: 9544.0
type: journal
type_publication: article
year: 2022
---
abstract: "Objectives\nThis study aimed to provide an overview of major data sources\
  \ in China that can be potentially used for epidemiology, health economics, and\
  \ outcomes research; compare them with similar data sources in other countries;\
  \ and discuss future directions of healthcare data development in China.\nMethods\n\
  The study was conducted in 2 phases. First, various data sources were identified\
  \ through a targeted literature review and recommendations by experts. Second, an\
  \ in-depth assessment was conducted to evaluate the strengths and limitations of\
  \ administrative claims and electronic health record data, which were further compared\
  \ with similar data sources in developed countries.\nResults\nSecondary databases,\
  \ including administrative claims and electronic health records, are the major types\
  \ of real-world data in China. There are substantial variations in available data\
  \ elements even within the same type of databases. Compared with similar databases\
  \ in developed countries, the secondary databases in China have some general limitations\
  \ such as variations in data quality, unclear data usage mechanism, and lack of\
  \ longitudinal follow-up data. In contrast, the large sample size and the potential\
  \ to collect additional data based on research needs present opportunities to further\
  \ improve real-world data in China.\nConclusions\nAlthough healthcare data have\
  \ expanded substantially in China, high-quality real-world evidence that can be\
  \ used to facilitate decision making remains limited in China. To support the generation\
  \ of real-world evidence, 2 fundamental issues in existing databases need to be\
  \ addressed\u2014data access/sharing and data quality."
author: Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia
  Zhong and Larry Liu
categories: Economics, Econometrics and Finance (miscellaneous) (Q2); Pharmacology,
  Toxicology and Pharmaceutics (miscellaneous) (Q2); Health Policy (Q3)
citable_docs._(3years): 197.0
cites_/_doc._(2years): 116.0
country: United States
coverage: 2012-2020
doi: 10.1016/j.vhri.2021.05.002
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '22121099'
issn1: '22121099'
issn2: '22121102'
issn3: '22121099'
jcr_value: null
keywords: administrative claims, data access, electronic health records, real-world
  data
publisher_x: Elsevier USA
publisher_y: null
ref._/_doc.: 3596.0
region: Northern America
scimago_value: 395.0
sjr_best_quartile: Q2
sourceid: 21100218529.0
title_bib: 'Real-world data for healthcare research in china: call for actions'
title_csv: Value in health regional issues
total_cites: .nan
total_cites_(3years): 265.0
total_docs._(2020): 79.0
total_docs._(3years): 207.0
total_refs.: 2841.0
type: journal
type_publication: article
year: 2022
---
abstract: "The effects of data governance (as a means to maximize big data value creation\
  \ in fire risk management) performance on fire risk was analyzed based on multi-source\
  \ statistical data of 105 cities in China from 2016 to 2018. Specifically, data\
  \ governance was first quantified with ten detailed indicators, which were then\
  \ selected for explaining urban fire risk through correlation analysis. Next, the\
  \ sample cities were clustered in terms of major socio-economic characteristics,\
  \ and then the effects of data governance were examined by constructing multivariate\
  \ regression models for each city cluster with ordinary least squares (OLS). The\
  \ results showed that the constructed regression models produced good interpretation\
  \ of fire risk in different types of cities, with coefficient of determination (R2)\
  \ in each model exceeding 0.65. Among the indicators, the development of infrastructures\
  \ (e.g. data collection devices and data analysis platforms), the level of data\
  \ use, and the updating of fire risk related data were proved to produce significant\
  \ effects on the reduction of fire frequency and fire consequence. Moreover, the\
  \ organizational maturity of data governance was proved to be helpful in reducing\
  \ fire frequency. For the cities with large population, the cross-department sharing\
  \ of high-value data was found to be another important determinant of urban fire\
  \ frequency. In comparison with existing statistical models which interpreted fire\
  \ risk with general social factors (with the highest R2\_=\_0.60), these new regression\
  \ models presented a better statistical performance (with the average R2\_=\_0.72).\
  \ These findings are expected to provide decision support for the local governments\
  \ of China and other jurisdictions to facilitate big data projects in improving\
  \ fire risk management."
author: Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas
categories: Geology (Q1); Geotechnical Engineering and Engineering Geology (Q1); Safety
  Research (Q1)
citable_docs._(3years): 844.0
cites_/_doc._(2years): 478.0
country: United Kingdom
coverage: 2012-2020
doi: 10.1016/j.ijdrr.2022.103138
eigenfactor_score: 0.010029999999999999
h_index: 45.0
isbn: null
issn: '22124209'
issn1: '22124209'
issn2: '22124209'
issn3: '22124209'
jcr_value: '4.320'
keywords: Urban fire risk, Fire risk management, Big data technologies, Data governance,
  Socio-economic factors, City-wide analysis
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 6377.0
region: Western Europe
scimago_value: 1161.0
sjr_best_quartile: Q1
sourceid: 21100228018.0
title_bib: 'Effects of governmental data governance on urban fire risk: a city-wide
  analysis in china'
title_csv: International journal of disaster risk reduction
total_cites: 6931.0
total_cites_(3years): 4363.0
total_docs._(2020): 562.0
total_docs._(3years): 856.0
total_refs.: 35838.0
type: journal
type_publication: article
year: 2022
---
abstract: The process controller in a precision grinder for bearing rings puts high
  performance demands on the machine to achieve desired quality in production. This
  paper presents a unique approach of adding additional sensors for machine condition
  monitoring for the purpose of learning and using high fidelity condition indicators.
  The consolidation of real-time sensor data and the process control signals yields
  high-dimensional dataset. Automatic segmentation helps optimize the amount of data
  for processing and data mining ahead of fault diagnosis. The proposed setup is state
  of the art for prognostics as part of condition-based maintenance in a production
  machine.
author: "Muhammad Ahmer and P\xE4r Marklund and Martin Gustafsson and Kim Berglund"
categories: Control and Systems Engineering; Industrial and Manufacturing Engineering
citable_docs._(3years): 3145.0
cites_/_doc._(2years): 240.0
country: Netherlands
coverage: 2012-2020
doi: 10.1016/j.procir.2020.04.094
eigenfactor_score: .nan
h_index: 65.0
isbn: null
issn: '22128271'
issn1: '22128271'
issn2: '22128271'
issn3: '22128271'
jcr_value: null
keywords: Analytics, Automation, Condition monitoring, Grinding, Machining, Maintenance,
  Manufacturing, Measurement, Process Monitoring, Sensor
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 2091.0
region: Western Europe
scimago_value: 683.0
sjr_best_quartile: '-'
sourceid: 21100243809.0
title_bib: A unified approach towards performance monitoring and condition-based maintenance
  in grinding machines
title_csv: Procedia cirp
total_cites: .nan
total_cites_(3years): 8225.0
total_docs._(2020): 1456.0
total_docs._(3years): 3191.0
total_refs.: 30451.0
type: conference and proceedings
type_publication: article
year: 2020
---
abstract: "Nature recreation and tourism is a substantial ecosystem service of Europe's\
  \ countryside that has a substantial economic value and contributes considerably\
  \ to income and employment of local communities. Highlighting the recreational value\
  \ and economic contribution of nature areas can be used as a strong argument for\
  \ the funding of protected and recreational areas. The total number of recreational\
  \ visits of a nature area has been recognised as a major determinant of its economic\
  \ recreational value and its contribution to local economies. This paper presents\
  \ an international geo-database on recreational visitor numbers to non-urban ecosystems,\
  \ containing 1267 observations at 518 separate case study areas throughout Europe.\
  \ The monitored sites are described by their centroid coordinates and shape files\
  \ displaying the exact extension of the sites. Therefore, the database illustrates\
  \ the spatial distribution of visitor counting throughout Europe and can be used\
  \ for secondary research, such as for validation of spatially explicit recreational\
  \ ecosystem service models and for identifying relevant drivers of recreational\
  \ ecosystem services. To develop the database, we review visitor monitoring literature\
  \ throughout Europe and give an overview of such activities with special attention\
  \ to visitor counting. We identify one major shortcoming in the available literature,\
  \ which relates to the presentation, study area definition and methodological reporting\
  \ of conducted visitor counting studies. Insufficient reporting hampers the identification\
  \ of the study area, the comparability of different studies and the evaluation of\
  \ the studies' quality. Based on our findings, we propose a standardised reporting\
  \ template for visitor counting studies and advanced data sharing for recreational\
  \ visitor data. Researchers and institutions are invited to report on their visitor\
  \ counting studies via our web interface at rris.biopama.org/visitor-reporting and\
  \ thereby contribute to a global visitor database that will be shared via the ESP\
  \ Visualisation tool.\nManagement implications\nThe total annual visitor number\
  \ is the most important variable for defining the relative importance and the economic\
  \ recreational value of different recreational areas. Due to the importance of visitor\
  \ counting and its increased attention in the scientific literature we: \u2022present\
  \ a geo-database on recreational visitor statistics for nature areas, which allows\
  \ identifying sites for which visitor statistics exist and which can be used for\
  \ secondary research\u2022review current practice in recreational visitor counting\
  \ across nature areas in Europe and give guidance for future applications,\u2022\
  identify shortcomings in the methodological reporting of recent visitor monitoring\
  \ and counting studies and\u2022present and recommend reporting standard for all\
  \ future visitor counting studies in order to improve their comparability and to\
  \ allow assessing their quality.\u2022The reporting standard is translated into\
  \ a web interface for visitor data collection, which allows for data sharing via\
  \ a global map-browser."
author: "Jan Philipp Sch\xE4gner and Joachim Maes and Luke Brander and Maria-Luisa\
  \ Paracchini and Volkmar Hartje and Gregoire Dubois"
categories: Tourism, Leisure and Hospitality Management (Q2)
citable_docs._(3years): 114.0
cites_/_doc._(2years): 288.0
country: United Kingdom
coverage: 2013-2020
doi: 10.1016/j.jort.2017.02.004
eigenfactor_score: .nan
h_index: 21.0
isbn: null
issn: '22130780'
issn1: '22130780'
issn2: '22130780'
issn3: '22130780'
jcr_value: null
keywords: Nature recreation, Visitor monitoring, Visitor counting review, Reporting
  standard, Visitor statistics, Visitor data sharing
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6277.0
region: Western Europe
scimago_value: 686.0
sjr_best_quartile: Q2
sourceid: 21100255547.0
title_bib: 'Monitoring recreation across european nature areas: a geo-database of
  visitor counts, a review of literature and a call for a visitor counting reporting
  standard'
title_csv: Journal of outdoor recreation and tourism
total_cites: .nan
total_cites_(3years): 381.0
total_docs._(2020): 65.0
total_docs._(3years): 116.0
total_refs.: 4080.0
type: journal
type_publication: article
year: 2017
---
abstract: "Machine learning techniques offer a precious tool box for use within astronomy\
  \ to solve problems involving so-called big data. They provide a means to make accurate\
  \ predictions about a particular system without prior knowledge of the underlying\
  \ physical processes of the data. In this article, and the companion papers of this\
  \ series, we present the set of Generalized Linear Models (GLMs) as a fast alternative\
  \ method for tackling general astronomical problems, including the ones related\
  \ to the machine learning paradigm. To demonstrate the applicability of GLMs to\
  \ inherently positive and continuous physical observables, we explore their use\
  \ in estimating the photometric redshifts of galaxies from their multi-wavelength\
  \ photometry. Using the gamma family with a log link function we predict redshifts\
  \ from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan\
  \ Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic\
  \ outlier rates as low as \u223C1% for simulated and \u223C2% for real data. Moreover,\
  \ we can easily obtain such levels of precision within a matter of seconds on a\
  \ normal desktop computer and with training sets that contain merely thousands of\
  \ galaxies. Our software is made publicly available as a user-friendly package developed\
  \ in Python, R and via an interactive web application. This software allows users\
  \ to apply a set of GLMs to their own photometric catalogues and generates publication\
  \ quality plots with minimum effort. By facilitating their ease of use to the astronomical\
  \ community, this paper series aims to make GLMs widely known and to encourage their\
  \ implementation in future large-scale projects, such as the Large Synoptic Survey\
  \ Telescope."
author: J. Elliott and R.S. {de Souza} and A. Krone-Martins and E. Cameron and E.E.O.
  Ishida and J. Hilbe
categories: Astronomy and Astrophysics (Q2); Computer Science Applications (Q2); Space
  and Planetary Science (Q2)
citable_docs._(3years): 132.0
cites_/_doc._(2years): 353.0
country: Netherlands
coverage: 2013-2020
doi: 10.1016/j.ascom.2015.01.002
eigenfactor_score: 0.0025
h_index: 31.0
isbn: null
issn: '22131337'
issn1: '22131337'
issn2: '22131337'
issn3: '22131337'
jcr_value: '1.927'
keywords: 'Techniques: photometric, Methods: statistical, Methods: analytical, Galaxies:
  distances and redshifts'
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4374.0
region: Western Europe
scimago_value: 692.0
sjr_best_quartile: Q2
sourceid: 21100241218.0
title_bib: 'The overlooked potential of generalized linear models in astronomy-ii:
  gamma regression and photometric redshifts'
title_csv: Astronomy and computing
total_cites: 796.0
total_cites_(3years): 475.0
total_docs._(2020): 46.0
total_docs._(3years): 133.0
total_refs.: 2012.0
type: journal
type_publication: article
year: 2015
---
abstract: Artificial and augmented intelligence (AI) and machine learning (ML) methods
  are expanding into the health care space. Big data are increasingly used in patient
  care applications, diagnostics, and treatment decisions in allergy and immunology.
  How these technologies will be evaluated, approved, and assessed for their impact
  is an important consideration for researchers and practitioners alike. With the
  potential of ML, deep learning, natural language processing, and other assistive
  methods to redefine health care usage, a scaffold for the impact of AI technology
  on research and patient care in allergy and immunology is needed. An American Academy
  of Asthma Allergy and Immunology Health Information Technology and Education subcommittee
  workgroup was convened to perform a scoping review of AI within health care as well
  as the specialty of allergy and immunology to address impacts on allergy and immunology
  practice and research as well as potential challenges including education, AI governance,
  ethical and equity considerations, and potential opportunities for the specialty.
  There are numerous potential clinical applications of AI in allergy and immunology
  that range from disease diagnosis to multidimensional data reduction in electronic
  health records or immunologic datasets. For appropriate application and interpretation
  of AI, specialists should be involved in the design, validation, and implementation
  of AI in allergy and immunology. Challenges include incorporation of data science
  and bioinformatics into training of future allergists-immunologists.
author: Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian
  Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider
categories: Immunology and Allergy (Q1)
citable_docs._(3years): 1067.0
cites_/_doc._(2years): 368.0
country: United States
coverage: 2013-2020
doi: 10.1016/j.jaip.2022.01.047
eigenfactor_score: .nan
h_index: 58.0
isbn: null
issn: '22132198'
issn1: '22132198'
issn2: '22132201'
issn3: '22132198'
jcr_value: null
keywords: Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis,
  Augmented intelligence, Clinical decision support, Electronic health records, Equity,
  Machine learning, Natural language processing, Medical education
publisher_x: Elsevier
publisher_y: null
ref._/_doc.: 2732.0
region: Northern America
scimago_value: 1731.0
sjr_best_quartile: Q1
sourceid: 21100239250.0
title_bib: "A framework for augmented intelligence in allergy and immunology practice\
  \ and research\u2014a work group report of the aaaai health informatics, technology,\
  \ and education committee"
title_csv: 'Journal of allergy and clinical immunology: in practice'
total_cites: .nan
total_cites_(3years): 4937.0
total_docs._(2020): 700.0
total_docs._(3years): 1357.0
total_refs.: 19123.0
type: journal
type_publication: article
year: 2022
---
abstract: "Summary\nBackground\nThe large amount of clinical signals in intensive\
  \ care units can easily overwhelm health-care personnel and can lead to treatment\
  \ delays, suboptimal care, or clinical errors. The aim of this study was to apply\
  \ deep machine learning methods to predict severe complications during critical\
  \ care in real time after cardiothoracic surgery.\nMethods\nWe used deep learning\
  \ methods (recurrent neural networks) to predict several severe complications (mortality,\
  \ renal failure with a need for renal replacement therapy, and postoperative bleeding\
  \ leading to operative revision) in post cardiosurgical care in real time. Adult\
  \ patients who underwent major open heart surgery from Jan 1, 2000, to Dec 31, 2016,\
  \ in a German tertiary care centre for cardiovascular diseases formed the main derivation\
  \ dataset. We measured the accuracy and timeliness of the deep learning model's\
  \ forecasts and compared predictive quality to that of established standard-of-care\
  \ clinical reference tools (clinical rule for postoperative bleeding, Simplified\
  \ Acute Physiology Score II for mortality, and the Kidney Disease: Improving Global\
  \ Outcomes staging criteria for acute renal failure) using positive predictive value\
  \ (PPV), negative predictive value, sensitivity, specificity, area under the curve\
  \ (AUC), and the F1 measure (which computes a harmonic mean of sensitivity and PPV).\
  \ Results were externally retrospectively validated with 5898 cases from the published\
  \ MIMIC-III dataset.\nFindings\nOf 47\u2008559 intensive care admissions (corresponding\
  \ to 42\u2008007 patients), we included 11\u2008492 (corresponding to 9269 patients).\
  \ The deep learning models yielded accurate predictions with the following PPV and\
  \ sensitivity scores: PPV 0\xB790 and sensitivity 0\xB785 for mortality, 0\xB787\
  \ and 0\xB794 for renal failure, and 0\xB784 and 0\xB774 for bleeding. The predictions\
  \ significantly outperformed the standard clinical reference tools, improving the\
  \ absolute complication prediction AUC by 0\xB729 (95% CI 0\xB723\u20130\xB735)\
  \ for bleeding, by 0\xB724 (0\xB719\u20130\xB729) for mortality, and by 0\xB724\
  \ (0\xB713\u20130\xB735) for renal failure (p<0\xB70001 for all three analyses).\
  \ The deep learning methods showed accurate predictions immediately after patient\
  \ admission to the intensive care unit. We also observed an increase in performance\
  \ in our validation cohort when the machine learning approach was tested against\
  \ clinical reference tools, with absolute improvements in AUC of 0\xB709 (95% CI\
  \ 0\xB703\u20130\xB715; p=0\xB70026) for bleeding, of 0\xB718 (0\xB707\u20130\xB7\
  29; p=0\xB70013) for mortality, and of 0\xB725 (0\xB718\u20130\xB732; p<0\xB70001)\
  \ for renal failure.\nInterpretation\nThe observed improvements in prediction for\
  \ all three investigated clinical outcomes have the potential to improve critical\
  \ care. These findings are noteworthy in that they use routinely collected clinical\
  \ data exclusively, without the need for any manual processing. The deep machine\
  \ learning method showed AUC scores that significantly surpass those of clinical\
  \ reference tools, especially soon after admission. Taken together, these properties\
  \ are encouraging for prospective deployment in critical care settings to direct\
  \ the staff's attention towards patients who are most at risk.\nFunding\nNo specific\
  \ funding."
author: "Alexander Meyer and Dina Zverinski and Boris Pfahringer and J\xF6rg Kempfert\
  \ and Titus Kuehne and Simon H S\xFCndermann and Christof Stamm and Thomas Hofmann\
  \ and Volkmar Falk and Carsten Eickhoff"
categories: Pulmonary and Respiratory Medicine (Q1)
citable_docs._(3years): 247.0
cites_/_doc._(2years): 676.0
country: United Kingdom
coverage: 2013-2020
doi: 10.1016/S2213-2600(18)30300-X
eigenfactor_score: .nan
h_index: 113.0
isbn: null
issn: '22132600'
issn1: '22132619'
issn2: '22132600'
issn3: '22132619'
jcr_value: null
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 1606.0
region: Western Europe
scimago_value: 9030.0
sjr_best_quartile: Q1
sourceid: 21100220495.0
title_bib: 'Machine learning for real-time prediction of complications in critical
  care: a retrospective study'
title_csv: Lancet respiratory medicine,the
total_cites: .nan
total_cites_(3years): 5271.0
total_docs._(2020): 330.0
total_docs._(3years): 813.0
total_refs.: 5300.0
type: journal
type_publication: article
year: 2018
---
abstract: "Massive parallel DNA sequencing combined with chromatin immunoprecipitation\
  \ and a large variety of DNA/RNA-enrichment methodologies is at the origin of data\
  \ resources of major importance. Indeed these resources, available for multiple\
  \ genomes, represent the most comprehensive catalogue of (i) cell, development and\
  \ signal transduction-specified patterns of binding sites for transcription factors\
  \ (\u2018cistromes\u2019) and for transcription and chromatin modifying machineries\
  \ and (ii) the patterns of specific local post-translational modifications of histones\
  \ and DNA (\u2018epigenome\u2019) or of regulatory chromatin binding factors. In\
  \ addition, (iii) the resources specifying chromatin structure alterations are emerging.\
  \ Importantly, these types of \u201Comics\u201D datasets populate increasingly public\
  \ repositories and provide highly valuable resources for the exploration of general\
  \ principles of cell function in a multi-dimensional genome\u2013transcriptome\u2013\
  epigenome\u2013chromatin structure context. However, data mining is critically dependent\
  \ on the data quality, an issue that, surprisingly, is still largely ignored by\
  \ scientists and well-financed consortia, data repositories and scientific journals.\
  \ So what determines the quality of ChIP-seq experiments and the datasets generated\
  \ therefrom and what refrains scientists from associating quality criteria to their\
  \ data? In this \u2018opinion\u2019 we trace the various parameters that influence\
  \ the quality of this type of datasets, as well as the computational efforts that\
  \ were made until now to qualify them. Moreover, we describe a universal quality\
  \ control (QC) certification approach that provides a quality rating for ChIP-seq\
  \ and enrichment-related assays. The corresponding QC tool and a regularly updated\
  \ database, from which at present the quality parameters of more than 8000 datasets\
  \ can be retrieved, are freely accessible at www.ngs-qc.org."
author: Marco Antonio Mendoza-Parra and Hinrich Gronemeyer
categories: Biotechnology (Q2); Biochemistry (Q3); Genetics (Q3); Molecular Medicine
  (Q3)
citable_docs._(3years): 102.0
cites_/_doc._(2years): 0.0
country: United States
coverage: 2013-2017
doi: 10.1016/j.gdata.2014.08.002
eigenfactor_score: .nan
h_index: 20.0
isbn: null
issn: '22135960'
issn1: '22135960'
issn2: '22135960'
issn3: '22135960'
jcr_value: null
keywords: ChIP sequencing, Massive parallel sequencing, Quality control, Omics data
  mining
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 549.0
sjr_best_quartile: Q2
sourceid: 21100283759.0
title_bib: 'Assessing quality standards for chip-seq and related massive parallel
  sequencing-generated datasets: when rating goes beyond avoiding the crisis'
title_csv: Genomics data
total_cites: .nan
total_cites_(3years): 196.0
total_docs._(2020): 0.0
total_docs._(3years): 102.0
total_refs.: 0.0
type: journal
type_publication: article
year: 2014
---
abstract: "As the global human population increases, livestock agriculture must adapt\
  \ to provide more livestock products and with improved efficiency while also addressing\
  \ concerns about animal welfare, environmental sustainability, and public health.\
  \ The purpose of this paper is to critically review the current state of the art\
  \ in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies,\
  \ specifically biometric sensors, big data, and blockchain technology. Biometric\
  \ sensors include either noninvasive or invasive sensors that monitor an individual\
  \ animal\u2019s health and behavior in real time, allowing farmers to integrate\
  \ this data for population-level analyses. Real-time information from biometric\
  \ sensors is processed and integrated using big data analytics systems that rely\
  \ on statistical algorithms to sort through large, complex data sets to provide\
  \ farmers with relevant trending patterns and decision-making tools. Sensors enabled\
  \ blockchain technology affords secure and guaranteed traceability of animal products\
  \ from farm to table, a key advantage in monitoring disease outbreaks and preventing\
  \ related economic losses and food-related health pandemics. Thanks to PLF technologies,\
  \ livestock agriculture has the potential to address the abovementioned pressing\
  \ concerns by becoming more transparent and fostering increased consumer trust.\
  \ However, new PLF technologies are still evolving and core component technologies\
  \ (such as blockchain) are still in their infancy and insufficiently validated at\
  \ scale. The next generation of PLF technologies calls for preventive and predictive\
  \ analytics platforms that can sort through massive amounts of data while accounting\
  \ for specific variables accurately and accessibly. Issues with data privacy, security,\
  \ and integration need to be addressed before the deployment of multi-farm shared\
  \ PLF solutions becomes commercially feasible."
author: Suresh Neethirajan and Bas Kemp
categories: Electrical and Electronic Engineering (Q1); Electronic, Optical and Magnetic
  Materials (Q1); Biotechnology (Q2); Signal Processing (Q2)
citable_docs._(3years): 140.0
cites_/_doc._(2years): 412.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.sbsr.2021.100408
eigenfactor_score: .nan
h_index: 28.0
isbn: null
issn: '22141804'
issn1: '22141804'
issn2: '22141804'
issn3: '22141804'
jcr_value: null
keywords: Precision Livestock Farming, digitalization, Digital Technologies in Livestock
  Systems, sensor technology, big data, blockchain, data models, livestock agriculture
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 3995.0
region: Western Europe
scimago_value: 770.0
sjr_best_quartile: Q1
sourceid: 21100356802.0
title_bib: Digital livestock farming
title_csv: Sensing and bio-sensing research
total_cites: .nan
total_cites_(3years): 713.0
total_docs._(2020): 80.0
total_docs._(3years): 140.0
total_refs.: 3196.0
type: journal
type_publication: article
year: 2021
---
abstract: Nowadays, IoT, cloud computing, mobile and social networks are generating
  a transformation in social processes. Nevertheless, this technological change rise
  to new threats and security attacks that produce new and complex cybersecurity scenarios
  with large volumes of data and different attack vectors that can exceeded the cognitive
  skills of security analysts. In this context, cognitive sciences can enhance the
  cognitive processes, which can help to security analysts to establish actions in
  less time and more efficiently within cybersecurity operations. This works presents
  a cognitive security model that integrates technological solutions such as Big Data,
  Machine Learning, and Support Decision Systems with the cognitive processes of security
  analysts used to generate knowledge, understanding and execution of security response
  actions. The model considers alternatives to establish the automation process in
  the execution of cognitive tasks defined in the cyber operations processes and includes
  the analyst as the central axis in the processes of validation and decision making
  through the use of MAPE-K, OODA and Human in the Loop.
author: Roberto O Andrade and Sang Guun Yoo
categories: Computer Networks and Communications (Q2); Safety, Risk, Reliability and
  Quality (Q2); Software (Q2)
citable_docs._(3years): 292.0
cites_/_doc._(2years): 543.0
country: United Kingdom
coverage: 2013-2020
doi: 10.1016/j.jisa.2019.06.008
eigenfactor_score: 0.00209
h_index: 40.0
isbn: null
issn: '22142126'
issn1: '22142126'
issn2: '22142134'
issn3: '22142126'
jcr_value: '3.872'
keywords: Cognitive security, Cognitive science, Situation awareness, Cyber operations
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 4677.0
region: Western Europe
scimago_value: 610.0
sjr_best_quartile: Q2
sourceid: 21100332403.0
title_bib: 'Cognitive security: a comprehensive study of cognitive science in cybersecurity'
title_csv: Journal of information security and applications
total_cites: 1526.0
total_cites_(3years): 1526.0
total_docs._(2020): 183.0
total_docs._(3years): 297.0
total_refs.: 8559.0
type: journal
type_publication: article
year: 2019
---
abstract: 'A series of weaknesses in creativity, research design, and quality of writing
  continue to handicap energy social science. Many studies ask uninteresting research
  questions, make only marginal contributions, and lack innovative methods or application
  to theory. Many studies also have no explicit research design, lack rigor, or suffer
  from mangled structure and poor quality of writing. To help remedy these shortcomings,
  this Review offers suggestions for how to construct research questions; thoughtfully
  engage with concepts; state objectives; and appropriately select research methods.
  Then, the Review offers suggestions for enhancing theoretical, methodological, and
  empirical novelty. In terms of rigor, codes of practice are presented across seven
  method categories: experiments, literature reviews, data collection, data analysis,
  quantitative energy modeling, qualitative analysis, and case studies. We also recommend
  that researchers beware of hierarchies of evidence utilized in some disciplines,
  and that researchers place more emphasis on balance and appropriateness in research
  design. In terms of style, we offer tips regarding macro and microstructure and
  analysis, as well as coherent writing. Our hope is that this Review will inspire
  more interesting, robust, multi-method, comparative, interdisciplinary and impactful
  research that will accelerate the contribution that energy social science can make
  to both theory and practice.'
author: Benjamin K. Sovacool and Jonn Axsen and Steve Sorrell
categories: Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Nuclear
  Energy and Engineering (Q1); Renewable Energy, Sustainability and the Environment
  (Q1); Social Sciences (miscellaneous) (Q1)
citable_docs._(3years): 801.0
cites_/_doc._(2years): 686.0
country: United Kingdom
coverage: 2014-2020
doi: 10.1016/j.erss.2018.07.007
eigenfactor_score: .nan
h_index: 63.0
isbn: null
issn: '22146296'
issn1: '22146296'
issn2: '22146296'
issn3: '22146296'
jcr_value: null
keywords: Validity, Research methods, Research methodology, Interdisciplinary research,
  Research excellence
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 7949.0
region: Western Europe
scimago_value: 2313.0
sjr_best_quartile: Q1
sourceid: 21100325067.0
title_bib: 'Promoting novelty, rigor, and style in energy social science: towards
  codes of practice for appropriate methods and research design'
title_csv: Energy research and social science
total_cites: .nan
total_cites_(3years): 5449.0
total_docs._(2020): 419.0
total_docs._(3years): 801.0
total_refs.: 33308.0
type: journal
type_publication: article
year: 2018
---
abstract: "The common and various forms of Twitter information render this one of\
  \ the best controlling and recording virtual environments of information. The growth\
  \ in social media nowadays gives internet users immense interest. In several pups\
  \ like prediction, advertisement, sentiment analysis \u2026, the data on such a\
  \ social network platform is used. People exchange good or bad views on problems,\
  \ items and administrations through the web and informal communities. The capacity\
  \ to assess such a data productively is presently observed as a noteworthy upper\
  \ hand in settling on choices all the more proficiently. In this sense, associations\
  \ use methods, for example, Sentiment Analysis (SA). The utilization of web based\
  \ life around the globe is growing, however, greatly speeding up mass data generation\
  \ and stopping us from providing useful insights in conventional SA systems. These\
  \ data volumes can be processed effectively, using SA and Big Data technology. Big\
  \ data is not a luxury, in fact, but an important prediction."
author: Harika Vanam and Jeberson {Retna Raj R}
categories: Materials Science (miscellaneous)
citable_docs._(3years): 10409.0
cites_/_doc._(2years): 124.0
country: United Kingdom
coverage: 2005, 2014-2020
doi: 10.1016/j.matpr.2020.11.486
eigenfactor_score: .nan
h_index: 47.0
isbn: null
issn: '22147853'
issn1: '22147853'
issn2: '22147853'
issn3: '22147853'
jcr_value: null
keywords: Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics,
  Machine learning algorithm
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 2215.0
region: Western Europe
scimago_value: 341.0
sjr_best_quartile: '-'
sourceid: 21100370037.0
title_bib: Analysis of twitter data through big data based sentiment analysis approaches
title_csv: 'Materials today: proceedings'
total_cites: .nan
total_cites_(3years): 14096.0
total_docs._(2020): 3996.0
total_docs._(3years): 10585.0
total_refs.: 88521.0
type: conference and proceedings
type_publication: article
year: 2021
---
abstract: "ABSTRACT\nThe world continues to experience a surge in data generation\
  \ and digital transformation. Historic data is increasingly being replaced by modernized\
  \ data, such as big data, which is regarded as data that exhibits the 5Vs: volume,\
  \ variety, velocity, veracity and value. The capacity to optimally use and comprehend\
  \ value from big data has become an indispensable aptitude for modern companies.\
  \ In contrast to commercial and technology firms, usage, management and governance\
  \ of data, including big data is a novel and evolving trend for mining and mineral\
  \ industries. Although the mining industry can be unenthusiastic to change, embracing\
  \ modernized data and big data is evolutionarily unavoidable, given many industry-wide\
  \ challenges (i.e., fluctuation in commodity prices, geotechnical and harsh ground\
  \ conditions, and ore grade), which corrode revenues and increase business risks,\
  \ including the possibility of regulatory non-compliance. The minerals industry\
  \ holds a genuine gold mine of data that were collected for scientific, engineering,\
  \ operational and other purposes. Data and data-centric workspaces that are targeted\
  \ towards innovation and experimentation, which if combined with in-discipline expertise\
  \ are two harmonious ingredients that can provide many practical solutions for the\
  \ mining and mineral industries. In this paper, the concept, the opportunity and\
  \ the necessity for a move towards a technology- and innovation-based, data-centric\
  \ \u2018dry laboratories\u2019 (common workspaces that facilitates data-centric\
  \ experimentation and innovation) in the minerals industry are assessed. We contend\
  \ that the dry laboratory environment maximizes the value of data for the minerals\
  \ industry. Toward the establishment of dry laboratories, we propose several essential\
  \ components of a framework that would enable the functionality of dry laboratories\
  \ in the minerals industry, while concomitantly examining the components from both\
  \ academia and industry perspectives."
author: Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau
categories: Development (Q1); Economic Geology (Q1); Geography, Planning and Development
  (Q1); Management, Monitoring, Policy and Law (Q1)
citable_docs._(3years): 313.0
cites_/_doc._(2years): 359.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.exis.2022.101089
eigenfactor_score: .nan
h_index: 29.0
isbn: null
issn: 2214790X
issn1: 2214790X
issn2: 2214790X
issn3: 2214790X
jcr_value: null
keywords: Dry laboratory, Data analytics, Process simulation, Mining industry, Data-centric,
  Data-driven
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6198.0
region: Western Europe
scimago_value: 999.0
sjr_best_quartile: Q1
sourceid: 21100305261.0
title_bib: 'Framework components for data-centric dry laboratories in the minerals
  industry: a path to science-and-technology-led innovation'
title_csv: Extractive industries and society
total_cites: .nan
total_cites_(3years): 1173.0
total_docs._(2020): 206.0
total_docs._(3years): 337.0
total_refs.: 12767.0
type: journal
type_publication: article
year: 2022
---
abstract: The massive rise of Big Data generated from smartphones, social media, Internet
  of Things (IoT), and multimedia, has produced an overwhelming flow of data in either
  structured or unstructured format. Big Data technologies are being developed and
  implemented in the food supply chain that gather and analyse these data. Such technologies
  demand new approaches in data collection, storage, processing and knowledge extraction.
  In this article, an overview of the recent developments in Big Data applications
  in food safety are presented. This review shows that the use of Big Data in food
  safety remains in its infancy but it is influencing the entire food supply chain.
  Big Data analysis is used to provide predictive insights in several steps in the
  food supply chain, support supply chain actors in taking real time decisions, and
  design the monitoring and sampling strategies. Lastly, the main research challenges
  that require research efforts are introduced.
author: Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke
  M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel}
  and Wouter Hoenderdaal and Hans J.P. Marvin
categories: Applied Microbiology and Biotechnology (Q1); Food Science (Q1)
citable_docs._(3years): 286.0
cites_/_doc._(2years): 516.0
country: Netherlands
coverage: 2015-2021
doi: 10.1016/j.cofs.2020.11.006
eigenfactor_score: 0.00491
h_index: 38.0
isbn: null
issn: '22147993'
issn1: '22147993'
issn2: '22147993'
issn3: '22147993'
jcr_value: '6.031'
keywords: null
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5725.0
region: Western Europe
scimago_value: 1297.0
sjr_best_quartile: Q1
sourceid: 21100370190.0
title_bib: Big data in food safety- a review
title_csv: Current opinion in food science
total_cites: 3298.0
total_cites_(3years): 1714.0
total_docs._(2020): 89.0
total_docs._(3years): 318.0
total_refs.: 5095.0
type: journal
type_publication: article
year: 2020
---
abstract: "Within CONNECTING Nature, we are dealing with developing innovative nature-based\
  \ solutions (NBS) for climate change adaptation, health and well-being, social cohesion\
  \ and sustainable economic development in European cities. In order to enable \u201C\
  learning by comparing\u201D and \u201Cgenerating new knowledge\u201D from multiple\
  \ NBS related studies, a novel data and knowledge base is needed which requires\
  \ a specified methodological approach for its development. This paper provides conceptual\
  \ and methodological context and techniques for constructing such a data and knowledge\
  \ base that will systematically support the process of NBS monitoring and assessment:\u2022\
  A methodology presents the comprehensive, multi-step approach to the NBS data and\
  \ knowledge development that helps to guide work and influence the quality of an\
  \ information included.\u2022The paper describes the methodology and main steps/phases\
  \ for developing a large data and knowledge base of NBS that will allow further\
  \ systematic review.\u2022The suggested methodology explains how to build NBS related\
  \ databases from the conceptualization and requirements phases through to implementation\
  \ and maintenance. In this regard, such a methodology is iterative, with extensive\
  \ NBS stakeholders\u2019 and end-user's involvement that are packaged with reusable\
  \ templates or deliverables offering a good opportunity for success when used by\
  \ practitioners and other end-users.\u2022The NBS data and knowledge base gathers\
  \ information about different NBS models and generations into one easy-to-find,\
  \ easy-to-use place and provides detailed descriptions of each of the 1490 NBS cases\
  \ from urban centers in Europe.\u2022The data and knowledge base thus helps users\
  \ identify the best and most appropriated NBS model/type for addressing the particular\
  \ goals and, at the same time, considers the local context and potential.\u2022\
  The data obtained can be used for the further meta-analysis by applying statistics\
  \ or searching for specific sample cases and thus enables to generate and expand\
  \ the knowledge from multiple NBS related studies, in both qualitative and quantitative\
  \ ways."
author: Diana Dushkova and Dagmar Haase
categories: Medical Laboratory Technology (Q2); Clinical Biochemistry (Q3)
citable_docs._(3years): 548.0
cites_/_doc._(2years): 184.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.mex.2020.101096
eigenfactor_score: .nan
h_index: 23.0
isbn: null
issn: '22150161'
issn1: '22150161'
issn2: '22150161'
issn3: '22150161'
jcr_value: null
keywords: Nature-based solutions (NBS), Data- and knowledge base, Climate change,
  Societal challenges, Sustainability, Resilience, Urban Europe
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 1809.0
region: Western Europe
scimago_value: 356.0
sjr_best_quartile: Q2
sourceid: 21100317906.0
title_bib: 'Methodology for development of a data and knowledge base for learning
  from existing nature-based solutions in europe: the connecting nature project'
title_csv: Methodsx
total_cites: .nan
total_cites_(3years): 1058.0
total_docs._(2020): 423.0
total_docs._(3years): 548.0
total_refs.: 7652.0
type: journal
type_publication: article
year: 2020
---
abstract: 'Summary

  There is widespread agreement by health-care providers, medical associations, industry,
  and governments that automation using digital technology could improve the delivery
  and quality of care in psychiatry, and reduce costs. Many benefits from technology
  have already been realised, along with the identification of many challenges. In
  this Review, we discuss some of the challenges to developing effective automation
  for psychiatry to optimise physician treatment of individual patients. Using the
  perspective of automation experts in other industries, three examples of automation
  in the delivery of routine care are reviewed: (1) effects of electronic medical
  records on the patient interview; (2) effects of complex systems integration on
  e-prescribing; and (3) use of clinical decision support to assist with clinical
  decision making. An increased understanding of the experience of automation from
  other sectors might allow for more effective deployment of technology in psychiatry.'
author: Michael Bauer and Scott Monteith and John Geddes and Michael J Gitlin and
  Paul Grof and Peter C Whybrow and Tasha Glenn
categories: Biological Psychiatry (Q1); Psychiatry and Mental Health (Q1)
citable_docs._(3years): 224.0
cites_/_doc._(2years): 488.0
country: United Kingdom
coverage: 2014-2020
doi: 10.1016/S2215-0366(19)30041-0
eigenfactor_score: .nan
h_index: 86.0
isbn: null
issn: '22150366'
issn1: '22150374'
issn2: '22150366'
issn3: '22150374'
jcr_value: null
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 1646.0
region: Western Europe
scimago_value: 7447.0
sjr_best_quartile: Q1
sourceid: 21100356804.0
title_bib: 'Automation to optimise physician treatment of individual patients: examples
  in psychiatry'
title_csv: Lancet psychiatry,the
total_cites: .nan
total_cites_(3years): 4496.0
total_docs._(2020): 347.0
total_docs._(3years): 937.0
total_refs.: 5712.0
type: journal
type_publication: article
year: 2019
---
abstract: Under the trend of economic globalization, intelligent manufacturing has
  attracted a lot of attention from academic and industry. Related enabling technologies
  make manufacturing industry more intelligent. As one of the key technologies in
  artificial intelligence, big data driven analysis improves the market competitiveness
  of manufacturing industry by mining the hidden knowledge value and potential ability
  of industrial big data, and helps enterprise leaders make wise decisions in various
  complex manufacturing environments. This paper provides a theoretical analysis basis
  for big data-driven technology to guide decision-making in intelligent manufacturing,
  fully demonstrating the practicability of big data-driven technology in the intelligent
  manufacturing industry, including key advantages and internal motivation. A conceptual
  framework of intelligent decision-making based on industrial big data-driven technology
  is proposed in this study, which provides valuable insights and thoughts for the
  severe challenges and future research directions in this field.
author: Chunquan Li and Yaqiong Chen and Yuling Shang
categories: Civil and Structural Engineering (Q1); Computer Networks and Communications
  (Q1); Electronic, Optical and Magnetic Materials (Q1); Fluid Flow and Transfer Processes
  (Q1); Hardware and Architecture (Q1); Mechanical Engineering (Q1); Metals and Alloys
  (Q1); Biomaterials (Q2)
citable_docs._(3years): 407.0
cites_/_doc._(2years): 509.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.jestch.2021.06.001
eigenfactor_score: .nan
h_index: 50.0
isbn: null
issn: '22150986'
issn1: '22150986'
issn2: '22150986'
issn3: '22150986'
jcr_value: null
keywords: Intelligent manufacturing, Artificial intelligence, Industrial big data,
  Big data-driven technology, Decision-making
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4084.0
region: Western Europe
scimago_value: 803.0
sjr_best_quartile: Q1
sourceid: 21100806003.0
title_bib: A review of industrial big data for decision making in intelligent manufacturing
title_csv: Engineering science and technology, an international journal
total_cites: .nan
total_cites_(3years): 2346.0
total_docs._(2020): 132.0
total_docs._(3years): 408.0
total_refs.: 5391.0
type: journal
type_publication: article
year: 2022
---
abstract: "Big data analytics and artificial intelligence, paired with blockchain\
  \ technology, the Internet of Things, and other emerging technologies, are poised\
  \ to revolutionise urban management. With massive amounts of data collected from\
  \ citizens, devices, and traditional sources such as routine and well-established\
  \ censuses, urban areas across the world have \u2013 for the first time in history\
  \ \u2013 the opportunity to monitor and manage their urban infrastructure in real-time.\
  \ This simultaneously provides previously unimaginable opportunities to shape the\
  \ future of cities, but also gives rise to new ethical challenges. This paper provides\
  \ a transdisciplinary synthesis of the developments, opportunities, and challenges\
  \ for urban management and planning under this ongoing \u2018digital revolution\u2019\
  \ to provide a reference point for the largely fragmented research efforts and policy\
  \ practice in this area. We consider both top-down systems engineering approaches\
  \ and the bottom-up emergent approaches to coordination of different systems and\
  \ functions, their implications for the existing physical and institutional constraints\
  \ on the built environment and various planning practices, as well as the social\
  \ and ethical considerations associated with this transformation from non-digital\
  \ urban management to data-driven urban management."
author: Zeynep Engin and Justin {van Dijk} and Tian Lan and Paul A. Longley and Philip
  Treleaven and Michael Batty and Alan Penn
categories: Urban Studies (Q1); Geography, Planning and Development (Q2); Public Administration
  (Q2)
citable_docs._(3years): 60.0
cites_/_doc._(2years): 313.0
country: Netherlands
coverage: 2012-2020
doi: 10.1016/j.jum.2019.12.001
eigenfactor_score: .nan
h_index: 11.0
isbn: null
issn: '22265856'
issn1: '25890360'
issn2: '22265856'
issn3: '25890360'
jcr_value: null
keywords: Data-driven society, Urban management and applications, Evidence-based decision
  making
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4028.0
region: Western Europe
scimago_value: 587.0
sjr_best_quartile: Q1
sourceid: 21100907127.0
title_bib: 'Data-driven urban management: mapping the landscape'
title_csv: Journal of urban management
total_cites: .nan
total_cites_(3years): 210.0
total_docs._(2020): 32.0
total_docs._(3years): 68.0
total_refs.: 1289.0
type: journal
type_publication: article
year: 2020
---
abstract: Systematic Literature Review (SLR) is a structured way of conducting a review
  of existing research works produced by the earlier researchers. The application
  of right data analysis technique during the SLR evaluation stage would give an insight
  to the researcher in achieving the SLR objective. This paper presents how descriptive
  analysis and text analysis can be applied to achieve one of the common SLR objectives
  which is to study the progress of specific research domain. These techniques have
  been demonstrated to synthesis the progress of Master Data Management research domain.
  Using descriptive analysis technique, this study has identified a trend of related
  literary works distribution by years, sources, and publication types. Meanwhile,
  text analysis shows the common terms and interest topics in the Master Data Management
  research which are 1) master data, 2) data quality, 3) business intelligence, 4)
  business process, 5) data integration, 6) big data, 7) data governance, 8) information
  governance, 9) data management and 10) product data. It is hoped that other researchers
  would be able to replicate these analysis techniques in performing SLR for other
  research domains.
author: Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan
categories: Computer Networks and Communications; Human-Computer Interaction; Information
  Systems
citable_docs._(3years): 161.0
cites_/_doc._(2years): 20.0
country: United States
coverage: 2013, 2017
doi: 10.1109/ICRIIS.2017.8002473
eigenfactor_score: .nan
h_index: 12.0
isbn: null
issn: '23248157'
issn1: '23248149'
issn2: '23248157'
issn3: '23248149'
jcr_value: null
keywords: Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text
  mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text
  Analysis;Master Data Management
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 178.0
sjr_best_quartile: '-'
sourceid: 21100298607.0
title_bib: 'Descriptive analysis and text analysis in systematic literature review:
  a review of master data management'
title_csv: International conference on research and innovation in information systems,
  icriis
total_cites: .nan
total_cites_(3years): 123.0
total_docs._(2020): 0.0
total_docs._(3years): 163.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2017
---
abstract: The effects of COVID-19 have quickly spread around the world, testing the
  limits of the population and the public health sector. High demand on medical services
  are offset by disruptions in daily operations as hospitals struggle to function
  in the face of overcapacity, understaffing and information gaps. Faced with these
  problems, new technologies are being deployed to fight this pandemic and help medical
  staff governments to reduce its spread. Among these technologies, we find blockchains
  and Big Data which have been used in tracking, prediction applications and others.
  However, despite the help that these new technologies have provided, they remain
  limited if the data with which they are fed are not of good quality. In this paper,
  we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic
  and some data quality issues that still present challenges to decision making. Finally
  we present a general Blockchain-based framework for data governance that aims to
  ensure a high level of data trust, security, and privacy.
author: Ezzine, Imane and Benhlima, Laila
categories: Computer Science Applications; Information Systems and Management; Management
  Science and Operations Research; Signal Processing
citable_docs._(3years): 128.0
cites_/_doc._(2years): 114.0
country: United States
coverage: '2015'
doi: 10.1109/CiSt49399.2021.9357200
eigenfactor_score: .nan
h_index: 11.0
isbn: null
issn: '23271884'
issn1: 2327185X
issn2: '23271884'
issn3: 2327185X
jcr_value: null
keywords: COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big
  Data;Data Quality;data governance
publisher_x: null
publisher_y: null
ref._/_doc.: 0.0
region: Northern America
scimago_value: 170.0
sjr_best_quartile: '-'
sourceid: 21100400809.0
title_bib: Technology against covid-19 a blockchain-based framework for data quality
title_csv: Colloquium in information science and technology, cist
total_cites: .nan
total_cites_(3years): 139.0
total_docs._(2020): 0.0
total_docs._(3years): 132.0
total_refs.: 0.0
type: conference and proceedings
type_publication: inproceedings
year: 2020
---
abstract: 'Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a
  crowd of moving human participants to acquire, or generate, data from their environment.
  As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more
  efficient operation of a smart city. Big data techniques employed on this data produce
  inferences about the participants'' environment, the smart city. However, sufficient
  amounts of data are not always available. Sometimes, the available data are scarce
  as it is obtained at different times, locations, and from different MCS participants
  who may not be present. As a consequence, the scale of data acquired may be small
  and susceptible to errors. In such scenarios, the MCS system requires techniques
  that acquire reliable inferences from such limited data sets. To that end, we resort
  to small data (SD) techniques that are relevant for scarce and erroneous scenarios.
  In this article, we discuss SD and propose schemes to tackle the problems associated
  with such limited data sets, in the context of the smart city. We propose two novel
  quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric
  (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within
  MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension
  with MAD-Q and MADBS-Q. We show the performance of all the presented techniques
  through closed-form mathematical expressions, with which simulation results were
  found to be consistent.'
author: Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.
categories: Computer Networks and Communications (Q1); Computer Science Applications
  (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems
  and Management (Q1); Signal Processing (Q1)
citable_docs._(3years): 1555.0
cites_/_doc._(2years): 1237.0
country: United States
coverage: 2014-2020
doi: 10.1109/JIOT.2020.2994556
eigenfactor_score: 0.032080000000000004
h_index: 97.0
isbn: null
issn: '23274662'
issn1: '23274662'
issn2: '23274662'
issn3: '23274662'
jcr_value: '9.471'
keywords: Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent
  sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile
  crowdsensing (MCS);small data (SD)
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 3472.0
region: Northern America
scimago_value: 2075.0
sjr_best_quartile: Q1
sourceid: 21100338350.0
title_bib: Quality estimation for scarce scenarios within mobile crowdsensing systems
title_csv: Ieee internet of things journal
total_cites: 21151.0
total_cites_(3years): 20461.0
total_docs._(2020): 1163.0
total_docs._(3years): 1594.0
total_refs.: 40380.0
type: journal
type_publication: article
year: 2020
---
abstract: 'Federated Learning (FL) has emerged as a privacy-preserving distributed
  machine learning paradigm. To motivate data owners to contribute towards FL, research
  on FL incentive mechanisms is gaining great interest. Existing monetary incentive
  mechanisms generally share the same FL model with all participants regardless of
  their contributions. Such an assumption can be unfair towards participants who contributed
  more and promote undesirable free-riding, especially when the final model is of
  great utility value to participants. In this paper, we propose a Fairness-Aware
  Incentive Mechanism for federated learning (FedFAIM) to address such problem. It
  satisfies two types of fairness notion: 1) aggregation fairness, which determines
  aggregation results according to data quality; 2) reward fairness, which assigns
  each participant a unique model with performance reflecting his contribution. Aggregation
  fairness is achieved through efficient gradient aggregation which examines local
  gradient quality and aggregates them based on data quality. Reward fairness is achieved
  through an efficient Shapley value-based contribution assessment method and a novel
  reward allocation method based on reputation and distribution of local and global
  gradients. We further prove reward fairness is theoretically guaranteed. Extensive
  experiments show that FedFAIM provides stronger incentives than similar non-monetary
  FL incentive mechanisms while achieving a high level of fairness.'
author: Shi, Zhuan and Zhang, Lan and Yao, Zhenyu and Lyu, Lingjuan and Chen, Cen
  and Wang, Li and Wang, Junhao and Li, Xiang-Yang
categories: Information Systems (Q1); Information Systems and Management (Q1)
citable_docs._(3years): 8.0
cites_/_doc._(2years): 411.0
country: United States
coverage: '2020'
doi: 10.1109/TBDATA.2022.3183614
eigenfactor_score: 0.00134
h_index: 6.0
isbn: null
issn: '23327790'
issn1: '23327790'
issn2: '23327790'
issn3: '23327790'
jcr_value: '3.344'
keywords: Computational modeling;Resource management;Servers;Training;Collaborative
  work;Particle measurements;Atmospheric measurements;Federated Learning;Incentive
  Mechanism;Fairness
publisher_x: Institute of Electrical and Electronics Engineers Inc.
publisher_y: null
ref._/_doc.: 1406.0
region: Northern America
scimago_value: 959.0
sjr_best_quartile: Q1
sourceid: 21101019393.0
title_bib: 'Fedfaim: a model performance-based fair incentive mechanism for federated
  learning'
title_csv: Ieee transactions on big data
total_cites: 636.0
total_cites_(3years): 37.0
total_docs._(2020): 71.0
total_docs._(3years): 9.0
total_refs.: 998.0
type: journal
type_publication: article
year: 2022
---
abstract: Prominent urbanization forces governments to rethink their management processes,
  incorporate new technologies and ensure quality of life through practices aligned
  with the concepts of smart and sustainable cities. With a qualitative approach through
  semi-structured interviews with commanders of two local police departments, this
  study investigated information orientation in the strategic decision-making process
  in the area of public safety in a small Brazilian city. The police departments have
  a limited ICT infrastructure to support the strategic decision-making process because
  their information systems are not connected. The results show that although the
  city of Pato Branco (Brazil) is considered a smart city in the area of public security,
  there are limited resources in several aspects of the police departments for the
  effective management of their ICT infrastructures. The impact of resource constraints
  reflects throughout the entire information use lifecycle - identification, collection,
  organization, processing, etc. - which fuels the strategic decision-making process.
  The implantation of an operations center could significantly reduce the effects
  of the problems identified in this research and further research may reveal the
  operational, technical, economic and financial viability of this proposal.
author: M. Colla and G.D. Santos
categories: Artificial Intelligence (Q2); Industrial and Manufacturing Engineering
  (Q2)
citable_docs._(3years): 3572.0
cites_/_doc._(2years): 179.0
country: Netherlands
coverage: 2015-2020
doi: 10.1016/j.promfg.2020.01.238
eigenfactor_score: .nan
h_index: 43.0
isbn: null
issn: '23519789'
issn1: '23519789'
issn2: '23519789'
issn3: '23519789'
jcr_value: null
keywords: Smart cities, Sustainable cities, Information orientation, Information,
  Communication Technologies, Decision-making
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 1997.0
region: Western Europe
scimago_value: 504.0
sjr_best_quartile: Q2
sourceid: 21100792109.0
title_bib: Public safety decision-making in the context of smart and sustainable cities
title_csv: Procedia manufacturing
total_cites: .nan
total_cites_(3years): 8346.0
total_docs._(2020): 1390.0
total_docs._(3years): 3628.0
total_refs.: 27760.0
type: journal
type_publication: article
year: 2019
---
abstract: Based on the concept and research status of big data, we analyze and examine
  the importance of constructing the knowledge system of nursing science for the development
  of the nursing discipline in the context of big data and propose that it is necessary
  to establish big data centers for nursing science to share resources, unify language
  standards, improve professional nursing databases, and establish a knowledge system
  structure.
author: Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and
  Zhiguang Duan
categories: Nursing (miscellaneous) (Q1)
citable_docs._(3years): 199.0
cites_/_doc._(2years): 262.0
country: Singapore
coverage: 2014-2020
doi: 10.1016/j.ijnss.2019.03.001
eigenfactor_score: .nan
h_index: 16.0
isbn: null
issn: '23520132'
issn1: '23520132'
issn2: '23520132'
issn3: '23520132'
jcr_value: null
keywords: Artificial intelligence, Data mining, Knowledge bases, Nursing
publisher_x: Elsevier (Singapore) Pte Ltd
publisher_y: null
ref._/_doc.: 3280.0
region: Asiatic Region
scimago_value: 703.0
sjr_best_quartile: Q1
sourceid: 21100469749.0
title_bib: 'The application of big data and the development of nursing science: a
  discussion paper'
title_csv: International journal of nursing sciences
total_cites: .nan
total_cites_(3years): 526.0
total_docs._(2020): 87.0
total_docs._(3years): 229.0
total_refs.: 2854.0
type: journal
type_publication: article
year: 2019
---
abstract: The existing prediction model of eco-environmental water demand has the
  problem of large prediction error. In order to solve the above problems, the prediction
  model of eco-environmental water demand is constructed based on big data analysis.
  In order to reduce the prediction error of the ecological environment water demand
  prediction model, the framework of the ecological environment water demand prediction
  model is built. On this basis, the principal component analysis method is used to
  select the auxiliary variables of the model. Based on the selected auxiliary variables,
  the minimum monthly average flow method is used to analyze the basic water demand
  of the ecological environment, the leakage water demand and the water surface evaporation
  ecological environment water demand, so as to analyze based on the results, the
  water demand of ecological environment is predicted by big data analysis technology,
  and the prediction of water demand of ecological environment is realized. The experimental
  results show that compared with the existing ecological environment water demand
  prediction model, the prediction error of the model is within 19.3, which fully
  shows that the constructed ecological environment water demand prediction model
  has better prediction effect and can provide a certain reference value for the actual
  use of water resources.
author: Lihong Zhao
categories: Environmental Science (miscellaneous) (Q1); Plant Science (Q1); Soil Science
  (Q1)
citable_docs._(3years): 310.0
cites_/_doc._(2years): 551.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.eti.2020.101196
eigenfactor_score: .nan
h_index: 28.0
isbn: null
issn: '23521864'
issn1: '23521864'
issn2: '23521864'
issn3: '23521864'
jcr_value: null
keywords: Big data analysis, Ecological environment, Water demand, Prediction
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5748.0
region: Western Europe
scimago_value: 866.0
sjr_best_quartile: Q1
sourceid: 21100385961.0
title_bib: Prediction model of ecological environmental water demand based on big
  data analysis
title_csv: Environmental technology and innovation
total_cites: .nan
total_cites_(3years): 1661.0
total_docs._(2020): 409.0
total_docs._(3years): 310.0
total_refs.: 23508.0
type: journal
type_publication: article
year: 2021
---
abstract: In the era of bioinformatics and big data, ecological research depends on
  large and easily accessible databases that make it possible to construct complex
  system models. Open-access data repositories for food webs via publications and
  ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain
  ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the
  trophic connections (predator-prey relationships) for the Danube River ecosystem
  as gathered from globally available literature data. Data are analyzed by Danube
  regions separately (Upper, Middle, Lower Danube) as well as an integrated master
  network version. The master version has been aggregated into larger taxonomic categories.
  Local and global metrics were used to analyze and compare each network. We find
  disparity between regions (the Middle Danube having most nodes, but still quite
  heterogenous), we identify the most important trophic groups, and explain ways on
  evaluating missing data using each aggregation stage. This data-driven approach,
  summarizing our presently documented knowledge, can be used for preparing preliminary
  models and to further refine the Danube River food web in the future.
author: "Katalin Patonai and Ferenc Jord\xE1n"
categories: Ecology (Q1); Ecology, Evolution, Behavior and Systematics (Q2)
citable_docs._(3years): 91.0
cites_/_doc._(2years): 262.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.fooweb.2021.e00203
eigenfactor_score: .nan
h_index: 15.0
isbn: null
issn: '23522496'
issn1: '23522496'
issn2: '23522496'
issn3: '23522496'
jcr_value: null
keywords: Aggregation, Danube River, Food web, Incomplete data, Taxonomy
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6240.0
region: Western Europe
scimago_value: 847.0
sjr_best_quartile: Q1
sourceid: 21100365104.0
title_bib: 'Integrating trophic data from the literature: the danube river food web'
title_csv: Food webs
total_cites: .nan
total_cites_(3years): 224.0
total_docs._(2020): 43.0
total_docs._(3years): 93.0
total_refs.: 2683.0
type: journal
type_publication: article
year: 2021
---
abstract: "Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter\
  \ electronic health records. Cerner Corporation secured appropriate data use agreements\
  \ and permissions from more than 100 health systems in the United States contributing\
  \ to the database as of March 2022. A subset of the database was extracted to include\
  \ data from only patients with SARS-CoV-2 infections and is referred to as the Cerner\
  \ COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients\
  \ and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9\
  \ billion, 486 million, and 11.5 billion records\_in the condition, medication,\
  \ procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19\
  \ Dataset consists of 130.1 million encounters from 3.8 million patients. The size\
  \ and longitudinal nature of\_CRWD can be leveraged for advanced analytics and artificial\
  \ intelligence in medical research across all specialties and is a rich source of\
  \ novel discoveries on a wide range of conditions including but not limited to COVID-19."
author: Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and
  Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster
categories: Education (Q4); Multidisciplinary (Q4)
citable_docs._(3years): 8.0
cites_/_doc._(2years): 113.0
country: Netherlands
coverage: 2014-2020
doi: 10.1016/j.dib.2022.108120
eigenfactor_score: .nan
h_index: 30.0
isbn: null
issn: '23523409'
issn1: '23523409'
issn2: '23523409'
issn3: '23523409'
jcr_value: null
keywords: "Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records\
  \ (EHR), HealtheIntent, HealtheDataLab\u2122, Cerner learning Health Network (LHN)"
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 837.0
region: Western Europe
scimago_value: 122.0
sjr_best_quartile: Q4
sourceid: 21100372856.0
title_bib: Cerner real-world data (crwd) - a de-identified multicenter electronic
  health records database
title_csv: Data in brief
total_cites: .nan
total_cites_(3years): 3980.0
total_docs._(2020): 1742.0
total_docs._(3years): 3537.0
total_refs.: 14584.0
type: journal
type_publication: article
year: 2022
---
abstract: "A series of well-characterized specimens, known as the Peabody-Yale Reference\
  \ Obsidians (PYRO) sets, has been designed to aid with calibrating and assessing\
  \ X-ray fluorescence analysis (XRF) data, including portable XRF (pXRF) measurements,\
  \ for obsidian sourcing. Each of these ten matched sets consists of 35 specimens:\
  \ 20 specimens for calibration and 15 specimens for evaluation. These sets include\
  \ not only obsidians with common geochemical compositions (i.e., alkaline rhyolites)\
  \ but also rarer ones (i.e., peralkaline rhyolitic, trachytic, and andesitic specimens,\
  \ including East African Rift obsidians). When used as described, the PYRO sets\
  \ are suitable to calibrate and evaluate XRF data for obsidians worldwide. A set\
  \ can be borrowed following loan policies of the Peabody Museum of Natural History,\
  \ which will also accession a set. Publishing all source information \u2013 from\
  \ their names and GPS coordinates to the datasets used for the recommended values\
  \ \u2013 not only allows the sets to be replicated by others but also fulfills the\
  \ demands of scientific transparency. Their main purpose is facilitating collaborations\
  \ and \u201Cbig data\u201D projects, and the PYRO sets were designed to complement\
  \ existing protocols for calibration and evaluation. In short, the sets are intended\
  \ as a tool for almost anyone (e.g., a student who borrows an instrument to source\
  \ artifacts) to meet \u2013 and even exceed \u2013 experts' practices involving\
  \ transparency, accuracy, and reproducibility."
author: Ellery Frahm
categories: Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1)
citable_docs._(3years): 1365.0
cites_/_doc._(2years): 167.0
country: Netherlands
coverage: 2015-2020
doi: 10.1016/j.jasrep.2019.101957
eigenfactor_score: .nan
h_index: 26.0
isbn: null
issn: 2352409X
issn1: 2352409X
issn2: 2352409X
issn3: 2352409X
jcr_value: null
keywords: Obsidian sourcing, EDXRF, Portable XRF, Accuracy, Reproducibility, Transparency,
  Collaboration
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6751.0
region: Western Europe
scimago_value: 840.0
sjr_best_quartile: Q1
sourceid: 21100369721.0
title_bib: 'Introducing the peabody-yale reference obsidians (pyro) sets: open-source
  calibration and evaluation standards for quantitative x-ray fluorescence analysis'
title_csv: 'Journal of archaeological science: reports'
total_cites: .nan
total_cites_(3years): 2514.0
total_docs._(2020): 527.0
total_docs._(3years): 1389.0
total_refs.: 35578.0
type: journal
type_publication: article
year: 2019
---
abstract: Advanced technologies are required in future mobile wireless networks to
  support services with highly diverse requirements in terms of high data rate and
  reliability, low latency, and massive access. Deep Learning (DL), one of the most
  exciting developments in machine learning and big data, has recently shown great
  potential in the study of wireless communications. In this article, we provide a
  literature review on the applications of DL in the physical layer. First, we analyze
  the limitations of existing signal processing techniques in terms of model accuracy,
  global optimality, and computational scalability. Next, we provide a brief review
  of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer
  technologies, including both DL-based signal processing modules and end-to-end systems.
  Deep neural networks are used to replace a single or several conventional functional
  modules, whereas the objective of the latter is to replace the entire transceiver
  structure. Lastly, we discuss the open issues and research directions of the DL-based
  physical layer in terms of model complexity, data quality, data representation,
  and algorithm reliability.
author: Siqi Liu and Tianyu Wang and Shaowei Wang
categories: Communication (Q1); Computer Networks and Communications (Q1); Hardware
  and Architecture (Q1)
citable_docs._(3years): 96.0
cites_/_doc._(2years): 881.0
country: China
coverage: 2015-2020
doi: 10.1016/j.dcan.2021.09.014
eigenfactor_score: 0.0013800000000000002
h_index: 26.0
isbn: null
issn: '23528648'
issn1: '23528648'
issn2: '24685925'
issn3: '23528648'
jcr_value: '6.797'
keywords: Data-driven, Deep learning, Physical layer, Wireless communications
publisher_x: Chongqing University of Posts and Telecommunications
publisher_y: null
ref._/_doc.: 4190.0
region: Asiatic Region
scimago_value: 1082.0
sjr_best_quartile: Q1
sourceid: 21100823476.0
title_bib: 'Toward intelligent wireless communications: deep learning - based physical
  layer technologies'
title_csv: Digital communications and networks
total_cites: 823.0
total_cites_(3years): 881.0
total_docs._(2020): 77.0
total_docs._(3years): 105.0
total_refs.: 3226.0
type: journal
type_publication: article
year: 2021
---
abstract: Global lockdowns in response to the COVID-19 pandemic have led to changes
  in the anthropogenic activities resulting in perceivable air quality improvements.
  Although several recent studies have analyzed these changes over different regions
  of the globe, these analyses have been constrained due to the usage of station based
  data which is mostly limited up to the metropolitan cities. Also the quantifiable
  changes have been reported only for the developed and developing regions leaving
  the poor economies (e.g. Africa) due to the shortage of in-situ data. Using a comprehensive
  set of high spatiotemporal resolution satellites and merged products of air pollutants,
  we analyze the air quality across the globe and quantify the improvement resulting
  from the suppressed anthropogenic activity during the lockdowns. In particular,
  we focus on megacities, capitals and cities with high standards of living to make
  the quantitative assessment. Our results offer valuable insights into the spatial
  distribution of changes in the air pollutants due to COVID-19 enforced lockdowns.
  Statistically significant reductions are observed over megacities with mean reduction
  by 19.74%, 7.38% and 49.9% in nitrogen dioxide (NO2), aerosol optical depth (AOD)
  and PM2.5 concentrations. Google Earth Engine empowered cloud computing based remote
  sensing is used and the results provide a testbed for climate sensitivity experiments
  and validation of chemistry-climate models. Additionally, Google Earth Engine based
  apps have been developed to visualize the changes in a real-time fashion.
author: Manmeet Singh and Bhupendra Bahadur Singh and Raunaq Singh and Badimela Upendra
  and Rupinder Kaur and Sukhpal Singh Gill and Mriganka Sekhar Biswas
categories: Geography, Planning and Development (Q1); Computers in Earth Sciences
  (Q2)
citable_docs._(3years): 220.0
cites_/_doc._(2years): 337.0
country: Netherlands
coverage: 2015-2020
doi: 10.1016/j.rsase.2021.100489
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '23529385'
issn1: '23529385'
issn2: '23529385'
issn3: '23529385'
jcr_value: null
keywords: COVID19, Google earth engine, PM, NO, AOD, Tropospheric ozone, Cloud computing
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5467.0
region: Western Europe
scimago_value: 703.0
sjr_best_quartile: Q1
sourceid: 21100416071.0
title_bib: Quantifying covid-19 enforced global changes in atmospheric pollutants
  using cloud computing based remote sensing
title_csv: 'Remote sensing applications: society and environment'
total_cites: .nan
total_cites_(3years): 780.0
total_docs._(2020): 143.0
total_docs._(3years): 220.0
total_refs.: 7818.0
type: journal
type_publication: article
year: 2021
---
abstract: Data quality issues have special implications in network data. Data glitches
  are propagated rapidly along pathways dictated by the hierarchy and topology of
  the network. In this paper, we use temporal data from a vast data network to study
  data glitches and their effect on network monitoring tasks such as anomaly detection.
  We demonstrate the consequences of cleaning the data, and develop targeted and customized
  cleaning strategies by exploiting the network hierarchy.
author: Loh, Ji Meng and Dasu, Tamraparni
categories: Computer Science Applications; Software
citable_docs._(3years): 546.0
cites_/_doc._(2years): 93.0
country: United States
coverage: '2015'
doi: 10.1109/ICDMW.2012.125
eigenfactor_score: .nan
h_index: 21.0
isbn: null
issn: '23759259'
issn1: '23759232'
issn2: '23759259'
issn3: '23759232'
jcr_value: null
keywords: Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information
  management;Data glitches;Big Data;missing values;outliers;network analysis;Earth
  Mover Distance
publisher_x: null
publisher_y: null
ref._/_doc.: 2648.0
region: Northern America
scimago_value: 212.0
sjr_best_quartile: '-'
sourceid: 21100398701.0
title_bib: Effect of data repair on mining network streams
title_csv: Ieee international conference on data mining workshops, icdmw
total_cites: .nan
total_cites_(3years): 789.0
total_docs._(2020): 131.0
total_docs._(3years): 558.0
total_refs.: 3469.0
type: conference and proceedings
type_publication: inproceedings
year: 2012
---
abstract: With the emergence of cyber-physical systems (CPS), we are now at the brink
  of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet
  of Things) is one of the foundations of this CPS revolution, which involves a large
  number of smart objects connected by networks. The volume of time series of SG equipment
  is tremendous and the raw time series are very likely to contain missing values
  because of undependable network transferring. The problem of storing a tremendous
  volume of raw time series thereby providing a solid support for precise time series
  analytics now becomes tricky. In this article, we propose a dependable time series
  analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable
  of providing a dependable data transforming from CPS to the target database with
  an extraction engine to preliminary refining raw data and further cleansing the
  data with a correction engine built on top of a sensor-network-regularization-based
  matrix factorization method. The experimental results reveal that our proposed DTSA
  framework is capable of effectively increasing the dependability of raw time series
  transforming between CPS and the target database system through the online lightweight
  extraction engine and the offline correction engine. Our proposed DTSA framework
  would be useful for other industrial big data practices.
author: Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar,
  P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping
categories: Artificial Intelligence (Q2); Computer Networks and Communications (Q2);
  Control and Optimization (Q2); Hardware and Architecture (Q2); Human-Computer Interaction
  (Q2)
citable_docs._(3years): 115.0
cites_/_doc._(2years): 320.0
country: United States
coverage: 2017-2020
doi: 10.1145/3145623
eigenfactor_score: .nan
h_index: 14.0
isbn: null
issn: 2378962X
issn1: '23789638'
issn2: 2378962X
issn3: '23789638'
jcr_value: null
keywords: cyber-physical-systems, dependable time series analytics, sensor-network-regularization-based
  matrix factorization, IoT-based smart grid
publisher_x: Association for Computing Machinery (ACM)
publisher_y: Association for Computing Machinery
ref._/_doc.: 4867.0
region: Northern America
scimago_value: 542.0
sjr_best_quartile: Q2
sourceid: 21100935201.0
title_bib: A dependable time series analytic framework for cyber-physical systems
  of iot-based smart grid
title_csv: Acm transactions on cyber-physical systems
total_cites: .nan
total_cites_(3years): 429.0
total_docs._(2020): 27.0
total_docs._(3years): 122.0
total_refs.: 1314.0
type: journal
type_publication: article
year: 2018
---
abstract: This article provides an artificial intelligence platform proposal for paint
  structure quality prediction using Big Data analytics methodologies. The whole proposal
  fits into the current trends that are outlined in the Industry 4.0 concept. The
  painting process is very complex, producing huge volumes of data, but the main problem
  is that the data comes from different data sources, often heterogeneous, and it
  is necessary to propose a way to collect and integrate them into a common repository.
  The motivation for this work were the industry requirements to solve specific problems
  that cannot be solved by standard methods but require a sophisticated and holistic
  approach. It is the application of artificial intelligence that suggests a solution
  that is not otherwise visible, and the use of standard methods would not give any
  satisfactory results. The result is the design of an artificial intelligence platform
  that has been deployed in a real manufacturing process, and the initial results
  confirm the correctness and validity of this step. We also present a data collection
  and integration architecture, which is an integral part of every big data analytics
  solution, and a principal component analysis that was used to reduce the dimensionality
  of the large number of production process data.
author: M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec
categories: Control and Systems Engineering (Q3)
citable_docs._(3years): 8351.0
cites_/_doc._(2years): 113.0
country: Austria
coverage: 2002-2019
doi: 10.1016/j.ifacol.2020.12.299
eigenfactor_score: .nan
h_index: 72.0
isbn: null
issn: '24058963'
issn1: '24058963'
issn2: '24058963'
issn3: '24058963'
jcr_value: null
keywords: artificial intelligence, automotive, big data analytics, industry 4.0, knowledge
  discovery, neural networks, prediction, principal component analysis
publisher_x: IFAC Secretariat
publisher_y: null
ref._/_doc.: 1663.0
region: Western Europe
scimago_value: 308.0
sjr_best_quartile: Q3
sourceid: 21100456158.0
title_bib: Artificial intelligence platform proposal for paint structure quality prediction
  within the industry 4.0 concept
title_csv: Ifac-papersonline
total_cites: .nan
total_cites_(3years): 9863.0
total_docs._(2020): 115.0
total_docs._(3years): 8407.0
total_refs.: 1913.0
type: journal
type_publication: article
year: 2020
---
abstract: This work assesses the quality of Internet of Things data not only as an
  intrinsic quality on how well it represents the related phenomenon but also, on
  how much information it contains to educate an artificial entity. The quality metrics
  here proposed are tested with real datasets. Also, they are implemented on OpenCPU,
  so the open data repositories can use them off-the-shelf to rate their datasets
  without computational cost and minimum human intervention, making them more attractive
  to potential users and gaining visibility and impact.
author: "Aurora Gonz\xE1lez-Vidal and Alfonso P. Ramallo-Gonz\xE1lez and Antonio F.\
  \ Skarmeta"
categories: Computer Networks and Communications (Q1); Hardware and Architecture (Q1);
  Information Systems (Q1); Software (Q1); Artificial Intelligence (Q2)
citable_docs._(3years): 140.0
cites_/_doc._(2years): 630.0
country: South Korea
coverage: 2015-2020
doi: 10.1016/j.icte.2022.06.001
eigenfactor_score: 0.0014
h_index: 22.0
isbn: null
issn: '24059595'
issn1: '24059595'
issn2: '24059595'
issn3: '24059595'
jcr_value: '4.317'
keywords: Data quality, Open data, IoT, Machine learning
publisher_x: Korean Institute of Communications Information Sciences
publisher_y: null
ref._/_doc.: 1827.0
region: Asiatic Region
scimago_value: 733.0
sjr_best_quartile: Q1
sourceid: 21100836194.0
title_bib: Intrinsic and extrinsic quality of data for open data repositories
title_csv: Ict express
total_cites: 789.0
total_cites_(3years): 813.0
total_docs._(2020): 85.0
total_docs._(3years): 141.0
total_refs.: 1553.0
type: journal
type_publication: article
year: 2022
---
abstract: "With the widespread use of digital technologies such as big data, cloud\
  \ computing and artificial intelligence in higher education, how to establish a\
  \ scientific and systematic evaluation system to turn the traditional classroom\
  \ with the one-way transmission of knowledge into an interactive space for exchanging\
  \ ideas and inspiring wisdom has become an essential task for human resource management\
  \ in universities, and a key to improving teaching quality. However, due to the\
  \ debate between scientism and humanism in teaching evaluation, studies related\
  \ to teaching performance have been isolated from human resource management, resulting\
  \ in the lack of a systematic vision and framework for such studies. Relevant studies\
  \ are still limited to the evaluation contents of different evaluation subjects.\
  \ Evaluations also tend to focus only on the teaching process, ignoring the objectives\
  \ of talent training, making it difficult for evaluations to play a goal-oriented\
  \ role and hindering the further development of relevant studies. Therefore, this\
  \ paper draws on human resource management methodologies and analyzes knowledge\
  \ teaching evaluation system characteristics in colleges and universities in a big\
  \ data context to construct a \u201Cmultiple evaluations, trinity and four-step\
  \ closed-loop\u201D big data-based knowledge teaching evaluation system. \u201C\
  Trinity\u201D represents evaluation from three performance dimensions: teaching\
  \ effect, teaching behavior and teaching ability. \u201CMultiple evaluations\u201D\
  \ represents the design of teaching performance indicators based on teaching data,\
  \ breaking the barriers between different evaluation subjects. \u201CFour-step closed-loop\u201D\
  \ draws on performance management theory to standardize the teaching performance\
  \ management process from four aspects: planning, implementation, evaluation, and\
  \ feedback. This evaluation system provides a systematic methodology for unifying\
  \ the theory and practice of innovative knowledge teaching evaluation system in\
  \ universities in a big data context."
author: Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan
categories: Business and International Management (Q1); Economics and Econometrics
  (Q1); Management of Technology and Innovation (Q1); Marketing (Q1)
citable_docs._(3years): 70.0
cites_/_doc._(2years): 1083.0
country: Netherlands
coverage: 2016-2020
doi: 10.1016/j.jik.2022.100197
eigenfactor_score: .nan
h_index: 20.0
isbn: null
issn: 2444569X
issn1: 2444569X
issn2: '25307614'
issn3: 2444569X
jcr_value: null
keywords: Big data, Knowledge teaching evaluation, Performance management
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 6751.0
region: Western Europe
scimago_value: 1720.0
sjr_best_quartile: Q1
sourceid: 21100932830.0
title_bib: Review on a big data-based innovative knowledge teaching evaluation system
  in universities
title_csv: Journal of innovation and knowledge
total_cites: .nan
total_cites_(3years): 691.0
total_docs._(2020): 37.0
total_docs._(3years): 72.0
total_refs.: 2498.0
type: journal
type_publication: article
year: 2022
---
abstract: 'Supply chain disruptions have serious consequences for society and this
  has made supply chain risk management (SCRM) an attractive area for researchers
  and managers. In this paper, we use an objective literature mapping approach to
  identify, classify, and analyze decision-making models and support systems for SCRM,
  providing an agenda for future research. Through bibliometric networks of articles
  published in the Scopus database, we analyze the most influential decision-making
  models and support systems for SCRM, evaluate the main areas of current research,
  and provide insights for future research in this field. The main results are the
  following: we found that the identity of the area is structured in three groups
  of risk decision support models: (i) quantitative multicriteria decision models,
  (ii) stochastic decision-making models, and (iii) computational simulation/optimization
  models. We mapped six current research clusters: (i) conceptual and qualitative
  risk models, (ii) upstream supply chain risk models, (iii) downstream supply chain
  risk models, (iv) supply chain sustainability risk models, (v) stochastic and multicriteria
  decision risk models, and (vi) emerging techniques risk models. We identified seven
  future research clusters, with insights from further studies for: (i) tools to operate
  SCRM data, (ii) validation of risk models, (iii) computational improvement for data
  analysis, (iv) multi-level and multi-period supply chains, (v) agrifood risks, (vi)
  energy risks and (vii) sustainability risks. Finally, the future research agenda
  should prioritize SCRM''s holistic vision, the relationship between Big Data, Industry
  4.0 and SCRM, as well as emerging social and environmental risks.'
author: "Marcus Vinicius Carvalho Fagundes and Eduardo Oliveira Teles and Silvio A.B.\
  \ {Vieira de Melo} and Francisco Gaud\xEAncio Mendon\xE7a Freires"
categories: Business and International Management (Q1); Marketing (Q1); Strategy and
  Management (Q1); Economics and Econometrics (Q2)
citable_docs._(3years): 63.0
cites_/_doc._(2years): 633.0
country: Spain
coverage: 2016-2020
doi: 10.1016/j.iedeen.2020.02.001
eigenfactor_score: 0.00054
h_index: 18.0
isbn: null
issn: '24448834'
issn1: '24448834'
issn2: '24448834'
issn3: '24448834'
jcr_value: '5.024'
keywords: risk model, multicriteria decision, stochastic and computational model,
  bibliometrics
publisher_x: European Academy of Management and Business Economics
publisher_y: null
ref._/_doc.: 5595.0
region: Western Europe
scimago_value: 1024.0
sjr_best_quartile: Q1
sourceid: 21100465205.0
title_bib: 'Decision-making models and support systems for supply chain risk: literature
  mapping and future research agenda'
title_csv: European research on management and business economics
total_cites: 395.0
total_cites_(3years): 427.0
total_docs._(2020): 22.0
total_docs._(3years): 63.0
total_refs.: 1231.0
type: journal
type_publication: article
year: 2020
---
abstract: "The wave of new technologies has opened up the opportunity for cost-effective\
  \ generation of high-throughput profiles of biological systems. This is generating\
  \ tons of biological data. It is thus leading us towards the \u201Cbig data\u201D\
  \ era which is creating a pressing need to bridge the gap between high-throughput\
  \ technological development and our ability for managing, analyzing, and integrating\
  \ the biological big data. To harness the maximum out of it, sufficient expertise\
  \ needs to be developed for big data management and analysis. In this review, we\
  \ discuss the challenges related to storage, transfer, access and analysis of unstructured\
  \ and structured biological big data. Subsequently, it provides a comprehensive\
  \ summary regarding the important strategies adopted for biological big data management\
  \ which includes a discussion on all the recently used tools or software built for\
  \ high throughput processing and analysis of biological big data. Finally it discusses\
  \ the future perspectives of big data bioinformatics."
author: Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur
  Ghosh
categories: Genetics (Q4)
citable_docs._(3years): 441.0
cites_/_doc._(2years): 72.0
country: Netherlands
coverage: 2015-2020
doi: 10.1016/j.genrep.2020.100869
eigenfactor_score: .nan
h_index: 9.0
isbn: null
issn: '24520144'
issn1: '24520144'
issn2: '24520144'
issn3: '24520144'
jcr_value: null
keywords: Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce,
  Machine learning
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 4164.0
region: Western Europe
scimago_value: 235.0
sjr_best_quartile: Q4
sourceid: 21100445644.0
title_bib: 'Big data in biology: the hope and present-day challenges in it'
title_csv: Gene reports
total_cites: .nan
total_cites_(3years): 331.0
total_docs._(2020): 407.0
total_docs._(3years): 442.0
total_refs.: 16946.0
type: journal
type_publication: article
year: 2020
---
abstract: "Purpose/Objective\nOutside of randomized clinical trials, it is difficult\
  \ to develop clinically relevant evidence-based recommendations for radiotherapy\
  \ (RT) practice guidelines due to lack of comprehensive real-world data. To address\
  \ this knowledge gap, we formed the Learning and Analytics from Multicenter Big\
  \ Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization,\
  \ develop software solutions for data analysis and recommend clinical practice change\
  \ based on real-world data analyzed. The first phase of this \u201CBig Data\u201D\
  \ study aimed at characterizing variability in clinical practice patterns of dosimetric\
  \ data for organs at risk (OAR), that would undermine subsequent use of large scale,\
  \ electronically aggregated data to characterize associations with outcomes. Evidence\
  \ from this study was used as the basis for practical recommendations to improve\
  \ data quality.\nMaterials/Methods\nDosimetric details of patients with H&N cancer\
  \ treated with RT between 2014 and 2019 were analyzed. Institutional patterns of\
  \ practice were characterized including structure nomenclature, volumes and frequency\
  \ of contouring. Dose volume histogram (DVH) distributions were characterized and\
  \ compared to institutional constraints and literature values.\nResults\nPlans for\
  \ 4664 patients treated to a mean plan dose of 64.4 \xB1 13.2 Gy in 32 \xB1 4 fractions\
  \ were aggregated. Prior to implementation of TG263 guidelines in each institution,\
  \ there was variability in OAR nomenclature across institutions and structures.\
  \ With evidence from this study, we identified a targeted and practical set of recommendations\
  \ aimed at improving the quality of real-world data.\nConclusion\nQuantifying similarities\
  \ and differences among institutions for OAR structures and DVH metrics is the launching\
  \ point for next steps to investigate potential relationships between DVH parameters\
  \ and patient outcomes."
author: Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman
  and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk
  and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee
  and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and
  Charles Mayo
categories: Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)
citable_docs._(3years): 264.0
cites_/_doc._(2years): 265.0
country: United States
coverage: 2016-2020
doi: 10.1016/j.adro.2022.100925
eigenfactor_score: .nan
h_index: 19.0
isbn: null
issn: '24521094'
issn1: '24521094'
issn2: '24521094'
issn3: '24521094'
jcr_value: null
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 2474.0
region: Northern America
scimago_value: 989.0
sjr_best_quartile: Q1
sourceid: 21100465104.0
title_bib: 'Head and neck radiotherapy patterns of practice variability identified
  as a challenge to real-world big data: results from the learning from analysis of
  multicentre big data aggregation (lambda) consortium'
title_csv: Advances in radiation oncology
total_cites: .nan
total_cites_(3years): 746.0
total_docs._(2020): 211.0
total_docs._(3years): 271.0
total_refs.: 5221.0
type: journal
type_publication: article
year: 2022
---
abstract: Typically, only a smaller portion of the monitorable operational data (e.g.
  from sensors and environment) from Offshore Support Vessels (OSVs) are used at present.
  Operational data, in addition to equipment performance data, design and construction
  data, creates large volumes of data with high veracity and variety. In most cases,
  such data richness is not well understood as to how to utilize it better during
  design and operation. It is, very often, too time consuming and resource demanding
  to estimate the final operational performance of vessel concept design solution
  in early design by applying simulations and model tests. This paper argues that
  there is a significant potential to integrate ship lifecycle data from different
  phases of its operation in large data repository for deliberate aims and evaluations.
  It is disputed discretely in the paper, evaluating performance of real similar type
  vessels during early stages of the design process, helps substantially improving
  and fine-tuning the performance criterion of the next generations of vessel design
  solutions. Producing learning from such a ship lifecycle data repository to find
  useful patterns and relationships among design parameters and existing fleet real
  performance data, requires the implementation of modern data mining techniques,
  such as big data and clustering concepts, which are introduced and applied in this
  paper. The analytics model introduced suggests and reviews all relevant steps of
  data knowledge discovery, including pre-processing (integration, feature selection
  and cleaning), processing (data analyzing) and post processing (evaluating and validating
  results) in this context.
author: Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf
  Brett
categories: Industrial and Manufacturing Engineering (Q1); Information Systems and
  Management (Q1)
citable_docs._(3years): 83.0
cites_/_doc._(2years): 1226.0
country: Netherlands
coverage: 2016-2020
doi: 10.1016/j.jii.2018.02.002
eigenfactor_score: 0.00148
h_index: 24.0
isbn: null
issn: 2452414X
issn1: 2452414X
issn2: 2452414X
issn3: 2452414X
jcr_value: '10.063'
keywords: External data, Internal data, Abnormality, Missing data, Outliers, Randomness,
  Multivariate analysis, Data integration, Clustering
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 7686.0
region: Western Europe
scimago_value: 2042.0
sjr_best_quartile: Q1
sourceid: 21100787106.0
title_bib: "Improving early osv design robustness by applying \u2018multivariate big\
  \ data analytics\u2019 on a ship's life cycle"
title_csv: Journal of industrial information integration
total_cites: 1149.0
total_cites_(3years): 1361.0
total_docs._(2020): 28.0
total_docs._(3years): 86.0
total_refs.: 2152.0
type: journal
type_publication: article
year: 2018
---
abstract: "The purpose of this paper (presented online as a keynote lecture at the\
  \ 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize\
  \ the agenda for data-centric geotechnics, an emerging field that attempts to prepare\
  \ geotechnical engineering for digital transformation. The agenda must include (1)\
  \ development of methods that make sense of all real-world data (not selective input\
  \ data for a physical model), (2) offering insights of significant value to critical\
  \ real-world decisions for current or future practice (not decisions for an ideal\
  \ world or decisions of minor concern to geotechnical engineers), and (3) sensitivity\
  \ to the physical context of geotechnics (not abstract data-driven analysis connected\
  \ to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience\
  \ base should be substantial). These three elements are termed \u201Cdata centricity\u201D\
  , \u201Cfit for (and transform) practice\u201D, and \u201Cgeotechnical context\u201D\
  \ in the agenda. Given that a knowledge of the site is central to any geotechnical\
  \ engineering project, data-driven site characterization (DDSC) must constitute\
  \ one key application domain in data-centric geotechnics, although other infrastructure\
  \ lifecycle phases such as project conceptualization, design, construction, operation,\
  \ and decommission/reuse would benefit from data-informed decision support as well.\
  \ One part of DDSC that addresses numerical soil data in a site investigation report\
  \ and soil property databases is pursued under Project DeepGeo. In principle, the\
  \ source of data can also go beyond site investigation, and the type of data can\
  \ go beyond numbers, such as categorical data, text, audios, images, videos, and\
  \ expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic\
  \ map of the subsurface volume below a full-scale project site and to estimate relevant\
  \ engineering properties at each spatial point based on actual site investigation\
  \ data and other relevant Big Indirect Data (BID). Uncertainty quantification is\
  \ necessary, as current real-world data is insufficient, incomplete, and/or not\
  \ directly relevant to construct a deterministic map. The value of a deterministic\
  \ map for decision support is debatable. The computational cost to do this for a\
  \ 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures\
  \ need to be a part of a completely smart infrastructure that fits the circular\
  \ economy and need to focus on delivering service to end-users and the community\
  \ from project conceptualization to decommission/reuse with full integration to\
  \ smart city and smart society. Although current geotechnical practice has been\
  \ very successful in taking \u201Ccalculated risk\u201D informed by limited data,\
  \ imperfect theories, prototype testing, observations, among others and exercising\
  \ judicious caution and engineering judgment, there is no clear pathway forward\
  \ to leverage on big data and digital technologies such as machine learning, BIM,\
  \ and digital twin to meet more challenging needs such as sustainability and resilience\
  \ engineering."
author: Kok-Kwang Phoon and Jianye Ching and Zijun Cao
categories: Building and Construction (Q1); Civil and Structural Engineering (Q1);
  Geotechnical Engineering and Engineering Geology (Q2)
citable_docs._(3years): 74.0
cites_/_doc._(2years): 285.0
country: China
coverage: 2016-2020
doi: 10.1016/j.undsp.2022.04.001
eigenfactor_score: .nan
h_index: 12.0
isbn: null
issn: '24679674'
issn1: '20962754'
issn2: '24679674'
issn3: '20962754'
jcr_value: null
keywords: Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization
  (DDSC), Project DeepGeo, Data-informed decision support index
publisher_x: Tongji University Press
publisher_y: null
ref._/_doc.: 4642.0
region: Asiatic Region
scimago_value: 774.0
sjr_best_quartile: Q1
sourceid: 21100939600.0
title_bib: Unpacking data-centric geotechnics
title_csv: Underground space (china)
total_cites: .nan
total_cites_(3years): 234.0
total_docs._(2020): 64.0
total_docs._(3years): 76.0
total_refs.: 2971.0
type: journal
type_publication: article
year: 2022
---
abstract: "The COSMOS Database (DB) was originally established to provide reliable\
  \ data for cosmetics-related chemicals within the COSMOS Project funded as part\
  \ of the SEURAT-1 Research Initiative. The database has subsequently been maintained\
  \ and developed further into COSMOS Next Generation (NG), a combination of database\
  \ and in silico tools, essential components of a knowledge base. COSMOS DB provided\
  \ a cosmetics inventory as well as other regulatory inventories, accompanied by\
  \ assessment results and in vitro and in vivo toxicity data. In addition to data\
  \ content curation, much effort was dedicated to data governance \u2013 data authorisation,\
  \ characterisation of quality, documentation of meta information, and control of\
  \ data use. Through this effort, COSMOS DB was able to merge and fuse data of various\
  \ types from different sources. Building on the previous effort, the COSMOS Minimum\
  \ Inclusion (MINIS) criteria for a toxicity database were further expanded to quantify\
  \ the reliability of studies. COSMOS NG features multiple fingerprints for analysing\
  \ structure similarity, and new tools to calculate molecular properties and screen\
  \ chemicals with endpoint-related public profilers, such as DNA and protein binders,\
  \ liver alerts and genotoxic alerts. The publicly available COSMOS NG enables users\
  \ to compile information and execute analyses such as category formation and read-across.\
  \ This paper provides a step-by-step guided workflow for a simple read-across case,\
  \ starting from a target structure and culminating in an estimation of a NOAEL confidence\
  \ interval. Given its strong technical foundation, inclusion of quality-reviewed\
  \ data, and provision of tools designed to facilitate communication between users,\
  \ COSMOS NG is a first step towards building a toxicological knowledge hub leveraging\
  \ many public data systems for chemical safety evaluation. We continue to monitor\
  \ the feedback from the user community at support@mn-am.com."
author: C. Yang and M.T.D. Cronin and K.B. Arvidson and B. Bienfait and S.J. Enoch
  and B. Heldreth and B. Hobocienski and K. Muldoon-Jacobs and Y. Lan and J.C. Madden
  and T. Magdziarz and J. Marusczyk and A. Mostrag and M. Nelms and D. Neagu and K.
  Przybylak and J.F. Rathman and J. Park and A-N Richarz and A.M. Richard and J.V.
  Ribeiro and O. Sacher and C. Schwab and V. Vitcheva and P. Volarath and A.P. Worth
categories: Computer Science Applications (Q2); Health, Toxicology and Mutagenesis
  (Q2); Toxicology (Q2)
citable_docs._(3years): 104.0
cites_/_doc._(2years): 248.0
country: Netherlands
coverage: 2017-2020
doi: 10.1016/j.comtox.2021.100175
eigenfactor_score: .nan
h_index: 12.0
isbn: null
issn: '24681113'
issn1: '24681113'
issn2: '24681113'
issn3: '24681113'
jcr_value: null
keywords: Toxicity, Database, Public database, Knowledge hub, Study reliability, Analogue
  selection, Guided workflow
publisher_x: Elsevier BV
publisher_y: null
ref._/_doc.: 5004.0
region: Western Europe
scimago_value: 754.0
sjr_best_quartile: Q2
sourceid: 21100824046.0
title_bib: "Cosmos next generation \u2013 a public knowledge base leveraging chemical\
  \ and biological data to support the regulatory assessment of chemicals"
title_csv: Computational toxicology
total_cites: .nan
total_cites_(3years): 300.0
total_docs._(2020): 26.0
total_docs._(3years): 106.0
total_refs.: 1301.0
type: journal
type_publication: article
year: 2021
---
abstract: "Purpose\nThough the domain of big data and artificial intelligence in health\
  \ care continues to evolve, there is a lack of systemic methods to improve data\
  \ quality and streamline the preparation process. To address this, we aimed to develop\
  \ an automated sorting system (RetiSort) that accurately labels the type and laterality\
  \ of retinal photographs.\nDesign\nCross-sectional study.\nParticipants\nRetiSort\
  \ was developed with retinal photographs from the Singapore Epidemiology of Eye\
  \ Diseases (SEED) study.\nMethods\nThe development of RetiSort was composed of 3\
  \ steps: 2 deep-learning (DL) algorithms and 1 rule-based classifier. For step 1,\
  \ a DL algorithm was developed to locate the optic disc, the \u201Clandmark feature.\u201D\
  \ For step 2, based on the location of the optic disc derived from step 1, a rule-based\
  \ classifier was developed to sort retinal photographs into 3 types: macular-centered,\
  \ optic disc\u2013centered, or related to other fields. Step 2 concurrently distinguished\
  \ laterality (i.e., the left or right eye) of macular-centered photographs. For\
  \ step 3, an additional DL algorithm was developed to differentiate the laterality\
  \ of disc-centered photographs. Via the 3 steps, RetiSort sorted and labeled retinal\
  \ images into (1) right macular\u2013centered, (2) left macular\u2013centered, (3)\
  \ right optic disc\u2013centered, (4) left optic disc\u2013centered, and (5) images\
  \ relating to other fields. Subsequently, the accuracy of RetiSort was evaluated\
  \ on 5000 randomly selected retinal images from SEED as well as on 3 publicly available\
  \ image databases (DIARETDB0, HEI-MED, and Drishti-GS). The main outcome measure\
  \ was the accuracy for sorting of retinal photographs.\nResults\nRetiSort mislabeled\
  \ 48 out of 5000 retinal images from SEED, representing an overall accuracy of 99.0%\
  \ (95% confidence interval [CI], 98.7\u201399.3). In external tests, RetiSort mislabeled\
  \ 1, 0, and 2 images, respectively, from DIARETDB0, HEI-MED, and Drishti-GS, representing\
  \ an accuracy of 99.2% (95% CI, 95.8\u201399.9), 100%, and 98.0% (95% CI, 93.1\u2013\
  99.8), respectively. Saliency maps consistently showed that the DL algorithm in\
  \ step 3 required pixels in the central left lateral border and optic disc of optic\
  \ disc\u2013centered retinal photographs to differentiate the laterality.\nConclusions\n\
  RetiSort is a highly accurate automated sorting system. It can aid in data preparation\
  \ and has practical applications in DL research that uses retinal photographs."
author: Tyler Hyungtaek Rim and Zhi Da Soh and Yih-Chung Tham and Henrik Hee Seung
  Yang and Geunyoung Lee and Youngnam Kim and Simon Nusinovici and Daniel Shu Wei
  Ting and Tien Yin Wong and Ching-Yu Cheng
categories: Ophthalmology (Q1)
citable_docs._(3years): 394.0
cites_/_doc._(2years): 234.0
country: United States
coverage: 2017-2020
doi: 10.1016/j.oret.2020.03.007
eigenfactor_score: .nan
h_index: 18.0
isbn: null
issn: '24686530'
issn1: '24686530'
issn2: '24686530'
issn3: '24686530'
jcr_value: null
keywords: null
publisher_x: Elsevier Inc.
publisher_y: null
ref._/_doc.: 1897.0
region: Northern America
scimago_value: 1517.0
sjr_best_quartile: Q1
sourceid: 21100925772.0
title_bib: Deep learning for automated sorting of retinal photographs
title_csv: Ophthalmology retina
total_cites: .nan
total_cites_(3years): 1327.0
total_docs._(2020): 254.0
total_docs._(3years): 586.0
total_refs.: 4819.0
type: journal
type_publication: article
year: 2020
---
abstract: "Summary\nHundreds of millions of people lack access to electricity. Decentralized\
  \ solar-battery systems are key for addressing this while avoiding carbon emissions\
  \ and air pollution but are hindered by relatively high costs and rural locations\
  \ that inhibit timely preventive maintenance. Accurate diagnosis of battery health\
  \ and prediction of end of life from operational data improves user experience and\
  \ reduces costs. However, lack of controlled validation tests and variable data\
  \ quality mean existing lab-based techniques fail to work. We apply a scalable probabilistic\
  \ machine learning approach to diagnose health in 1,027 solar-connected lead-acid\
  \ batteries, each running for 400\u2013760\_days, totaling 620 million data rows.\
  \ We demonstrate 73% accurate prediction of end of life, 8\_weeks in advance, rising\
  \ to 82% at the point of failure. This work highlights the opportunity to estimate\
  \ health from existing measurements using \u201Cbig data\u201D techniques, without\
  \ additional equipment, extending lifetime and improving performance in real-world\
  \ applications."
author: Antti Aitio and David A. Howey
categories: Energy (miscellaneous) (Q1)
citable_docs._(3years): 512.0
cites_/_doc._(2years): 2467.0
country: United States
coverage: 2017-2020
doi: 10.1016/j.joule.2021.11.006
eigenfactor_score: 0.046239999999999996
h_index: 84.0
isbn: null
issn: '25424351'
issn1: '25424351'
issn2: '25424351'
issn3: '25424351'
jcr_value: '41.248'
keywords: battery, health, machine learning, rural electrification, Gaussian process,
  classification, Kalman filter
publisher_x: Cell Press
publisher_y: null
ref._/_doc.: 4470.0
region: Northern America
scimago_value: 12532.0
sjr_best_quartile: Q1
sourceid: 21100834904.0
title_bib: Predicting battery end of life from solar off-grid system field data using
  machine learning
title_csv: Joule
total_cites: 17275.0
total_cites_(3years): 14739.0
total_docs._(2020): 237.0
total_docs._(3years): 598.0
total_refs.: 10595.0
type: journal
type_publication: article
year: 2021
---
abstract: 'Summary

  Big data is important to new developments in global clinical science that aim to
  improve the lives of patients. Technological advances have led to the regular use
  of structured electronic health-care records with the potential to address key deficits
  in clinical evidence that could improve patient care. The COVID-19 pandemic has
  shown this potential in big data and related analytics but has also revealed important
  limitations. Data verification, data validation, data privacy, and a mandate from
  the public to conduct research are important challenges to effective use of routine
  health-care data. The European Society of Cardiology and the BigData@Heart consortium
  have brought together a range of international stakeholders, including representation
  from patients, clinicians, scientists, regulators, journal editors, and industry
  members. In this Review, we propose the CODE-EHR minimum standards framework to
  be used by researchers and clinicians to improve the design of studies and enhance
  transparency of study methods. The CODE-EHR framework aims to develop robust and
  effective utilisation of health-care data for research purposes.'
author: "Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D\
  \ Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and\
  \ Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and\
  \ Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna\
  \ Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and\
  \ Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert\
  \ Kazmierski and Susanne L\xF8gstrup and R Thomas Lumbers and Thomas F L\xFCscher\
  \ and Paul McGreavy and Ileana L Pi\xF1a and Lothar Roessig and Carl Steinbeisser\
  \ and Mats Sundgren and Beno\xEEt Tyl and Ghislaine van Thiel and Kees van Bochove\
  \ and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz\
  \ Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee"
categories: Decision Sciences (miscellaneous) (Q1); Health Informatics (Q1); Health
  Information Management (Q1); Medicine (miscellaneous) (Q1)
citable_docs._(3years): 26.0
cites_/_doc._(2years): 394.0
country: United Kingdom
coverage: 2019-2020
doi: 10.1016/S2589-7500(22)00151-0
eigenfactor_score: .nan
h_index: 14.0
isbn: null
issn: '25897500'
issn1: '25897500'
issn2: '25897500'
issn3: '25897500'
jcr_value: null
keywords: null
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 1670.0
region: Western Europe
scimago_value: 2346.0
sjr_best_quartile: Q1
sourceid: 21100922606.0
title_bib: Code-ehr best-practice framework for the use of structured electronic health-care
  records in clinical research
title_csv: Lancet digital health, the
total_cites: .nan
total_cites_(3years): 319.0
total_docs._(2020): 138.0
total_docs._(3years): 81.0
total_refs.: 2305.0
type: journal
type_publication: article
year: 2022
---
abstract: "Background\nArtificial intelligence (AI) promises to provide useful information\
  \ to clinicians specializing in hypertension. Already, there are some significant\
  \ AI applications on large validated data sets.\nMethods and results\nThis review\
  \ presents the use of AI to predict clinical outcomes in big data i.e. data with\
  \ high volume, variety, veracity, velocity and value. Four examples are included\
  \ in this review. In the first example, deep learning and support vector machine\
  \ (SVM) predicted the occurrence of cardiovascular events with 56%\u201357% accuracy.\
  \ In the second example, in a data base of 378,256 patients, a neural network algorithm\
  \ predicted the occurrence of cardiovascular events during 10 year follow up with\
  \ sensitivity (68%) and specificity (71%). In the third example, a machine learning\
  \ algorithm classified 1,504,437 patients on the presence or absence of hypertension\
  \ with 51% sensitivity, 99% specificity and area under the curve 87%. In example\
  \ four, wearable biosensors and portable devices were used in assessing a person's\
  \ risk of developing hypertension using photoplethysmography to separate persons\
  \ who were at risk of developing hypertension with sensitivity higher than 80% and\
  \ positive predictive value higher than 90%. The results of the above studies were\
  \ adjusted for demographics and the traditional risk factors for atherosclerotic\
  \ disease.\nConclusion\nThese examples describe the use of artificial intelligence\
  \ methods in the field of hypertension."
author: Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis
  and Stavros Zinonos and William J. Kostis
categories: Cardiology and Cardiovascular Medicine (Q4); Internal Medicine (Q4)
citable_docs._(3years): 16.0
cites_/_doc._(2years): 71.0
country: Canada
coverage: 2019-2020
doi: 10.1016/j.ijchy.2020.100027
eigenfactor_score: .nan
h_index: 2.0
isbn: null
issn: '25900862'
issn1: '25900862'
issn2: '25900862'
issn3: '25900862'
jcr_value: null
keywords: Machine learning, Deep neural networks, Hypertension, Disease management,
  Personalized disease network
publisher_x: Canadian Medical Association
publisher_y: null
ref._/_doc.: 2998.0
region: Northern America
scimago_value: 194.0
sjr_best_quartile: Q4
sourceid: 21100922613.0
title_bib: Uses and opportunities for machine learning in hypertension research
title_csv: 'International journal of cardiology: hypertension'
total_cites: .nan
total_cites_(3years): 12.0
total_docs._(2020): 43.0
total_docs._(3years): 17.0
total_refs.: 1289.0
type: journal
type_publication: article
year: 2020
---
abstract: "We present BiDaML 2.0, an integrated suite of visual languages and supporting\
  \ tool to help multidisciplinary teams with the design of big data analytics solutions.\
  \ BiDaML tool support provides a platform for efficiently producing BiDaML diagrams\
  \ and facilitating their design, creation, report and code generation. We evaluated\
  \ BiDaML using two types of evaluations, a theoretical analysis using the \u201C\
  physics of notations\u201D, and an empirical study with 1) a group of 12 target\
  \ end-users and 2) five individual end-users.\_Participants mostly agreed that BiDaML\
  \ was straightforward to understand/learn, and prefer BiDaML for supporting complex\
  \ data analytics solution modeling than other modeling languages."
author: Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John
  Grundy and John Hosking and Qiang He
categories: Computer Networks and Communications (Q3); Human-Computer Interaction
  (Q3); Software (Q3)
citable_docs._(3years): 47.0
cites_/_doc._(2years): 212.0
country: United Kingdom
coverage: 2019-2020
doi: 10.1016/j.cola.2020.100964
eigenfactor_score: 0.0001
h_index: 6.0
isbn: null
issn: '25901184'
issn1: '25901184'
issn2: '26659182'
issn3: '25901184'
jcr_value: '1.271'
keywords: Big data analytics, Big data modeling, Big data toolkits, Domain-specific
  visual languages, Multidisciplinary teams, End-user tools
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5247.0
region: Western Europe
scimago_value: 254.0
sjr_best_quartile: Q3
sourceid: 21100904890.0
title_bib: An end-to-end model-based approach to support big data analytics development
title_csv: Journal of computer languages
total_cites: 78.0
total_cites_(3years): 104.0
total_docs._(2020): 36.0
total_docs._(3years): 49.0
total_refs.: 1889.0
type: journal
type_publication: article
year: 2020
---
abstract: "Amid the burgeoning interest in and use of academic and learning analytics\
  \ through learning management systems (LMS), the implications of big data and their\
  \ uses should be central to computers and writing scholarship. In this case study\
  \ we describe the UMN Canvas LMS experience in such as way so that writing instructors\
  \ might become more familiar with levels of access to academic and learning analytics,\
  \ more acquainted with the analytical capabilities in LMSs, and more mindful of\
  \ implications of learning analytics stemming from LMS use in writing pedagogy.\
  \ We provide a historical account on the development and infusion of LMS in writing\
  \ pedagogy and demonstrate how these systems are affecting the way computers and\
  \ composition scholars consider writing instruction and assessment. We then respond\
  \ critically to the collection of data drawn from the authors\u2019 use of these\
  \ systems in on-campus and online teaching. We conclude with implications for writing\
  \ pedagogy along with a matrix for addressing ethical concerns."
author: Ann Hill Duin and Jason Tham
categories: Language and Linguistics (Q1); Linguistics and Language (Q1); Computer
  Science (miscellaneous) (Q2); Education (Q2)
citable_docs._(3years): 102.0
cites_/_doc._(2years): 150.0
country: United Kingdom
coverage: 1983-2020
doi: 10.1016/j.compcom.2020.102544
eigenfactor_score: .nan
h_index: 35.0
isbn: null
issn: '87554615'
issn1: '87554615'
issn2: '87554615'
issn3: '87554615'
jcr_value: null
keywords: Learning management systems, Academic and learning analytics, Writing pedagogy,
  Student privacy, Access
publisher_x: Elsevier Ltd.
publisher_y: null
ref._/_doc.: 5244.0
region: Western Europe
scimago_value: 521.0
sjr_best_quartile: Q1
sourceid: 26168.0
title_bib: 'The current state of analytics: implications for learning management system
  (lms) use in writing pedagogy'
title_csv: Computers and composition
total_cites: .nan
total_cites_(3years): 159.0
total_docs._(2020): 43.0
total_docs._(3years): 112.0
total_refs.: 2255.0
type: journal
type_publication: article
year: 2020
